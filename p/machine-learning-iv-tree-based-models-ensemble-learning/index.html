<!DOCTYPE html>
<html lang="en-us" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.
 Classification and Regression Trees (CART) Decision tree for classification Given a labeled dataset, a classification tree learns a sequence of if-else questions about individual features in order to infer the labels. In contrast to linear models, trees are able to capture non-linear relationships between features and labels. In addition, trees don&amp;rsquo;t require the features to be on the same scale through standardization for example.'><title>Machine Learning IV (Tree-Based Models &amp; Ensemble Learning)</title>

<link rel='canonical' href='https://wanghaoming177.netlify.app/p/machine-learning-iv-tree-based-models-ensemble-learning/'>

<link rel="stylesheet" href="/scss/style.min.6cb794b99473add9625fc6fcb4fd8782c21b06365b632b8fbb1076efe11c688e.css"><meta property='og:title' content='Machine Learning IV (Tree-Based Models &amp; Ensemble Learning)'>
<meta property='og:description' content='All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.
 Classification and Regression Trees (CART) Decision tree for classification Given a labeled dataset, a classification tree learns a sequence of if-else questions about individual features in order to infer the labels. In contrast to linear models, trees are able to capture non-linear relationships between features and labels. In addition, trees don&amp;rsquo;t require the features to be on the same scale through standardization for example.'>
<meta property='og:url' content='https://wanghaoming177.netlify.app/p/machine-learning-iv-tree-based-models-ensemble-learning/'>
<meta property='og:site_name' content='Haoming Wang'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='scikit-learn' /><meta property='article:published_time' content='2022-02-02T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2022-02-02T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="Machine Learning IV (Tree-Based Models &amp; Ensemble Learning)">
<meta name="twitter:description" content="All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.
 Classification and Regression Trees (CART) Decision tree for classification Given a labeled dataset, a classification tree learns a sequence of if-else questions about individual features in order to infer the labels. In contrast to linear models, trees are able to capture non-linear relationships between features and labels. In addition, trees don&amp;rsquo;t require the features to be on the same scale through standardization for example.">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            localStorage.setItem(colorSchemeKey, "dark");
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/IMG_0770_hu86b91b9497cdda82d5c488540c7342b2_101927_300x0_resize_q75_box.jpg" width="300"
                            height="273" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">üåè</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">Haoming Wang</a></h1>
            <h2 class="site-description">We choose to go to the moon in this decade and do the other things, not because they are easy, but because they are hard.</h2>
        </div>
    </header><ol class="social-menu">
            
                <li>
                    <a 
                        href='https://github.com/floating15'
                        target="_blank"
                        title="GitHub"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://www.linkedin.com/in/%E6%B5%A9%E9%93%AD-%E7%8E%8B-b0a164171/'
                        target="_blank"
                        title="Linkedin"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="28" height="28" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <rect x="4" y="4" width="16" height="16" rx="2" />
  <line x1="8" y1="11" x2="8" y2="16" />
  <line x1="8" y1="8" x2="8" y2="8.01" />
  <line x1="12" y1="16" x2="12" y2="11" />
  <path d="M16 16v-3a2 2 0 0 0 -4 0" />
</svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://follow.it/haoming-wang?action=followPub'
                        target="_blank"
                        title="r-mail"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-mailbox" width="28" height="28" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M10 21v-6.5a3.5 3.5 0 0 0 -7 0v6.5h18v-6a4 4 0 0 0 -4 -4h-10.5" />
  <path d="M12 11v-8h4l2 2l-2 2h-4" />
  <path d="M6 15h1" />
</svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://twitter.com'
                        target="_blank"
                        title="Twitter"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M22 4.01c-1 .49 -1.98 .689 -3 .99c-1.121 -1.265 -2.783 -1.335 -4.38 -.737s-2.643 2.06 -2.62 3.737v1c-3.245 .083 -6.135 -1.395 -8 -4c0 0 -4.182 7.433 4 11c-1.872 1.247 -3.739 2.088 -6 2c3.308 1.803 6.913 2.423 10.034 1.517c3.58 -1.04 6.522 -3.723 7.651 -7.742a13.84 13.84 0 0 0 .497 -3.753c-.002 -.249 1.51 -2.772 1.818 -4.013z" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        

        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="#FF1C02" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        

        <li >
            <a href='/about/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="#35FF02" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>About</span>
            </a>
        </li>
        
        

        <li >
            <a href='/notes/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-math-function" width="28" height="28" viewBox="0 0 24 24" stroke-width="1.5" stroke="#02EAFF" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M14 10h1c1 0 1 1 2.016 3.527c.984 2.473 .984 3.473 1.984 3.473h1" />
  <path d="M13 17c1.5 0 3 -2 4 -3.5s2.5 -3.5 4 -3.5" />
  <path d="M3 19c0 1.5 .5 2 2 2s2 -4 3 -9s1.5 -9 3 -9s2 .5 2 2" />
  <line x1="5" y1="12" x2="11" y2="12" />
</svg>
                
                <span>Notes</span>
            </a>
        </li>
        
        

        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="#0223FF" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        

        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="#F402FF" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        

        <li >
            <a href='/links/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>Links</span>
            </a>
        </li>
        

        <div class="menu-bottom-section">
            
            
        </div>
    </ol>
</aside>
<main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/ml/" style="background-color: false; color: false;">
                Machine Learning
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/machine-learning-iv-tree-based-models-ensemble-learning/">Machine Learning IV (Tree-Based Models &amp; Ensemble Learning)</a>
        </h2>
    
        
    </div>

    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Feb 02, 2022</time>
            </div>
        

        
    </footer>
    

    
</div>
</header>

    <section class="article-content">
    
    
    <blockquote>
<p>All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.</p>
</blockquote>
<h1 id="classification-and-regression-trees-cart">Classification and Regression Trees (CART)</h1>
<h2 id="decision-tree-for-classification">Decision tree for classification</h2>
<p>Given a labeled dataset, a classification tree learns a sequence of if-else questions about individual features in order to infer the labels. In contrast to linear models, trees are able to capture <strong>non-linear</strong> relationships between features and labels. In addition, trees don&rsquo;t require the features to be on the <strong>same scale</strong> through standardization for example.</p>
<p>To understand trees more concretely, we&rsquo;ll try to predict whether a tumor is malignant or benign in the Wisconsin Breast Cancer dataset using only 2 features. The figure here shows a scatterplot of two cancerous cell features with malignant-tumors in blue and benign-tumors in red.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205120957482.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>When a classification tree is trained on this dataset, the tree learns a sequence of if-else questions with each question involving <strong>one feature</strong> and one split-point. Take a look at the tree diagram here.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205120958499.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>At the top, the tree asks whether the concave-points mean of an instance is &lt;= 0.051. If it is, the instance traverses the True branch; otherwise, it traverses the False branch. Similarly, the instance keeps traversing the internal branches until it reaches an end. The label of the instance is then predicted to be that of the prevailing class at that end. The maximum number of branches separating the top from an extreme-end is known as the <strong>maximum depth</strong> which is equal to 2 here.</p>
<p>To understand the tree&rsquo;s predictions more concretely, let&rsquo;s see how it classifies instances in the feature-space. A classification-model divides the feature-space into regions where all instances in one region are assigned to only one class-label. These regions are known as <strong>decision-regions</strong>. Decision-regions are separated by surfaces called <strong>decision-boundaries</strong>.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205121012971.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>The left figure here shows the decision-regions of a linear-classifier. Note how the boundary is a straight-line. In contrast, as shown here on the right, a classification-tree produces rectangular decision-regions in the feature-space. This happens because at each split made by the tree, only one feature is involved. In our tumor prediction case, the decision tree divides the sample feature space into four decision-regions:</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205121005964.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>To fit a decision tree with scikit-learn:</p>
<ul>
<li>import <code>DecisionTreeClassifier</code> from <code>sklearn.tree</code>, import the functions <code>train_test_split()</code> from <code>sklearn.model_selection</code> and <code>accuracy_score()</code> from <code>sklearn.metrics</code>.</li>
<li>split the data into 80% train and 20% test using <code>train_test_split()</code>. Set the parameter <code>stratify</code> to <code>y</code> in order for the train and test sets to have the same proportion of class labels as the unsplit dataset.</li>
<li>use <code>DecisionTreeClassifier()</code> to instantiate a tree classifier, <code>dt</code> with a maximum depth of 2 by setting the parameter <code>max_depth</code> to <code>2</code>. Note that the parameter <code>random_state</code> is set to <code>1</code> for reproducibility.</li>
<li>call the <code>fit</code> method on <code>dt</code> and pass <code>X_train</code> and <code>y_train</code>.</li>
<li>To predict the labels of the test-set, call the <code>predict</code> method on dt.</li>
<li>print the accuracy of the test set using <code>accuracy_score()</code>.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import </span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Instantiate a DecisionTreeClassifier &#39;dt&#39; with a maximum depth of 6</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>

<span class="c1"># Fit dt to the training set</span>
<span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict test set labels</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Compute test set accuracy  </span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Test set accuracy: </span><span class="si">{:.2f}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>[0 0 0 1 0]
Test set accuracy: 0.89
</code></pre><p>As we can see, a classification tree divides the feature space into rectangular regions. In contrast, a linear model such as logistic regression produces only a single linear decision boundary dividing the feature space into two decision regions:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import LogisticRegression from sklearn.linear_model</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span>  <span class="n">LogisticRegression</span>

<span class="c1"># Instatiate logreg</span>
<span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Fit logreg to the training set</span>
<span class="n">logreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Define a list called clfs containing the two classifiers logreg and dt</span>
<span class="n">clfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">logreg</span><span class="p">,</span> <span class="n">dt</span><span class="p">]</span>

<span class="c1"># Review the decision regions of the two classifiers</span>
<span class="n">plot_labeled_decision_regions</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">clfs</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205121024047.png"
	
	
	
	loading="lazy"
	
	
></p>
<h2 id="classification-tree-learning">Classification tree Learning</h2>
<p>Let&rsquo;s define some terms: A decision-tree is a <strong>data-structure</strong> consisting of a hierarchy of individual units called <strong>nodes</strong>. A node is a point that involves either a question or a prediction.</p>
<ul>
<li>The <strong>root</strong> is the node at which the decision-tree starts growing. It has no parent node and involves a question that gives rise to 2 children nodes through two branches.</li>
<li>An <strong>internal node</strong> is a node that has a parent. It also involves a question that gives rise to 2 children nodes.</li>
<li>A <strong>leaf</strong> has one parent node and involves no questions and has no children. It&rsquo;s where a prediction is made.</li>
</ul>
<p>Recall that when a classification tree is trained on a labeled dataset, the tree learns patterns from the features in such a way to produce the purest leafs. In other words the tree is trained in such a way so that, in each leaf, one class-label is predominant.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205121042764.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>In the tree diagram shown here, consider the case where an instance traverses the tree to reach the leaf on the left. In this leaf, there are 257 instances classified as benign and 7 instances classified as malignant. As a result, the tree&rsquo;s prediction for this instance would be: &lsquo;benign&rsquo;. In order to understand how a classification tree produces the purest leafs possible, let&rsquo;s first define the concept of <strong>information gain</strong>.</p>
<p>The nodes of a classification tree are grown recursively; in other words, the obtention of an internal node or a leaf depends on the state of its predecessors. To produce the purest leafs possible, at each node, a tree asks a question involving one feature <code>f</code> and a split-point <code>sp</code>. But how does it know which feature and which split-point to pick? It does so by <strong>maximizing Information gain</strong>. The tree considers that every node contains information and aims at maximizing the Information Gain obtained after each split. Consider the case where a node with N samples is split into a left-node with Nleft samples and a right-node with Nright samples.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205121043994.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>The information gain for such split is given by the following formula:</p>
<p>$$
I G(\underbrace{f}<em>{\text {feature }}, \underbrace{s p}</em>{\text {split-point }})=I(\text { parent })-\left(\frac{N_{\text {left }}}{N} I(\text { left })+\frac{N_{\text {right }}}{N} I(\text { right })\right)
$$</p>
<p>The criterion function $I(\cdot)$ here used is to measure the impurity of a node, there are different criteria you can use among which are the gini-index or entropy. you can set the information criterion of the estimator <code>dt</code> by setting the <code>criterion</code> parameter to &lsquo;gini&rsquo; (default) or &lsquo;entropy&rsquo;.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Instantiate dt_entropy, set &#39;entropy&#39; as the information criterion</span>
<span class="n">dt_entropy</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;entropy&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Fit, predict and evaluate</span>
<span class="n">dt_entropy</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span><span class="o">=</span> <span class="n">dt_entropy</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy_entropy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Print accuracy_entropy and accuracy_gini</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Accuracy achieved by using entropy: </span><span class="si">{</span><span class="n">accuracy_entropy</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Accuracy achieved by using the gini index: </span><span class="si">{</span><span class="n">accuracy_gini</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Accuracy achieved by using entropy: 0.991
Accuracy achieved by using the gini index: 1.000
</code></pre><p>When an <strong>unconstrained tree</strong> is trained, the nodes are grown recursively. In other words, a node exists based on the state of its predecessors. At a non-leaf node, the data is split based on feature <code>f</code> and split-point <code>sp</code> in such a way to maximize information gain. If the information gain obtained by splitting a node is null, the node is declared a leaf. Note that these rules are not for <strong>constrained trees</strong>. If you constrain the maximum depth of a tree to 2 for example, all nodes having a depth of 2 will be declared leafs even if the information gain obtained by splitting such nodes is not null.</p>
<h2 id="decision-tree-for-regression">Decision tree for regression</h2>
<p>Recall that in regression, the target variable is continuous. In other words, the output of your model is a real value. Let&rsquo;s motivate our discussion of regression by introducing the automobile miles-per-gallon dataset from the UCI Machine Learning Repository. This dataset consists of 6 features corresponding to the characteristics of a car and a continuous target variable labeled <code>mpg</code> which stands for miles-per-gallon. Our task is to predict the <code>mpg</code> consumption of a car given these six features. To simplify the problem, here the analysis is restricted to only one feature corresponding to the displacement of a car. This feature is denoted by <code>displ</code>.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205121107912.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>A 2D scatter plot of <code>mpg</code> versus <code>displ</code> shows that the mpg-consumption decreases <strong>nonlinearly</strong> with displacement. Note that linear models such as linear regression would not be able to capture such a non-linear trend.</p>
<p>Here, it&rsquo;s important to note that, when a regression tree is trained on a dataset, the impurity of a node is measured using the mean-squared error of the targets in that node. This means that the regression tree tries to find the splits that produce leafs where in each leaf the target values are on average, the closest possible to the mean-value of the labels in that particular leaf.</p>
<p>$$
I(\text { node })=\underbrace{\operatorname{MSE}(\text { node })}<em>{\text {mean-squared-error }}=\frac{1}{N</em>{\text {node }}} \sum_{i \in \text { node }}\left(y^{(i)}-\hat{y}_{\text {node }}\right)^{2}
$$</p>
<p>$$
\underbrace{\hat{y}<em>{\text {node }}}</em>{\text {mean-target-value }}=\frac{1}{N_{\text {node }}} \sum_{i \in \text { node }} y^{(i)}
$$
As a new instance traverses the tree and reaches a certain leaf, its target-variable &lsquo;y&rsquo; is computed as the average of the target-variables contained in that leaf as shown in this formula.
$$
\hat{y}<em>{\text {pred }}(\text { leaf })=\frac{1}{N</em>{\text {leaf }}} \sum_{i \in \text { leaf }} y^{(i)}
$$</p>
<p>To highlight the importance of the flexibility of regression trees, take a look at this figure. On the left we have a scatter plot of the data in blue along with the predictions of a linear regression model shown in black. The linear model fails to capture the non-linear trend exhibited by the data. On the right, we have the same scatter plot along with a red line corresponding to the predictions of the regression tree. The regression tree shows a greater flexibility and is able to capture the non-linearity.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205121120794.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>To train a decision tree with scikit-learn to solve this regression problem:</p>
<ul>
<li>import <code>DecisionTreeRegressor</code> from <code>sklearn.tree</code> and the functions <code>train_test_split()</code> from <code>sklearn.model_selection</code> and <code>mean_squared_error</code> as <code>MSE()</code> from <code>sklearn.metrics</code>.</li>
<li>split the data into 80%-train and 20%-test using <code>train_test_split</code>.</li>
<li>instantiate the <code>DecisionTreeRegressor()</code> with a maximum depth of 4 by setting the parameter <code>max_depth</code> to <code>4</code>. In addition, set the parameter <code>min_sample_leaf</code> to <code>0.1</code> to impose a stopping condition in which each leaf has to contain at least 10% of the training data.</li>
<li>fit <code>dt</code> to the training set and predict the test set labels. Finally, evaluate the mean-squared error</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import DecisionTreeRegressor from sklearn.tree</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span> <span class="k">as</span> <span class="n">MSE</span>

<span class="c1"># Instantiate dt</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span>
	<span class="n">max_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mf">0.13</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>

<span class="c1"># Fit, predict and evaluate</span>
<span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">mse_dt</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">rmse_dt</span> <span class="o">=</span> <span class="n">mse_dt</span> <span class="o">**</span> <span class="mf">0.5</span>

<span class="c1"># Print rmse_dt</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Test set RMSE of dt: </span><span class="si">{:.2f}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rmse_dt</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Test set RMSE of dt: 4.37
</code></pre><p>Compare with Linear regression</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred_lr</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">mse_lr</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lr</span><span class="p">)</span>
<span class="n">rmse_lr</span> <span class="o">=</span> <span class="n">mse_lr</span> <span class="o">**</span> <span class="mf">0.5</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Linear Regression test set RMSE: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rmse_lr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Regression Tree test set RMSE: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rmse_dt</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Linear Regression test set RMSE: 5.10
Regression Tree test set RMSE: 4.37
</code></pre><h1 id="the-bias-variance-tradeoff">The Bias-Variance Tradeoff</h1>
<h2 id="generalization-error">Generalization error</h2>
<p>In supervised learning, you make the assumption that there&rsquo;s a mapping $f$ between features and labels. You can express this as $y=f(x)$. $f$ which is shown in red here is an unknown function that you want to determine. In reality, data generation is always accompanied with randomness or noise like the blue points shown here.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205121209099.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>Our goal is to find a model $\hat{f}$ that best approximates $f$. When training $\hat{f}$, we want to make sure that noise is discarded as much as possible. At the end, $\hat{f}$ should achieve a low predictive error on unseen datasets. We may encounter two difficulties when approximating $f$. The first is overfitting, it&rsquo;s when $\hat{f}$ fits the noise in the training set. The second is underfitting, it&rsquo;s when $\hat{f}$ is not flexible enough to approximate f.</p>
<p>When a model overfits the training set, its predictive power on unseen datasets is pretty low. This is illustrated by the predictions of the decision tree regressor shown here in red. The model clearly memorized the noise present in the training set. Such model achieves a low training set error and a high test set error.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205121212477.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>When a model underfits the data, like the regression tree whose predictions are shown here in red, the training set error is roughly equal to the test set error. However, both errors are relatively high. Now the trained model isn&rsquo;t flexible enough to capture the complex dependency between features and labels.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205121213334.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>The <strong>generalization error</strong> of a model tells you how much it generalizes on unseen data. It can be decomposed into 3 terms: <strong>bias, variance and irreducible error</strong> where the irreducible error is the error contribution of noise.</p>
<p>The bias term tells you, on average, how much $\hat{f}$ and $f$ are different. To illustrate this consider the high bias model shown here in black; this model is not flexible enough to approximate the true function $f$ shown in red. High bias models lead to <strong>underfitting</strong>.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205121215970.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>The variance term tells you how much $\hat{f}$ is inconsistent over different training sets. Consider the high variance model shown here in black; in this case, $\hat{f}$ follows the training data points so closely that it misses the true function $f$ shown in red. High variance models lead to <strong>overfitting</strong>.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205121215651.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>The <strong>complexity</strong> of a model sets its flexibility to approximate the true function $f$. For example: increasing the maximum-tree-depth increases the complexity of a decision tree. The diagram here shows how <strong>the best model complexity corresponds to the lowest generalization error</strong>. When the model complexity increases, the variance increases while the bias decreases. Conversely, when model complexity decreases, variance decreases and bias increases. Our goal is to find the model complexity that achieves the lowest generalization error. Since this error is the sum of three terms with the irreducible error being constant, we need to find a balance between bias and variance because as one increases the other decreases. This is known as the <strong>bias-variance trade-off</strong>.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205121217760.png"
	
	
	
	loading="lazy"
	
	
></p>
<h2 id="diagnose-bias-and-variance-problems">Diagnose bias and variance problems</h2>
<p>Given that you&rsquo;ve trained a supervised machine learning model labeled $\hat{f}$, how do you estimate the $\hat{f}$&rsquo;s generalization error? This cannot be done directly because:</p>
<ul>
<li>$f$ is unknown,</li>
<li>usually you only have one dataset,</li>
<li>you don&rsquo;t have access to the error term due to noise.</li>
</ul>
<p>A solution to this is to first split the data into a training and test set. The model $\hat{f}$ can then be fit to the training set and its error can be evaluated on the test set. The generalization error of $\hat{f}$ is roughly approximated by $\hat{f}$&rsquo;s error on the test set.</p>
<p>Usually, the test set should be kept untouched until one is confident about $\hat{f}$&rsquo;s performance. It should only be used to evaluate $\hat{f}$&rsquo;s final performance or error. Now, evaluating $\hat{f}$&rsquo;s performance on the training set may produce an optimistic estimation of the error because $\hat{f}$ was already exposed to the training set when it was fit. To obtain a reliable estimate of $\hat{f}$&rsquo;s performance, you should use cross-validation or CV.</p>
<p>The diagram here illustrates this technique for K=10:</p>
<ul>
<li>First, the training set (T) is split randomly into 10 partitions or folds,</li>
<li>The error of $\hat{f}$ is evaluated 10 times on the 10 folds,</li>
<li>Each time, one fold is picked for evaluation after training $\hat{f}$ on the other 9 folds.</li>
<li>At the end, you&rsquo;ll obtain a list of 10 errors.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205131045333.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>Finally, as shown in this formula, the CV-error is computed as the mean of the 10 obtained errors.</p>
<p>$$
\mathrm{CV} \text { error }=\frac{E_{1}+\ldots+E_{10}}{10}
$$</p>
<p>Once you have computed $\hat{f}$&rsquo;s cross-validation-error, you can check if it is greater than $\hat{f}$&rsquo;s training set error. If it is greater, <strong>$\hat{f}$ is said to suffer from high variance</strong>. In such case, $\hat{f}$ has <strong>overfit</strong> the training set. To remedy this try decreasing $\hat{f}$&rsquo;s complexity. For example, in a decision tree you can reduce the maximum-tree-depth or increase the maximum-samples-per-leaf. In addition, you may also gather more data to train $\hat{f}$.</p>
<p>On the other hand, <strong>$\hat{f}$ is said to suffer from high bias</strong> if its cross-validation-error is roughly equal to the training error but much greater than the desired error. In such case $\hat{f}$ <strong>underfits</strong> the training set. To remedy this try increasing the model&rsquo;s complexity or gather more relevant features for the problem.</p>
<p>To perform K-fold-cross-validation using scikit-learn</p>
<ul>
<li>import the function <code>cross_val_score()</code> from <code>sklearn.model_selection</code>.</li>
<li>split the dataset using <code>train_test_split()</code>.</li>
<li>instantiate a <code>DecisionTreeRegressor()</code> <code>dt</code> with the parameters <code>max_depth</code> set to 4 and <code>min_samples_leaf</code> to 0.14.</li>
<li>call <code>cross_val_score()</code> by passing <code>dt</code>, <code>X_train</code>, <code>y_train</code>; set the parameters</li>
<li>
<ul>
<li><code>cv</code> to 10 for 10-fold-cross-validation</li>
</ul>
</li>
<li>
<ul>
<li><code>scoring</code> to &lsquo;neg_mean_squared_error&rsquo; to compute the negative-mean-squared-errors. The scoring parameter was set so because <code>cross_val_score()</code> does not allow computing the mean-squared-errors directly.</li>
</ul>
</li>
<li>
<ul>
<li><code>n_jobs</code> to -1 to exploit all available CPUs in computation.</li>
</ul>
</li>
</ul>
<p>The result is a numpy-array of the 10 negative mean-squared-errors achieved on the 10-folds. You can multiply the result by minus-one to obtain an array of <code>MSE_CV_scores</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="c1"># Set SEED for reproducibility</span>
<span class="n">SEED</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Split the data into 70% train and 30% test</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
	<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
	<span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> 
	<span class="n">random_state</span><span class="o">=</span><span class="n">SEED</span>
<span class="p">)</span>

<span class="c1"># Instantiate a DecisionTreeRegressor dt</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span>
	<span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
	<span class="n">min_samples_leaf</span><span class="o">=</span><span class="mf">0.26</span><span class="p">,</span> 
	<span class="n">random_state</span><span class="o">=</span><span class="n">SEED</span>
<span class="p">)</span>

<span class="c1"># Compute the array containing the 10-folds CV MSEs</span>
<span class="n">MSE_CV_scores</span> <span class="o">=</span> <span class="o">-</span> <span class="n">cross_val_score</span><span class="p">(</span>
    <span class="n">dt</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> 
    <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
    <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Compute the 10-folds CV RMSE</span>
<span class="n">RMSE_CV</span> <span class="o">=</span> <span class="p">(</span><span class="n">MSE_CV_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">**</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Print RMSE_CV</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;CV RMSE: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">RMSE_CV</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>CV RMSE: 5.14
</code></pre><p>After that, fit <code>dt</code> to the training set and evaluate the labels of the training and test sets. Finally, you can use the function <code>MSE</code> to evaluate the train and test set mean-squared-errors.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import mean_squared_error from sklearn.metrics as MSE</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span> <span class="k">as</span> <span class="n">MSE</span>

<span class="c1"># Fit dt to the training set</span>
<span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict the labels</span>
<span class="n">y_pred_train</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluate the RMSE of dt</span>
<span class="n">RMSE_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">MSE</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">RMSE_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Print RMSE</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train RMSE: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">RMSE_train</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test RMSE: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">RMSE_test</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Train RMSE: 3.32
Test RMSE: 5.15
</code></pre><p>Given that the training set error is smaller than the CV-error, we can deduce that dt <strong>overfits</strong> the training set and that it suffers from high variance. Notice how the CV and test set errors are roughly equal.</p>
<h2 id="ensemble-learning">Ensemble Learning</h2>
<p>Let&rsquo;s first recap what we learned from the previous chapter about CARTs. CARTs present many <strong>advantages</strong>:</p>
<ul>
<li>they are easy to understand and their output is easy to interpret.</li>
<li>CARTs are easy to use and their flexibility gives them an ability to describe nonlinear dependencies between features and labels.</li>
<li>you don&rsquo;t need a lot of feature preprocessing to train a CART. In contrast to other models, you don&rsquo;t have to standardize or normalize features before feeding them to a CART.</li>
</ul>
<p>CARTs also have <strong>limitations</strong>.</p>
<ul>
<li>A classification tree is only able to produce orthogonal decision boundaries.</li>
<li>CARTs are also very sensitive to small variations in the training set. Sometimes, when a single point is removed from the training set, a CART&rsquo;s learned parameters may changed drastically.</li>
<li>CARTs also suffer from high variance when they are trained without constraints. In such case, they may overfit the training set.</li>
</ul>
<p>A solution that takes advantage of the flexibility of CARTs while reducing their tendency to memorize noise is <strong>ensemble learning</strong>. Ensemble learning can be summarized as follows:</p>
<ul>
<li>As a first step, different models are trained on the same dataset.</li>
<li>Each model makes its own predictions.</li>
<li>A meta-model then aggregates the predictions of individual models and outputs a final prediction.</li>
<li>The final prediction is more robust and less prone to errors than each individual model.</li>
</ul>
<p>Let&rsquo;s take a look at the diagram here to visually understand how ensemble learning works for a classification problem. First, the training set is fed to different classifiers. Each classifier learns its parameters and makes predictions. Then these predictions are fed to a meta model which aggregates them and outputs a final prediction.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205131135463.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>Let&rsquo;s now take a look at an ensemble technique known as the <strong>voting classifier</strong>. More concretely, we&rsquo;ll consider a binary classification task. The ensemble here consists of N classifiers making the predictions P0,P1,to,PN with P=0 or 1. The meta model outputs the final prediction by hard voting.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205131137353.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>To understand hard voting, consider a voting classifier that consists of 3 trained classifiers as shown in the diagram here. While classifiers 1 and 3 predict the label of 1 for a new data-point, classifier 2 predicts the label 0. In this case, 1 has 2 votes while 0 has 1 vote. As a result, the voting classifier predicts 1.</p>
<p>To train a voting classifier using scikit-learn.</p>
<ul>
<li>import <code>LogisticRegression</code>, <code>DecisionTreeClassifier</code> and <code>KNeighborsClassifier</code>. You also need to import <code>VotingClassifier</code> from <code>sklearn.ensemble</code>.</li>
<li>split the data and instantiate the different models.</li>
<li>define a list named <code>classifiers</code> that contains tuples corresponding the the name of the models and the models themselves.</li>
<li>write a for loop to iterate over the list classifiers; fit each classifier to the training set, evaluate its accuracy on the test set and print the result.</li>
<li>instantiate a <code>VotingClassifier</code> by setting the <code>estimators</code> parameter to <code>classifiers</code>. Fitting it to the training set, evaluate its accuracy on the test set</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Set seed for reproducibility</span>
<span class="n">SEED</span><span class="o">=</span><span class="mi">1</span>

<span class="c1"># Instantiate lr</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>

<span class="c1"># Instantiate knn</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span>

<span class="c1"># Instantiate dt</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">min_samples_leaf</span><span class="o">=</span><span class="mf">0.13</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>

<span class="c1"># Define the list classifiers</span>
<span class="n">classifiers</span> <span class="o">=</span> <span class="p">[</span>
	<span class="p">(</span><span class="s1">&#39;Logistic Regression&#39;</span><span class="p">,</span> <span class="n">lr</span><span class="p">),</span> 
	<span class="p">(</span><span class="s1">&#39;K Nearest Neighbours&#39;</span><span class="p">,</span> <span class="n">knn</span><span class="p">),</span> 
	<span class="p">(</span><span class="s1">&#39;Classification Tree&#39;</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
<span class="p">]</span>

<span class="c1"># Iterate over the pre-defined list of classifiers</span>
<span class="k">for</span> <span class="n">clf_name</span><span class="p">,</span> <span class="n">clf</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">:</span>    
 
    <span class="c1"># Fit clf to the training set</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>    
   
    <span class="c1"># Predict y_pred</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="c1"># Calculate accuracy</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span> 
   
    <span class="c1"># Evaluate clf&#39;s accuracy on the test set</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{:s}</span><span class="s1"> : </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf_name</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">))</span>

<span class="c1"># Instantiate a VotingClassifier vc</span>
<span class="n">vc</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="n">classifiers</span><span class="p">)</span>    

<span class="c1"># Fit vc to the training set</span>
<span class="n">vc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>   

<span class="c1"># Evaluate the test set predictions</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">vc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Calculate accuracy score</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Voting Classifier: </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Logistic Regression : 0.741
K Nearest Neighbours : 0.701
Classification Tree : 0.707
Voting Classifier: 0.764
</code></pre><p>Note that the accuracy of Voting Classifier is higher than that achieved by any of the individual models in the ensemble.</p>
<h1 id="bagging-and-random-forests">Bagging and Random Forests</h1>
<h2 id="bagging">Bagging</h2>
<p>In the last chapter, we discussed that the Voting Classifier is an ensemble of models that are fit to the same training set using different algorithms. we also saw that the final predictions were obtained by majority voting. In Bagging, the ensemble is formed by models that use the same training algorithm. However, these models are not trained on the entire training set. Instead, each model is trained on a different subset of the data.</p>
<p>In fact, bagging stands for <strong>bootstrap aggregation</strong>. Its name refers to the fact that it uses a technique known as the bootstrap. Overall, Bagging has the effect of <strong>reducing the variance</strong> of individual models in the ensemble.</p>
<p>The basic process of bagging is:</p>
<ul>
<li>A data set containing M samples is randomly sampled m times <strong>with put-backs</strong>, such that a sample set with m samples is obtained. (m &lt;= M, in scikit-learn, m = M by default)</li>
<li>Take N such sample sets.</li>
<li>Train a base estimator for each sample set.</li>
<li>Combine:</li>
<li>
<ul>
<li>for the classification task, the final prediction is obtained by majority <strong>voting</strong>. The corresponding classifier in scikit-learn is <a class="link" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html?highlight=baggingclassifier#sklearn.ensemble.BaggingClassifier"  target="_blank" rel="noopener"
    >BaggingClassifier</a>.</li>
</ul>
</li>
<li>
<ul>
<li>for the regression task, the final prediction is the <strong>average</strong> of the predictions made by the individual models forming the ensemble. The corresponding regressor in scikit-learn is <a class="link" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor"  target="_blank" rel="noopener"
    >BaggingRegressor</a>.</li>
</ul>
</li>
</ul>
<p>Let&rsquo;s train a <code>BaggingClassifier</code> in scikit-learn on the breast cancer dataset.</p>
<ul>
<li>import <code>BaggingClassifier</code> from <code>sklearn.ensemble</code></li>
<li>split the data.</li>
<li>instantiate a <code>DecisionTreeClassifier</code> <code>dt</code> with the parameters <code>max_depth</code> set to 4 and <code>min_samples_leaf</code> set to 0.16.</li>
<li>instantiate a <code>BaggingClassifier</code> <code>bc</code> with parameters</li>
<li>
<ul>
<li><code>base_estimator</code> set to <code>dt</code> and</li>
</ul>
</li>
<li>
<ul>
<li><code>n_estimators</code> set to 700, which means bc consists of 700 classification trees <code>dt</code>.</li>
</ul>
</li>
<li>
<ul>
<li><code>n_jobs</code> set to -1 so that all CPU cores are used in computation.</li>
</ul>
</li>
</ul>
<p>Once we are done, fit <code>bc</code> to the training set, predict the test set labels and finally, evaluate the test set accuracy.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>

<span class="c1"># Instantiate dt</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mf">0.16</span>
<span class="p">)</span>

<span class="c1"># Instantiate bc</span>
<span class="n">bc</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span>
    <span class="n">base_estimator</span><span class="o">=</span><span class="n">dt</span><span class="p">,</span> 
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">700</span><span class="p">,</span> 
    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Fit bc, dt to the training set</span>
<span class="n">bc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict test set labels</span>
<span class="n">y_bc_pred</span> <span class="o">=</span> <span class="n">bc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_dt_pred</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluate acc_test</span>
<span class="n">bc_acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_bc_pred</span><span class="p">)</span>
<span class="n">dt_acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_dt_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set accuracy of bc: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">bc_acc_test</span><span class="p">))</span> 
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set accuracy of dt: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dt_acc_test</span><span class="p">))</span> 
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Test set accuracy of bc: 0.71
Test set accuracy of dt: 0.66
</code></pre><p>The output shows that a <code>BaggingClassifier</code> achieves a test set accuracy of 71%. Training the classification tree <code>dt</code>, which is the base estimator here, to the same training set would lead to a test set accuracy of 66%. The result highlights how bagging outperforms the base estimator <code>dt</code>.</p>
<h2 id="out-of-bag-evaluation">Out of Bag Evaluation</h2>
<p>Recall that in bagging, some instances may be sampled several times for one model. On the other hand, other instance may not be sampled at all (since we sample with put-backs). On average, for each model, 63% of the training instances are sampled. The remaining 37% that are not sampled constitute what is known as the <strong>Out-of-bag</strong> or <strong>OOB instances</strong>.</p>
<p>Since OOB instances are not seen by a model during training, these can be used to estimate the performance of the ensemble without the need for cross-validation. This technique is known as <strong>OOB-evaluation</strong>. To understand OOB-evaluation more concretely, take a look at this diagram.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205131339444.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>Here, for each model, the bootstrap instances are shown in blue while the OOB-instances are shown in red. Each of the N models constituting the ensemble is then trained on its corresponding bootstrap samples and evaluated on the OOB instances. This leads to the obtainment of N OOB scores labeled OOB1 to OOBN. The OOB-score of the bagging ensemble is evaluated as the average of these N OOB scores as shown by the formula on top.</p>
<p>Now it&rsquo;s time to see OOB-evaluation in action.</p>
<ul>
<li>import <code>BaggingClassifier</code></li>
<li>split the dataset in a stratified way by setting the parameter <code>stratify</code> to <code>y</code>.</li>
<li>instantiate a <code>DecisionTreeClassifier</code> called <code>dt</code></li>
<li>instantiate a <code>BaggingClassifier</code> called <code>bc</code>. Importantly, set the parameter <code>oob_score</code> to <code>True</code> in order to evaluate the OOB-accuracy of <code>bc</code> after training.</li>
</ul>
<p>Note that in scikit-learn, the OOB-score corresponds to the <strong>accuracy</strong> for classifiers and the <strong>r-squared score</strong> for regressors.</p>
<ul>
<li>fit <code>bc</code> to the training set and predict the test set labels.</li>
<li>evaluate the OOB-accuracy of bc by extracting the attribute <code>oob_score_</code> from the trained instance.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>

<span class="c1"># Instantiate dt</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span>
	<span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
	<span class="n">random_state</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Instantiate bc</span>
<span class="n">bc</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span>
    <span class="n">base_estimator</span><span class="o">=</span><span class="n">dt</span><span class="p">,</span> 
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Fit, predict</span>
<span class="n">bc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">bc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluate</span>
<span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">acc_oob</span> <span class="o">=</span> <span class="n">bc</span><span class="o">.</span><span class="n">oob_score_</span>

<span class="c1"># Print acc_test and acc_oob</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set accuracy: </span><span class="si">{:.3f}</span><span class="s1">, </span><span class="se">\n</span><span class="s1">OOB accuracy: </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc_test</span><span class="p">,</span> <span class="n">acc_oob</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Test set accuracy: 0.698, 
OOB accuracy: 0.700
</code></pre><p>The test-set accuracy is about 69.8% and the OOB-accuracy is about 70%. The two obtained accuracies are pretty close though not exactly equal. These results highlight how OOB-evaluation can be an efficient technique to obtain a performance estimate of a bagged-ensemble on unseen data without performing cross-validation.</p>
<h2 id="random-forests-rf">Random Forests (RF)</h2>
<p>Recall that in bagging the base estimator could be any model including a decision tree, logistic regression or even a neural network. Each estimator is trained on a distinct bootstrap sample drawn from the training set using all available features.</p>
<p>Random Forests is an ensemble method that uses a decision tree as a base estimator. In Random Forests, each estimator is trained on a different bootstrap sample having the same size as the training set. Random forests introduces further randomization than bagging when training each of the base estimators. When each tree is trained, <strong>only d features can be sampled</strong> at each node with put-backs, where d is a number smaller than the total number of features.</p>
<p>The reason for doing so is, before start training, it is impossible to know which features' data have outliers and which features best determine the classification results. the process of random forests reduces the influence of two influencing factors on the classification results.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205131411718.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>The diagram here shows the training procedure for random forests. Notice how each tree forming the ensemble is trained on a different bootstrap sample from the training set. In addition, when a tree is trained, at each node, only d features are sampled from all features with put-backs. The node is then split using the sampled feature that maximizes information gain. In scikit-learn d defaults to the square-root of the number of features. For example, if there are 100 features, only 10 features are sampled at each node.</p>
<p>Once trained, predictions can be made on new instances. When a new instance is fed to the different base estimators, each of them outputs a prediction. The predictions are then collected by the random forests meta-classifier and a final prediction is made depending on the nature of the problem.</p>
<ul>
<li>For classification, the final prediction is made by majority voting. The corresponding scikit-learn class is <code>RandomForestClassifier</code>.</li>
<li>For regression, the final prediction is the average of all the labels predicted by the base estimators. The corresponding scikit-learn class is <code>RandomForestRegressor</code>.</li>
</ul>
<p>In general, Random Forests achieves a <strong>lower variance</strong> than individual trees.</p>
<p>To train a random forests regressor</p>
<ul>
<li>importing <code>RandomForestRegressor</code></li>
<li>split the dataset</li>
<li>instantiate a <code>RandomForestRegressor</code> called <code>rf</code> consisting of 700 regression trees. This can be done by setting <code>n_estimators</code> to 700.
In addition, you can control the complexity of the model by setting <code>min_samples_leaf</code> or <code>max_depth</code> (recall that the base model of random forest is decision tree)</li>
<li>fit <code>rf</code> to the training set and predict the test set labels.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span> <span class="k">as</span> <span class="n">MSE</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Instantiate rf</span>
<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">700</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Fit rf to the training set</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict the test set labels</span>
<span class="n">rf_y_pred</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">dt_y_pred</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluate the test set RMSE</span>
<span class="n">rf_rmse_test</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
<span class="n">dt_rmse_test</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">dt_y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>

<span class="c1"># Print rmse_test</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set RMSE of rf: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rf_rmse_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set RMSE of dt: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dt_rmse_test</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Test set RMSE of rf: 0.50
Test set RMSE of dt: 0.74
</code></pre><p>The result shows that rf achieves a test set RMSE of 0.50; this error is smaller than that achieved by a single regression tree which is 0.74.</p>
<p>When a tree based method is trained, the predictive power of a feature or its importance can be assessed. In scikit-learn, <strong>feature importance</strong> is assessed by measuring how much the tree nodes use a particular feature to reduce impurity. Note that the importance of a feature is expressed as a percentage indicating the weight of that feature in training and prediction. Once you train a tree-based model in scikit-learn, the features importance can be accessed by extracting the <code>feature_importance_</code> attribute from the model.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Create a pd.Series of features importances</span>
<span class="n">importances</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">rf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span>
    <span class="n">index</span><span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">feature_names</span>
<span class="p">)</span>

<span class="c1"># Sort importances</span>
<span class="n">importances_sorted</span> <span class="o">=</span> <span class="n">importances</span><span class="o">.</span><span class="n">sort_values</span><span class="p">()</span>

<span class="c1"># Draw a horizontal barplot of importances_sorted</span>
<span class="n">importances_sorted</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;barh&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightgreen&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Features Importances&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;/Users/wanghaoming/Desktop/fig.jpg&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205131454907.jpg"
	
	
	
	loading="lazy"
	
	
></p>
<blockquote>
<p>recommend reading: <a class="link" href="https://blog.csdn.net/siyuangulu/article/details/122508931"  target="_blank" rel="noopener"
    >article</a></p>
</blockquote>
<h1 id="boosting">Boosting</h1>
<h2 id="adaboost">Adaboost</h2>
<p>Boosting refers to an ensemble method in which many predictors are trained and each predictor learns from the errors of its predecessor. More formally, in boosting many weak learners are combined to form a strong learner. A weak learner is a model doing slightly better than random guessing. For example, a decision tree with a maximum-depth of one, known as a decision-stump, is a weak learner.</p>
<p>In boosting, an ensemble of predictors are trained sequentially and each predictor tries to correct the errors made by its predecessor. The two boosting methods we will discuss are AdaBoost and Gradient Boosting.</p>
<p>AdaBoost stands for Adaptive Boosting. In AdaBoost, each predictor pays more attention to the samples wrongly predicted by its predecessor by constantly changing the weights of training samples. Furthermore, each predictor is assigned a coefficient $\alpha$ that weighs its contribution in the ensemble&rsquo;s final prediction. Alpha depends on the predictor&rsquo;s training error.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205141022190.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>As shown in the diagram, there are N predictors in total. First, predictor1 is trained on the initial dataset (X,y), and the training error for predictor1 is determined. This error can then be used to determine $\alpha_1$ which is predictor1&rsquo;s coefficient. Alpha1 is then used to determine the weights W(2) of the training samples for predictor2. <strong>Notice how the incorrectly predicted samples shown in green acquire higher weights</strong>. When the weighted samples are used to train predictor2, this predictor is forced to pay more attention to the incorrectly predicted samples. This process is repeated sequentially, until the N predictors forming the ensemble are trained.</p>
<blockquote>
<p>This process is like doing exercises, at the beginning for all the topic of Ex are equally, once done, check the answers, to separate the right and wrong, and then transfer the wrong topics to the wrong set, the next time you do the questions is not the same for all the topics, should focus on the wrong set, and the wrong set of similar topics should draw more attention, but the simple questions can be simply taken over. This way, simple topics and hard topics are solved.</p>
</blockquote>
<p>An important parameter used in training is the learning rate, $\eta$. $\eta$ is a number between 0 and 1; it is used to shrink the coefficient $\alpha$ of a trained predictor. It&rsquo;s important to note that there&rsquo;s a <strong>trade-off</strong> between $\eta$ and the number of estimators. A smaller value of $\eta$ should be compensated by a greater number of estimators.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205141031976.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>Once all the predictors in the ensemble are trained, the label of a new sample can be predicted depending on the nature of the problem.</p>
<ul>
<li>For classification, each predictor predicts the label of the new sample and the ensemble&rsquo;s prediction is obtained by <strong>weighted majority voting</strong> by <code>AdaBoostClassifier</code>.</li>
<li>For regression, the same procedure is applied and the ensemble&rsquo;s prediction is obtained by performing a <strong>weighted average</strong> by <code>AdaBoostRegressor</code>.</li>
</ul>
<p>It&rsquo;s important to note that individual predictors need not to be CARTs. However CARTs are used most of the time in boosting because of their high variance.</p>
<ul>
<li>import <code>AdaBoostClassifier</code>, <code>DecisionTreeClassifier</code>, <code>roc_auc_score</code> and others.</li>
<li>instantiate a <code>DecisionTreeClassifier</code> called <code>dt</code> with the parameter <code>max_depth</code> set to 2.</li>
<li>instantiate an <code>AdaBoostClassifier</code> called <code>ada</code> consisting of 180 decision-stumps. This can be done by setting the parameters</li>
<li>
<ul>
<li><code>base_estimator</code> to <code>dt</code> and</li>
</ul>
</li>
<li>
<ul>
<li><code>n_estimators</code> to <code>180</code>.</li>
</ul>
</li>
<li>fit <code>ada</code> to the training set and predict the probability of obtaining the positive class in the test set. Call the function <code>roc_auc_score</code> and passing the parameters <code>y_test</code> and <code>y_pred_proba</code>.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Instantiate dt</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Instantiate ada</span>
<span class="n">ada</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span>
    <span class="n">base_estimator</span><span class="o">=</span><span class="n">dt</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Fit ada to the training set</span>
<span class="n">ada</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Compute the probabilities of obtaining the positive class</span>
<span class="n">y_pred_proba</span> <span class="o">=</span> <span class="n">ada</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Evaluate test-set roc_auc_score</span>
<span class="n">ada_roc_auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_proba</span><span class="p">)</span>

<span class="c1"># Print roc_auc_score</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ROC AUC score: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ada_roc_auc</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>ROC AUC score: 0.99
</code></pre><p>The result shows that the <code>AdaBoostClassifier</code> achieves a ROC-AUC score of about 0.99.</p>
<h2 id="gradient-boosting-gb">Gradient Boosting (GB)</h2>
<p>In gradient boosting, each predictor in the ensemble corrects its predecessor&rsquo;s error. In contrast to AdaBoost, the weights of the training samples are not tweaked. Instead, <strong>each predictor is trained using the residual errors of its predecessor as labels</strong>. We will focus on the technique known as gradient boosted trees where the base learner is a CART.</p>
<p>To understand how gradient boosted trees are trained for a <strong>regression</strong> problem, take a look at the diagram here. The ensemble consists of N trees. Tree1 is trained using the features matrix X and the dataset labels y. The predictions labeled y1hat are used to determine the training set residual errors r1. Tree2 is then trained using the features matrix X and the residual errors r1 of Tree1 as labels. The predicted residuals r1hat are then used to determine the residuals of residuals which are labeled r2. This process is repeated until all of the N trees forming the ensemble are trained.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205141100025.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>An important parameter used in training gradient boosted trees is <strong>shrinkage</strong>. Shrinkage refers to the fact that the prediction of each tree in the ensemble is shrinked after it is multiplied by a learning rate $\eta$ which is a number between 0 and 1. Similarly to AdaBoost, there&rsquo;s a <strong>trade-off</strong> between $\eta$ and the number of estimators. Decreasing the learning rate needs to be compensated by increasing the number of estimators in order for the ensemble to reach a certain performance.</p>
<p>Once all trees in the ensemble are trained, prediction can be made. When a new sample is available, each tree predicts a label and the final ensemble prediction is given by the formula here.</p>
<p>$$
y_{\text {pred }}=y_{1}+\eta r_{1}+\ldots+\eta r_{N}
$$</p>
<p>In scikit-learn, the class for a gradient boosting regressor is <code>GradientBoostingRegressor</code>. Though not discussed in this blog, a similar algorithm is used for classification problems. The class implementing gradient boosted classification in scikit-learn is <code>GradientBoostingClassifier</code>.</p>
<ul>
<li>import <code>GradientBoostingRegressor</code> from <code>sklearn.ensemble</code>. also, import the functions <code>mean_squared_error</code> as <code>MSE</code>.</li>
<li>instantiate a <code>GradientBoostingRegressor</code> <code>gbt</code> consisting of 500 CARTs. This can be done by setting the parameters</li>
<li>
<ul>
<li><code>n_estimators</code> to 500 and</li>
</ul>
</li>
<li>
<ul>
<li><code>max_depth</code> to 4.</li>
</ul>
</li>
<li>fit <code>gbt</code> to the training set and predict the test set labels. Compute the test set RMSE and print the value.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span> <span class="k">as</span> <span class="n">MSE</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Instantiate gbt</span>
<span class="n">gbt</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">max_features</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>

<span class="c1"># Fit gb to the training set</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">gbt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;time cost: </span><span class="si">{</span><span class="n">t2</span><span class="o">-</span><span class="n">t1</span><span class="si">}</span><span class="s2"> s&#34;</span><span class="p">)</span>

<span class="c1"># Predict test set labels</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">gbt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Compute MSE</span>
<span class="n">mse_test</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Compute RMSE</span>
<span class="n">rmse_test</span> <span class="o">=</span> <span class="n">mse_test</span> <span class="o">**</span> <span class="mf">0.5</span>

<span class="c1"># Print RMSE</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set RMSE of gb: </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rmse_test</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>time cost: 14.910254716873169 s
Test set RMSE of gbt: 0.478
</code></pre><p>The result shows that <code>gbt</code> achieves a test set RMSE of 0.478.</p>
<h2 id="stochastic-gradient-boosting-sgb">Stochastic Gradient Boosting (SGB)</h2>
<p>Gradient boosting involves an exhaustive search procedure. Each tree in the ensemble is trained to find the best split-points and the best features. This procedure may lead to CARTs that use the same split-points and possibly the same features.</p>
<p>To mitigate these effects, you can use an algorithm known as stochastic gradient boosting. In stochastic gradient boosting, each CART is trained on a random subset of the training data. This subset is sampled with put-backs. Furthermore, at the level of each node, features are sampled with put-backs when choosing the best split-points. As a result, this creates further diversity in the ensemble and the net effect is adding more variance to the ensemble of trees and can significantly improve the prediction speed.</p>
<p>Let&rsquo;s take a closer look at the training procedure used in stochastic gradient boosting by examining the diagram shown below. First, instead of providing all the training samples to a tree, <strong>only a fraction of these samples</strong> are provided through sampling with put-backs. The sampled data is then used for training a tree. However, not all features are considered when a split is made. Instead, <strong>only a certain randomly sampled fraction of these features</strong> are used for this purpose. Once a tree is trained, predictions are made and the residual errors can be computed. These residual errors are multiplied by the learning rate $\eta$ and are fed to the next tree in the ensemble. This procedure is repeated sequentially until all the trees in the ensemble are trained. The prediction procedure for a new sample in stochastic gradient boosting is similar to that of gradient boosting.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205141131632.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>To put this into practice, define a stochastic gradient boosting regressor consisting of 500 CARTs. This can be done by</p>
<ul>
<li>instantiate a <code>GradientBoostingRegressor</code> <code>sgbt</code>, and set the parameters</li>
<li>
<ul>
<li><code>max_depth=4</code></li>
</ul>
</li>
<li>
<ul>
<li><code>n_estimators=500</code></li>
</ul>
</li>
<li>
<ul>
<li><code>subsample=0.8</code> in order for each tree to sample 80% of the data for training.</li>
</ul>
</li>
<li>
<ul>
<li><code>max_features=0.2</code> so that each tree uses 20% of available features to perform the best-split.</li>
</ul>
</li>
</ul>
<p>Once done, fit <code>sgbt</code> to the training set and predict the test set labels. Finally, compute the test set RMSE and print it.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span> <span class="k">as</span> <span class="n">MSE</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Instantiate gbt</span>
<span class="n">sgbt</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">max_features</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>

<span class="c1"># Fit gb to the training set</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">sgbt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;time cost: </span><span class="si">{</span><span class="n">t2</span><span class="o">-</span><span class="n">t1</span><span class="si">}</span><span class="s2"> s&#34;</span><span class="p">)</span>

<span class="c1"># Predict test set labels</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">sgbt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Compute MSE</span>
<span class="n">mse_test</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Compute RMSE</span>
<span class="n">rmse_test</span> <span class="o">=</span> <span class="n">mse_test</span> <span class="o">**</span> <span class="mf">0.5</span>

<span class="c1"># Print RMSE</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set RMSE of sgbt: </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rmse_test</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>time cost: 6.8205671310424805 s
Test set RMSE of sgbt: 0.451
</code></pre><p>As we can see, SGBT improves the accuracy and prediction speed.</p>
<h1 id="model-tuning">Model Tuning</h1>
<h2 id="tuning-a-carts-hyperparameters">Tuning a CART&rsquo;s Hyperparameters</h2>
<p>Machine learning models are characterized by parameters and hyperparameters. Parameters are learned from data through training; examples of parameters include the split-feature and the split-point of a node in a CART. Hyperparameters are not learned from data; they should be set prior to training. Examples of hyperparameters include the maximum-depth and the splitting-criterion of a CART.</p>
<p>Hyperparameter tuning consists of searching for the set of optimal hyperparameters for the learning algorithm. The solution involves finding the set of optimal hyperparameters yielding an optimal model. The optimal model yields an optimal score. The score function measures the agreement between true labels and a model&rsquo;s predictions. In sklearn, it defaults to <strong>accuracy for classifiers and r-squared for regressors</strong>. A model&rsquo;s generalization performance is evaluated using cross-validation.</p>
<p>There are many approaches for hyperparameter tuning including: Grid Search, Random Search, Bayesian Optimization, Genetic Algorithms, and so on. We&rsquo;ll only focus on grid-search.</p>
<p>In grid-search cross-validation, first you manually set a grid of discrete hyperparameter values. Then, you pick a metric for scoring model performance and you search exhaustively through the grid. For each set of hyperparameters, you evaluate each model&rsquo;s score. The optimal hyperparameters are those for which the model achieves the best cross-validation score. Note that grid-search suffers from the <strong>curse of dimensionality</strong>, i.e. the bigger the grid, the longer it takes to find the solution.</p>
<p>Let&rsquo;s now see how we can inspect the hyperparameters of a CART in scikit-learn. You can first instantiate a <code>DecisionTreeClassifier</code> called <code>dt</code>. call dt&rsquo;s <code>.get_params()</code> method. This prints out a dictionary where the keys are the hyperparameter names and values are the default values fo hyperparameter. In the following, we&rsquo;ll only be optimizing <code>max_depth</code> and <code>min_samples_leaf</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dt</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>{'ccp_alpha': 0.0,
 'class_weight': None,
 'criterion': 'gini',
 'max_depth': None,
 'max_features': None,
 'max_leaf_nodes': None,
 'min_impurity_decrease': 0.0,
 'min_impurity_split': None,
 'min_samples_leaf': 1,
 'min_samples_split': 2,
 'min_weight_fraction_leaf': 0.0,
 'random_state': 1,
 'splitter': 'best'}
</code></pre><p>Let&rsquo;s now tune <code>dt</code> on the breast cancer dataset:</p>
<ul>
<li>import <code>GridSearchCV</code> from <code>sklearn.model_selection</code>.</li>
<li>define a dictionary called <code>params_dt</code> containing the names of the hyperparameters to tune as keys and lists of hyperparameter-values as values.</li>
<li>instantiate a <code>GridSearchCV</code> object <code>grid_dt</code> by setting</li>
<li>
<ul>
<li><code>estimator=dt</code></li>
</ul>
</li>
<li>
<ul>
<li><code>param_grid=params_dt</code></li>
</ul>
</li>
<li>
<ul>
<li><code>scoring='roc_auc'</code>  (or &lsquo;accuracy&rsquo; by default)</li>
</ul>
</li>
<li>
<ul>
<li><code>cv=5</code></li>
</ul>
</li>
</ul>
<p>Finally, fit <code>grid_dt</code> to the training set. After training <code>grid_dt</code>, we can obtain</p>
<ul>
<li>the best set of hyperparameter-values by <code>grid_dt.best_params_</code></li>
<li>the best cross validation accuracy by <code>grid_dt.best_score_</code></li>
<li>the best-model by <code>grid_dt.best_estimator</code></li>
</ul>
<p>Note that this model is fitted on the whole training set because the <code>refit</code> parameter of <code>GridSearchCV</code> is set to <code>True</code> by default. Finally, you can evaluate this model&rsquo;s test set accuracy or roc-auc using the <code>score</code> method or <code>roc_auc_score</code> method respectively.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Instantiate a DecisionTreeClassifier &#39;dt&#39;</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Define params_dt</span>
<span class="n">params_dt</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.12</span><span class="p">,</span> <span class="mf">0.14</span><span class="p">,</span> <span class="mf">0.16</span><span class="p">,</span> <span class="mf">0.18</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># Instantiate grid_dt</span>
<span class="n">grid_dt</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="n">dt</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="o">=</span><span class="n">params_dt</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="n">grid_dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">best_param</span> <span class="o">=</span> <span class="n">grid_dt</span><span class="o">.</span><span class="n">best_params_</span>
<span class="n">best_rocauc</span> <span class="o">=</span> <span class="n">grid_dt</span><span class="o">.</span><span class="n">best_score_</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">grid_dt</span><span class="o">.</span><span class="n">best_estimator_</span>

<span class="n">y_pred_prob</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">roc_auc_score</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_prob</span><span class="p">)</span>
<span class="n">accuracy_score</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;best parameters: </span><span class="si">{</span><span class="n">best_param</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;best roc-auc score of cv on training set: </span><span class="si">{</span><span class="n">best_rocauc</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;best roc-auc score on test set: </span><span class="si">{</span><span class="n">roc_auc_score</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;accuracy score on test set: </span><span class="si">{</span><span class="n">accuracy_score</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>best parameters: {'max_depth': 3, 'min_samples_leaf': 0.14}
best roc-auc score of cv on training set: 0.9483149425287356
best roc-auc score on test set: 0.9642231308411215
accuracy score on test set: 0.9064327485380117
</code></pre><blockquote>
<p>As a note, hyperparameter tuning is computationally expensive and may sometimes lead only to very slight improvement of a model&rsquo;s performance. For this reason, it is desired to weigh the impact of tuning on the pipeline of your data analysis project as a whole in order to understand if it is worth pursuing.</p>
</blockquote>
<h2 id="tuning-a-rfs-hyperparameters">Tuning a RF&rsquo;s Hyperparameters</h2>
<p>In addition to the hyperparameters of the CARTs forming random forests, the ensemble itself is characterized by other hyperparameters such as the number of estimators, whether it uses bootstraping or not and so on.</p>
<p>To inspect the hyperparameters of a <code>RandomForestRegressor</code>, first, import <code>RandomForestRegressor</code> from sklearn.ensemble and then instantiate a <code>RandomForestRegressor</code> called <code>rf</code>. The hyperparameters of <code>rf</code> along with their default values can be accessed by calling <code>rf</code>&rsquo;s <code>.get_params()</code> method. In the following, we&rsquo;ll be optimizing <code>n_estimators</code>, <code>max_depth</code>, <code>min_samples_leaf</code> and <code>max_features</code>.</p>
<p>We&rsquo;ll perform grid-search cross-validation on the auto-dataset:</p>
<ul>
<li>import <code>mean_squared_error</code> as <code>MSE</code> from <code>sklearn.metrics</code> and <code>GridSearchCV</code> from <code>sklearn.model_selection</code>.</li>
<li>define a dictionary called <code>params_rf</code> containing the grid of hyperparameters.</li>
<li>instantiate a <code>GridSearchCV</code> object called <code>grid_rf</code> and set the parameters</li>
<li>
<ul>
<li><code>estimator=rf</code>,</li>
</ul>
</li>
<li>
<ul>
<li><code>param_grid=params_rf</code>.</li>
</ul>
</li>
<li>
<ul>
<li><code>cv=3</code></li>
</ul>
</li>
<li>
<ul>
<li><code>scoring='neg_mean_squared_error'</code> in order to use negative mean squared error as a metric.</li>
</ul>
</li>
<li>
<ul>
<li><code>verbose=1</code> this parameter controls verbosity; the higher its value, the more messages are printed during fitting.
<code>&gt;1</code> : the computation time for each fold and parameter candidate is displayed;
<code>&gt;2</code> : the score is also displayed;
<code>&gt;3</code> : the fold and candidate parameter indexes are also displayed together with the starting time of the computation.</li>
</ul>
</li>
</ul>
<p>We can now fit <code>grid_rf</code> to the training set. The output shows messages related to grid fitting as well as the obtained optimal model. We can extract rf&rsquo;s best hyperparameters by getting the attribute <code>best_params_</code> from <code>grid_rf</code>. We can also extract the best model from rf. This enables us to predict the test set labels and evaluate the test-set MSE.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span> <span class="k">as</span> <span class="n">MSE</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Instantiate &#39;rf&#39;</span>
<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">()</span>

<span class="n">params_rf</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">500</span><span class="p">],</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
    <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
    <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;log2&#39;</span><span class="p">,</span> <span class="s1">&#39;sqrt&#39;</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># Instantiate &#39;grid_rf&#39;</span>
<span class="n">grid_rf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="n">rf</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="o">=</span><span class="n">params_rf</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="n">grid_rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">best_param</span> <span class="o">=</span> <span class="n">grid_rf</span><span class="o">.</span><span class="n">best_params_</span>
<span class="n">best_mse</span> <span class="o">=</span> <span class="o">-</span> <span class="n">grid_rf</span><span class="o">.</span><span class="n">best_score_</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">grid_rf</span><span class="o">.</span><span class="n">best_estimator_</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">msescore</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">r2score</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="c1"># compute r^2 by default for regression in sklearn</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;best parameters: </span><span class="si">{</span><span class="n">best_param</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;best MSE score of cv on training set: </span><span class="si">{</span><span class="n">best_mse</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;best MSE score on test set: </span><span class="si">{</span><span class="n">msescore</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;r^2 score on test set: </span><span class="si">{</span><span class="n">r2score</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Fitting 3 folds for each of 36 candidates, totalling 108 fits
best parameters: {'max_depth': 6, 'max_features': 'log2', 'min_samples_leaf': 0.1, 'n_estimators': 400}
best MSE score of cv on training set: 0.7203497070079746
best MSE score on test set: 0.758856243156018
r^2 score on test set: 0.4442806821766565
</code></pre><h1 id="compendium">Compendium</h1>
<h2 id="sklearntree">sklearn.tree</h2>
<p><a class="link" href="https://scikit-learn.org/stable/modules/classes.html?highlight=tree#module-sklearn.tree"  target="_blank" rel="noopener"
    >sklearn.tree</a></p>
<ul>
<li><a class="link" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"  target="_blank" rel="noopener"
    >DecisionTreeClassifier</a></li>
<li><a class="link" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor"  target="_blank" rel="noopener"
    >DecisionTreeRegressor</a></li>
</ul>
<h2 id="sklearnensemble">sklearn.ensemble</h2>
<p><a class="link" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble"  target="_blank" rel="noopener"
    >sklearn.ensemble</a></p>
<ul>
<li><a class="link" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor"  target="_blank" rel="noopener"
    >BaggingRegressor¬∂</a></li>
<li><a class="link" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier"  target="_blank" rel="noopener"
    >BaggingClassifier</a></li>
<li><a class="link" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier"  target="_blank" rel="noopener"
    >VotingClassifier</a></li>
<li><a class="link" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier"  target="_blank" rel="noopener"
    >RandomForestClassifier</a></li>
<li><a class="link" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?highlight=adaboost#sklearn.ensemble.AdaBoostClassifier"  target="_blank" rel="noopener"
    >AdaBoostClassifier</a></li>
<li><a class="link" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html?highlight=adaboost#sklearn.ensemble.AdaBoostRegressor"  target="_blank" rel="noopener"
    >AdaBoostRegressor</a></li>
<li><a class="link" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier"  target="_blank" rel="noopener"
    >GradientBoostingClassifier</a></li>
<li><a class="link" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor"  target="_blank" rel="noopener"
    >GradientBoostingRegressor </a></li>
</ul>
<h2 id="custom-functions-used">Custom functions used</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span><span class="lnt">248
</span><span class="lnt">249
</span><span class="lnt">250
</span><span class="lnt">251
</span><span class="lnt">252
</span><span class="lnt">253
</span><span class="lnt">254
</span><span class="lnt">255
</span><span class="lnt">256
</span><span class="lnt">257
</span><span class="lnt">258
</span><span class="lnt">259
</span><span class="lnt">260
</span><span class="lnt">261
</span><span class="lnt">262
</span><span class="lnt">263
</span><span class="lnt">264
</span><span class="lnt">265
</span><span class="lnt">266
</span><span class="lnt">267
</span><span class="lnt">268
</span><span class="lnt">269
</span><span class="lnt">270
</span><span class="lnt">271
</span><span class="lnt">272
</span><span class="lnt">273
</span><span class="lnt">274
</span><span class="lnt">275
</span><span class="lnt">276
</span><span class="lnt">277
</span><span class="lnt">278
</span><span class="lnt">279
</span><span class="lnt">280
</span><span class="lnt">281
</span><span class="lnt">282
</span><span class="lnt">283
</span><span class="lnt">284
</span><span class="lnt">285
</span><span class="lnt">286
</span><span class="lnt">287
</span><span class="lnt">288
</span><span class="lnt">289
</span><span class="lnt">290
</span><span class="lnt">291
</span><span class="lnt">292
</span><span class="lnt">293
</span><span class="lnt">294
</span><span class="lnt">295
</span><span class="lnt">296
</span><span class="lnt">297
</span><span class="lnt">298
</span><span class="lnt">299
</span><span class="lnt">300
</span><span class="lnt">301
</span><span class="lnt">302
</span><span class="lnt">303
</span><span class="lnt">304
</span><span class="lnt">305
</span><span class="lnt">306
</span><span class="lnt">307
</span><span class="lnt">308
</span><span class="lnt">309
</span><span class="lnt">310
</span><span class="lnt">311
</span><span class="lnt">312
</span><span class="lnt">313
</span><span class="lnt">314
</span><span class="lnt">315
</span><span class="lnt">316
</span><span class="lnt">317
</span><span class="lnt">318
</span><span class="lnt">319
</span><span class="lnt">320
</span><span class="lnt">321
</span><span class="lnt">322
</span><span class="lnt">323
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="k">def</span> <span class="nf">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">clf</span><span class="p">,</span>
                          <span class="n">feature_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">filler_feature_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">filler_feature_ranges</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">X_highlight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">zoom_factor</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                          <span class="n">legend</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">hide_spines</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                          <span class="n">markers</span><span class="o">=</span><span class="s1">&#39;s^oxv&lt;&gt;&#39;</span><span class="p">,</span>
                          <span class="n">colors</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;#1f77b4,#ff7f0e,#3ca02c,#d62728,&#39;</span>
                                  <span class="s1">&#39;#9467bd,#8c564b,#e377c2,&#39;</span>
                                  <span class="s1">&#39;#7f7f7f,#bcbd22,#17becf&#39;</span><span class="p">),</span>
                          <span class="n">scatter_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">contourf_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">scatter_highlight_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;Plot decision regions of a classifier.
</span><span class="s2">
</span><span class="s2">    Please note that this functions assumes that class labels are
</span><span class="s2">    labeled consecutively, e.g,. 0, 1, 2, 3, 4, and 5. If you have class
</span><span class="s2">    labels with integer labels &gt; 4, you may want to provide additional colors
</span><span class="s2">    and/or markers as `colors` and `markers` arguments.
</span><span class="s2">    See http://matplotlib.org/examples/color/named_colors.html for more
</span><span class="s2">    information.
</span><span class="s2">
</span><span class="s2">    Parameters
</span><span class="s2">    ----------
</span><span class="s2">    X : array-like, shape = [n_samples, n_features]
</span><span class="s2">        Feature Matrix.
</span><span class="s2">
</span><span class="s2">    y : array-like, shape = [n_samples]
</span><span class="s2">        True class labels.
</span><span class="s2">
</span><span class="s2">    clf : Classifier object.
</span><span class="s2">        Must have a .predict method.
</span><span class="s2">
</span><span class="s2">    feature_index : array-like (default: (0,) for 1D, (0, 1) otherwise)
</span><span class="s2">        Feature indices to use for plotting. The first index in
</span><span class="s2">        `feature_index` will be on the x-axis, the second index will be
</span><span class="s2">        on the y-axis.
</span><span class="s2">
</span><span class="s2">    filler_feature_values : dict (default: None)
</span><span class="s2">        Only needed for number features &gt; 2. Dictionary of feature
</span><span class="s2">        index-value pairs for the features not being plotted.
</span><span class="s2">
</span><span class="s2">    filler_feature_ranges : dict (default: None)
</span><span class="s2">        Only needed for number features &gt; 2. Dictionary of feature
</span><span class="s2">        index-value pairs for the features not being plotted. Will use the
</span><span class="s2">        ranges provided to select training samples for plotting.
</span><span class="s2">
</span><span class="s2">    ax : matplotlib.axes.Axes (default: None)
</span><span class="s2">        An existing matplotlib Axes. Creates
</span><span class="s2">        one if ax=None.
</span><span class="s2">
</span><span class="s2">    X_highlight : array-like, shape = [n_samples, n_features] (default: None)
</span><span class="s2">        An array with data points that are used to highlight samples in `X`.
</span><span class="s2">
</span><span class="s2">    zoom_factor : float (default: 1.0)
</span><span class="s2">        Controls the scale of the x- and y-axis of the decision plot.
</span><span class="s2">
</span><span class="s2">    hide_spines : bool (default: True)
</span><span class="s2">        Hide axis spines if True.
</span><span class="s2">
</span><span class="s2">    legend : int (default: 1)
</span><span class="s2">        Integer to specify the legend location.
</span><span class="s2">        No legend if legend is 0.
</span><span class="s2">
</span><span class="s2">    markers : str (default: &#39;s^oxv&lt;&gt;&#39;)
</span><span class="s2">        Scatterplot markers.
</span><span class="s2">
</span><span class="s2">    colors : str (default: &#39;red,blue,limegreen,gray,cyan&#39;)
</span><span class="s2">        Comma separated list of colors.
</span><span class="s2">
</span><span class="s2">    scatter_kwargs : dict (default: None)
</span><span class="s2">        Keyword arguments for underlying matplotlib scatter function.
</span><span class="s2">
</span><span class="s2">    contourf_kwargs : dict (default: None)
</span><span class="s2">        Keyword arguments for underlying matplotlib contourf function.
</span><span class="s2">
</span><span class="s2">    scatter_highlight_kwargs : dict (default: None)
</span><span class="s2">        Keyword arguments for underlying matplotlib scatter function.
</span><span class="s2">
</span><span class="s2">    Returns
</span><span class="s2">    ---------
</span><span class="s2">    ax : matplotlib.axes.Axes object
</span><span class="s2">
</span><span class="s2">    Examples
</span><span class="s2">    -----------
</span><span class="s2">    For usage examples, please see
</span><span class="s2">    http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/
</span><span class="s2">
</span><span class="s2">    &#34;&#34;&#34;</span>

    <span class="n">check_Xy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_int</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Validate X and y arrays</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>

    <span class="n">plot_testdata</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X_highlight</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">X_highlight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;X_highlight must be a NumPy array or None&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">plot_testdata</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_highlight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;X_highlight must be a 2D array&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">feature_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Unpack and validate the feature_index values</span>
        <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;feature_index requires more than one training feature&#39;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">x_index</span><span class="p">,</span> <span class="n">y_index</span> <span class="o">=</span> <span class="n">feature_index</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;Unable to unpack feature_index. Make sure feature_index &#39;</span>
                <span class="s1">&#39;only has two dimensions.&#39;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">X</span><span class="p">[:,</span> <span class="n">x_index</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">y_index</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span>
                <span class="s1">&#39;feature_index values out of range. X.shape is </span><span class="si">{}</span><span class="s1">, but &#39;</span>
                <span class="s1">&#39;feature_index is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">feature_index</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">feature_index</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x_index</span><span class="p">,</span> <span class="n">y_index</span> <span class="o">=</span> <span class="n">feature_index</span>

    <span class="c1"># Extra input validation for higher number of training features</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">filler_feature_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Filler values must be provided when &#39;</span>
                             <span class="s1">&#39;X has more than 2 training features.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">filler_feature_ranges</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">set</span><span class="p">(</span><span class="n">filler_feature_values</span><span class="p">)</span> <span class="o">==</span> <span class="nb">set</span><span class="p">(</span><span class="n">filler_feature_ranges</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;filler_feature_values and filler_feature_ranges must &#39;</span>
                    <span class="s1">&#39;have the same keys&#39;</span><span class="p">)</span>

        <span class="c1"># Check that all columns in X are accounted for</span>
        <span class="n">column_check</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">filler_feature_values</span><span class="p">:</span>
            <span class="n">column_check</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">feature_index</span><span class="p">:</span>
            <span class="n">column_check</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">column_check</span><span class="p">):</span>
            <span class="n">missing_cols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="o">~</span><span class="n">column_check</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;Column(s) </span><span class="si">{}</span><span class="s1"> need to be accounted for in either &#39;</span>
                <span class="s1">&#39;feature_index or filler_feature_values&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">missing_cols</span><span class="p">))</span>

    <span class="n">marker_gen</span> <span class="o">=</span> <span class="n">cycle</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">markers</span><span class="p">))</span>

    <span class="n">n_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">colors</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
    <span class="n">colors_gen</span> <span class="o">=</span> <span class="n">cycle</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="nb">next</span><span class="p">(</span><span class="n">colors_gen</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)]</span>

    <span class="c1"># Get minimum and maximum</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">x_index</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">1.</span><span class="o">/</span><span class="n">zoom_factor</span><span class="p">,</span>
                    <span class="n">X</span><span class="p">[:,</span> <span class="n">x_index</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1.</span><span class="o">/</span><span class="n">zoom_factor</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">y_index</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">1.</span><span class="o">/</span><span class="n">zoom_factor</span><span class="p">,</span>
                        <span class="n">X</span><span class="p">[:,</span> <span class="n">y_index</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1.</span><span class="o">/</span><span class="n">zoom_factor</span><span class="p">)</span>

    <span class="n">xnum</span><span class="p">,</span> <span class="n">ynum</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">dpi</span> <span class="o">*</span> <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">get_size_inches</span><span class="p">()</span>
    <span class="n">xnum</span><span class="p">,</span> <span class="n">ynum</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="n">xnum</span><span class="p">),</span> <span class="n">ceil</span><span class="p">(</span><span class="n">ynum</span><span class="p">)</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">xnum</span><span class="p">),</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">ynum</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">X_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">X_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span>
        <span class="n">X_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X_grid</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dim</span><span class="p">))</span>
        <span class="n">X_predict</span><span class="p">[:,</span> <span class="n">x_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_grid</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">X_predict</span><span class="p">[:,</span> <span class="n">y_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_grid</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">feature_idx</span> <span class="ow">in</span> <span class="n">filler_feature_values</span><span class="p">:</span>
                <span class="n">X_predict</span><span class="p">[:,</span> <span class="n">feature_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">filler_feature_values</span><span class="p">[</span><span class="n">feature_idx</span><span class="p">]</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_predict</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c1"># Plot decisoin region</span>
    <span class="c1"># Make sure contourf_kwargs has backwards compatible defaults</span>
    <span class="n">contourf_kwargs_default</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">0.45</span><span class="p">,</span> <span class="s1">&#39;antialiased&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
    <span class="n">contourf_kwargs</span> <span class="o">=</span> <span class="n">format_kwarg_dictionaries</span><span class="p">(</span>
                        <span class="n">default_kwargs</span><span class="o">=</span><span class="n">contourf_kwargs_default</span><span class="p">,</span>
                        <span class="n">user_kwargs</span><span class="o">=</span><span class="n">contourf_kwargs</span><span class="p">,</span>
                        <span class="n">protected_keys</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;colors&#39;</span><span class="p">,</span> <span class="s1">&#39;levels&#39;</span><span class="p">])</span>
    <span class="n">cset</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span>
                       <span class="n">colors</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span>
                       <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span>
                       <span class="o">**</span><span class="n">contourf_kwargs</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cset</span><span class="o">.</span><span class="n">levels</span><span class="p">,</span>
               <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span>
               <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
               <span class="n">antialiased</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">()])</span>

    <span class="c1"># Scatter training data samples</span>
    <span class="c1"># Make sure scatter_kwargs has backwards compatible defaults</span>
    <span class="n">scatter_kwargs_default</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span> <span class="s1">&#39;black&#39;</span><span class="p">}</span>
    <span class="n">scatter_kwargs</span> <span class="o">=</span> <span class="n">format_kwarg_dictionaries</span><span class="p">(</span>
                        <span class="n">default_kwargs</span><span class="o">=</span><span class="n">scatter_kwargs_default</span><span class="p">,</span>
                        <span class="n">user_kwargs</span><span class="o">=</span><span class="n">scatter_kwargs</span><span class="p">,</span>
                        <span class="n">protected_keys</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;marker&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">y_data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">c</span><span class="p">]]</span>
            <span class="n">x_data</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">c</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">y_data</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">c</span><span class="p">,</span> <span class="n">y_index</span><span class="p">]</span>
            <span class="n">x_data</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">c</span><span class="p">,</span> <span class="n">x_index</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">dim</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">filler_feature_ranges</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">class_mask</span> <span class="o">=</span> <span class="n">y</span> <span class="o">==</span> <span class="n">c</span>
            <span class="n">feature_range_mask</span> <span class="o">=</span> <span class="n">get_feature_range_mask</span><span class="p">(</span>
                            <span class="n">X</span><span class="p">,</span> <span class="n">filler_feature_values</span><span class="o">=</span><span class="n">filler_feature_values</span><span class="p">,</span>
                            <span class="n">filler_feature_ranges</span><span class="o">=</span><span class="n">filler_feature_ranges</span><span class="p">)</span>
            <span class="n">y_data</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">class_mask</span> <span class="o">&amp;</span> <span class="n">feature_range_mask</span><span class="p">,</span> <span class="n">y_index</span><span class="p">]</span>
            <span class="n">x_data</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">class_mask</span> <span class="o">&amp;</span> <span class="n">feature_range_mask</span><span class="p">,</span> <span class="n">x_index</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_data</span><span class="p">,</span>
                   <span class="n">y</span><span class="o">=</span><span class="n">y_data</span><span class="p">,</span>
                   <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
                   <span class="n">marker</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">marker_gen</span><span class="p">),</span>
                   <span class="n">label</span><span class="o">=</span><span class="n">c</span><span class="p">,</span>
                   <span class="o">**</span><span class="n">scatter_kwargs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">hide_spines</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;bottom&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">([])</span>

    <span class="k">if</span> <span class="n">plot_testdata</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x_data</span> <span class="o">=</span> <span class="n">X_highlight</span>
            <span class="n">y_data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">X_highlight</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">x_data</span> <span class="o">=</span> <span class="n">X_highlight</span><span class="p">[:,</span> <span class="n">x_index</span><span class="p">]</span>
            <span class="n">y_data</span> <span class="o">=</span> <span class="n">X_highlight</span><span class="p">[:,</span> <span class="n">y_index</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">feature_range_mask</span> <span class="o">=</span> <span class="n">get_feature_range_mask</span><span class="p">(</span>
                    <span class="n">X_highlight</span><span class="p">,</span> <span class="n">filler_feature_values</span><span class="o">=</span><span class="n">filler_feature_values</span><span class="p">,</span>
                    <span class="n">filler_feature_ranges</span><span class="o">=</span><span class="n">filler_feature_ranges</span><span class="p">)</span>
            <span class="n">y_data</span> <span class="o">=</span> <span class="n">X_highlight</span><span class="p">[</span><span class="n">feature_range_mask</span><span class="p">,</span> <span class="n">y_index</span><span class="p">]</span>
            <span class="n">x_data</span> <span class="o">=</span> <span class="n">X_highlight</span><span class="p">[</span><span class="n">feature_range_mask</span><span class="p">,</span> <span class="n">x_index</span><span class="p">]</span>

        <span class="c1"># Make sure scatter_highlight_kwargs backwards compatible defaults</span>
        <span class="n">scatter_highlight_defaults</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="s1">&#39;none&#39;</span><span class="p">,</span>
                                      <span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span>
                                      <span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
                                      <span class="s1">&#39;linewidths&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                                      <span class="s1">&#39;marker&#39;</span><span class="p">:</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span>
                                      <span class="s1">&#39;s&#39;</span><span class="p">:</span> <span class="mi">80</span><span class="p">}</span>
        <span class="n">scatter_highlight_kwargs</span> <span class="o">=</span> <span class="n">format_kwarg_dictionaries</span><span class="p">(</span>
                                    <span class="n">default_kwargs</span><span class="o">=</span><span class="n">scatter_highlight_defaults</span><span class="p">,</span>
                                    <span class="n">user_kwargs</span><span class="o">=</span><span class="n">scatter_highlight_kwargs</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span>
                   <span class="n">y_data</span><span class="p">,</span>
                   <span class="o">**</span><span class="n">scatter_highlight_kwargs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">legend</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">filler_feature_ranges</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">handles</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
                      <span class="n">framealpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">scatterpoints</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">legend</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ax</span>


<span class="k">def</span> <span class="nf">plot_labeled_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">models</span><span class="p">):</span>    
    <span class="s1">&#39;&#39;&#39;
</span><span class="s1">    Function producing a scatter plot of the instances contained 
</span><span class="s1">    in the 2D dataset (X,y) along with the decision 
</span><span class="s1">    regions of two trained classification models contained in the
</span><span class="s1">    list &#39;models&#39;.
</span><span class="s1">            
</span><span class="s1">    Parameters
</span><span class="s1">    ----------
</span><span class="s1">    X: pandas DataFrame corresponding to two numerical features 
</span><span class="s1">    y: pandas Series corresponding the class labels
</span><span class="s1">    models: list containing two trained classifiers 
</span><span class="s1">    
</span><span class="s1">    &#39;&#39;&#39;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">models</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;
</span><span class="s1">        Models should be a list containing only two trained classifiers.
</span><span class="s1">        &#39;&#39;&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;
</span><span class="s1">        X has to be a pandas DataFrame with two numerical features.
</span><span class="s1">        &#39;&#39;&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;
</span><span class="s1">        y has to be a pandas Series corresponding to the labels.
</span><span class="s1">        &#39;&#39;&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">6.0</span><span class="p">,</span><span class="mf">2.7</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span>
        <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></td></tr></table>
</div>
</div>
</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/scikit-learn/">scikit-learn</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css"integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js"integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js"integrity="sha384-vZTG03m&#43;2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ]
        });})
</script>
    
</article>

    

    

<aside class="related-contents--wrapper">
    <h2 class="section-title">Related contents</h2>
    <div class="related-contents">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/p/machine-learning-iii-linear-classifiers/">
        
        

        <div class="article-details">
            <h2 class="article-title">Machine Learning III (Linear Classifiers)</h2>
        </div>
    </a>
</article>
            
                
<article class="">
    <a href="/p/machine-learning-ii-unsupervised-learning/">
        
        

        <div class="article-details">
            <h2 class="article-title">Machine Learning II (Unsupervised Learning)</h2>
        </div>
    </a>
</article>
            
                
<article class="">
    <a href="/p/machine-learning-i-supervised-learning-by-scikit-learn/">
        
        

        <div class="article-details">
            <h2 class="article-title">Machine Learning I (Supervised Learning by Scikit-Learn)</h2>
        </div>
    </a>
</article>
            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2022 Haoming Wang
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.11.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>

</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >

            </main>
    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#classification-and-regression-trees-cart">Classification and Regression Trees (CART)</a>
      <ol>
        <li><a href="#decision-tree-for-classification">Decision tree for classification</a></li>
        <li><a href="#classification-tree-learning">Classification tree Learning</a></li>
        <li><a href="#decision-tree-for-regression">Decision tree for regression</a></li>
      </ol>
    </li>
    <li><a href="#the-bias-variance-tradeoff">The Bias-Variance Tradeoff</a>
      <ol>
        <li><a href="#generalization-error">Generalization error</a></li>
        <li><a href="#diagnose-bias-and-variance-problems">Diagnose bias and variance problems</a></li>
        <li><a href="#ensemble-learning">Ensemble Learning</a></li>
      </ol>
    </li>
    <li><a href="#bagging-and-random-forests">Bagging and Random Forests</a>
      <ol>
        <li><a href="#bagging">Bagging</a></li>
        <li><a href="#out-of-bag-evaluation">Out of Bag Evaluation</a></li>
        <li><a href="#random-forests-rf">Random Forests (RF)</a></li>
      </ol>
    </li>
    <li><a href="#boosting">Boosting</a>
      <ol>
        <li><a href="#adaboost">Adaboost</a></li>
        <li><a href="#gradient-boosting-gb">Gradient Boosting (GB)</a></li>
        <li><a href="#stochastic-gradient-boosting-sgb">Stochastic Gradient Boosting (SGB)</a></li>
      </ol>
    </li>
    <li><a href="#model-tuning">Model Tuning</a>
      <ol>
        <li><a href="#tuning-a-carts-hyperparameters">Tuning a CART&rsquo;s Hyperparameters</a></li>
        <li><a href="#tuning-a-rfs-hyperparameters">Tuning a RF&rsquo;s Hyperparameters</a></li>
      </ol>
    </li>
    <li><a href="#compendium">Compendium</a>
      <ol>
        <li><a href="#sklearntree">sklearn.tree</a></li>
        <li><a href="#sklearnensemble">sklearn.ensemble</a></li>
        <li><a href="#custom-functions-used">Custom functions used</a></li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

<script
    src="https://cdn.jsdelivr.net/gh/zhixuan2333/gh-blog@v0.1.0/js/ribbon.min.js"
    integrity="sha384-UEK8ZiP3VgFNP8KnKMKDmd4pAUAOJ59Y2Jo3ED2Z5qKQf6HLHovMxq7Beb9CLPUe"
    crossorigin="anonymous"
    size="300"
    alpha="0.6"
    zindex="-1"
    defer
></script>


    </body>
</html>
