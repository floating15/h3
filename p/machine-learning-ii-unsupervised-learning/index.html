<!DOCTYPE html>
<html lang="en-us" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.
 Clustering Unsupervised learning is a class of machine learning techniques for discovering patterns in data. It learns without labels. It is pure pattern discovery, unguided by a prediction task.
Kmeans k-means finds a specified number of clusters in the samples. To start,
  import KMeans from sklearn.'><title>Machine Learning II (Unsupervised Learning)</title>

<link rel='canonical' href='https://wanghaoming177.netlify.app/p/machine-learning-ii-unsupervised-learning/'>

<link rel="stylesheet" href="/scss/style.min.6cb794b99473add9625fc6fcb4fd8782c21b06365b632b8fbb1076efe11c688e.css"><meta property='og:title' content='Machine Learning II (Unsupervised Learning)'>
<meta property='og:description' content='All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.
 Clustering Unsupervised learning is a class of machine learning techniques for discovering patterns in data. It learns without labels. It is pure pattern discovery, unguided by a prediction task.
Kmeans k-means finds a specified number of clusters in the samples. To start,
  import KMeans from sklearn.'>
<meta property='og:url' content='https://wanghaoming177.netlify.app/p/machine-learning-ii-unsupervised-learning/'>
<meta property='og:site_name' content='Haoming Wang'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='scikit-learn' /><meta property='article:published_time' content='2022-03-15T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2022-03-15T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="Machine Learning II (Unsupervised Learning)">
<meta name="twitter:description" content="All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.
 Clustering Unsupervised learning is a class of machine learning techniques for discovering patterns in data. It learns without labels. It is pure pattern discovery, unguided by a prediction task.
Kmeans k-means finds a specified number of clusters in the samples. To start,
  import KMeans from sklearn.">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            localStorage.setItem(colorSchemeKey, "dark");
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/IMG_0770_hu86b91b9497cdda82d5c488540c7342b2_101927_300x0_resize_q75_box.jpg" width="300"
                            height="273" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">üåè</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">Haoming Wang</a></h1>
            <h2 class="site-description">We choose to go to the moon in this decade and do the other things, not because they are easy, but because they are hard.</h2>
        </div>
    </header><ol class="social-menu">
            
                <li>
                    <a 
                        href='https://github.com/floating15'
                        target="_blank"
                        title="GitHub"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://twitter.com'
                        target="_blank"
                        title="Linkedin"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="28" height="28" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <rect x="4" y="4" width="16" height="16" rx="2" />
  <line x1="8" y1="11" x2="8" y2="16" />
  <line x1="8" y1="8" x2="8" y2="8.01" />
  <line x1="12" y1="16" x2="12" y2="11" />
  <path d="M16 16v-3a2 2 0 0 0 -4 0" />
</svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://follow.it/haoming-wang?action=followPub'
                        target="_blank"
                        title="r-mail"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-mailbox" width="28" height="28" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M10 21v-6.5a3.5 3.5 0 0 0 -7 0v6.5h18v-6a4 4 0 0 0 -4 -4h-10.5" />
  <path d="M12 11v-8h4l2 2l-2 2h-4" />
  <path d="M6 15h1" />
</svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://twitter.com'
                        target="_blank"
                        title="Twitter"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M22 4.01c-1 .49 -1.98 .689 -3 .99c-1.121 -1.265 -2.783 -1.335 -4.38 -.737s-2.643 2.06 -2.62 3.737v1c-3.245 .083 -6.135 -1.395 -8 -4c0 0 -4.182 7.433 4 11c-1.872 1.247 -3.739 2.088 -6 2c3.308 1.803 6.913 2.423 10.034 1.517c3.58 -1.04 6.522 -3.723 7.651 -7.742a13.84 13.84 0 0 0 .497 -3.753c-.002 -.249 1.51 -2.772 1.818 -4.013z" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        

        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="#FF1C02" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        

        <li >
            <a href='/about/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="#35FF02" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>About</span>
            </a>
        </li>
        
        

        <li >
            <a href='/notes/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-math-function" width="28" height="28" viewBox="0 0 24 24" stroke-width="1.5" stroke="#02EAFF" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M14 10h1c1 0 1 1 2.016 3.527c.984 2.473 .984 3.473 1.984 3.473h1" />
  <path d="M13 17c1.5 0 3 -2 4 -3.5s2.5 -3.5 4 -3.5" />
  <path d="M3 19c0 1.5 .5 2 2 2s2 -4 3 -9s1.5 -9 3 -9s2 .5 2 2" />
  <line x1="5" y1="12" x2="11" y2="12" />
</svg>
                
                <span>Notes</span>
            </a>
        </li>
        
        

        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="#0223FF" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        

        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="#F402FF" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        

        <li >
            <a href='/links/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>Links</span>
            </a>
        </li>
        

        <div class="menu-bottom-section">
            
            
        </div>
    </ol>
</aside>
<main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/ml/" style="background-color: false; color: false;">
                Machine Learning
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/machine-learning-ii-unsupervised-learning/">Machine Learning II (Unsupervised Learning)</a>
        </h2>
    
        
    </div>

    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Mar 15, 2022</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    37 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>
</header>

    <section class="article-content">
    
    
    <blockquote>
<p>All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.</p>
</blockquote>
<h1 id="clustering">Clustering</h1>
<p>Unsupervised learning is a class of machine learning techniques for discovering patterns in data. It learns without labels. It is pure pattern discovery, unguided by a prediction task.</p>
<h2 id="kmeans">Kmeans</h2>
<p>k-means finds a specified number of clusters in the samples. To start,</p>
<ul>
<li>
<p>import <code>KMeans</code> from <code>sklearn.cluster</code>.</p>
</li>
<li>
<p>instantiate <code>KMeans</code>, specifying the number of clusters we want to find by <code>n_clusters</code> argument.</p>
</li>
<li>
<p>call the <code>.fit()</code> method of the model, passing the array of samples.</p>
</li>
<li>
<p>use the <code>.predict()</code> method of the model on these <strong>same</strong> samples. This returns a cluster label for each sample, indicating to which cluster a sample belongs. Alternatively, you can use <code>.fit_predict()</code> method to assemble <code>.fit()</code> and <code>.predict()</code>.</p>
</li>
</ul>
<p>k-means remembers the mean (or average) of the samples in each cluster. These are called the &ldquo;centroids&rdquo;. We can attain these centroids by referring <code>cluster_centers_</code> attribute on the model.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Create a KMeans instance with 3 clusters: model</span>
<span class="n">kms</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Fit model to points</span>
<span class="n">kms</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>

<span class="c1"># Determine the cluster labels of new_points: labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">kms</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_points</span><span class="p">)</span>

<span class="c1"># Print cluster labels of new_points</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="c1"># Assign the columns of new_points: xs and ys</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">new_points</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">new_points</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Make a scatter plot of xs and ys, using labels to define the colors</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Assign the cluster centers: centroids</span>
<span class="n">centroids</span> <span class="o">=</span> <span class="n">kms</span><span class="o">.</span><span class="n">cluster_centers_</span>

<span class="c1"># Assign the columns of centroids: centroids_x, centroids_y</span>
<span class="n">centroids_x</span> <span class="o">=</span> <span class="n">centroids</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">centroids_y</span> <span class="o">=</span> <span class="n">centroids</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Make a scatter plot of centroids_x and centroids_y</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centroids_x</span><span class="p">,</span> <span class="n">centroids_y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>[1 2 0 1 2 1 2 2 2 0 1 2 2 0 0 2 0 0 2 2 0 2 1 2 1 0 2 0 0 1 1 2 2 2 0 1 2
	2 1 2 0 1 1 0 1 2 0 0 2 2 2 2 0 0 1 1 0 0 0 1 1 2 2 2 1 2 0 2 1 0 1 1 1 2
	1 0 0 1 2 0 1 0 1 2 0 2 0 1 2 2 2 1 2 2 1 0 0 0 0 1 2 1 0 0 1 1 2 1 0 0 1
	0 0 0 2 2 2 2 0 0 2 1 2 0 2 1 0 2 0 0 2 0 2 0 1 2 1 1 2 0 1 2 1 1 0 2 2 1
	0 1 0 2 1 0 0 1 0 2 2 0 2 0 0 2 2 1 2 2 0 1 0 1 1 2 1 2 2 1 1 0 1 1 1 0 2
	2 1 0 1 0 0 2 2 2 1 2 2 2 0 0 1 2 1 1 1 0 2 2 2 2 2 2 0 0 2 0 0 0 0 2 0 0
	2 2 1 0 1 1 0 1 0 1 0 2 2 0 2 2 2 0 1 1 0 2 2 0 2 0 0 2 0 0 1 0 1 1 1 2 0
	0 0 1 2 1 0 1 0 0 2 1 1 1 0 2 2 2 1 2 0 0 2 1 1 0 1 1 0 1 2 1 0 0 0 0 2 0
	0 2 2 1]
</code></pre><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205030706966.png"
	
	
	
	loading="lazy"
	
	
></p>
<h2 id="evaluating">Evaluating</h2>
<h3 id="inertia_"><code>inertia_</code></h3>
<p>We need a way to measure the quality of a clustering that uses only the clusters and the samples themselves. A good clustering has tight clusters, meaning that the samples in each cluster are bunched together, not spread out.</p>
<p>How spread out the samples within each cluster are can be measured by the inertia.  The inertia of a kmeans model is measured automatically when <code>fit</code> methods are called, and is available afterwards as the <code>inertia_</code> attribute. Intuitively, inertia measures how far samples are from their centroids. We want clusters that are not spread out, so lower values of the inertia are better.</p>
<p>Intuitively, inertia decreases monotonically with the number of classes, Thus a good rule of thumb is to choose an <strong>elbow</strong> in the inertia plot, that is, a point where the inertia begins to decrease more slowly.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">ks</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">inertias</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">ks</span><span class="p">:</span>
    <span class="c1"># Create a KMeans instance with k clusters: model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Fit model to samples</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
    
    <span class="c1"># Append the inertia to the list of inertias</span>
    <span class="n">inertias</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>
    
<span class="c1"># Plot ks vs inertias</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">inertias</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;number of clusters, k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;inertia&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">ks</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205030727245.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>In this case, <code>n_clusters=3</code> would be a good choice.</p>
<h3 id="crosstab">crosstab</h3>
<p>If the dataset is labeled before, we can use crosstab to evaluate the clustering.</p>
<ul>
<li>Import <code>pandas</code>,</li>
<li>create a two-column DataFrame, where the first column is cluster labels and the second column is the real label</li>
<li>use the pandas <code>crosstab</code> function to build the cross tabulation, passing the two columns of the DataFrame.</li>
</ul>
<p>Cross tabulations like these provide great insights into which sort of samples are in which cluster. For example, the grain variety for each sample <code>varieties</code> is given:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Create a KMeans model with 3 clusters: model</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model3</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">model4</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Use fit_predict to fit model and obtain cluster labels: labels</span>
<span class="n">labels2</span> <span class="o">=</span> <span class="n">model2</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="n">labels3</span> <span class="o">=</span> <span class="n">model3</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="n">labels4</span> <span class="o">=</span> <span class="n">model4</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Create a DataFrame with labels and varieties as columns: df</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">labels2</span><span class="p">,</span> <span class="s1">&#39;varieties&#39;</span><span class="p">:</span> <span class="n">varieties</span><span class="p">})</span>
<span class="n">df3</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">labels3</span><span class="p">,</span> <span class="s1">&#39;varieties&#39;</span><span class="p">:</span> <span class="n">varieties</span><span class="p">})</span>
<span class="n">df4</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">labels4</span><span class="p">,</span> <span class="s1">&#39;varieties&#39;</span><span class="p">:</span> <span class="n">varieties</span><span class="p">})</span>

<span class="c1"># Create crosstab: ct</span>
<span class="n">ct2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">],</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;varieties&#39;</span><span class="p">])</span>
<span class="n">ct3</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">df3</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">],</span> <span class="n">df3</span><span class="p">[</span><span class="s1">&#39;varieties&#39;</span><span class="p">])</span>
<span class="n">ct4</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">df4</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">],</span> <span class="n">df4</span><span class="p">[</span><span class="s1">&#39;varieties&#39;</span><span class="p">])</span>

<span class="c1"># Display inertia_</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model2</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model3</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model4</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="c1"># Display ct</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct4</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>1011.7123453151189
587.318611594043
471.49830938958627
varieties  Canadian wheat  Kama wheat  Rosa wheat
labels                                           
0                      70          55           1
1                       0          15          69
varieties  Canadian wheat  Kama wheat  Rosa wheat
labels                                           
0                       0           1          60
1                      68           9           0
2                       2          60          10
varieties  Canadian wheat  Kama wheat  Rosa wheat
labels                                           
0                      64           8           0
1                       0           0          48
2                       6          53           0
3                       0           9          22
</code></pre><p>If we assume that each cluster has similar size, then we can see <code>n_cluster=3</code> is indeed the best choice, just like we concluded from the inertia plot.</p>
<h2 id="standardization">Standardization</h2>
<p>In sometimes, the features of the dataset have very different variances. In KMeans clustering, the variance of a feature corresponds to its influence on the clustering algorithm. To give every feature a chance, the data needs to be transformed so that features have equal variance. This can be achieved with the <code>StandardScaler</code> from scikit-learn.</p>
<ul>
<li>import <code>StandardScaler</code> from <code>sklearn.preprocessing</code>.</li>
<li>create a <code>StandardScaler</code> object</li>
<li><code>fit</code> it to the samples. The <code>transform</code> method can now be used to standardize any samples, either the same ones, or completely new ones.</li>
</ul>
<p>The APIs of <code>StandardScaler</code> and <code>KMeans</code> are similar, but there is an important difference. <code>StandardScaler</code> transforms data, and so has a <code>transform</code> method. KMeans, in contrast, assigns cluster labels to samples, and this done using the <code>predict</code> method.</p>
<p>So we need to perform two steps. Firstly, to standardize the data using <code>StandardScaler</code>, and secondly to take the standardized data and cluster it using <code>KMeans</code>. This can be conveniently achieved by combining the two steps using a scikit-learn pipeline. Data then flows from one step into the next, automatically.</p>
<ul>
<li>creating a <code>StandardScaler</code> and a <code>KMeans</code> object.</li>
<li>import the <code>make_pipeline</code> function from <code>sklearn.pipeline</code>.</li>
<li>Apply the <code>make_pipeline</code> function to the steps that we want to compose, in this case, the <code>scaler</code> and the <code>kmeans</code> objects.</li>
<li>use the <code>fit</code> method of the <code>pipeline</code> to fit both the <code>scaler</code> and <code>kmeans</code>, and use its <code>predict</code> method to obtain the cluster labels.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Perform the necessary imports</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Create scaler: scaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="c1"># Create KMeans instance: kmeans</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Create pipeline: pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">scaler</span><span class="p">,</span> <span class="n">kmeans</span><span class="p">)</span>

<span class="c1"># Fit the pipeline to samples</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Calculate the cluster labels: labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Create a DataFrame with labels and species as columns: df</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s1">&#39;labels&#39;</span> <span class="p">:</span> <span class="n">labels</span><span class="p">,</span>
        <span class="s1">&#39;species&#39;</span> <span class="p">:</span> <span class="n">species</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="c1"># Create crosstab: ct</span>
<span class="n">ct</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span>

<span class="c1"># Display ct</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>

<span class="c1"># Un-standardized Data</span>
<span class="n">kmeans2</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">labels2</span> <span class="o">=</span> <span class="n">kmeans2</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s1">&#39;labels&#39;</span> <span class="p">:</span> <span class="n">labels2</span><span class="p">,</span>
        <span class="s1">&#39;species&#39;</span> <span class="p">:</span> <span class="n">species</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">ct2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">],</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct2</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>species  Bream  Pike  Roach  Smelt
labels                            
0            0    17      0      0
1           33     0      1      0
2            0     0      0     13
3            1     0     19      1
species  Bream  Pike  Roach  Smelt
labels                            
0           17    10      3      0
1            0     4      0      0
2            1     1     17     14
3           16     2      0      0
</code></pre><p>We can see that standardization improves the clustering ability.</p>
<p>There are others kinds of data transformation methods in scikit-learn, for example <code>MaxAbsScaler</code> and <code>Normalizer</code>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import Normalizer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Normalizer</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Create a normalizer: normalizer</span>
<span class="n">normalizer</span> <span class="o">=</span> <span class="n">Normalizer</span><span class="p">()</span>

<span class="c1"># Create a KMeans model with 10 clusters: kmeans</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Make a pipeline chaining normalizer and kmeans: pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">normalizer</span><span class="p">,</span> <span class="n">kmeans</span><span class="p">)</span>

<span class="c1"># Fit pipeline to the daily price movements</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">movements</span><span class="p">)</span>

<span class="c1"># Predict the cluster labels: labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">movements</span><span class="p">)</span>

<span class="c1"># Create a DataFrame aligning labels and companies: df</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">labels</span><span class="p">,</span> <span class="s1">&#39;companies&#39;</span><span class="p">:</span> <span class="n">companies</span><span class="p">})</span>

<span class="c1"># Display df sorted by cluster label</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>    labels          companies
3        1   American express
7        5              Canon
47       4           Symantec
41       8      Philip Morris
14       4               Dell
1        1                AIG
34       5         Mitsubishi
9        2  Colgate-Palmolive
4        3             Boeing
10       7     ConocoPhillips
</code></pre><h1 id="hierarchical-clustering-and-t-sne">Hierarchical Clustering and t-SNE</h1>
<h2 id="hierarchical-clustering">Hierarchical clustering</h2>
<h3 id="visualization">visualization</h3>
<p>Hierarchical clustering can organize any sort of data into a hierarchy. Let&rsquo;s consider a new type of dataset, describing how countries scored performances at the Eurovision 2016 song contest. The result of applying hierarchical clustering to the Eurovision scores can be visualized as a tree-like diagram called a &ldquo;<strong>dendrogram</strong>&rdquo;. This single picture reveals a great deal of information about the voting behavior of countries at the Eurovision.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205030833885.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>Hierarchical clustering proceeds in steps. In the beginning, every country is its own cluster - so there are as many clusters as there are countries. At each step, the two closest clusters are merged. This decreases the number of clusters, and eventually, there is only one cluster left, and it contains all the countries. The entire process of the hierarchical clustering is encoded in the dendrogram. To understand better, let&rsquo;s zoom in and look at just one part of this dendrogram.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205030838906.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>In the beginning, there are six clusters, each containing only one country. The first merging is the node where the Cyprus and Greece are merged together. Later on, the new cluster is merged with the cluster containing Bulgaria. Shortly after that, the clusters containing Moldova and Russia are merged, which later is in turn merged with the cluster containing Armenia. Later still, the two big composite clusters are merged together. This process continues until there is only one cluster left, and it contains all the countries.</p>
<p>To perform a hierarchical clustering:</p>
<ul>
<li>import the <code>linkage</code> and <code>dendrogram</code> functions from <code>scipy.cluster.hierarchy</code>.</li>
<li>Use the <code>linkage</code> function to obtain a hierarchical clustering of the samples. Notice there is an extra method argument <code>method='complete'</code>, we&rsquo;ll cover this later.</li>
<li>pass the output of <code>linkage</code> to the <code>dendrogram</code> function to visualize the result. For the dendrogram, we&rsquo;ll need a list of varieties of each observation to specify the <code>labels</code> argument.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Perform the necessary imports</span>
<span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">dendrogram</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Calculate the linkage: mergings</span>
<span class="n">mergings</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;complete&#39;</span><span class="p">)</span>

<span class="c1"># Plot the dendrogram, using varieties as labels</span>
<span class="n">dendrogram</span><span class="p">(</span><span class="n">mergings</span><span class="p">,</span>
           <span class="n">labels</span><span class="o">=</span><span class="n">varieties</span><span class="p">,</span>
           <span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span>
           <span class="n">leaf_font_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">varieties</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>(42, 7) 42
</code></pre><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205030852293.png"
	
	
	
	loading="lazy"
	
	
></p>
<h3 id="cluster-labels">cluster labels</h3>
<p>An intermediate stage in the hierarchical clustering is specified by choosing a height on the dendrogram. For example, choosing a height of 15 defines a clustering in which Bulgaria, Cyprus and Greece are in one cluster, Russia and Moldova are in another, and Armenia is in a cluster on its own.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205040645298.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>The y-axis of the dendrogram encodes the distance between merging clusters. For example, the distance between the cluster containing Cyprus and the cluster containing Greece was approximately 6 when they were merged into a single cluster. When this new cluster was merged with the cluster containing Bulgaria, the distance between them was 12. So the height that specifies an intermediate clustering corresponds to a <strong>distance</strong>. This specifies that the hierarchical clustering should stop merging clusters when all clusters are at least this far apart.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205040647406.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>The distance between two clusters is measured using a &ldquo;linkage method&rdquo;. In our example, we used &ldquo;<code>complete</code>&rdquo; linkage, where <strong>the distance between two clusters is the maximum of the distances between their samples.</strong> This was specified via the &ldquo;<code>method</code>&rdquo; parameter. There are many other linkage methods, like &ldquo;<code>single</code>&rdquo; linkage, the distance between two clusters is the minimum of the distances between their samples. See more method <a class="link" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html"  target="_blank" rel="noopener"
    >here</a>.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205040719390.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>The cluster labels for any intermediate stage of the hierarchical clustering can be extracted using the <code>fcluster</code> function, specifying the linkage returned, height and <code>criterion='distance'</code>. This returns a numpy array containing the cluster labels for all the countries.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Perform the necessary imports</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">dendrogram</span><span class="p">,</span> <span class="n">fcluster</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Calculate the linkage: mergings</span>
<span class="n">mergings</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">)</span>

<span class="c1"># Plot the dendrogram</span>
<span class="n">dendrogram</span><span class="p">(</span>
    <span class="n">mergings</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="n">country_names</span><span class="p">,</span>
    <span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span>
    <span class="n">leaf_font_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Use fcluster to extract labels: labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">fcluster</span><span class="p">(</span><span class="n">mergings</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span><span class="p">)</span>

<span class="c1"># Create a DataFrame with labels and varieties as columns: df</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">labels</span><span class="p">,</span> <span class="s1">&#39;varieties&#39;</span><span class="p">:</span> <span class="n">varieties</span><span class="p">})</span>

<span class="c1"># Create crosstab: ct</span>
<span class="n">ct</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;varieties&#39;</span><span class="p">])</span>

<span class="c1"># Display ct</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>varieties  Canadian wheat  Kama wheat  Rosa wheat
labels                                           
1                      14           3           0
2                       0           0          14
3                       0          11           0
</code></pre><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205040706364.png"
	
	
	
	loading="lazy"
	
	
></p>
<h2 id="t-sne">t-SNE</h2>
<p>t-SNE stands for &ldquo;t-distributed stochastic neighbor embedding&rdquo;. It maps samples from their high-dimensional space into a 2 or 3 dimensional space so they can visualized.</p>
<p>t-SNE is available in scikit-learn, but it works a little differently to the <code>fit</code>/<code>transform</code> components we&rsquo;ve already met. The samples are in a 2-dimensional numpy array <code>samples</code>, and there is a list <code>variety_numbers</code> giving the species of each sample.</p>
<ul>
<li>import <code>TSNE</code> from <code>sklearn.manifold</code></li>
<li>create a <code>TSNE</code> object, with argument <code>learning_rate</code>.</li>
<li>apply the <code>fit_transform()</code> method to the samples, and then make a scatter plot of the result, coloring the points using the species.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import TSNE</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="c1"># Create a TSNE instance: model</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="c1"># Apply fit_transform to samples: tsne_features</span>
<span class="n">tsne_features</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Select the 0th feature: xs</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">tsne_features</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Select the 1st feature: ys</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">tsne_features</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Scatter plot, coloring by variety_numbers</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">variety_numbers</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205040729033.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>There are three aspects that deserve special attention:</p>
<ul>
<li>the <code>fit_transform</code> method
t-SNE only has a <code>fit_transform</code> method which simultaneously fits the model and transforms the data. However, t-SNE does not have separate <code>fit</code> and <code>transform</code> methods. This means that you can&rsquo;t extend a t-SNE map to include new samples. Instead, you have to start over each time.</li>
<li>and the <code>learning_rate</code>.
The <code>learning_rate</code> makes the use of t-SNE more complicated than some other techniques. You may need to try different learning rates for different datasets. Normally it&rsquo;s enough to try a few values between 50 and 200.</li>
<li>A final thing to be aware of is that the axes of a t-SNE plot do not have any interpretable meaning. In fact, they are different every time t-SNE is applied, even on the same data.</li>
</ul>
<p>t-SNE provides great visualizations when the individual samples can be labeled. Here we apply t-SNE to the company stock price data. The stock price movements for each company are in the array <code>normalized_movements</code> (which have been normalized). The list <code>companies</code> gives the name of each company.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import TSNE</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="c1"># Create a TSNE instance: model</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="c1"># Apply fit_transform to normalized_movements: tsne_features</span>
<span class="n">tsne_features</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">normalized_movements</span><span class="p">)</span>

<span class="c1"># Select the 0th feature: xs</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">tsne_features</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Select the 1th feature: ys</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">tsne_features</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Scatter plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Annotate the points</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">company</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">companies</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">company</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205040732207.png"
	
	
	
	loading="lazy"
	
	
></p>
<h1 id="dimension-reduction">Dimension Reduction</h1>
<p>Dimension reduction finds patterns in data, and uses these patterns to re-express it in a compressed form. This makes subsequent computation with the data much more efficient, and this can be a big deal in a world of big datasets. However, the most important function of dimension reduction is to reduce a dataset to its &ldquo;bare bones&rdquo;, discarding noisy features that cause big problems for supervised learning tasks like regression and classification. In many real-world applications, it&rsquo;s dimension reduction that makes prediction possible.</p>
<h2 id="pca">PCA</h2>
<p>The most fundamental of dimension reduction techniques is called &ldquo;Principal Component Analysis&rdquo;, or &ldquo;PCA&rdquo; for short. PCA performs dimension reduction in two steps, and the first one, called &ldquo;de-correlation&rdquo;, doesn&rsquo;t change the dimension of the data at all. It&rsquo;s this first step that we&rsquo;ll focus on in this section.</p>
<p>In this first step, PCA <strong>rotates</strong> the samples so that they are aligned with the coordinate axes; Furthermore PCA also <strong>shifts</strong> the samples so that they have mean zero. These scatter plots show the effect of PCA applied to two features of the wine dataset. Notice that no information is lost - this is true no matter how many features the dataset has.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205040744152.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>scikit-learn has an implementation of PCA de-correlation, and it has <code>fit</code> and <code>transform</code> methods just like <code>StandardScaler</code>. The <code>fit</code> method learns how to shift and how to rotate the samples, but doesn&rsquo;t actually change them. The <code>transform</code> method, on the other hand, applies the transformation that fit learned. In particular, the transform method can be applied to new, unseen samples.</p>
<ul>
<li>import <code>PCA</code> from <code>sklearn.decomposition</code></li>
<li>create a PCA object, and <code>fit</code> it to the samples.</li>
<li>use the fit PCA object to <code>transform</code> the samples.</li>
</ul>
<p>This returns a new array of transformed samples. This new array has the same number of rows and columns as the original sample array, each row and column represents a transformed sample and feature respectively.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Perform the necessary imports</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">pearsonr</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="c1"># Create PCA instance: model</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>

<span class="c1"># Apply the fit_transform method of model to grains: pca_features</span>
<span class="n">pca_features</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">grains</span><span class="p">)</span>

<span class="c1"># Assign 0th column of pca_features: xs</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">pca_features</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Assign 1st column of pca_features: ys</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">pca_features</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Assign the 0th column of grains: width</span>
<span class="n">width</span> <span class="o">=</span> <span class="n">grains</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Assign the 1st column of grains: length</span>
<span class="n">length</span> <span class="o">=</span> <span class="n">grains</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Calculate the Pearson correlation</span>
<span class="n">correlationo</span><span class="p">,</span> <span class="n">pvalueo</span> <span class="o">=</span> <span class="n">pearsonr</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
<span class="n">correlationt</span><span class="p">,</span> <span class="n">pvaluet</span> <span class="o">=</span> <span class="n">pearsonr</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>

<span class="c1"># Display the correlation</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;origin corr: &#39;</span><span class="p">,</span> <span class="n">correlationo</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;transformed corr: &#39;</span><span class="p">,</span> <span class="n">correlationt</span><span class="p">)</span>

<span class="c1"># Scatter plot xs vs ys</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;origin&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;transform&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205040801332.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>Finally, PCA is called &ldquo;principal component analysis&rdquo; because it learns the &ldquo;principal components&rdquo; of the data. These are the directions in which the samples vary the most, depicted here in red. It is the principal components that PCA aligns with the coordinate axes. After a PCA model has been fit, the principal components are available as the <code>components_</code> attribute. This is numpy array with one row for each principal component.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Make a scatter plot of the untransformed points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">grains</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">grains</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Create a PCA instance: model</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>

<span class="c1"># Fit model to points</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">grains</span><span class="p">)</span>

<span class="c1"># Get the mean of the grain samples: mean</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">mean_</span>

<span class="c1"># Get the first principal component: first_pc</span>
<span class="n">first_pc0</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>
<span class="n">first_pc1</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">)</span>

<span class="c1"># Plot first_pc as an arrow, starting at mean</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mean</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">first_pc0</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">first_pc0</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mean</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">first_pc1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">first_pc1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Keep axes on same scale</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>[[ 0.63910027  0.76912343]
 [-0.76912343  0.63910027]]
</code></pre><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205040852402.png"
	
	
	
	loading="lazy"
	
	
></p>
<h2 id="intrinsic-dimension">Intrinsic dimension</h2>
<p>The intrinsic dimension of a dataset is the number of features required to approximate it. The intrinsic dimension informs dimension reduction, because it tells us how much a dataset can be compressed.</p>
<p>To better illustrate the intrinsic dimension, let&rsquo;s consider an example dataset containing only some of the samples from the iris dataset. Specifically, let&rsquo;s take three measurements from the iris versicolor samples: sepal length, sepal width, and petal width. So each sample is represented as a point in 3-dimensional space. However, if we make a 3d scatter plot of the samples, we see that they all lie very close to a flat, 2-dimensional sheet. This means that the data can be approximated by using only two coordinates, without losing much information. So this dataset has intrinsic dimension 2.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205040842294.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>But scatter plots are only possible if there are 3 features or less. So how can the intrinsic dimension be identified, even if there are many features? This is where PCA is really helpful. <strong>The intrinsic dimension can be identified by counting the PCA features that have high variance</strong>.</p>
<p>To see how, let&rsquo;s see what happens when PCA is applied to the dataset of versicolor samples. PCA rotates and shifts the samples to align them with the coordinate axes. This expresses the samples using three PCA features. The PCA features are in a special order. Here is a bar graph showing the variance of each of the PCA features. As you can see, each PCA feature has less variance than the last, and in this case the last PCA feature has very low variance. This agrees with the scatter plot of the PCA features, where the samples don&rsquo;t vary much in the vertical direction. In the other two directions, however, the variance is apparent.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205040845210.png"
	
	
	
	loading="lazy"
	
	
></p>
<p><strong>The intrinsic dimension is the number of PCA features that have significant variance</strong>. In our example, only the first two PCA features have significant variance. So this dataset has intrinsic dimension 2, which agrees with what we observed when inspecting the scatter plot.</p>
<p>To plot the variances of the PCA features in practice:</p>
<ul>
<li>imports <code>PCA</code> from <code>sklearn.decomposition</code>.</li>
<li>create a PCA model <code>pca</code>, and fit it to the samples.</li>
<li>create a range enumerating the number of PCA features which is available as the <code>.n_components_ attribute</code> of <code>pca</code></li>
<li>make a bar plot of the variances; the variances are available as the <code>explained_variance</code> attribute of the <code>pca</code></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Perform the necessary imports</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Create scaler: scaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="c1"># Create a PCA instance: pca</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>

<span class="c1"># Create pipeline: pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">scaler</span><span class="p">,</span> <span class="n">pca</span><span class="p">)</span>

<span class="c1"># Fit the pipeline to &#39;samples&#39;</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Plot the explained variances</span>
<span class="n">features</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">n_components_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;PCA feature&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;variance&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205040856911.png"
	
	
	
	loading="lazy"
	
	
></p>
<h2 id="dimension-reduction-with-pca">Dimension reduction with PCA</h2>
<p>Dimension reduction represents the same data using less features and is vital for building machine learning pipelines using real-world data. We&rsquo;ve seen already that the PCA features are in decreasing order of variance. PCA performs dimension reduction by discarding the PCA features with lower variance, which it assumes to be noise, and retaining the higher variance PCA features, which it assumes to be informative.</p>
<p>To use PCA for dimension reduction, you need to specify how many PCA features to keep. A good choice is the intrinsic dimension of the dataset, if you know it:</p>
<ul>
<li>import <code>PCA</code> as usual.</li>
<li>create a <code>PCA</code> model specifying <code>n_components=2</code>,</li>
<li><code>fit</code> the model and <code>transform</code> the samples as usual.</li>
</ul>
<p>Printing the shape of the transformed samples, we see that there are only two features.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="c1"># Create a PCA model with 2 components: pca</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Fit and transform the PCA instance to the scaled samples</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scaled_samples</span><span class="p">)</span>
<span class="n">pca_features</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">scaled_samples</span><span class="p">)</span>

<span class="c1"># Print the shape of pca_features</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scaled_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pca_features</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>(85, 6)
(85, 2)
</code></pre><p>PCA discards the low variance features, and assumes that the higher variance features are informative. Like all assumptions, there are cases where this doesn&rsquo;t hold.</p>
<h2 id="truncatedsvd"><code>TruncatedSVD</code></h2>
<p>In some cases, an alternative implementation of PCA needs to be used. Word frequency arrays are a great example. In a word-frequency array, each row corresponds to a document, and each column corresponds to a word from a fixed vocabulary. The entries of the word-frequency array measure how often each word appears in each document. Only some of the words from the vocabulary appear in any one document, so most entries of the word frequency array are zero.</p>
<p>Arrays like this are said to be &ldquo;<strong>sparse</strong>&rdquo;, and are often represented using a special type of array called a &ldquo;<code>scipy.sparse.csr_matrix</code>&rdquo;. <code>csr_matrices</code> save space by remembering only the non-zero entries of the array.</p>
<p>For example, <code>TfidfVectorizer</code> transforms a list of documents into a word frequency array, which it outputs as a <code>csr_matrix</code>. It has <code>fit()</code> and <code>transform()</code> methods like other sklearn objects. <code>documents</code> is a list of toy documents about pets:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="c1"># Create a TfidfVectorizer: tfidf</span>
<span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span> 

<span class="c1"># Apply fit_transform to document: csr_mat</span>
<span class="n">csr_mat</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

<span class="c1"># Print result of toarray() method</span>
<span class="nb">print</span><span class="p">(</span><span class="n">csr_mat</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>

<span class="c1"># Get the words: words</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>

<span class="c1"># Print words</span>
<span class="nb">print</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>[[0.51785612 0.         0.         0.68091856 0.51785612 0.        ]
 [0.         0.         0.51785612 0.         0.51785612 0.68091856]
 [0.51785612 0.68091856 0.51785612 0.         0.         0.        ]]
['cats', 'chase', 'dogs', 'meow', 'say', 'woof']
</code></pre><p>Scikit-learn&rsquo;s PCA doesn&rsquo;t support <code>csr_matrices</code>, and we&rsquo;ll need to use <code>sklearn.decomposition.TruncatedSVD</code> instead. <code>TruncatedSVD</code> performs the same transformation as <code>PCA</code>, but accepts <code>csr_matrices</code> as input. Other than that, you interact with <code>TruncatedSVD</code> and <code>PCA</code> in exactly the same way.</p>
<p>Now we will combine our knowledge of <code>TruncatedSVD</code> and <code>kmeans</code> to cluster some popular pages from Wikipedia.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Perform the necessary imports</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Create a TruncatedSVD instance: svd</span>
<span class="n">svd</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="c1"># Create a KMeans instance: kmeans</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="c1"># Create a pipeline: pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">svd</span><span class="p">,</span> <span class="n">kmeans</span><span class="p">)</span>

<span class="c1"># Fit the pipeline to articles</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">articles</span><span class="p">)</span>

<span class="c1"># Calculate the cluster labels: labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">articles</span><span class="p">)</span>

<span class="c1"># Create a DataFrame aligning labels and titles: df</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;label&#39;</span><span class="p">:</span> <span class="n">labels</span><span class="p">,</span> <span class="s1">&#39;article&#39;</span><span class="p">:</span> <span class="n">titles</span><span class="p">})</span>

<span class="c1"># Display df sorted by cluster label</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;label&#39;</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>    label                                        article
59      0                                    Adam Levine
57      0                          Red Hot Chili Peppers
56      0                                       Skrillex
55      0                                  Black Sabbath
54      0                                 Arctic Monkeys
53      0                                   Stevie Nicks
52      0                                     The Wanted
51      0                                     Nate Ruess
50      0                                   Chad Kroeger
58      0                                         Sepsis
30      1                  France national football team
31      1                              Cristiano Ronaldo
32      1                                   Arsenal F.C.
33      1                                 Radamel Falcao
37      1                                       Football
35      1                Colombia national football team
36      1              2014 FIFA World Cup qualification
38      1                                         Neymar
39      1                                  Franck Rib√©ry
34      1                             Zlatan Ibrahimoviƒá
26      2                                     Mila Kunis
28      2                                  Anne Hathaway
27      2                                 Dakota Fanning
25      2                                  Russell Crowe
29      2                               Jennifer Aniston
23      2                           Catherine Zeta-Jones
22      2                              Denzel Washington
21      2                             Michael Fassbender
20      2                                 Angelina Jolie
24      2                                   Jessica Biel
10      3                                 Global warming
11      3       Nationally Appropriate Mitigation Action
13      3                               Connie Hedegaard
14      3                                 Climate change
12      3                                   Nigel Lawson
16      3                                        350.org
17      3  Greenhouse gas emissions by the United States
18      3  2010 United Nations Climate Change Conference
19      3  2007 United Nations Climate Change Conference
15      3                                 Kyoto Protocol
8       4                                        Firefox
1       4                                 Alexa Internet
2       4                              Internet Explorer
3       4                                    HTTP cookie
4       4                                  Google Search
5       4                                         Tumblr
6       4                    Hypertext Transfer Protocol
7       4                                  Social search
49      4                                       Lymphoma
42      4                                    Doxycycline
47      4                                          Fever
46      4                                     Prednisone
44      4                                           Gout
43      4                                       Leukemia
9       4                                       LinkedIn
48      4                                     Gabapentin
0       4                                       HTTP 404
45      5                                    Hepatitis C
41      5                                    Hepatitis B
40      5                                    Tonsillitis
</code></pre><h1 id="non-negative-matrix-factorization-nmf">Non-negative Matrix Factorization (NMF)</h1>
<h2 id="basics-of-nmf">Basics of NMF</h2>
<p>NMF stands for &ldquo;non-negative matrix factorization&rdquo;. NMF, like PCA, is a dimension reduction technique. In constract to PCA, however, NMF models are interpretable. This means an NMF models are easier to understand yourself, and much easier for you to explain to others. NMF can not be applied to every dataset, however. It is required that the sample features be &ldquo;non-negative&rdquo;, so greater than or equal to 0. NMF achieves its interpretability by decomposing samples as sums of their parts. For example, NMF decomposes documents as combinations of common themes, and images as combinations of common patterns.</p>
<p>NMF is available in scikit learn, and follows the same <code>fit</code>/<code>transform</code> pattern as PCA. However, unlike PCA, the argument <code>n_components</code> must always be specified. NMF works both with numpy arrays and sparse arrays in the <code>csr_matrix</code> format.</p>
<p>Let&rsquo;s see an application of NMF to a toy example of a <strong>word-frequency array</strong> <code>articles</code>. Each row represents a document, and each column represents a word. And the entries of the array measure the frequency of each word in each document using what&rsquo;s known as &ldquo;<code>tf-idf</code>&rdquo;. &ldquo;<code>tf</code>&rdquo; is the frequency of the word in the document. &ldquo;<code>idf</code>&rdquo; is a weighting scheme that reduces the influence of frequent words like &ldquo;the&rdquo;. Let&rsquo;s now see how to use NMF in Python.</p>
<ul>
<li>import <code>NMF</code> from <code>sklearn.decomposition</code>.</li>
<li>create a model, specifying the desired number of components <code>n_components=2</code>.</li>
<li>fit the model to the samples, then transform to obtain the dimension-reducted features.</li>
</ul>
<p>As we saw with PCA, our transformed data in this example will have 6 columns, corresponding to our 6 new features.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import NMF and pandas</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">NMF</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Create an NMF instance: model</span>
<span class="n">nmf</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="c1"># Fit the model to articles</span>
<span class="n">nmf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">articles</span><span class="p">)</span>

<span class="c1"># Transform the articles: nmf_features</span>
<span class="n">nmf_features</span> <span class="o">=</span> <span class="n">nmf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">articles</span><span class="p">)</span>

<span class="c1"># Print the NMF features</span>
<span class="nb">print</span><span class="p">(</span><span class="n">articles</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nmf_features</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nmf_features</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="mi">10</span><span class="p">,:])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--------------------------------&#39;</span><span class="p">)</span>

<span class="c1"># Create a pandas DataFrame: df</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">nmf_features</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">titles</span><span class="p">)</span>

<span class="c1"># Print the row for &#39;Anne Hathaway&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;Anne Hathaway&#39;</span><span class="p">])</span>

<span class="c1"># Print the row for &#39;Denzel Washington&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;Denzel Washington&#39;</span><span class="p">])</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>(60, 13125)
(60, 6)
[[0.   0.   0.   0.   0.   0.57]
 [0.   0.   0.   0.   0.   0.4 ]
 [0.   0.   0.   0.   0.   0.38]
 [0.   0.   0.   0.   0.   0.49]
 [0.01 0.01 0.01 0.03 0.   0.33]
 [0.   0.   0.02 0.   0.01 0.36]
 [0.   0.   0.   0.   0.   0.49]
 [0.02 0.01 0.   0.02 0.03 0.48]
 [0.01 0.03 0.03 0.07 0.02 0.34]]
--------------------------------
    0    0.004
    1    0.000
    2    0.000
    3    0.576
    4    0.000
    5    0.000
    Name: Anne Hathaway, dtype: float64
    0    0.000
    1    0.006
    2    0.000
    3    0.422
    4    0.000
    5    0.000
    Name: Denzel Washington, dtype: float64
</code></pre><p>The NMF feature values are always non-negative. The entries of the NMF components are non-negative, as well. Just as PCA has principal components, NMF has components which it learns from the samples, and as with PCA, the dimension of the components is the same as the dimension of the samples. These components are avaliable as <code>components_</code> attribute, each row represents one component and each column represents  the projection of a component on each feature dimension, just like PCA.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="nb">print</span><span class="p">(</span><span class="n">nmf</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nmf</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>[[0.01 0.   0.   ... 0.   0.   0.  ]
 [0.   0.   0.01 ... 0.   0.   0.  ]
 [0.   0.   0.   ... 0.   0.   0.  ]
 [0.   0.   0.   ... 0.   0.01 0.  ]
 [0.   0.   0.   ... 0.   0.   0.  ]
 [0.   0.   0.01 ... 0.   0.   0.  ]]
(6, 13125)
</code></pre><p><strong>The features and the components of an NMF model can be combined to approximately reconstruct the original data samples</strong>: if we multiply each NMF components by the corresponding NMF feature value, and add up each column, we get something very close to the original sample. So a sample can be reconstructed by multiplying the NMF components by the NMF feature values of the sample, and adding up. This calculation also can be expressed as a product of matrice, that&rsquo;s where the &ldquo;matrix factorization&rdquo;, or &ldquo;MF&rdquo;, in NMF comes from.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205051158091.png"
	
	
	
	loading="lazy"
	
	
></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="nb">print</span><span class="p">(</span><span class="n">nmf_features</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nmf</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">articles</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>(60, 6)
(6, 13125)
(60, 13125)
</code></pre><p>So the feature matrix $\cdot$ the components matrix is almost the original sample matrix.</p>
<h2 id="nmf-learns-interpretable-parts">NMF learns interpretable parts</h2>
<h3 id="text-analysis">Text analysis</h3>
<p>The components of NMF represent patterns that frequently occur in the samples. Let&rsquo;s consider a concrete example, where scientific articles are represented by their word frequencies. There are 20000 articles, and 800 words. So the array has 800 columns.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205081033910.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>Let&rsquo;s fit an NMF model with 10 components to the articles. The 10 components are stored as the 10 rows of a 2-dimensional numpy array. The rows, or components, live in an 800-dimensional space - there is one dimension for each of the words. Aligning the words of our vocabulary with the columns of the NMF components allows them to be interpreted. Choosing a component, and looking at which words have the highest values,</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205081025661.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>we see that they fit a theme: the words are &lsquo;species&rsquo;, &lsquo;plant&rsquo;, &lsquo;plants&rsquo;, &lsquo;genetic&rsquo;, &lsquo;evolution&rsquo; and &lsquo;life&rsquo;. So if NMF is applied to documents, then <strong>the components correspond to topics, and the NMF features reconstruct the documents from the topics</strong>.</p>
<p><code>words</code> is a list of the words that label the columns of the word-frequency array:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import pandas</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Create a DataFrame: components_df</span>
<span class="n">components_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">nmf</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">words</span><span class="p">)</span>

<span class="c1"># Print the shape of the DataFrame</span>
<span class="nb">print</span><span class="p">(</span><span class="n">components_df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Select row 3: component</span>
<span class="n">component</span> <span class="o">=</span> <span class="n">components_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

<span class="c1"># Print result of nlargest</span>
<span class="nb">print</span><span class="p">(</span><span class="n">component</span><span class="o">.</span><span class="n">nlargest</span><span class="p">())</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>(6, 13125)
film       0.628
award      0.253
starred    0.245
role       0.211
actress    0.186
Name: 3, dtype: float64
</code></pre><h3 id="grayscale-image">Grayscale image</h3>
<p>An image in which all the pixels are shades of gray ranging from black to white is called a &ldquo;grayscale image&rdquo;. Since there are only shades of grey, a grayscale image can be encoded by the brightness of every pixel. Representing the brightness as a number between 0 and 1, where 0 is totally black and 1 is totally white, the image can be represented as 2-dimensional array of numbers. These 2-dimensional arrays of numbers can then be flattened by enumerating the entries. For instance, we could read-off the values row-by-row, from left-to-right and top to bottom. The grayscale image is now represented by a flat array of non-negative numbers.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205081031872.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>A collection of grayscale images of the same size can thus be encoded as a 2-dimensional array, in which each row represents an image as a flattened array, and each column represents a pixel, and the pixels as features, we see that the data is arranged similarly to the word frequency array. Indeed, the entries of this array are non-negative, so NMF can be used to learn the parts of the images. To recover the image, use the <code>reshape</code> method of the sample, specifying the dimensions of the original image as a tuple. This yields the 2-dimensional array of pixel brightnesses. To display the corresponding image, import <code>pyplot</code>, and pass the 2-dimensional array to the <code>plt.imshow</code> function.</p>
<p>If NMF is applied to a collection of images, then <strong>the NMF components represent patterns that frequently occur in the images</strong>. For instance, NMF decomposes images from an LED display into the individual cells of the display.</p>
<p>Suppose we are given 100 images as a 2D array <code>samples</code>, where each row represents a single 13x8 image. The images in the dataset are pictures of a LED digital display:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import pyplot</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Select the 0th row: digit</span>
<span class="n">digit</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Print digit</span>
<span class="nb">print</span><span class="p">(</span><span class="n">digit</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">digit</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-----------------------&#39;</span><span class="p">)</span>

<span class="c1"># Reshape digit to a 13x8 array: bitmap</span>
<span class="n">bitmap</span> <span class="o">=</span> <span class="n">digit</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Print bitmap</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bitmap</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bitmap</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-----------------------&#39;</span><span class="p">)</span>

<span class="c1"># Use plt.imshow to display bitmap</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">bitmap</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>(100, 104)
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0.]
(104,)
-----------------------
[[0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 1. 1. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]]
(13, 8)
-----------------------
</code></pre><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205081042909.png"
	
	
	
	loading="lazy"
	
	
></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import NMF</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">NMF</span>

<span class="c1"># Create an NMF model: model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="c1"># Apply fit_transform to samples: features</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Call show_as_image on each component</span>
<span class="k">def</span> <span class="nf">show_as_image</span><span class="p">(</span><span class="n">sample</span><span class="p">):</span>
    <span class="n">bitmap</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">13</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
        <span class="n">bitmap</span><span class="p">,</span> 
        <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span>
        <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span>
    <span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">i</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">components_</span><span class="p">:</span>
    <span class="n">show_as_image</span><span class="p">(</span><span class="n">component</span><span class="p">)</span>
    <span class="n">i</span><span class="o">+=</span><span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Select the 0th row of features: digit_features</span>
<span class="n">digit_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>

<span class="c1"># Print digit_features</span>
<span class="nb">print</span><span class="p">(</span><span class="n">digit_features</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>[4.76823559e-01 0.00000000e+00 0.00000000e+00 5.90605054e-01
 4.81559442e-01 0.00000000e+00 7.37562716e-16]
</code></pre><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205081101288.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>Thus we can see that <strong>NMF transform the feature form Pixel (100,104) to Cell (100,7). And each of components (7,104) represents the Pixel of the Cell</strong>.</p>
<p>Unlike NMF, PCA doesn&rsquo;t learn the parts of things. Its components do not correspond to topics (in the case of documents) or to parts of images, when trained on images.  Notice that the components of PCA do not represent meaningful parts of images of LED digits!</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="c1"># Create a PCA instance: model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="c1"># Apply fit_transform to samples: features</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Call show_as_image on each component</span>
<span class="k">def</span> <span class="nf">show_as_image</span><span class="p">(</span><span class="n">sample</span><span class="p">):</span>
    <span class="n">bitmap</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">13</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
        <span class="n">bitmap</span><span class="p">,</span> 
        <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span>
        <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span>
    <span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">i</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">components_</span><span class="p">:</span>
    <span class="n">show_as_image</span><span class="p">(</span><span class="n">component</span><span class="p">)</span>
    <span class="n">i</span><span class="o">+=</span><span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205081112564.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>Although NMF has the ability to learn patterns, the effect of learning depends on the parameters <code>n_components</code>. For example, when studying LED images, the best value for this parameter is 7, because LEDs have only 7 beads:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)]</span>
<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">show_as_image</span><span class="p">(</span><span class="n">sample</span><span class="p">):</span>
    <span class="n">bitmap</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">13</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
        <span class="n">bitmap</span><span class="p">,</span> 
        <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span>
        <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span>
    <span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">models</span><span class="p">:</span>
    <span class="n">mc</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">components_</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">mc</span><span class="p">:</span>
        <span class="n">show_as_image</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;n=</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">5</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205081132812.png"
	
	
	
	loading="lazy"
	
	
></p>
<h2 id="building-recommender-systems-using-nmf">Building recommender systems using NMF</h2>
<p>Given an article, how can we find articles that have similar topics? Our strategy for solving this problem is to apply NMF to the word-frequency array of the articles, and to use the resulting NMF features. We have known <strong>these NMF features describe the topic mixture of an article</strong>. So similar articles will have similar NMF features.</p>
<p>Suppose that we are given a word frequency array articles corresponding to the collection of newspaper articles in question. Import <code>NMF</code>, create the model, and use the <code>fit_transform</code> method to obtain the transformed articles. Now we&rsquo;ve got NMF features for every article, given by the columns of the new array. Now we need to define how to compare articles using their NMF features. Similar documents have similar topics, but it isn&rsquo;t always the case that the NMF feature values are exactly the same. For instance, one version of a document might use very direct language, whereas other versions might interleave the same content with meaningless chatter. Meaningless chatter reduces the frequency of the topic words overall, which reduces the values of the NMF features representing the topics. However, on a scatter plot of the NMF features, all these versions lie on a single line passing through the origin.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205081223190.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>For this reason, when comparing two documents, it&rsquo;s a good idea to compare these lines. We&rsquo;ll compare them using what is known as the <strong>cosine similarity</strong>, which uses the angle between the two lines. Higher values indicate greater similarity.</p>
<p>With the help of a pandas DataFrame, we can label the similarities with the article titles. To compute the cosine similarity.</p>
<ul>
<li>import the <code>normalize</code> function from <code>sklearn.preprocessing</code>, apply it to the array of all NMF features.</li>
<li>create a DataFrame whose rows are the normalized features, using the titles as an index.</li>
<li>use the <code>loc</code> method of the DataFrame to select the normalized feature values for the current article</li>
<li>calculate the cosine similarities using the <code>dot</code> method of the DataFrame.</li>
<li>use the <code>nlargest</code> method of the resulting pandas Series to find the articles with the highest cosine similarity.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Perform the necessary imports</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">NMF</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">normalize</span>

<span class="c1"># Create an NMF instance: model</span>
<span class="n">nmf</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="c1"># Fit the model to articles</span>
<span class="n">nmf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">articles</span><span class="p">)</span>

<span class="c1"># Transform the articles: nmf_features</span>
<span class="n">nmf_features</span> <span class="o">=</span> <span class="n">nmf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">articles</span><span class="p">)</span>

<span class="c1"># Normalize the NMF features: norm_features</span>
<span class="n">norm_features</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">nmf_features</span><span class="p">)</span>

<span class="c1"># Create a DataFrame: df</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">norm_features</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">titles</span><span class="p">)</span>

<span class="c1"># Select the row corresponding to &#39;Cristiano Ronaldo&#39;: article</span>
<span class="n">article</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;Cristiano Ronaldo&#39;</span><span class="p">]</span>

<span class="c1"># Compute the dot products: similarities</span>
<span class="n">similarities</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">article</span><span class="p">)</span>

<span class="c1"># Display those with the largest cosine similarity</span>
<span class="nb">print</span><span class="p">(</span><span class="n">similarities</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Cristiano Ronaldo                    1.000
Franck Rib√©ry                        1.000
Radamel Falcao                       1.000
Zlatan Ibrahimoviƒá                   1.000
France national football team        1.000
Colombia national football team      1.000
Neymar                               0.999
2014 FIFA World Cup qualification    0.998
Arsenal F.C.                         0.998
Football                             0.975
dtype: float64
</code></pre><p>Suppose that we are given a sparse array <code>artists</code> whose rows correspond to artists and whose columns correspond to users. The entries give the number of times each artist was listened to by each user. The names of the musical artists are available as the list <code>artist_names</code>.</p>
<p>We will build a pipeline and transform the array into normalized NMF features. <code>MaxAbsScaler</code>, transforms the data so that all users have the same influence on the model, regardless of how many different artists they&rsquo;ve listened to. After <code>fit_transform</code> was called on the pipeline , <code>norm_features</code> is an array containing the normalized NMF features as rows.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Perform the necessary imports</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">NMF</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Normalizer</span><span class="p">,</span> <span class="n">MaxAbsScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Create a MaxAbsScaler: scaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MaxAbsScaler</span><span class="p">()</span>

<span class="c1"># Create an NMF model: nmf</span>
<span class="n">nmf</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># Create a Normalizer: normalizer</span>
<span class="n">normalizer</span> <span class="o">=</span> <span class="n">Normalizer</span><span class="p">()</span>

<span class="c1"># Create a pipeline: pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">scaler</span><span class="p">,</span> <span class="n">nmf</span><span class="p">,</span> <span class="n">normalizer</span><span class="p">)</span>

<span class="c1"># Apply fit_transform to artists: norm_features</span>
<span class="n">norm_features</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">artists</span><span class="p">)</span>

<span class="c1"># Create a DataFrame: df</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">norm_features</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">artist_names</span><span class="p">)</span>

<span class="c1"># Select row of &#39;Bruce Springsteen&#39;: artist</span>
<span class="n">artist</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;Bruce Springsteen&#39;</span><span class="p">]</span>

<span class="c1"># Compute cosine similarities: similarities</span>
<span class="n">similarities</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">artist</span><span class="p">)</span>

<span class="c1"># Display those with highest cosine similarity</span>
<span class="nb">print</span><span class="p">(</span><span class="n">similarities</span><span class="o">.</span><span class="n">nlargest</span><span class="p">())</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Bruce Springsteen    1.000
Neil Young           0.956
Van Morrison         0.872
Leonard Cohen        0.865
Bob Dylan            0.859
dtype: float64
</code></pre><h1 id="compendium">Compendium</h1>
<p>sklearn.cluster</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">sklearn</span><span class="o">.</span><span class="n">cluster</span>
	<span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
		<span class="o">.</span><span class="n">fit</span><span class="p">()</span>
		<span class="o">.</span><span class="n">predict</span><span class="p">()</span>
		<span class="o">.</span><span class="n">fit_predict</span><span class="p">()</span>
		<span class="o">.</span><span class="n">cluster_centers_</span>
		<span class="o">.</span><span class="n">inertia_</span>
</code></pre></td></tr></table>
</div>
</div><p>sklearn.manifold</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">sklearn</span><span class="o">.</span><span class="n">manifold</span>
	<span class="n">TSNE</span><span class="p">()</span>
		<span class="o">.</span><span class="n">learning_rate</span>
		<span class="o">.</span><span class="n">fit_transform</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p>sklearn.decomposition</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">sklearn</span><span class="o">.</span><span class="n">decomposition</span>
	<span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
		<span class="o">.</span><span class="n">fit</span><span class="p">()</span>
		<span class="o">.</span><span class="n">transform</span><span class="p">()</span>
		<span class="o">.</span><span class="n">fit_transform</span><span class="p">()</span>
		<span class="o">.</span><span class="n">components_</span>
		<span class="o">.</span><span class="n">n_components_</span>
		<span class="o">.</span><span class="n">explained_variance_</span>
	
	<span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

	<span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
		<span class="o">.</span><span class="n">fit</span><span class="p">()</span>
		<span class="o">.</span><span class="n">transform</span><span class="p">()</span>
		<span class="o">.</span><span class="n">components_</span>
</code></pre></td></tr></table>
</div>
</div><p>scipy.cluster.hierarchy</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">scipy</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">hierarchy</span>
	<span class="n">linkage</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">)</span>
	
	<span class="n">dendrogram</span><span class="p">(</span>
		<span class="n">linkage</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">),</span>
		<span class="n">labels</span><span class="o">=</span><span class="p">,</span>
		<span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span>
    	<span class="n">leaf_font_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
	<span class="p">)</span>

	<span class="n">fcluster</span><span class="p">(</span>
		<span class="n">linkage</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">),</span>
		<span class="mi">6</span><span class="p">,</span>
		<span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span>
	<span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div>
</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/scikit-learn/">scikit-learn</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css"integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js"integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js"integrity="sha384-vZTG03m&#43;2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ]
        });})
</script>
    
</article>

    

    

<aside class="related-contents--wrapper">
    <h2 class="section-title">Related contents</h2>
    <div class="related-contents">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/p/machine-learning-iii-linear-classifiers/">
        
        

        <div class="article-details">
            <h2 class="article-title">Machine Learning III (Linear Classifiers)</h2>
        </div>
    </a>
</article>
            
                
<article class="">
    <a href="/p/machine-learning-i-supervised-learning-by-scikit-learn/">
        
        

        <div class="article-details">
            <h2 class="article-title">Machine Learning I (Supervised Learning by Scikit-Learn)</h2>
        </div>
    </a>
</article>
            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2022 Haoming Wang
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.11.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>

</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >

            </main>
    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#clustering">Clustering</a>
      <ol>
        <li><a href="#kmeans">Kmeans</a></li>
        <li><a href="#evaluating">Evaluating</a>
          <ol>
            <li><a href="#inertia_"><code>inertia_</code></a></li>
            <li><a href="#crosstab">crosstab</a></li>
          </ol>
        </li>
        <li><a href="#standardization">Standardization</a></li>
      </ol>
    </li>
    <li><a href="#hierarchical-clustering-and-t-sne">Hierarchical Clustering and t-SNE</a>
      <ol>
        <li><a href="#hierarchical-clustering">Hierarchical clustering</a>
          <ol>
            <li><a href="#visualization">visualization</a></li>
            <li><a href="#cluster-labels">cluster labels</a></li>
          </ol>
        </li>
        <li><a href="#t-sne">t-SNE</a></li>
      </ol>
    </li>
    <li><a href="#dimension-reduction">Dimension Reduction</a>
      <ol>
        <li><a href="#pca">PCA</a></li>
        <li><a href="#intrinsic-dimension">Intrinsic dimension</a></li>
        <li><a href="#dimension-reduction-with-pca">Dimension reduction with PCA</a></li>
        <li><a href="#truncatedsvd"><code>TruncatedSVD</code></a></li>
      </ol>
    </li>
    <li><a href="#non-negative-matrix-factorization-nmf">Non-negative Matrix Factorization (NMF)</a>
      <ol>
        <li><a href="#basics-of-nmf">Basics of NMF</a></li>
        <li><a href="#nmf-learns-interpretable-parts">NMF learns interpretable parts</a>
          <ol>
            <li><a href="#text-analysis">Text analysis</a></li>
            <li><a href="#grayscale-image">Grayscale image</a></li>
          </ol>
        </li>
        <li><a href="#building-recommender-systems-using-nmf">Building recommender systems using NMF</a></li>
      </ol>
    </li>
    <li><a href="#compendium">Compendium</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

<script
    src="https://cdn.jsdelivr.net/gh/zhixuan2333/gh-blog@v0.1.0/js/ribbon.min.js"
    integrity="sha384-UEK8ZiP3VgFNP8KnKMKDmd4pAUAOJ59Y2Jo3ED2Z5qKQf6HLHovMxq7Beb9CLPUe"
    crossorigin="anonymous"
    size="300"
    alpha="0.6"
    zindex="-1"
    defer
></script>


    </body>
</html>
