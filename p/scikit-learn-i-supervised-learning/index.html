<!DOCTYPE html>
<html lang="en-us" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.
 Classification k-Nearest Neighbors We have a set of labeled data and we want to build a classifier that takes unlabeled data as input and outputs a label. We first need choose a type of classifier and it needs to learn from the already labeled data. For this reason, we call the already labeled data the training data.'><title>Scikit-Learn I (Supervised Learning)</title>

<link rel='canonical' href='https://wanghaoming177.netlify.app/p/scikit-learn-i-supervised-learning/'>

<link rel="stylesheet" href="/scss/style.min.6cb794b99473add9625fc6fcb4fd8782c21b06365b632b8fbb1076efe11c688e.css"><meta property='og:title' content='Scikit-Learn I (Supervised Learning)'>
<meta property='og:description' content='All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.
 Classification k-Nearest Neighbors We have a set of labeled data and we want to build a classifier that takes unlabeled data as input and outputs a label. We first need choose a type of classifier and it needs to learn from the already labeled data. For this reason, we call the already labeled data the training data.'>
<meta property='og:url' content='https://wanghaoming177.netlify.app/p/scikit-learn-i-supervised-learning/'>
<meta property='og:site_name' content='Haoming Wang'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='scikit-learn' /><meta property='article:published_time' content='2021-08-15T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2021-08-15T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="Scikit-Learn I (Supervised Learning)">
<meta name="twitter:description" content="All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.
 Classification k-Nearest Neighbors We have a set of labeled data and we want to build a classifier that takes unlabeled data as input and outputs a label. We first need choose a type of classifier and it needs to learn from the already labeled data. For this reason, we call the already labeled data the training data.">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "dark");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/IMG_0770_hu86b91b9497cdda82d5c488540c7342b2_101927_300x0_resize_q75_box.jpg" width="300"
                            height="273" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">üåè</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">Haoming Wang</a></h1>
            <h2 class="site-description">We choose to go to the moon in this decade and do the other things, not because they are easy, but because they are hard.</h2>
        </div>
    </header><ol class="social-menu">
            
                <li>
                    <a 
                        href='https://github.com/floating15'
                        target="_blank"
                        title="GitHub"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://twitter.com'
                        target="_blank"
                        title="Linkedin"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="28" height="28" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <rect x="4" y="4" width="16" height="16" rx="2" />
  <line x1="8" y1="11" x2="8" y2="16" />
  <line x1="8" y1="8" x2="8" y2="8.01" />
  <line x1="12" y1="16" x2="12" y2="11" />
  <path d="M16 16v-3a2 2 0 0 0 -4 0" />
</svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://follow.it/haoming-wang?action=followPub'
                        target="_blank"
                        title="r-mail"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-mailbox" width="28" height="28" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M10 21v-6.5a3.5 3.5 0 0 0 -7 0v6.5h18v-6a4 4 0 0 0 -4 -4h-10.5" />
  <path d="M12 11v-8h4l2 2l-2 2h-4" />
  <path d="M6 15h1" />
</svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://twitter.com'
                        target="_blank"
                        title="Twitter"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M22 4.01c-1 .49 -1.98 .689 -3 .99c-1.121 -1.265 -2.783 -1.335 -4.38 -.737s-2.643 2.06 -2.62 3.737v1c-3.245 .083 -6.135 -1.395 -8 -4c0 0 -4.182 7.433 4 11c-1.872 1.247 -3.739 2.088 -6 2c3.308 1.803 6.913 2.423 10.034 1.517c3.58 -1.04 6.522 -3.723 7.651 -7.742a13.84 13.84 0 0 0 .497 -3.753c-.002 -.249 1.51 -2.772 1.818 -4.013z" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        

        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="#FF1C02" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        

        <li >
            <a href='/about/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="#35FF02" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>About</span>
            </a>
        </li>
        
        

        <li >
            <a href='/notes/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-math-function" width="28" height="28" viewBox="0 0 24 24" stroke-width="1.5" stroke="#02EAFF" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M14 10h1c1 0 1 1 2.016 3.527c.984 2.473 .984 3.473 1.984 3.473h1" />
  <path d="M13 17c1.5 0 3 -2 4 -3.5s2.5 -3.5 4 -3.5" />
  <path d="M3 19c0 1.5 .5 2 2 2s2 -4 3 -9s1.5 -9 3 -9s2 .5 2 2" />
  <line x1="5" y1="12" x2="11" y2="12" />
</svg>
                
                <span>Notes</span>
            </a>
        </li>
        
        

        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="#0223FF" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        

        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="#F402FF" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        

        <li >
            <a href='/links/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>Links</span>
            </a>
        </li>
        

        <div class="menu-bottom-section">
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>
<main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/ml/" style="background-color: false; color: false;">
                Machine Learning
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/scikit-learn-i-supervised-learning/">Scikit-Learn I (Supervised Learning)</a>
        </h2>
    
        
    </div>

    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Aug 15, 2021</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    45 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>
</header>

    <section class="article-content">
    
    
    <blockquote>
<p>All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.</p>
</blockquote>
<h1 id="classification">Classification</h1>
<h2 id="k-nearest-neighbors">k-Nearest Neighbors</h2>
<p>We have a set of labeled data and we want to build a classifier that takes unlabeled data as input and outputs a label. We first need choose a type of classifier and it needs to learn from the already labeled data. For this reason, we call the already labeled data the training data.</p>
<p>There is a simple algorithm called K-nearest neighbors (KNN). The basic idea of KNN, is to predict the label of any data point by looking at the K, for example, 3, closest labeled data points and getting them to vote on what label the unlabeled point should have.</p>
<p>In this image, there&rsquo;s an example of KNN in two dimensions: if k equals 3, you would classify it as red
<img src="https://raw.githubusercontent.com/floating15/img/main/202204301028926.png"
	
	
	
	loading="lazy"
	
	
>
if k equals 5, as green.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202204301030634.png"
	
	
	
	loading="lazy"
	
	
></p>
<p><strong>All machine learning models in scikit-learn are implemented as python classes</strong>. These classes serve two purposes: they implement the algorithms for <strong>learning</strong> a model, and <strong>predicting</strong>, while also storing all the information that is learned from the data.</p>
<ul>
<li>Training a model on the data is also called fitting the model to the data. In scikit-learn, we use the <code>.fit()</code> method to do this.</li>
<li>The <code>.predict()</code> method is what we use to predict the label of an, unlabeled data point.</li>
</ul>
<p>To fit our first classifier using scikit-learn,</p>
<ul>
<li>We first need to import <code>KNeighborsClassifie</code>r from <code>sklearn.neighbors</code>.</li>
<li>We then <strong>instantiate</strong> our <code>KNeighborsClassifier</code>, set the number of neighbors k to argument <code>n_neighbors</code>, and assign it to the the instance (the classifier).</li>
<li>Then we can fit this classifier to our training set, the labeled data. To do so, we apply the method <code>.fit()</code> to the classifier and pass it two arguments: the features as a NumPy array and the labels (or target) as a NumPy array.
When we fit the classifier, it returns the classifier itself and modifies it to fit it to the data. And then we can use it to predict on some unlabeled data.</li>
<li>We use the <code>.predict()</code> method on the classifier and pass it the data to be predicted. And it return a array with a prediction for each observation or row in these data.</li>
</ul>
<blockquote>
<p>The scikit-learn API requires firstly that you have the data as a <strong>NumPy array</strong> or <strong>pandas DataFrame</strong>, and the data with <strong>features in columns</strong> and <strong>observations in rows</strong>; It also requires that the features take on <strong>continuous values</strong>, such as the price of a house, as opposed to categories, such as &lsquo;male&rsquo; or &lsquo;female&rsquo;. It also requires that there are <strong>no missing values</strong> in the data.</p>
</blockquote>
<p>Now take an overview of our data:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Data To Be Trained: &#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;------------------------------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;party&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">())</span> <span class="c1"># target</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;------------------------------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Data To Be Predicted: &#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_new</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Data To Be Trained: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 435 entries, 0 to 434 ### &lt;-- 435 observations (samples)
Data columns (total 17 columns): ### &lt;-- 16 feature + 1 target
party                435 non-null object
infants              435 non-null int64
water                435 non-null int64
budget               435 non-null int64
physician            435 non-null int64
salvador             435 non-null int64
religious            435 non-null int64
satellite            435 non-null int64
aid                  435 non-null int64
missile              435 non-null int64
immigration          435 non-null int64
synfuels             435 non-null int64
education            435 non-null int64
superfund            435 non-null int64
crime                435 non-null int64
duty_free_exports    435 non-null int64
eaa_rsa              435 non-null int64
dtypes: int64(16), object(1)
memory usage: 57.9+ KB
None
------------------------------------------
0    republican
1    republican
2      democrat
3      democrat
4      democrat
Name: party, dtype: object
------------------------------------------
Data To Be Predicted: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1 entries, 0 to 0
Data columns (total 16 columns):
0     1 non-null float64
1     1 non-null float64
2     1 non-null float64
3     1 non-null float64
4     1 non-null float64
5     1 non-null float64
6     1 non-null float64
7     1 non-null float64
8     1 non-null float64
9     1 non-null float64
10    1 non-null float64
11    1 non-null float64
12    1 non-null float64
13    1 non-null float64
14    1 non-null float64
15    1 non-null float64
dtypes: float64(16)
memory usage: 208.0 bytes
None
</code></pre><p>The first column of <code>df</code>, <code>df['party']</code> is the target, the following 16 column <code>df.iloc[:, 1:17]</code> is the feature.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import KNeighborsClassifier from sklearn.neighbors</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span> 

<span class="c1"># Create arrays for the features and the response variable</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;party&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">17</span><span class="p">]</span>

<span class="c1"># Create a k-NN classifier with 6 neighbors: knn</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="c1"># Fit the classifier to the data</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Predict and print the label for the new data point X_new</span>
<span class="n">new_prediction</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Prediction: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">new_prediction</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Prediction: ['democrat']
</code></pre><h2 id="measuring-model-performance">Measuring model performance</h2>
<p>Now that we know how to fit a classifier and use it to predict the labels of previously unseen data, we need to figure out how to measure its performance. In classification problems, accuracy is a commonly-used metric. The accuracy of a classifier is defined as the number of <strong>correct predictions divided by the total number of data points</strong>.</p>
<p>To created data to compute accuracy, a common practice is to split the data into two sets, a <strong>training set</strong> and a <strong>test set</strong>. You train or fit the classifier on the training set. Then you make predictions on the labeled test set and compare these predictions with the known labels. You then compute the accuracy of your predictions.</p>
<p>To do this, we first import <code>train_test_split</code> from <code>sklearn.model_selection</code>. We then use the <code>train_test_split</code> function to randomly split our data.</p>
<ul>
<li>The first argument will be the feature data <code>X</code>, the second the targets or labels <code>y</code>.</li>
<li>The <code>test_size</code> argument specifies what proportion of the original data is used for the test set.</li>
<li>The <code>random_state</code> kwarg sets a seed for the random number generator that splits the data into train and test. Setting the seed with the same argument later will allow you to reproduce the exact split and your downstream results.</li>
<li>It is also best practice to perform our split so that the split reflects the labels on the data.  That is, we want the labels to be distributed in train and test sets as they are in the original dataset. To achieve this, we use the keyword argument <code>stratify</code> equals <code>y</code>, where <code>y</code> the list or array containing the labels.</li>
</ul>
<p><code>train_test_split</code> returns four arrays: X train, X test, y train, and y test in that order, respectively. By default, train test split splits the data into 75% training data and 25% test data, which is a good rule of thumb.</p>
<p>We then instantiate our K-nearest neighbors classifier, fit it to the training data using the <code>.fit()</code> method. To check out the <strong>accuracy</strong> of our model, we use the <code>.score()</code> method of the classifier and pass it <code>X_test</code> and <code>y_test</code> (note that we can even do not call <code>.predict()</code>).</p>
<p>Generally, complex models (in these case, small k) run the risk of being sensitive to noise in the specific data that you have, rather than reflecting general trends in the data. This is know as <strong>overfitting</strong>. But if you increase k even more and make the model even simpler, then the model will perform less well on both test and training sets, as indicated in this schematic figure, known as a model <strong>complexity curve</strong>. This is called <strong>underfitting</strong>.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202204301129588.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>In the following context, we&rsquo;ll work with the MNIST digits recognition dataset, which is one of scikit-learn&rsquo;s included datasets. The dataset contains 1797 samples, each sample in this scikit-learn dataset is an 8x8 image representing a handwritten digit. Each pixel is represented by an integer in the range 0 to 16, indicating varying levels of black.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Load the digits dataset: digits</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])
(1797, 8, 8)
(1797, 64)
[[ 0.  0.  5. 13.  9.  1.  0.  0.]
 [ 0.  0. 13. 15. 10. 15.  5.  0.]
 [ 0.  3. 15.  2.  0. 11.  8.  0.]
 [ 0.  4. 12.  0.  0.  8.  8.  0.]
 [ 0.  5.  8.  0.  0.  9.  8.  0.]
 [ 0.  4. 11.  0.  1. 12.  7.  0.]
 [ 0.  2. 14.  5. 10. 12.  0.  0.]
 [ 0.  0.  6. 13. 10.  0.  0.  0.]]
</code></pre><p>Recall that scikit-learn&rsquo;s built-in datasets are of type <strong>Bunch</strong>, which are <strong>dictionary-like objects</strong>. Helpfully for the MNIST dataset, scikit-learn provides an &lsquo;images&rsquo; key in addition to the &lsquo;data&rsquo; and &lsquo;target&rsquo; keys. Because it is a 2D array of the images corresponding to each sample, this &lsquo;images&rsquo; key is useful for visualizing the images. On the other hand, the &lsquo;data&rsquo; key contains the feature array - that is, the &lsquo;images&rsquo; as a flattened array of 64 pixels.</p>
<p>Notice that you can access the keys of these Bunch objects in two different ways: By using the <code>.</code> notation, as in <code>digits.images</code>, or the <code>[]</code> notation, as in <code>digits['images']</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># # Display some digits</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
            <span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">j</span><span class="p">],</span>
            <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">,</span>
            <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span>
        <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://raw.githubusercontent.com/floating15/img/main/202204301141928.png"
	
	
	
	loading="lazy"
	
	
></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Create feature and target arrays</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Split into training and test set</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
	<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
	<span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> 
	<span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> 
	<span class="n">stratify</span><span class="o">=</span><span class="n">y</span>
<span class="p">)</span>

<span class="c1"># Create a k-NN classifier with 7 neighbors: knn</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="c1"># Fit the classifier to the training data</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Print the accuracy</span>
<span class="nb">print</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>0.9833333333333333
</code></pre><p>We will compute and plot the training and testing accuracy scores for a variety of different neighbor values. By observing how the accuracy scores differ for the training and testing sets with different values of k, we will develop the intuition for overfitting and underfitting.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Setup arrays to store different k</span>
<span class="n">neighbors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>

<span class="c1"># Setup arrays to store train and test accuracies</span>
<span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">neighbors</span><span class="p">))</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">neighbors</span><span class="p">))</span>

<span class="c1"># Loop over different values of k</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">neighbors</span><span class="p">):</span>
    <span class="c1"># Setup a k-NN Classifier with k neighbors: knn</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

    <span class="c1"># Fit the classifier to the training data</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1">#Compute accuracy on the training set</span>
    <span class="n">train_accuracy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1">#Compute accuracy on the testing set</span>
    <span class="n">test_accuracy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c1"># Generate plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;k-NN: Varying Number of Neighbors&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">test_accuracy</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Testing Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Training Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Neighbors&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://raw.githubusercontent.com/floating15/img/main/202204301205222.png"
	
	
	
	loading="lazy"
	
	
></p>
<h1 id="regression">Regression</h1>
<p>Checking the type of <code>X</code> and <code>y</code>, we see that both are NumPy arrays. To turn them into NumPy arrays of the desired shape (<strong>2D array</strong>), we apply the <code>reshape</code> method to <strong>keep the first dimension, but add another dimension of size one</strong> to <code>X</code> and <code>y</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import numpy and pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Read the CSV file into a DataFrame: df</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;gapminder.csv&#39;</span><span class="p">)</span>

<span class="c1"># Create arrays for features and target variable</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">life</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">fertility</span>

<span class="c1"># Print the dimensions of y and X before reshaping</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Dimensions of y before reshaping: &#34;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Dimensions of X before reshaping: &#34;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Reshape X and y</span>
<span class="n">y_reshaped</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_reshaped</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Print the dimensions of y_reshaped and X_reshaped</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Dimensions of y after reshaping: &#34;</span><span class="p">,</span> <span class="n">y_reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Dimensions of X after reshaping: &#34;</span><span class="p">,</span> <span class="n">X_reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Dimensions of y before reshaping:  (139,)
Dimensions of X before reshaping:  (139,)
Dimensions of y after reshaping:  (139, 1)
Dimensions of X after reshaping:  (139, 1)
</code></pre><h2 id="ols">OLS</h2>
<p>The API of regression is almost same as classification.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Create the regressor: reg</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Fit the model to the data</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_fertility</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Print R^2 </span>
<span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_fertility</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

<span class="c1"># Create the regression line</span>
<span class="n">prediction_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">X_fertility</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">X_fertility</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">prediction_space</span><span class="p">)</span>

<span class="c1"># Plot scatter plot and regression line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_fertility</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1">#  scatter plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">prediction_space</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1">#  regression line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>0.6192442167740035
</code></pre><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205010953334.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>Multi-variable regression is totally like single-variable regression:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span> 
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Create training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Shape of X: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">Shape of y: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>

<span class="c1"># Create the regressor: reg_all</span>
<span class="n">reg_all</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Fit the regressor to the training data</span>
<span class="n">reg_all</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict on the test data: y_pred</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">reg_all</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Compute and print R^2 and RMSE</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;R^2: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reg_all</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Root Mean Squared Error: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rmse</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Shape of X: (139, 8),
Shape of y: (139,)
R^2: 0.838046873142936
Root Mean Squared Error: 3.2476010800377213
</code></pre><h2 id="cross-validation">Cross-validation</h2>
<p>We have talked about train test split and computing model performance metrics on test set. But there is a potential pitfall of this process: if you&rsquo;re computing R^2 on the test set, the R^2 returned is dependent on the way that you split up the data.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">X_train1</span><span class="p">,</span> <span class="n">X_test1</span><span class="p">,</span> <span class="n">y_train1</span><span class="p">,</span> <span class="n">y_test1</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">41</span><span class="p">)</span>
<span class="n">X_train2</span><span class="p">,</span> <span class="n">X_test2</span><span class="p">,</span> <span class="n">y_train2</span><span class="p">,</span> <span class="n">y_test2</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train3</span><span class="p">,</span> <span class="n">X_test3</span><span class="p">,</span> <span class="n">y_train3</span><span class="p">,</span> <span class="n">y_test3</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">43</span><span class="p">)</span>

<span class="n">reg1</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">reg2</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">reg3</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="n">reg1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train1</span><span class="p">,</span> <span class="n">y_train1</span><span class="p">)</span>
<span class="n">reg2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train2</span><span class="p">,</span> <span class="n">y_train2</span><span class="p">)</span>
<span class="n">reg3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train3</span><span class="p">,</span> <span class="n">y_train3</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;R^2 for reg1: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reg1</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test1</span><span class="p">,</span> <span class="n">y_test1</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;R^2 for reg2: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reg2</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test2</span><span class="p">,</span> <span class="n">y_test2</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;R^2 for reg3: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reg3</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test3</span><span class="p">,</span> <span class="n">y_test3</span><span class="p">)))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>R^2 for reg1: 0.8785550178323325
R^2 for reg2: 0.838046873142936
R^2 for reg3: 0.8852965321157533
</code></pre><p>You can see that 0.83 and 0.88 is a big distinction. The data points in the test set may have some peculiarities that mean the R^2 computed on it is not representative of the model&rsquo;s ability to generalize to unseen data.</p>
<p>To combat this dependence on what is essentially an arbitrary split, we use a technique called <strong>cross-validation</strong>. We begin by splitting the dataset into five folds. Then we hold out the first fold as a test set, fit our model on the remaining four folds, predict on the test set, and compute the metric of interest. Next, we hold out the second fold as our test set, fit on the remaining data, predict on the test set, and compute the metric of interest. Then similarly with the third, fourth, and fifth fold.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205011026338.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>As a result we get five values of R^2 from which we can compute statistics of interest, such as the mean and median and 95% confidence intervals. As we split the dataset into five folds, we call this process 5-fold cross validation. More generally, if you use k folds, it is called k-fold cross validation or k-fold CV.</p>
<p>To perform k-fold CV in scikit-learn, we first import <code>cross_val_score</code> from <code>sklearn.model_selection</code>. Note that <code>cross_val_score</code> is a function:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>&lt;class 'function'&gt;
</code></pre><p>Then, we instantiate our model. We then call <code>cross_val_score</code> with the regressor <code>reg</code>, the feature data <code>X</code>, and the target data <code>y</code> as the first three positional arguments. (Note that we don&rsquo;t need to split <code>X</code>, <code>y</code> into train and test here). We also specify the number of folds with the keyword argument, <code>cv</code>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">reg</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Note that <code>cross_val_score</code> is just a function that performs splitting and fits model on splitted data repeatedly, thus you can pass others model in it, like the instance of <code>Rigid</code> or <code>Lasso</code>.</p>
</blockquote>
<p>This returns an array of cross-validation scores, which we assign to <code>cv_results</code>. The length of the array is the number of folds utilized. Note that the score reported is R^2, as this is the default score for linear regression.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import the necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="c1"># Create a linear regression object: reg</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Compute 5-fold cross-validation scores: cv_scores5</span>
<span class="n">cv_scores5</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">reg</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cv_scores5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Average 5-Fold CV Score: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cv_scores5</span><span class="p">)))</span>

<span class="c1"># Compute 7-fold cross-validation scores: cv_scores7</span>
<span class="n">cv_scores7</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">reg</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cv_scores7</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Average 7-Fold CV Score: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cv_scores7</span><span class="p">)))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>[0.81720569 0.82917058 0.90214134 0.80633989 0.94495637]
Average 5-Fold CV Score: 0.8599627722793232
[0.66129291 0.90449445 0.8350383  0.90814522 0.7705185 0.88507953 0.96175008]
Average 7-Fold CV Score: 0.8466169980423074
</code></pre><h2 id="regularized-regression">Regularized regression</h2>
<p>Recall that what fitting a linear regression does is minimize a loss function to choose a coefficient $a_i$ for each feature variable. If we allow these coefficients or parameters to be super large, we can get <strong>over-fitting</strong>. If your data sit in a high-dimensional space with large coefficients, it gets easy to predict nearly anything. For this reason, it is common practice to alter the loss function so that it penalizes for large coefficients. This is called <strong>regularization</strong>. We will introduce two class <code>Ridge</code> and <code>Lasso</code>, which are similar with <code>LinearRegression</code>.</p>
<h3 id="ridge-regression">Ridge regression</h3>
<p>The first type of regularized regression that we&rsquo;ll look at is called ridge regression in which our loss function is the standard OLS loss function plus the squared value of each coefficient multiplied by some constant alpha.
$$
Loss_{rigid} = Loss_{OLS} + \alpha\cdot \sum_{i=1}^{n}a_{i}^{2}
$$</p>
<p>Thus, when minimizing the loss function to fit to our data, models are penalized for coefficients with a large magnitude: large positive and large negative coefficients. Note that $\alpha$ is a parameter we need to choose in order to fit and predict. Essentially, we can select the $\alpha$ for which our model performs best. Picking $\alpha$ for ridge regression is similar to picking k in KNN. This is called hyperparameter tuning. This $\alpha$ can be thought of as a parameter that controls model <strong>complexity</strong> (recall that more complex the model is, the more likely it is to over-fitting).</p>
<p>Notice that when $\alpha$ is equal to zero, we get back OLS. Large coefficients in this case are not penalized and the <strong>over-fitting</strong> problem is not accounted for. A very high alpha means that large coefficients are significantly penalized, which can lead to a model that is too simple and ends up <strong>under-fitting</strong> the data.</p>
<p>The method of performing ridge regression with scikit-learn mirrors the other models that we have seen.</p>
<ul>
<li>Import <code>Ridge</code> from <code>sklearn.linear_model</code>,</li>
<li>Instantiate class <code>Ridge</code>, with argument <code>alpha</code> and <code>normalize</code>. Setting <code>normalize=True</code> ensures that all our variables are on the same scale and we will talk about this in more depth later.</li>
<li>Split our data into test and train, <code>.fit()</code> on the training, and <code>.predict()</code> on the test.</li>
</ul>
<p>Recall that <code>Rigid</code> is a class, you can specify argument by</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">rigid</span> <span class="o">=</span> <span class="n">Rigid</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">argument</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>or</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">rigid</span> <span class="o">=</span> <span class="n">Rigid</span><span class="p">()</span>
<span class="n">rigid</span><span class="o">.</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span>
<span class="n">rigid</span><span class="o">.</span><span class="n">argument</span><span class="o">=</span><span class="kc">True</span>
</code></pre></td></tr></table>
</div>
</div><p>Now, we will practice fitting ridge regression models over a range of different alphas, and plot cross-validated  scores for each.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="c1"># Setup the array of alphas and lists to store scores</span>
<span class="n">alpha_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">ridge_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ridge_scores_std</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Create a ridge regressor: ridge</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute scores over range of alphas</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alpha_space</span><span class="p">:</span>

    <span class="c1"># Specify the alpha value to use: ridge.alpha</span>
    <span class="n">ridge</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
    
    <span class="c1"># Perform 10-fold CV: ridge_cv_scores</span>
    <span class="n">ridge_cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">ridge</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    
    <span class="c1"># Append the mean of ridge_cv_scores to ridge_scores</span>
    <span class="n">ridge_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ridge_cv_scores</span><span class="p">))</span>
    
    <span class="c1"># Append the std of ridge_cv_scores to ridge_scores_std</span>
    <span class="n">ridge_scores_std</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ridge_cv_scores</span><span class="p">))</span>

<span class="c1"># Plots the score as well as standard error for each alpha:</span>
<span class="k">def</span> <span class="nf">display_plot</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">,</span> <span class="n">cv_scores_std</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha_space</span><span class="p">,</span> <span class="n">cv_scores</span><span class="p">)</span>

    <span class="n">std_error</span> <span class="o">=</span> <span class="n">cv_scores_std</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">alpha_space</span><span class="p">,</span> <span class="n">cv_scores</span> <span class="o">+</span> <span class="n">std_error</span><span class="p">,</span> <span class="n">cv_scores</span> <span class="o">-</span> <span class="n">std_error</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;CV Score +/- Std Error&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Alpha&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">),</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;.5&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="n">alpha_space</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha_space</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">display_plot</span><span class="p">(</span><span class="n">ridge_scores</span><span class="p">,</span> <span class="n">ridge_scores_std</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205011129278.png"
	
	
	
	loading="lazy"
	
	
></p>
<h3 id="lasso-regression">Lasso regression</h3>
<p>There is another type of regularized regression called lasso regression, in which our loss function is the standard OLS loss function plus the absolute value of each coefficient multiplied by some constant alpha.</p>
<p>$$
Loss_{lasso} = Loss_{OLS} + \alpha\cdot \sum_{i=1}^{n}|a_{i}|
$$</p>
<p>The method of performing lasso regression in scikit-learn mirrors ridge regression: we import <code>Lasso</code> from <code>sklearn.linear_model</code>, and the others steps are totally same.</p>
<p>One of the really cool aspects of lasso regression is that it can be used to <strong>select important features of a dataset</strong>. This is <strong>because it tends to shrink the coefficients of less important features to be exactly zero</strong>. The features whose coefficients are not shrunk to zero are &lsquo;selected&rsquo; by the LASSO algorithm. Thus we can extract the coefficients of variables (features) to investigate the importance of each one.</p>
<p>To show this, we import <code>Lasso</code> as before. We then instantiate our regressor, <code>.fit()</code> it to the data as always. Then we can <strong>extract the <code>coef_</code> attribute</strong> and plot the features and their <code>coef_</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import Lasso</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="c1"># Instantiate a lasso regressor: lasso</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Fit the regressor to the data</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Compute and print the coefficients</span>
<span class="n">lasso_coef</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lasso_coef</span><span class="p">)</span>

<span class="c1"># Plot the coefficients</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_columns</span><span class="p">)),</span> <span class="n">lasso_coef</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_columns</span><span class="p">)),</span> <span class="n">df_columns</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">margins</span><span class="p">(</span><span class="mf">0.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>[-0.         -0.         -0.          0.          0.          0.         -0.         -0.07087587]
</code></pre><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205011114628.png"
	
	
	
	loading="lazy"
	
	
></p>
<h1 id="fine-tuning">Fine-tuning</h1>
<h2 id="metrics-for-classification">Metrics for classification</h2>
<p>In classification, we&rsquo;ve used accuracy, the fraction of correctly classified samples, to measure model performance. However, accuracy is not always a useful metric. Consider a spam classification problem in which 99% of emails are real and only 1% are spam. I could build a model that classifies all emails as real; this model would be correct 99% of the time and thus have an accuracy of 99%, which is great. However, this naive classifier does a horrible job of predicting spam: it never predicts spam at all, so it completely fails at its original purpose.</p>
<p>The situation when one class is more frequent is called class imbalance because the class of real emails contains way more instances than the class of spam. This is a very common situation in practice and requires a more nuanced metric to assess the performance of our model.</p>
<p>Given a binary classifier, such as our spam email example, we can draw up a 2-by-2 matrix that summarizes predictive performance called a <strong>confusion matrix</strong>: across the top are the predicted labels, down the side the actual labels:</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205011215791.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>Given any model, we can fill in the confusion matrix according to its predictions. In the top left square, we have the number of spam emails correctly labeled; in the bottom right square, we have the number of real emails correctly labeled; in the top right, the number of spam emails incorrectly labeled; and in the bottom left, the number of real emails incorrectly labeled. Note that correctly labeled spam emails are referred to as true positives and correctly labeled real emails as true negatives. While incorrectly labeled spam will be referred to as false negatives and incorrectly labeled real emails as false positives.</p>
<p>Usually, the &ldquo;class of interest&rdquo; is called the positive class. As we are trying to detect spam, this makes spam the positive class. Notice that we can retrieve <strong>accuracy</strong> from the confusion matrix: it&rsquo;s the sum of the diagonal divided by the total sum of the matrix.</p>
<p>$$
accuracy = \frac{tp+tn}{tp+fp+tn+fn} =\frac{t}{t+f} .
$$</p>
<p>There are several other important metrics we can easily calculate from the confusion matrix. <strong>Precision</strong>, which is the number of true positives divided by the total number of true positives and false positives. It is also called the positive predictive value or PPV.</p>
<p>$$
precision = \frac{tp}{tp+fp} = \frac{tp}{p}
$$</p>
<p>In our case, this is the number of <strong>correctly labeled spam emails divided by the total number of emails classified as spam</strong>.</p>
<p><strong>Recall</strong>, which is the number of true positives divided by the total number of true positives and false negatives. This is also called sensitivity, hit rate, or true positive rate.</p>
<p>$$
recall = \frac{tp}{tp+fn}
$$</p>
<p>The <strong>F1-score</strong> is defined as two times the product of the precision and recall divided by the sum of the precision and recall, in other words, it&rsquo;s the harmonic mean of precision and recall.</p>
<p>$$
f1=2\times \frac{prs \cdot rcl}{prs+rcl}
$$</p>
<p>To put it in plain language,</p>
<ul>
<li>high precision means that our classifier had a low false positive rate, that is, not many real emails were predicted as being spam.</li>
<li>high recall means that our classifier predicted most positive or spam emails correctly.</li>
</ul>
<p>To compute the confusion matrix, along with the metrics for the classifier,</p>
<ul>
<li>we import <code>classification_report</code> and <code>confusion_matrix</code> from <code>sklearn.metrics</code>.</li>
<li>As always, we instantiate our classifier, split the data into train and test, <code>.fit()</code> the training data, and <code>.predict()</code> the labels of the test set.</li>
<li>To compute the confusion matrix, we pass the test set labels (<code>y_test</code>) and the predicted labels (<code>y_pred</code>) to the function <code>confusion_matrix</code>.</li>
<li>To compute the resulting metrics, we pass the same arguments to <code>classification_report</code>, which outputs a string containing all the relevant metrics.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="c1"># Create training and test set</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Instantiate a k-NN classifier: knn</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="c1"># Fit the classifier to the training data</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict the labels of the test data: y_pred</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Generate the confusion matrix and classification report</span>
<span class="nb">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;------------------------------&#34;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>[[176  30]
[ 52  50]]
------------------------------
			  precision    recall  f1-score   support

		  0       0.77      0.85      0.81       206
		  1       0.62      0.49      0.55       102

avg / total       0.72      0.73      0.72       308
</code></pre><p>Note that the definition of positive and negative are entirely subjective. Thus conventionally, for all metrics in scikit-learn, the first argument is always the true label (<code>y_test</code>) and the prediction (<code>y_label</code>) is always the second argument. Then the first line (0) of the output of <code>classification_report</code> is what we need. The second line (1) is the inverse of positive and negative of (0), for example, precision of (0) is tp/(tp+fp), but precision of (1) if tn/(tn+fn); and  recall of (0) is tp/(tp+fn), but recall of (1) if tn/(tn+fp).</p>
<h2 id="logistic-regression-and-the-roc-curve">Logistic regression and the ROC curve</h2>
<h3 id="building-a-logistic-regression-model">Building a logistic regression model</h3>
<p>Logistic regression, despite its name, is used in classification problems, not regression problems. When we have two possible labels for the target variable.</p>
<p>Using logistic regression in scikit-learn follows exactly the same formula that we have known so well:</p>
<ul>
<li>perform the necessary imports,</li>
<li>instantiate the classifier,</li>
<li>split data into training and test sets,</li>
<li><code>.fit()</code> the model on the training data, and <code>.predict()</code> on the test set.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import the necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="c1"># Create training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create the classifier: logreg</span>
<span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="c1"># Fit the classifier to the training data</span>
<span class="n">logreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict the labels of the test set: y_pred</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Compute and print the confusion matrix and classification report</span>
<span class="nb">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>[[176  30]
 [ 35  67]]
			  precision    recall  f1-score   support

		  0       0.83      0.85      0.84       206
		  1       0.69      0.66      0.67       102

avg / total       0.79      0.79      0.79       308
</code></pre><p>Notice that in defining logistic regression, we have specified a threshold of 0.5 for the probability, a threshold that defines our model. This is not particular for log reg but also could be used for KNN. When we call <code>.predict()</code>, given one feature, <strong>log reg will calculate a probability, p which is independent with the threshold</strong>, with respect to the target variable. If p is greater than 0.5, label will be predicted as &lsquo;1&rsquo;; and if less than 0.5, label will be &lsquo;0&rsquo;.</p>
<p>To extract the probability which is calculated by log reg, we call <code>predict_proba</code> method and pass the test data (<code>X_test</code>), <code>predict_proba</code> returns an array with two columns which contain the probabilities for the 0 and 1 respectively.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="nb">print</span><span class="p">(</span><span class="n">logreg</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">,:])</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>[[0.60409835 0.39590165]
 [0.76042394 0.23957606]
 [0.79670177 0.20329823]
 [0.77236009 0.22763991]
 [0.57194882 0.42805118]]
</code></pre><h3 id="plot-roc">plot ROC</h3>
<p>When the threshold equals zero, the model predicts &lsquo;1&rsquo; for all the data, which means the true positive rate (tp/(tp+fn)) is equal to the false positive rate (fp/(fp+tn)) is equal to one. When the threshold equals &lsquo;1&rsquo;, the model predicts &lsquo;0&rsquo; for all data, which means that both true and false positive rates are 0.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205011337133.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>If we vary the threshold between these two extremes, we get a series of different false positive and true positive rates. The set of points we get when trying all possible thresholds is called the receiver operating characteristic curve or <strong>ROC curve</strong>.</p>
<p>To plot the ROC curve, after instantiating <code>LogisticRegression</code>, fitting the model:</p>
<ul>
<li>import <code>roc_curve</code> from <code>sklearn.metrics</code>;</li>
<li>call <code>predict_proba(X_text)</code> on the model;</li>
<li>then call the function <code>roc_curve</code>; the first argument is given by the <strong>actual labels</strong> (<code>y_test</code>), the second by the <strong>predicted probabilities for 1</strong>.</li>
</ul>
<p>Here we used the predicted probabilities for &ldquo;1&rdquo; (positive), which is the second column of the outcome of <code>predict_prob</code>. Note that <strong>these probabilities don&rsquo;t rely on any threshold</strong>. Given a threshold t, the observations in <code>X_test</code> whose probabilities for &ldquo;1&rdquo; are greater than t will be labeled by 1 (w.r.t. threshold t), and the others will be labeled by 0 (w.r.t. threshold t). And then, combining with <code>y_test</code>, we can calculate the tpr and fpr (w.r.t. threshold t), which performs a point on ROC.</p>
<ul>
<li>We unpack the result into three variables: false positive rate, FPR; true positive rate, TPR; and the thresholds. We can then plot the FPR and TPR using <code>plt.plot</code> function.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span>

<span class="c1"># Compute predicted probabilities: y_pred_prob</span>
<span class="n">y_pred_prob</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Generate ROC curve values: fpr, tpr, thresholds</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_prob</span><span class="p">)</span>

<span class="c1"># Plot ROC curve</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ROC Curve&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://raw.githubusercontent.com/floating15/img/main/202205011409119.png"
	
	
	
	loading="lazy"
	
	
></p>
<h3 id="calculate-auc">Calculate AUC</h3>
<p>Given the ROC curve, we can extract a metric of interest. Because the larger the area under the ROC curve, the better our model is. The way to think about this is the following: if we had a model which produced an ROC curve that had a single point at (1,0), the upper left corner, representing a true positive rate of one and a false positive rate of zero, this would be a great model. For this reason, the area under the ROC, commonly denoted as AUC, is another popular metric for classification models.</p>
<p>To compute the AUC,</p>
<ul>
<li>import <code>roc_auc_score</code> from <code>sklearn.metrics</code>,</li>
<li>instantiate the classifier, split our data into train and test sets, and fit the model to the training set,</li>
<li>compute the predicted probabilities by call ing <code>predict_proba</code> on <code>logreg</code>,</li>
<li>pass the true labels (<code>y_test</code>) and the predicted probabilities to <code>roc_auc_score</code>.</li>
</ul>
<p>We can also compute the AUC using cross-validation. To do so,</p>
<ul>
<li>import and use the function <code>cross_val_score</code> as before,</li>
<li>passing it the estimator <code>logreg</code>, the features <code>X</code>, and the target <code>y</code>, and <code>cv</code>. We then additionally pass it the keyword argument <code>scoring=&quot;roc auc&quot;</code>.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="c1"># Compute predicted probabilities: y_pred_prob</span>
<span class="n">y_pred_prob</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Compute and print AUC score</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;AUC: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_prob</span><span class="p">)))</span>

<span class="c1"># Compute cross-validated AUC scores: cv_auc</span>
<span class="n">cv_auc</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">logreg</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;roc_auc&#34;</span><span class="p">)</span>

<span class="c1"># Print list of AUC scores</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;AUC scores computed using 5-fold cross-validation: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cv_auc</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>AUC: 0.8254806777079764
AUC scores computed using 5-fold cross-validation: [0.80148148 0.8062963  0.81481481 0.86245283 0.8554717 ]
</code></pre><h2 id="hyperparameter-tuning">Hyperparameter tuning</h2>
<p>We have seen that when fitting a linear regression, what we are really doing is choosing parameters for the model that fit the data the best. We also saw that we had to choose a value for the <code>alpha</code> in ridge and lasso regression before fitting it. Analogously, before fitting and predicting K-nearest neighbors, we need to choose <code>n_neighbors</code>. Such parameters, ones that need to be specified before fitting a model, are called <strong>hyperparameters</strong>. In other words, these are parameters that cannot be explicitly learned by fitting the model.</p>
<p>Herein lies a fundamental key for building a successful model: choosing the correct hyperparameter. The basic idea is to try a whole bunch of different values, fit all of them separately, see how well each performs, and choose the best one. This is called <strong>hyperparameter tuning</strong>. Now, when fitting different values of a hyperparameter, it is essential to use cross-validation as using train test split alone would risk overfitting the hyperparameter to the test set.</p>
<p>The basic idea is as follows: we choose a grid of possible values we want to try for the hyperparameter or hyperparameters. We then perform k-fold cross-validation for each point in the grid, that is, for each pair of hyperparameters. We then choose for our model the choice of hyperparameters that performed the best.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205011452783.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>This is called a grid search and in scikit-learn we implement it using the class <code>GridSearchCV</code>.</p>
<ul>
<li>
<p>import <code>GridSearchCV</code> from <code>sklearn.model_selection</code>.</p>
</li>
<li>
<p>then specify the hyperparameter as a dictionary in which the keys are the hyperparameter names, such as <code>n_neighbors</code> in KNN or <code>alpha</code> in lasso regression.
The values in the grid dictionary are lists containing the values we wish to tune the relevant hyperparameter or hyperparameters over. If we specify multiple parameters, all possible combinations will be tried.</p>
</li>
<li>
<p>instantiate classifier.</p>
</li>
<li>
<p>then use <code>GridSearchCV</code> and pass it</p>
<ul>
<li>our model (instance),</li>
<li>the grid we wish to tune over</li>
<li>the number of folds <code>cv</code> that we wish to use.</li>
</ul>
<p>This returns a <code>GridSearch</code> object that you can then call <code>.fit()</code> with data (<code>X</code> and <code>y</code>) on it to performs the actual grid search and cross validation.</p>
</li>
<li>
<p>We can then apply the attributes <code>best_params_</code> and <code>best_score_</code>, respectively, to retrieve the hyperparameters that perform the best along with the mean cross-validation score over that fold.</p>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Setup the hyperparameter grid</span>
<span class="n">c_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="n">c_space</span><span class="p">}</span>

<span class="c1"># Instantiate a logistic regression classifier: logreg</span>
<span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="c1"># Instantiate the GridSearchCV object: logreg_cv</span>
<span class="n">logreg_cv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">logreg</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Fit it to the data</span>
<span class="n">logreg_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Print the tuned parameters and score</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tuned Logistic Regression Parameters: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logreg_cv</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span> 
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Best score is </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logreg_cv</span><span class="o">.</span><span class="n">best_score_</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Tuned Logistic Regression Parameters: {'C': 3.727593720314938}
Best score is 0.7708333333333334
</code></pre><p><code>GridSearchCV</code> can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use <code>RandomizedSearchCV</code>, in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions.</p>
<p>For example, Decision trees have many parameters that can be tuned, such as <code>max_features</code>, <code>max_depth</code>, and <code>min_samples_leaf</code>: This makes it an ideal use case for <code>RandomizedSearchCV</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import necessary modules</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">randint</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>

<span class="c1"># Setup the parameters and distributions to sample from: param_dist</span>
<span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;max_depth&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
              <span class="s2">&#34;max_features&#34;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span>
              <span class="s2">&#34;min_samples_leaf&#34;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span>
              <span class="s2">&#34;criterion&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;gini&#34;</span><span class="p">,</span> <span class="s2">&#34;entropy&#34;</span><span class="p">]}</span>

<span class="c1"># Instantiate a Decision Tree classifier: tree</span>
<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>

<span class="c1"># Instantiate the RandomizedSearchCV object: tree_cv</span>
<span class="n">tree_cv</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">param_dist</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Fit it to the data</span>
<span class="n">tree_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Print the tuned parameters and score</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tuned Decision Tree Parameters (RGS): </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tree_cv</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Best score is (RGS): </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tree_cv</span><span class="o">.</span><span class="n">best_score_</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Tuned Decision Tree Parameters (RGS): {'criterion': 'gini', 'max_depth': 3, 'max_features': 5, 'min_samples_leaf': 2}
Best score is (RGS): 0.7395833333333334
</code></pre><h2 id="hold-out-set">Hold-out set</h2>
<p>After using K-fold cross-validation to tune my model&rsquo;s hyperparameters, I may want to report how well my model can be expected to perform on a dataset that it has never seen before, given my scoring function of choice. So, I want to use my model to predict on some labeled data, compare my prediction to the actual labels, and compute the scoring function. However, if I have used all of my data for cross-validation, estimating my model performance on any of it may not provide an accurate picture of how it will perform on unseen data.</p>
<p>For this reason, it is important to split all of my data at the very beginning into a training set and <strong>hold-out set</strong>, then perform cross-validation on the training set to tune my model&rsquo;s hyperparameters. After this, I can select the best hyperparameters and use the hold-out set, which has not been used at all, to test how well the model can be expected to perform on a dataset that it has never seen before.</p>
<h3 id="classification-1">Classification</h3>
<p>Logistic regression has a &lsquo;<code>penalty</code>&rsquo; hyperparameter which specifies whether to use &lsquo;<code>l1</code>&rsquo; or &lsquo;<code>l2</code>&rsquo; regularization. Here we create a hold-out set, tune the &lsquo;<code>C</code>&rsquo; and &lsquo;<code>penalty</code>&rsquo; hyperparameters of a logistic regression classifier using <code>GridSearchCV</code> on the training set.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Create the hyperparameter grid</span>
<span class="n">c_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="n">c_space</span><span class="p">,</span> <span class="s2">&#34;penalty&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">]}</span>

<span class="c1"># Instantiate the logistic regression classifier: logreg</span>
<span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="c1"># Create train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Instantiate the GridSearchCV object: logreg_cv</span>
<span class="n">logreg_cv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">logreg</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Fit it to the training data</span>
<span class="n">logreg_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Print the optimal parameters and best score</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tuned Logistic Regression Parameter: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logreg_cv</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tuned Logistic Regression Accuracy: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logreg_cv</span><span class="o">.</span><span class="n">best_score_</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Tuned Logistic Regression Parameter: {'C': 0.4393970560760795, 'penalty': 'l1'}
Tuned Logistic Regression Accuracy: 0.7652173913043478
</code></pre><h3 id="regression-1">Regression</h3>
<p>Recall that Lasso used the L1 penalty to regularize, while ridge used the L2 penalty. There is another type of regularized regression known as the <strong>elastic net</strong>. In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties:
$$
a\cdot L_1 + (1-a)\cdot L_2
$$</p>
<p>In scikit-learn, $a$ is represented by the &lsquo;<code>l1_ratio</code>&rsquo; parameter of <code>ElasticNet</code> class.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># Import necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">train_test_split</span>

<span class="c1"># Create train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create the hyperparameter grid</span>
<span class="n">l1_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;l1_ratio&#34;</span><span class="p">:</span> <span class="n">l1_space</span><span class="p">}</span>

<span class="c1"># Instantiate the ElasticNet regressor: elastic_net</span>
<span class="n">elastic_net</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">()</span>

<span class="c1"># Setup the GridSearchCV object: gm_cv</span>
<span class="n">gm_cv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">elastic_net</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Fit it to the training data</span>
<span class="n">gm_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict on the test set and compute metrics</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">gm_cv</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">r2</span> <span class="o">=</span> <span class="n">gm_cv</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>  <span class="c1">#  &lt;-- Notice here !!</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  <span class="c1">#  &lt;-- Notice here !!</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tuned ElasticNet l1 ratio: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gm_cv</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tuned ElasticNet R squared: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">r2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tuned ElasticNet MSE: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mse</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Tuned ElasticNet l1 ratio: {'l1_ratio': 0.20689655172413793}
Tuned ElasticNet R squared: 0.8668305372460283
Tuned ElasticNet MSE: 10.05791413339844
</code></pre><h1 id="preprocessing-and-pipelines">Preprocessing and pipelines</h1>
<h2 id="dummies-variable">Dummies variable</h2>
<p>Suppose we are dealing with a dataset that has categorical features, such as &lsquo;red&rsquo; or &lsquo;blue&rsquo;, or &lsquo;male&rsquo; or &lsquo;female&rsquo;. As these are not numerical values, the scikit-learn API will not accept them and we will have to preprocess these features into the correct format. Our goal is to convert these features so that they are numerical. The way we achieve this by splitting the feature into a number of binary features called &lsquo;dummy variables&rsquo;, one for each category: &lsquo;0&rsquo; means the observation was not that category, while &lsquo;1&rsquo; means it was.</p>
<p>For example, if we are dealing with a car dataset like this</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">auto</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;/Users/wanghaoming/Downloads/auto.csv&#34;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">auto</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>    mpg  displ   hp  weight  accel  origin  size
0  18.0  250.0   88    3139   14.5      US  15.0
1   9.0  304.0  193    4732   18.5      US  20.0
2  36.1   91.0   60    1800   16.4    Asia  10.0
3  18.5  250.0   98    3525   19.0      US  15.0
4  34.3   97.0   78    2188   15.8  Europe  10.0
</code></pre><p>The target variable here is miles per gallon or <code>mpg</code>, and the dataset has a &lsquo;<code>origin</code>&rsquo; feature with three different possible values: &lsquo;<code>US</code>&rsquo;, &lsquo;<code>Asia</code>&rsquo;, and &lsquo;<code>Europe</code>&rsquo;. We create binary features for each of the origins, as each car is made in exactly one country, each row in the dataset will have a one in exactly one of the three columns and zeros in the other two. Notice that in this case, if a car is from neither <code>US</code> nor <code>Asia</code>, then implicitly, it is from <code>Europe</code>. That means that we do not actually need three separate features, but only two, so we can delete the &lsquo;<code>Europe</code>&rsquo; column. If we do not do this, we are duplicating information, which might be an issue for some models.</p>
<p><img src="https://raw.githubusercontent.com/floating15/img/main/202205021115096.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>There are several ways to create dummy variables in Python. In scikit-learn, we can use <code>OneHotEncoder</code>. Or we can use pandas <code>get_dummies</code> function. Here, we will use <code>get_dummies</code>. So there is one categorical feature, <code>origin</code>, with three possible values: &lsquo;<code>US</code>&rsquo;, &lsquo;<code>Asia</code>&rsquo;, and &lsquo;<code>Europe</code>&rsquo;. We import <code>pandas</code>, read in the <code>DataFrame</code>, and then apply the <code>get_dummies</code> function.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">auto_dum</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">auto</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">auto_dum</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>   mpg  displ   hp  weight  accel  size    origin_Asia  origin_Europe     origin_US 
0  18.0  250.0   88    3139   14.5  15.0            0              0              1   
1   9.0  304.0  193    4732   18.5  20.0            0              0              1   
2  36.1   91.0   60    1800   16.4  10.0            1              0              0   
3  18.5  250.0   98    3525   19.0  15.0            0              0              1   
4  34.3   97.0   78    2188   15.8  10.0            0              1              0   
</code></pre><p>Notice, pandas deletes the original categories column <code>origin</code>. In the third row, <code>origin_USA</code> and <code>origin_Europe</code> have zeroes, while <code>origin_Asia</code> has a one, indicating that the car is of Asian origin. But if <code>origin_USA</code> and <code>origin_Europe</code> are zero, then we already know that the car is Asian. So, we <code>drop</code> the <code>origin_Asia</code> column. Alternatively, we can set attribute &ldquo;<code>drop_first=True</code>&rdquo; to <code>get_dummies</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">auto_dum1</span> <span class="o">=</span> <span class="n">auto_dum</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&#34;origin_Asia&#34;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">auto_dum1</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="n">auto_dum2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">auto</span><span class="p">,</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">auto_dum2</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>    mpg  displ   hp  weight  accel  size  origin_Europe  origin_US
0  18.0  250.0   88    3139   14.5  15.0              0          1
1   9.0  304.0  193    4732   18.5  20.0              0          1
2  36.1   91.0   60    1800   16.4  10.0              0          0
3  18.5  250.0   98    3525   19.0  15.0              0          1
4  34.3   97.0   78    2188   15.8  10.0              1          0
    mpg  displ   hp  weight  accel  size  origin_Europe  origin_US
0  18.0  250.0   88    3139   14.5  15.0              0          1
1   9.0  304.0  193    4732   18.5  20.0              0          1
2  36.1   91.0   60    1800   16.4  10.0              0          0
3  18.5  250.0   98    3525   19.0  15.0              0          1
4  34.3   97.0   78    2188   15.8  10.0              1          0
</code></pre><p>Notice that the new column names have the following structure: &ldquo;<code>original column name</code>&rdquo; + <code>_</code> + &ldquo;<code>value name</code>&rdquo;. Once we have created our dummy variables, we can fit models as before.</p>
<h2 id="missing-data">Missing data</h2>
<h3 id="drop">drop</h3>
<p>We say that data is missing when there is no value for a given feature in a particular row. Missing values can be encoded in a number of different ways, such as by zeroes, or question marks, or negative ones.</p>
<p>Checking out the dataset, it looks as though there are observations where insulin is zero. And triceps, which is the thickness of the skin, is zero. These are not possible and, as we have no indication of the real values, the data is, for all intents and purposes, missing.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">dia</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;~/Downloads/diab.csv&#34;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dia</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>   pregnancies  glucose  diastolic  triceps  insulin   bmi    dpf  age     diabetes  
0            6      148         72       35        0  33.6  0.627   50         1
1            1       85         66       29        0  26.6  0.351   31         0   
2            8      183         64        0        0  23.3  0.672   32         1   
3            1       89         66       23       94  28.1  0.167   21         0   
4            0      137         40       35      168  43.1  2.288   33         1   
</code></pre><p>Before we go any further, let&rsquo;s make all these entries <code>NaN</code> using the <code>replace</code> method on the relevant columns.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">dia</span><span class="o">.</span><span class="n">insulin</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dia</span><span class="o">.</span><span class="n">triceps</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dia</span><span class="o">.</span><span class="n">bmi</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dia</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 768 entries, 0 to 767
Data columns (total 9 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   pregnancies  768 non-null    int64  
 1   glucose      768 non-null    int64  
 2   diastolic    768 non-null    int64  
 3   triceps      541 non-null    float64
 4   insulin      394 non-null    float64
 5   bmi          757 non-null    float64
 6   dpf          768 non-null    float64
 7   age          768 non-null    int64  
 8   diabetes     768 non-null    int64  
dtypes: float64(4), int64(5)
memory usage: 54.1 KB
</code></pre><p>So, how do we deal with missing data? One way is to drop all rows containing missing data. We can do so using the pandas DataFrame method <code>dropna</code> with attribute <code>axis=0</code>. (Notice that <code>dropna(axis=0)</code> will drop the <strong>rows</strong> that contain missing values)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">dia1</span> <span class="o">=</span> <span class="n">dia</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dia1</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 393 entries, 3 to 765
Data columns (total 9 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   pregnancies  393 non-null    int64  
 1   glucose      393 non-null    int64  
 2   diastolic    393 non-null    int64  
 3   triceps      393 non-null    float64
 4   insulin      393 non-null    float64
 5   bmi          393 non-null    float64
 6   dpf          393 non-null    float64
 7   age          393 non-null    int64  
 8   diabetes     393 non-null    int64  
dtypes: float64(4), int64(5)
memory usage: 30.7 KB
</code></pre><p>Checking out the shape of the resulting data frame, we see that we now have only approximately half the rows left, this is unacceptable. Thus we need a more robust method. It is generally an equally bad idea to remove columns that contain NaNs.</p>
<h3 id="fill">fill</h3>
<p>Another option is to impute missing data. A common strategy is, in any given column with missing values, to compute the mean of all the non-missing entries and to replace all missing values with the mean.</p>
<ul>
<li>import <code>SimpleImputer</code> from <code>sklearn.impute</code></li>
<li>instantiate an instance of the <code>SimpleImputer</code>: imp.
<ul>
<li>argument <code>missing_values='NaN'</code> here specifies that missing values are represented by NaN;</li>
<li>argument <code>strategy='mean'</code> specifies that we will use the mean as described above;</li>
</ul>
</li>
<li>call <code>.fit()</code> method on <code>imp</code> to calculate the relative values of the each column, which can be used when filling the missing data later.</li>
<li>call <code>.transform()</code> on <code>imp</code> to fill the missing data of each column and return the filled data, which is a np.ndarray.</li>
</ul>
<blockquote>
<p>Alternatively, you can call <code>.fit_transform()</code> method to combine <code>fit</code> and <code>transform</code> methods together.</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="n">imp</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">missing_values</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">imp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dia</span><span class="p">)</span>
<span class="n">dia</span> <span class="o">=</span> <span class="n">imp</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">dia</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dia</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">,:])</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>[[6.00000000e+00 1.48000000e+02 7.20000000e+01 3.50000000e+01
  1.55548223e+02 3.36000000e+01 6.27000000e-01 5.00000000e+01
  1.00000000e+00]
 [1.00000000e+00 8.50000000e+01 6.60000000e+01 2.90000000e+01
  1.55548223e+02 2.66000000e+01 3.51000000e-01 3.10000000e+01
  0.00000000e+00]
 [8.00000000e+00 1.83000000e+02 6.40000000e+01 2.91534196e+01
  1.55548223e+02 2.33000000e+01 6.72000000e-01 3.20000000e+01
  1.00000000e+00]
 [1.00000000e+00 8.90000000e+01 6.60000000e+01 2.30000000e+01
  9.40000000e+01 2.81000000e+01 1.67000000e-01 2.10000000e+01
  0.00000000e+00]
 [0.00000000e+00 1.37000000e+02 4.00000000e+01 3.50000000e+01
  1.68000000e+02 4.31000000e+01 2.28800000e+00 3.30000000e+01
  1.00000000e+00]]
</code></pre><p>Due to their ability to transform our data as such, imputers are known as <strong>transformers</strong>, and any model that can transform data this way, using the <code>transform</code> method, is called a transformer.</p>
<h3 id="pipeline">pipeline</h3>
<p>After transforming the data, we could then fit our supervised learning model to it. But we can use the scikit-learn <strong>pipeline</strong> object to do all these at once</p>
<ul>
<li>import <code>Pipeline</code> from <code>sklearn.pipeline</code> and <code>SimpleImputer</code> from <code>sklearn.impute</code>.</li>
<li>instantiate a <code>SimpleImputer</code> and a estimator, such as <code>LogisticRegression</code>.</li>
<li>construct a <strong>list</strong> of steps in the pipeline, where each step is a <strong>2-tuple</strong> containing the name we wish to give the relevant step and the instance (in this case, the transformer and estimator).</li>
<li>instantiate <code>Pipeline</code> and pass the <strong>list</strong>.</li>
</ul>
<p>So far, we have spliced the pipeline, and now we need to inject water into the pipeline. Note that, in a pipeline, each step but the last must be a transformer and the last must be an estimator, such as, a classifier or a regressor.</p>
<ul>
<li>use <code>train_test_split</code> method to split the data into training and test sets</li>
<li>call <code>.fit()</code> method on <code>Pipeline</code> instance and pass the training data to train the estimator.</li>
<li>call <code>.predict()</code> method on <code>Pipeline</code> instance and pass the test data.</li>
<li>call <code>.score()</code> method on <code>Pipeline</code> instance and pass the test data to compute accuracy.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># instantiate transformer and estimator </span>
<span class="n">imp</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">missing_values</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">logreg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="c1"># create steps list and instance pipeline</span>
<span class="n">step</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;transformer&#39;</span><span class="p">,</span> <span class="n">imp</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;estimator&#39;</span><span class="p">,</span> <span class="n">logreg</span><span class="p">)</span>
<span class="p">]</span>
<span class="n">pip</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>

<span class="c1"># handle data</span>
<span class="n">dia</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;~/Downloads/diab.csv&#34;</span><span class="p">)</span>
<span class="n">dia</span><span class="o">.</span><span class="n">insulin</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dia</span><span class="o">.</span><span class="n">triceps</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dia</span><span class="o">.</span><span class="n">bmi</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dia</span><span class="o">.</span><span class="n">diabetes</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dia</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;diabetes&#39;</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># train and evaluate model</span>
<span class="n">pip</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">pip</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>0.2790835169520589
</code></pre><h2 id="centering-and-scaling">Centering and scaling</h2>
<h3 id="standardization">standardization</h3>
<p>Data imputation is one of several important preprocessing steps for machine learning. Let&rsquo;s use <code>describe</code> method to check out the ranges of the feature variables in the red wine quality dataset. The features are chemical properties such as acidity, pH, and alcohol content. The target value is good or bad, encoded as &lsquo;1&rsquo; and &lsquo;0&rsquo;, respectively. We see that the ranges vary widely: &lsquo;density&rsquo; varies from (point) 99 to to 1 and &lsquo;total sulfur dioxide&rsquo; from 6 to 289!</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">rw</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;~/Downloads/redw.csv&#34;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rw</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>       fixed acidity  volatile acidity  citric acid  residual sugar  \
count    1599.000000       1599.000000  1599.000000     1599.000000   
mean        8.319637          0.527821     0.270976        2.538806   
std         1.741096          0.179060     0.194801        1.409928   
min         4.600000          0.120000     0.000000        0.900000   
25%         7.100000          0.390000     0.090000        1.900000   
50%         7.900000          0.520000     0.260000        2.200000   
75%         9.200000          0.640000     0.420000        2.600000   
max        15.900000          1.580000     1.000000       15.500000   

         chlorides  free sulfur dioxide  total sulfur dioxide      density  \
count  1599.000000          1599.000000           1599.000000  1599.000000   
mean      0.087467            15.874922             46.467792     0.996747   
std       0.047065            10.460157             32.895324     0.001887   
min       0.012000             1.000000              6.000000     0.990070   
25%       0.070000             7.000000             22.000000     0.995600   
50%       0.079000            14.000000             38.000000     0.996750   
75%       0.090000            21.000000             62.000000     0.997835   
max       0.611000            72.000000            289.000000     1.003690   

                pH    sulphates      alcohol      quality  
count  1599.000000  1599.000000  1599.000000  1599.000000  
mean      3.311113     0.658149    10.422983     5.636023  
std       0.154386     0.169507     1.065668     0.807569  
min       2.740000     0.330000     8.400000     3.000000  
25%       3.210000     0.550000     9.500000     5.000000  
50%       3.310000     0.620000    10.200000     6.000000  
75%       3.400000     0.730000    11.100000     6.000000  
max       4.010000     2.000000    14.900000     8.000000
</code></pre><p>Many machine learning models use some form of distance to inform them so if you have features on far larger scales, they can unduly influence your model. For example, K-nearest neighbors uses distance explicitly when making predictions. For this reason, we actually want features to be on a similar scale. To achieve this, we do what is called normalizing or scaling and centering.</p>
<p>There are several ways to normalize the data: given any column,</p>
<ul>
<li>you can subtract the mean and divide by the std so that all features are centred around zero and have variance one. This is called <strong>standardization</strong>.</li>
<li>you can also subtract the minimum and divide by the range of the data so the normalized dataset has minimum zero and maximum one.</li>
<li>you can also normalize so that data ranges from -1 to 1 instead.</li>
</ul>
<p>Here we will perform standardization. To do this</p>
<ul>
<li>import <code>scale</code> from <code>sklearn.preprocessing</code>.</li>
<li>pass the feature data to <code>scale</code> and this returns our scaled data. (a np.ndarray)</li>
</ul>
<p>Looking at the mean and standard deviation of the columns of both the original and scaled data verifies this.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">scale</span>
<span class="n">tsdo</span> <span class="o">=</span> <span class="n">rw</span><span class="p">[</span><span class="s2">&#34;total sulfur dioxide&#34;</span><span class="p">]</span>
<span class="n">tsds</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">tsdo</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tsdo</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">tsdo</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tsds</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">tsds</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>46.46779237023139 32.88503665178367
4.443669391870545e-17 0.9999999999999999
</code></pre><h3 id="pipeline-1">pipeline</h3>
<p>We can also put a scalar in a pipeline object. To do so, we</p>
<ul>
<li>import <code>StandardScaler</code> from <code>sklearn.preprocessing</code></li>
<li>instantiate the scaler, estimator and pipline. Here we&rsquo;ll use a K-nearest neighbors algorithm.</li>
<li>split the dataset in training and test sets,</li>
<li>fit the pipeline to training set, and predict on test set. computing the accuracy (score)</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">scl</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">scl</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;knn&#39;</span><span class="p">,</span> <span class="n">knn</span><span class="p">)]</span>
<span class="n">pip</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>

<span class="n">rw</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;~/Downloads/redw.csv&#34;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">rw</span><span class="p">[</span><span class="s2">&#34;quality&#34;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">rw</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;quality&#39;</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">pip</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;scaled: &#39;</span><span class="p">,</span> <span class="n">pip</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="n">knno</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="n">knno</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;original: &#39;</span><span class="p">,</span> <span class="n">knno</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>scaled:  0.5729166666666666
original:  0.44166666666666665
</code></pre><p>Scaling did improve our model performance.</p>
<h3 id="use-gridsearch-in-pipeline">use <code>GridSearch</code> in pipeline</h3>
<p>we can use cross-validation with a supervised learning pipeline:</p>
<ul>
<li>build our pipeline at first.</li>
<li>then specify our hyperparameter space by creating a dictionary.</li>
</ul>
<p>Note that the keys must follows the format: <code>&lt;pipeline step name&gt;__&lt;hyperparameter name&gt;</code>; the corresponding value is a list or an array of the values to try for that particular hyperparameter. In this case, we are tuning only the <code>n_neighbors</code> in the KNN model.</p>
<ul>
<li>split our data into cross-validation and hold-out sets.</li>
<li>perform a <code>GridSearch</code> over the parameters in the pipeline by instantiating the <code>GridSearchCV</code> object</li>
<li>fit <code>GridSearchCV</code> to training data.</li>
<li>call <code>predict</code> method will the best found parameters  on the hold-out set.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Setup the pipeline</span>
<span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
         <span class="p">(</span><span class="s1">&#39;SVM&#39;</span><span class="p">,</span> <span class="n">SVC</span><span class="p">())]</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>

<span class="c1"># Specify the hyperparameter space</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;SVM__C&#39;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
              <span class="s1">&#39;SVM__gamma&#39;</span><span class="p">:[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">]}</span>

<span class="c1"># Create train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">21</span><span class="p">)</span>

<span class="c1"># Instantiate the GridSearchCV object: cv</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Fit to the training set</span>
<span class="n">cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict the labels of the test set: y_pred</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Compute and print metrics</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Accuracy: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tuned Model Parameters: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Accuracy: 0.7795918367346939
				precision    recall  f1-score   support

		False       0.83      0.85      0.84       662
		True       0.67      0.63      0.65       318

avg / total       0.78      0.78      0.78       980

Tuned Model Parameters: {'SVM__C': 10, 'SVM__gamma': 0.1}
</code></pre><p>Another comprehensive example:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Setup the pipeline steps: steps</span>
<span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;imputation&#39;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">missing_values</span><span class="o">=</span><span class="s1">&#39;NaN&#39;</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)),</span>
         <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
         <span class="p">(</span><span class="s1">&#39;elasticnet&#39;</span><span class="p">,</span> <span class="n">ElasticNet</span><span class="p">())]</span>

<span class="c1"># Create the pipeline: pipeline </span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>

<span class="c1"># Specify the hyperparameter space</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;elasticnet__l1_ratio&#39;</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">30</span><span class="p">)}</span>

<span class="c1"># Create train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create the GridSearchCV object: gm_cv</span>
<span class="n">gm_cv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Fit to the training set</span>
<span class="n">gm_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Compute and print the metrics</span>
<span class="n">r2</span> <span class="o">=</span> <span class="n">gm_cv</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tuned ElasticNet Alpha: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gm_cv</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tuned ElasticNet R squared: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">r2</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre tabindex="0"><code>Tuned ElasticNet Alpha: {'elasticnet__l1_ratio': 1.0}
Tuned ElasticNet R squared: 0.8862016570888217
</code></pre><h1 id="compendium">Compendium</h1>
<p>sklearn.neighbors</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">sklearn</span><span class="o">.</span><span class="n">neighbors</span>
	<span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">)</span>
		<span class="o">.</span><span class="n">fit</span><span class="p">()</span>
		<span class="o">.</span><span class="n">predict</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p>sklearn.linear_model</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">sklearn</span><span class="o">.</span><span class="n">linear_model</span>
	<span class="n">LinearRegression</span><span class="p">()</span>
		<span class="o">.</span><span class="n">fit</span><span class="p">()</span>
		<span class="o">.</span><span class="n">predict</span><span class="p">()</span>
		<span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

	<span class="n">Lasso</span><span class="p">()</span>
		<span class="o">.</span><span class="n">fit</span><span class="p">()</span>
		<span class="o">.</span><span class="n">predict</span><span class="p">()</span>
		<span class="o">.</span><span class="n">alpha</span>
		<span class="o">.</span><span class="n">normalize</span>
		<span class="o">.</span><span class="n">coef_</span>

	<span class="n">Ridge</span><span class="p">()</span>
		<span class="o">.</span><span class="n">fit</span><span class="p">()</span>
		<span class="o">.</span><span class="n">predict</span><span class="p">()</span>
		<span class="o">.</span><span class="n">alpha</span>
		<span class="o">.</span><span class="n">normalize</span>

	<span class="n">ElasticNet</span><span class="p">()</span>
		<span class="o">.</span><span class="n">fit</span><span class="p">()</span>
		<span class="o">.</span><span class="n">predict</span><span class="p">()</span>
		<span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>	
		<span class="o">.</span><span class="n">l1_ratio</span>

	<span class="n">LogisticRegression</span><span class="p">()</span>
		<span class="o">.</span><span class="n">fit</span><span class="p">()</span>
		<span class="o">.</span><span class="n">predict</span><span class="p">()</span>
		<span class="o">.</span><span class="n">predict_proba</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p>sklearn.metrics</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span>
	<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

	<span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">),</span> 

	<span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">),</span>

	<span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_prob</span><span class="p">),</span>

	<span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_prob</span><span class="p">),</span>
</code></pre></td></tr></table>
</div>
</div><p>sklearn.model_selection</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">sklearn</span><span class="o">.</span><span class="n">model_selection</span>
	<span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

	<span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span> <span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="p">)</span>

	<span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">cv</span><span class="p">)</span>
		<span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
		<span class="o">.</span><span class="n">best_params_</span>
		<span class="o">.</span><span class="n">best_score_</span>

	<span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">cv</span><span class="p">)</span>
		<span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
		<span class="o">.</span><span class="n">best_params_</span>
		<span class="o">.</span><span class="n">best_score_</span>		
</code></pre></td></tr></table>
</div>
</div><p>sklearn.impute</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">sklearn</span><span class="o">.</span><span class="n">impute</span> 
	<span class="n">SimpleImputer</span><span class="p">()</span>
		<span class="o">.</span><span class="n">missing_values</span>
		<span class="o">.</span><span class="n">strategy</span>
</code></pre></td></tr></table>
</div>
</div><p>sklearn.pipeline</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">sklearn</span><span class="o">.</span><span class="n">pipeline</span> 
	<span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>
		<span class="o">.</span><span class="n">fit</span><span class="p">()</span>
		<span class="o">.</span><span class="n">predict</span><span class="p">()</span>
		<span class="o">.</span><span class="n">score</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">sklearn</span><span class="o">.</span><span class="n">preprocessing</span>
	<span class="n">scale</span><span class="p">()</span>

	<span class="n">StandardScaler</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div>
</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/scikit-learn/">scikit-learn</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css"integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js"integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js"integrity="sha384-vZTG03m&#43;2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ]
        });})
</script>
    
</article>

    

    

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2022 Haoming Wang
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.11.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>

</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >

            </main>
    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#classification">Classification</a>
      <ol>
        <li><a href="#k-nearest-neighbors">k-Nearest Neighbors</a></li>
        <li><a href="#measuring-model-performance">Measuring model performance</a></li>
      </ol>
    </li>
    <li><a href="#regression">Regression</a>
      <ol>
        <li><a href="#ols">OLS</a></li>
        <li><a href="#cross-validation">Cross-validation</a></li>
        <li><a href="#regularized-regression">Regularized regression</a>
          <ol>
            <li><a href="#ridge-regression">Ridge regression</a></li>
            <li><a href="#lasso-regression">Lasso regression</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#fine-tuning">Fine-tuning</a>
      <ol>
        <li><a href="#metrics-for-classification">Metrics for classification</a></li>
        <li><a href="#logistic-regression-and-the-roc-curve">Logistic regression and the ROC curve</a>
          <ol>
            <li><a href="#building-a-logistic-regression-model">Building a logistic regression model</a></li>
            <li><a href="#plot-roc">plot ROC</a></li>
            <li><a href="#calculate-auc">Calculate AUC</a></li>
          </ol>
        </li>
        <li><a href="#hyperparameter-tuning">Hyperparameter tuning</a></li>
        <li><a href="#hold-out-set">Hold-out set</a>
          <ol>
            <li><a href="#classification-1">Classification</a></li>
            <li><a href="#regression-1">Regression</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#preprocessing-and-pipelines">Preprocessing and pipelines</a>
      <ol>
        <li><a href="#dummies-variable">Dummies variable</a></li>
        <li><a href="#missing-data">Missing data</a>
          <ol>
            <li><a href="#drop">drop</a></li>
            <li><a href="#fill">fill</a></li>
            <li><a href="#pipeline">pipeline</a></li>
          </ol>
        </li>
        <li><a href="#centering-and-scaling">Centering and scaling</a>
          <ol>
            <li><a href="#standardization">standardization</a></li>
            <li><a href="#pipeline-1">pipeline</a></li>
            <li><a href="#use-gridsearch-in-pipeline">use <code>GridSearch</code> in pipeline</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#compendium">Compendium</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

<script
    src="https://cdn.jsdelivr.net/gh/zhixuan2333/gh-blog@v0.1.0/js/ribbon.min.js"
    integrity="sha384-UEK8ZiP3VgFNP8KnKMKDmd4pAUAOJ59Y2Jo3ED2Z5qKQf6HLHovMxq7Beb9CLPUe"
    crossorigin="anonymous"
    size="300"
    alpha="0.6"
    zindex="-1"
    defer
></script>


    </body>
</html>
