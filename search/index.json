[{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Classification and Regression Trees (CART) Decision tree for classification Given a labeled dataset, a classification tree learns a sequence of if-else questions about individual features in order to infer the labels. In contrast to linear models, trees are able to capture non-linear relationships between features and labels. In addition, trees don\u0026rsquo;t require the features to be on the same scale through standardization for example.\nTo understand trees more concretely, we\u0026rsquo;ll try to predict whether a tumor is malignant or benign in the Wisconsin Breast Cancer dataset using only 2 features. The figure here shows a scatterplot of two cancerous cell features with malignant-tumors in blue and benign-tumors in red.\nWhen a classification tree is trained on this dataset, the tree learns a sequence of if-else questions with each question involving one feature and one split-point. Take a look at the tree diagram here.\nAt the top, the tree asks whether the concave-points mean of an instance is \u0026lt;= 0.051. If it is, the instance traverses the True branch; otherwise, it traverses the False branch. Similarly, the instance keeps traversing the internal branches until it reaches an end. The label of the instance is then predicted to be that of the prevailing class at that end. The maximum number of branches separating the top from an extreme-end is known as the maximum depth which is equal to 2 here.\nTo understand the tree\u0026rsquo;s predictions more concretely, let\u0026rsquo;s see how it classifies instances in the feature-space. A classification-model divides the feature-space into regions where all instances in one region are assigned to only one class-label. These regions are known as decision-regions. Decision-regions are separated by surfaces called decision-boundaries.\nThe left figure here shows the decision-regions of a linear-classifier. Note how the boundary is a straight-line. In contrast, as shown here on the right, a classification-tree produces rectangular decision-regions in the feature-space. This happens because at each split made by the tree, only one feature is involved. In our tumor prediction case, the decision tree divides the sample feature space into four decision-regions:\nTo fit a decision tree with scikit-learn:\n import DecisionTreeClassifier from sklearn.tree, import the functions train_test_split() from sklearn.model_selection and accuracy_score() from sklearn.metrics. split the data into 80% train and 20% test using train_test_split(). Set the parameter stratify to y in order for the train and test sets to have the same proportion of class labels as the unsplit dataset. use DecisionTreeClassifier() to instantiate a tree classifier, dt with a maximum depth of 2 by setting the parameter max_depth to 2. Note that the parameter random_state is set to 1 for reproducibility. call the fit method on dt and pass X_train and y_train. To predict the labels of the test-set, call the predict method on dt. print the accuracy of the test set using accuracy_score().  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # Import  from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # Instantiate a DecisionTreeClassifier \u0026#39;dt\u0026#39; with a maximum depth of 6 dt = DecisionTreeClassifier(max_depth=6, random_state=SEED) # Fit dt to the training set dt.fit(X_train, y_train) # Predict test set labels y_pred = dt.predict(X_test) # Compute test set accuracy  acc = accuracy_score(y_pred, y_test) print(y_pred[0:5]) print(\u0026#34;Test set accuracy: {:.2f}\u0026#34;.format(acc))   [0 0 0 1 0] Test set accuracy: 0.89 As we can see, a classification tree divides the feature space into rectangular regions. In contrast, a linear model such as logistic regression produces only a single linear decision boundary dividing the feature space into two decision regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # Import LogisticRegression from sklearn.linear_model from sklearn.linear_model import LogisticRegression # Instatiate logreg logreg = LogisticRegression(random_state=1) # Fit logreg to the training set logreg.fit(X_train, y_train) # Define a list called clfs containing the two classifiers logreg and dt clfs = [logreg, dt] # Review the decision regions of the two classifiers plot_labeled_decision_regions(X_test, y_test, clfs)   Classification tree Learning Let\u0026rsquo;s define some terms: A decision-tree is a data-structure consisting of a hierarchy of individual units called nodes. A node is a point that involves either a question or a prediction.\n The root is the node at which the decision-tree starts growing. It has no parent node and involves a question that gives rise to 2 children nodes through two branches. An internal node is a node that has a parent. It also involves a question that gives rise to 2 children nodes. A leaf has one parent node and involves no questions and has no children. It\u0026rsquo;s where a prediction is made.  Recall that when a classification tree is trained on a labeled dataset, the tree learns patterns from the features in such a way to produce the purest leafs. In other words the tree is trained in such a way so that, in each leaf, one class-label is predominant.\nIn the tree diagram shown here, consider the case where an instance traverses the tree to reach the leaf on the left. In this leaf, there are 257 instances classified as benign and 7 instances classified as malignant. As a result, the tree\u0026rsquo;s prediction for this instance would be: \u0026lsquo;benign\u0026rsquo;. In order to understand how a classification tree produces the purest leafs possible, let\u0026rsquo;s first define the concept of information gain.\nThe nodes of a classification tree are grown recursively; in other words, the obtention of an internal node or a leaf depends on the state of its predecessors. To produce the purest leafs possible, at each node, a tree asks a question involving one feature f and a split-point sp. But how does it know which feature and which split-point to pick? It does so by maximizing Information gain. The tree considers that every node contains information and aims at maximizing the Information Gain obtained after each split. Consider the case where a node with N samples is split into a left-node with Nleft samples and a right-node with Nright samples.\nThe information gain for such split is given by the following formula:\n$$ I G(\\underbrace{f}{\\text {feature }}, \\underbrace{s p}{\\text {split-point }})=I(\\text { parent })-\\left(\\frac{N_{\\text {left }}}{N} I(\\text { left })+\\frac{N_{\\text {right }}}{N} I(\\text { right })\\right) $$\nThe criterion function $I(\\cdot)$ here used is to measure the impurity of a node, there are different criteria you can use among which are the gini-index or entropy. you can set the information criterion of the estimator dt by setting the criterion parameter to \u0026lsquo;gini\u0026rsquo; (default) or \u0026lsquo;entropy\u0026rsquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Import from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # Instantiate dt_entropy, set \u0026#39;entropy\u0026#39; as the information criterion dt_entropy = DecisionTreeClassifier(max_depth=8, criterion=\u0026#39;entropy\u0026#39;, random_state=1) # Fit, predict and evaluate dt_entropy.fit(X_train, y_train) y_pred= dt_entropy.predict(X_test) accuracy_entropy = accuracy_score(y_test, y_pred) # Print accuracy_entropy and accuracy_gini print(f\u0026#39;Accuracy achieved by using entropy: {accuracy_entropy:.3f}\u0026#39;) print(f\u0026#39;Accuracy achieved by using the gini index: {accuracy_gini:.3f}\u0026#39;)   Accuracy achieved by using entropy: 0.991 Accuracy achieved by using the gini index: 1.000 When an unconstrained tree is trained, the nodes are grown recursively. In other words, a node exists based on the state of its predecessors. At a non-leaf node, the data is split based on feature f and split-point sp in such a way to maximize information gain. If the information gain obtained by splitting a node is null, the node is declared a leaf. Note that these rules are not for constrained trees. If you constrain the maximum depth of a tree to 2 for example, all nodes having a depth of 2 will be declared leafs even if the information gain obtained by splitting such nodes is not null.\nDecision tree for regression Recall that in regression, the target variable is continuous. In other words, the output of your model is a real value. Let\u0026rsquo;s motivate our discussion of regression by introducing the automobile miles-per-gallon dataset from the UCI Machine Learning Repository. This dataset consists of 6 features corresponding to the characteristics of a car and a continuous target variable labeled mpg which stands for miles-per-gallon. Our task is to predict the mpg consumption of a car given these six features. To simplify the problem, here the analysis is restricted to only one feature corresponding to the displacement of a car. This feature is denoted by displ.\nA 2D scatter plot of mpg versus displ shows that the mpg-consumption decreases nonlinearly with displacement. Note that linear models such as linear regression would not be able to capture such a non-linear trend.\nHere, it\u0026rsquo;s important to note that, when a regression tree is trained on a dataset, the impurity of a node is measured using the mean-squared error of the targets in that node. This means that the regression tree tries to find the splits that produce leafs where in each leaf the target values are on average, the closest possible to the mean-value of the labels in that particular leaf.\n$$ I(\\text { node })=\\underbrace{\\operatorname{MSE}(\\text { node })}{\\text {mean-squared-error }}=\\frac{1}{N{\\text {node }}} \\sum_{i \\in \\text { node }}\\left(y^{(i)}-\\hat{y}_{\\text {node }}\\right)^{2} $$\n$$ \\underbrace{\\hat{y}{\\text {node }}}{\\text {mean-target-value }}=\\frac{1}{N_{\\text {node }}} \\sum_{i \\in \\text { node }} y^{(i)} $$ As a new instance traverses the tree and reaches a certain leaf, its target-variable \u0026lsquo;y\u0026rsquo; is computed as the average of the target-variables contained in that leaf as shown in this formula. $$ \\hat{y}{\\text {pred }}(\\text { leaf })=\\frac{1}{N{\\text {leaf }}} \\sum_{i \\in \\text { leaf }} y^{(i)} $$\nTo highlight the importance of the flexibility of regression trees, take a look at this figure. On the left we have a scatter plot of the data in blue along with the predictions of a linear regression model shown in black. The linear model fails to capture the non-linear trend exhibited by the data. On the right, we have the same scatter plot along with a red line corresponding to the predictions of the regression tree. The regression tree shows a greater flexibility and is able to capture the non-linearity.\nTo train a decision tree with scikit-learn to solve this regression problem:\n import DecisionTreeRegressor from sklearn.tree and the functions train_test_split() from sklearn.model_selection and mean_squared_error as MSE() from sklearn.metrics. split the data into 80%-train and 20%-test using train_test_split. instantiate the DecisionTreeRegressor() with a maximum depth of 4 by setting the parameter max_depth to 4. In addition, set the parameter min_sample_leaf to 0.1 to impose a stopping condition in which each leaf has to contain at least 10% of the training data. fit dt to the training set and predict the test set labels. Finally, evaluate the mean-squared error  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # Import DecisionTreeRegressor from sklearn.tree from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error as MSE # Instantiate dt dt = DecisionTreeRegressor( max_depth=8, min_samples_leaf=0.13, random_state=3 ) # Fit, predict and evaluate dt.fit(X_train, y_train) y_pred = dt.predict(X_test) mse_dt = MSE(y_test, y_pred) rmse_dt = mse_dt ** 0.5 # Print rmse_dt print(\u0026#34;Test set RMSE of dt: {:.2f}\u0026#34;.format(rmse_dt))   Test set RMSE of dt: 4.37 Compare with Linear regression\n1 2 3 4 5 6 7 8 9 10  from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(X_train, y_train) y_pred_lr = lr.predict(X_test) mse_lr = MSE(y_test, y_pred_lr) rmse_lr = mse_lr ** 0.5 print(\u0026#39;Linear Regression test set RMSE: {:.2f}\u0026#39;.format(rmse_lr)) print(\u0026#39;Regression Tree test set RMSE: {:.2f}\u0026#39;.format(rmse_dt))   Linear Regression test set RMSE: 5.10 Regression Tree test set RMSE: 4.37 The Bias-Variance Tradeoff Generalization error In supervised learning, you make the assumption that there\u0026rsquo;s a mapping $f$ between features and labels. You can express this as $y=f(x)$. $f$ which is shown in red here is an unknown function that you want to determine. In reality, data generation is always accompanied with randomness or noise like the blue points shown here.\nOur goal is to find a model $\\hat{f}$ that best approximates $f$. When training $\\hat{f}$, we want to make sure that noise is discarded as much as possible. At the end, $\\hat{f}$ should achieve a low predictive error on unseen datasets. We may encounter two difficulties when approximating $f$. The first is overfitting, it\u0026rsquo;s when $\\hat{f}$ fits the noise in the training set. The second is underfitting, it\u0026rsquo;s when $\\hat{f}$ is not flexible enough to approximate f.\nWhen a model overfits the training set, its predictive power on unseen datasets is pretty low. This is illustrated by the predictions of the decision tree regressor shown here in red. The model clearly memorized the noise present in the training set. Such model achieves a low training set error and a high test set error.\nWhen a model underfits the data, like the regression tree whose predictions are shown here in red, the training set error is roughly equal to the test set error. However, both errors are relatively high. Now the trained model isn\u0026rsquo;t flexible enough to capture the complex dependency between features and labels.\nThe generalization error of a model tells you how much it generalizes on unseen data. It can be decomposed into 3 terms: bias, variance and irreducible error where the irreducible error is the error contribution of noise.\nThe bias term tells you, on average, how much $\\hat{f}$ and $f$ are different. To illustrate this consider the high bias model shown here in black; this model is not flexible enough to approximate the true function $f$ shown in red. High bias models lead to underfitting.\nThe variance term tells you how much $\\hat{f}$ is inconsistent over different training sets. Consider the high variance model shown here in black; in this case, $\\hat{f}$ follows the training data points so closely that it misses the true function $f$ shown in red. High variance models lead to overfitting.\nThe complexity of a model sets its flexibility to approximate the true function $f$. For example: increasing the maximum-tree-depth increases the complexity of a decision tree. The diagram here shows how the best model complexity corresponds to the lowest generalization error. When the model complexity increases, the variance increases while the bias decreases. Conversely, when model complexity decreases, variance decreases and bias increases. Our goal is to find the model complexity that achieves the lowest generalization error. Since this error is the sum of three terms with the irreducible error being constant, we need to find a balance between bias and variance because as one increases the other decreases. This is known as the bias-variance trade-off.\nDiagnose bias and variance problems Given that you\u0026rsquo;ve trained a supervised machine learning model labeled $\\hat{f}$, how do you estimate the $\\hat{f}$\u0026rsquo;s generalization error? This cannot be done directly because:\n $f$ is unknown, usually you only have one dataset, you don\u0026rsquo;t have access to the error term due to noise.  A solution to this is to first split the data into a training and test set. The model $\\hat{f}$ can then be fit to the training set and its error can be evaluated on the test set. The generalization error of $\\hat{f}$ is roughly approximated by $\\hat{f}$\u0026rsquo;s error on the test set.\nUsually, the test set should be kept untouched until one is confident about $\\hat{f}$\u0026rsquo;s performance. It should only be used to evaluate $\\hat{f}$\u0026rsquo;s final performance or error. Now, evaluating $\\hat{f}$\u0026rsquo;s performance on the training set may produce an optimistic estimation of the error because $\\hat{f}$ was already exposed to the training set when it was fit. To obtain a reliable estimate of $\\hat{f}$\u0026rsquo;s performance, you should use cross-validation or CV.\nThe diagram here illustrates this technique for K=10:\n First, the training set (T) is split randomly into 10 partitions or folds, The error of $\\hat{f}$ is evaluated 10 times on the 10 folds, Each time, one fold is picked for evaluation after training $\\hat{f}$ on the other 9 folds. At the end, you\u0026rsquo;ll obtain a list of 10 errors.  Finally, as shown in this formula, the CV-error is computed as the mean of the 10 obtained errors.\n$$ \\mathrm{CV} \\text { error }=\\frac{E_{1}+\\ldots+E_{10}}{10} $$\nOnce you have computed $\\hat{f}$\u0026rsquo;s cross-validation-error, you can check if it is greater than $\\hat{f}$\u0026rsquo;s training set error. If it is greater, $\\hat{f}$ is said to suffer from high variance. In such case, $\\hat{f}$ has overfit the training set. To remedy this try decreasing $\\hat{f}$\u0026rsquo;s complexity. For example, in a decision tree you can reduce the maximum-tree-depth or increase the maximum-samples-per-leaf. In addition, you may also gather more data to train $\\hat{f}$.\nOn the other hand, $\\hat{f}$ is said to suffer from high bias if its cross-validation-error is roughly equal to the training error but much greater than the desired error. In such case $\\hat{f}$ underfits the training set. To remedy this try increasing the model\u0026rsquo;s complexity or gather more relevant features for the problem.\nTo perform K-fold-cross-validation using scikit-learn\n import the function cross_val_score() from sklearn.model_selection. split the dataset using train_test_split(). instantiate a DecisionTreeRegressor() dt with the parameters max_depth set to 4 and min_samples_leaf to 0.14. call cross_val_score() by passing dt, X_train, y_train; set the parameters   cv to 10 for 10-fold-cross-validation     scoring to \u0026lsquo;neg_mean_squared_error\u0026rsquo; to compute the negative-mean-squared-errors. The scoring parameter was set so because cross_val_score() does not allow computing the mean-squared-errors directly.     n_jobs to -1 to exploit all available CPUs in computation.    The result is a numpy-array of the 10 negative mean-squared-errors achieved on the 10-folds. You can multiply the result by minus-one to obtain an array of MSE_CV_scores.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  # Import from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score # Set SEED for reproducibility SEED = 1 # Split the data into 70% train and 30% test X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=SEED ) # Instantiate a DecisionTreeRegressor dt dt = DecisionTreeRegressor( max_depth=4, min_samples_leaf=0.26, random_state=SEED ) # Compute the array containing the 10-folds CV MSEs MSE_CV_scores = - cross_val_score( dt, X_train, y_train, cv=10, scoring=\u0026#39;neg_mean_squared_error\u0026#39;, n_jobs=-1 ) # Compute the 10-folds CV RMSE RMSE_CV = (MSE_CV_scores.mean())**(0.5) # Print RMSE_CV print(\u0026#39;CV RMSE: {:.2f}\u0026#39;.format(RMSE_CV))   CV RMSE: 5.14 After that, fit dt to the training set and evaluate the labels of the training and test sets. Finally, you can use the function MSE to evaluate the train and test set mean-squared-errors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # Import mean_squared_error from sklearn.metrics as MSE from sklearn.metrics import mean_squared_error as MSE # Fit dt to the training set dt.fit(X_train, y_train) # Predict the labels y_pred_train = dt.predict(X_train) y_pred_test = dt.predict(X_test) # Evaluate the RMSE of dt RMSE_train = (MSE(y_train, y_pred_train))**(0.5) RMSE_test = (MSE(y_test, y_pred_test))**(0.5) # Print RMSE print(\u0026#39;Train RMSE: {:.2f}\u0026#39;.format(RMSE_train)) print(\u0026#39;Test RMSE: {:.2f}\u0026#39;.format(RMSE_test))   Train RMSE: 3.32 Test RMSE: 5.15 Given that the training set error is smaller than the CV-error, we can deduce that dt overfits the training set and that it suffers from high variance. Notice how the CV and test set errors are roughly equal.\nEnsemble Learning Let\u0026rsquo;s first recap what we learned from the previous chapter about CARTs. CARTs present many advantages:\n they are easy to understand and their output is easy to interpret. CARTs are easy to use and their flexibility gives them an ability to describe nonlinear dependencies between features and labels. you don\u0026rsquo;t need a lot of feature preprocessing to train a CART. In contrast to other models, you don\u0026rsquo;t have to standardize or normalize features before feeding them to a CART.  CARTs also have limitations.\n A classification tree is only able to produce orthogonal decision boundaries. CARTs are also very sensitive to small variations in the training set. Sometimes, when a single point is removed from the training set, a CART\u0026rsquo;s learned parameters may changed drastically. CARTs also suffer from high variance when they are trained without constraints. In such case, they may overfit the training set.  A solution that takes advantage of the flexibility of CARTs while reducing their tendency to memorize noise is ensemble learning. Ensemble learning can be summarized as follows:\n As a first step, different models are trained on the same dataset. Each model makes its own predictions. A meta-model then aggregates the predictions of individual models and outputs a final prediction. The final prediction is more robust and less prone to errors than each individual model.  Let\u0026rsquo;s take a look at the diagram here to visually understand how ensemble learning works for a classification problem. First, the training set is fed to different classifiers. Each classifier learns its parameters and makes predictions. Then these predictions are fed to a meta model which aggregates them and outputs a final prediction.\nLet\u0026rsquo;s now take a look at an ensemble technique known as the voting classifier. More concretely, we\u0026rsquo;ll consider a binary classification task. The ensemble here consists of N classifiers making the predictions P0,P1,to,PN with P=0 or 1. The meta model outputs the final prediction by hard voting.\nTo understand hard voting, consider a voting classifier that consists of 3 trained classifiers as shown in the diagram here. While classifiers 1 and 3 predict the label of 1 for a new data-point, classifier 2 predicts the label 0. In this case, 1 has 2 votes while 0 has 1 vote. As a result, the voting classifier predicts 1.\nTo train a voting classifier using scikit-learn.\n import LogisticRegression, DecisionTreeClassifier and KNeighborsClassifier. You also need to import VotingClassifier from sklearn.ensemble. split the data and instantiate the different models. define a list named classifiers that contains tuples corresponding the the name of the models and the models themselves. write a for loop to iterate over the list classifiers; fit each classifier to the training set, evaluate its accuracy on the test set and print the result. instantiate a VotingClassifier by setting the estimators parameter to classifiers. Fitting it to the training set, evaluate its accuracy on the test set  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  # Import from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import VotingClassifier from sklearn.metrics import accuracy_score # Set seed for reproducibility SEED=1 # Instantiate lr lr = LogisticRegression(random_state=SEED) # Instantiate knn knn = KNeighborsClassifier(n_neighbors=27) # Instantiate dt dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED) # Define the list classifiers classifiers = [ (\u0026#39;Logistic Regression\u0026#39;, lr), (\u0026#39;K Nearest Neighbours\u0026#39;, knn), (\u0026#39;Classification Tree\u0026#39;, dt) ] # Iterate over the pre-defined list of classifiers for clf_name, clf in classifiers: # Fit clf to the training set clf.fit(X_train, y_train) # Predict y_pred y_pred = clf.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) # Evaluate clf\u0026#39;s accuracy on the test set print(\u0026#39;{:s}: {:.3f}\u0026#39;.format(clf_name, accuracy)) # Instantiate a VotingClassifier vc vc = VotingClassifier(estimators=classifiers) # Fit vc to the training set vc.fit(X_train, y_train) # Evaluate the test set predictions y_pred = vc.predict(X_test) # Calculate accuracy score accuracy = accuracy_score(y_test, y_pred) print(\u0026#39;Voting Classifier: {:.3f}\u0026#39;.format(accuracy))   Logistic Regression : 0.741 K Nearest Neighbours : 0.701 Classification Tree : 0.707 Voting Classifier: 0.764 Note that the accuracy of Voting Classifier is higher than that achieved by any of the individual models in the ensemble.\nBagging and Random Forests Bagging In the last chapter, we discussed that the Voting Classifier is an ensemble of models that are fit to the same training set using different algorithms. we also saw that the final predictions were obtained by majority voting. In Bagging, the ensemble is formed by models that use the same training algorithm. However, these models are not trained on the entire training set. Instead, each model is trained on a different subset of the data.\nIn fact, bagging stands for bootstrap aggregation. Its name refers to the fact that it uses a technique known as the bootstrap. Overall, Bagging has the effect of reducing the variance of individual models in the ensemble.\nThe basic process of bagging is:\n A data set containing M samples is randomly sampled m times with put-backs, such that a sample set with m samples is obtained. (m \u0026lt;= M, in scikit-learn, m = M by default) Take N such sample sets. Train a base estimator for each sample set. Combine:   for the classification task, the final prediction is obtained by majority voting. The corresponding classifier in scikit-learn is BaggingClassifier.     for the regression task, the final prediction is the average of the predictions made by the individual models forming the ensemble. The corresponding regressor in scikit-learn is BaggingRegressor.    Let\u0026rsquo;s train a BaggingClassifier in scikit-learn on the breast cancer dataset.\n import BaggingClassifier from sklearn.ensemble split the data. instantiate a DecisionTreeClassifier dt with the parameters max_depth set to 4 and min_samples_leaf set to 0.16. instantiate a BaggingClassifier bc with parameters   base_estimator set to dt and     n_estimators set to 700, which means bc consists of 700 classification trees dt.     n_jobs set to -1 so that all CPU cores are used in computation.    Once we are done, fit bc to the training set, predict the test set labels and finally, evaluate the test set accuracy.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  # Import from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier # Instantiate dt dt = DecisionTreeClassifier( random_state=1, max_depth = 4, min_samples_leaf=0.16 ) # Instantiate bc bc = BaggingClassifier( base_estimator=dt, n_estimators=700, random_state=1, n_jobs=-1 ) # Fit bc, dt to the training set bc.fit(X_train, y_train) dt.fit(X_train, y_train) # Predict test set labels y_bc_pred = bc.predict(X_test) y_dt_pred = dt.predict(X_test) # Evaluate acc_test bc_acc_test = accuracy_score(y_test, y_bc_pred) dt_acc_test = accuracy_score(y_test, y_dt_pred) print(\u0026#39;Test set accuracy of bc: {:.2f}\u0026#39;.format(bc_acc_test)) print(\u0026#39;Test set accuracy of dt: {:.2f}\u0026#39;.format(dt_acc_test))   Test set accuracy of bc: 0.71 Test set accuracy of dt: 0.66 The output shows that a BaggingClassifier achieves a test set accuracy of 71%. Training the classification tree dt, which is the base estimator here, to the same training set would lead to a test set accuracy of 66%. The result highlights how bagging outperforms the base estimator dt.\nOut of Bag Evaluation Recall that in bagging, some instances may be sampled several times for one model. On the other hand, other instance may not be sampled at all (since we sample with put-backs). On average, for each model, 63% of the training instances are sampled. The remaining 37% that are not sampled constitute what is known as the Out-of-bag or OOB instances.\nSince OOB instances are not seen by a model during training, these can be used to estimate the performance of the ensemble without the need for cross-validation. This technique is known as OOB-evaluation. To understand OOB-evaluation more concretely, take a look at this diagram.\nHere, for each model, the bootstrap instances are shown in blue while the OOB-instances are shown in red. Each of the N models constituting the ensemble is then trained on its corresponding bootstrap samples and evaluated on the OOB instances. This leads to the obtainment of N OOB scores labeled OOB1 to OOBN. The OOB-score of the bagging ensemble is evaluated as the average of these N OOB scores as shown by the formula on top.\nNow it\u0026rsquo;s time to see OOB-evaluation in action.\n import BaggingClassifier split the dataset in a stratified way by setting the parameter stratify to y. instantiate a DecisionTreeClassifier called dt instantiate a BaggingClassifier called bc. Importantly, set the parameter oob_score to True in order to evaluate the OOB-accuracy of bc after training.  Note that in scikit-learn, the OOB-score corresponds to the accuracy for classifiers and the r-squared score for regressors.\n fit bc to the training set and predict the test set labels. evaluate the OOB-accuracy of bc by extracting the attribute oob_score_ from the trained instance.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  # Import from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier # Instantiate dt dt = DecisionTreeClassifier( min_samples_leaf=8, random_state=1 ) # Instantiate bc bc = BaggingClassifier( base_estimator=dt, n_estimators=500, oob_score=True, random_state=1 ) # Fit, predict bc.fit(X_train, y_train) y_pred = bc.predict(X_test) # Evaluate acc_test = accuracy_score(y_test, y_pred) acc_oob = bc.oob_score_ # Print acc_test and acc_oob print(\u0026#39;Test set accuracy: {:.3f}, \\nOOB accuracy: {:.3f}\u0026#39;.format(acc_test, acc_oob))   Test set accuracy: 0.698, OOB accuracy: 0.700 The test-set accuracy is about 69.8% and the OOB-accuracy is about 70%. The two obtained accuracies are pretty close though not exactly equal. These results highlight how OOB-evaluation can be an efficient technique to obtain a performance estimate of a bagged-ensemble on unseen data without performing cross-validation.\nRandom Forests (RF) Recall that in bagging the base estimator could be any model including a decision tree, logistic regression or even a neural network. Each estimator is trained on a distinct bootstrap sample drawn from the training set using all available features.\nRandom Forests is an ensemble method that uses a decision tree as a base estimator. In Random Forests, each estimator is trained on a different bootstrap sample having the same size as the training set. Random forests introduces further randomization than bagging when training each of the base estimators. When each tree is trained, only d features can be sampled at each node with put-backs, where d is a number smaller than the total number of features.\nThe reason for doing so is, before start training, it is impossible to know which features' data have outliers and which features best determine the classification results. the process of random forests reduces the influence of two influencing factors on the classification results.\nThe diagram here shows the training procedure for random forests. Notice how each tree forming the ensemble is trained on a different bootstrap sample from the training set. In addition, when a tree is trained, at each node, only d features are sampled from all features with put-backs. The node is then split using the sampled feature that maximizes information gain. In scikit-learn d defaults to the square-root of the number of features. For example, if there are 100 features, only 10 features are sampled at each node.\nOnce trained, predictions can be made on new instances. When a new instance is fed to the different base estimators, each of them outputs a prediction. The predictions are then collected by the random forests meta-classifier and a final prediction is made depending on the nature of the problem.\n For classification, the final prediction is made by majority voting. The corresponding scikit-learn class is RandomForestClassifier. For regression, the final prediction is the average of all the labels predicted by the base estimators. The corresponding scikit-learn class is RandomForestRegressor.  In general, Random Forests achieves a lower variance than individual trees.\nTo train a random forests regressor\n importing RandomForestRegressor split the dataset instantiate a RandomForestRegressor called rf consisting of 700 regression trees. This can be done by setting n_estimators to 700. In addition, you can control the complexity of the model by setting min_samples_leaf or max_depth (recall that the base model of random forest is decision tree) fit rf to the training set and predict the test set labels.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  # Import from sklearn.metrics import mean_squared_error as MSE from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from sklearn import datasets W = datasets.fetch_california_housing() X = W.data y = W.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # Instantiate rf rf = RandomForestRegressor( n_estimators=700, random_state=2 ) dt = DecisionTreeRegressor(random_state=2) # Fit rf to the training set rf.fit(X_train, y_train) dt.fit(X_train, y_train) # Predict the test set labels rf_y_pred = rf.predict(X_test) dt_y_pred = dt.predict(X_test) # Evaluate the test set RMSE rf_rmse_test = MSE(y_test, rf_y_pred) ** 0.5 dt_rmse_test = MSE(y_test, dt_y_pred) ** 0.5 # Print rmse_test print(\u0026#39;Test set RMSE of rf: {:.2f}\u0026#39;.format(rf_rmse_test)) print(\u0026#39;Test set RMSE of dt: {:.2f}\u0026#39;.format(dt_rmse_test))   Test set RMSE of rf: 0.50 Test set RMSE of dt: 0.74 The result shows that rf achieves a test set RMSE of 0.50; this error is smaller than that achieved by a single regression tree which is 0.74.\nWhen a tree based method is trained, the predictive power of a feature or its importance can be assessed. In scikit-learn, feature importance is assessed by measuring how much the tree nodes use a particular feature to reduce impurity. Note that the importance of a feature is expressed as a percentage indicating the weight of that feature in training and prediction. Once you train a tree-based model in scikit-learn, the features importance can be accessed by extracting the feature_importance_ attribute from the model.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # Import import matplotlib.pyplot as plt import pandas as pd # Create a pd.Series of features importances importances = pd.Series( data=rf.feature_importances_, index= W.feature_names ) # Sort importances importances_sorted = importances.sort_values() # Draw a horizontal barplot of importances_sorted importances_sorted.plot(kind=\u0026#39;barh\u0026#39;, color=\u0026#39;lightgreen\u0026#39;) plt.title(\u0026#39;Features Importances\u0026#39;) plt.savefig(\u0026#39;/Users/wanghaoming/Desktop/fig.jpg\u0026#39;, dpi=400)    recommend reading: article\n Boosting Adaboost Boosting refers to an ensemble method in which many predictors are trained and each predictor learns from the errors of its predecessor. More formally, in boosting many weak learners are combined to form a strong learner. A weak learner is a model doing slightly better than random guessing. For example, a decision tree with a maximum-depth of one, known as a decision-stump, is a weak learner.\nIn boosting, an ensemble of predictors are trained sequentially and each predictor tries to correct the errors made by its predecessor. The two boosting methods we will discuss are AdaBoost and Gradient Boosting.\nAdaBoost stands for Adaptive Boosting. In AdaBoost, each predictor pays more attention to the samples wrongly predicted by its predecessor by constantly changing the weights of training samples. Furthermore, each predictor is assigned a coefficient $\\alpha$ that weighs its contribution in the ensemble\u0026rsquo;s final prediction. Alpha depends on the predictor\u0026rsquo;s training error.\nAs shown in the diagram, there are N predictors in total. First, predictor1 is trained on the initial dataset (X,y), and the training error for predictor1 is determined. This error can then be used to determine $\\alpha_1$ which is predictor1\u0026rsquo;s coefficient. Alpha1 is then used to determine the weights W(2) of the training samples for predictor2. Notice how the incorrectly predicted samples shown in green acquire higher weights. When the weighted samples are used to train predictor2, this predictor is forced to pay more attention to the incorrectly predicted samples. This process is repeated sequentially, until the N predictors forming the ensemble are trained.\n This process is like doing exercises, at the beginning for all the topic of Ex are equally, once done, check the answers, to separate the right and wrong, and then transfer the wrong topics to the wrong set, the next time you do the questions is not the same for all the topics, should focus on the wrong set, and the wrong set of similar topics should draw more attention, but the simple questions can be simply taken over. This way, simple topics and hard topics are solved.\n An important parameter used in training is the learning rate, $\\eta$. $\\eta$ is a number between 0 and 1; it is used to shrink the coefficient $\\alpha$ of a trained predictor. It\u0026rsquo;s important to note that there\u0026rsquo;s a trade-off between $\\eta$ and the number of estimators. A smaller value of $\\eta$ should be compensated by a greater number of estimators.\nOnce all the predictors in the ensemble are trained, the label of a new sample can be predicted depending on the nature of the problem.\n For classification, each predictor predicts the label of the new sample and the ensemble\u0026rsquo;s prediction is obtained by weighted majority voting by AdaBoostClassifier. For regression, the same procedure is applied and the ensemble\u0026rsquo;s prediction is obtained by performing a weighted average by AdaBoostRegressor.  It\u0026rsquo;s important to note that individual predictors need not to be CARTs. However CARTs are used most of the time in boosting because of their high variance.\n import AdaBoostClassifier, DecisionTreeClassifier, roc_auc_score and others. instantiate a DecisionTreeClassifier called dt with the parameter max_depth set to 2. instantiate an AdaBoostClassifier called ada consisting of 180 decision-stumps. This can be done by setting the parameters   base_estimator to dt and     n_estimators to 180.   fit ada to the training set and predict the probability of obtaining the positive class in the test set. Call the function roc_auc_score and passing the parameters y_test and y_pred_proba.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  # Import from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier from sklearn.metrics import roc_auc_score from sklearn import datasets from sklearn.model_selection import train_test_split W = datasets.load_breast_cancer() X = W.data y = W.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # Instantiate dt dt = DecisionTreeClassifier(max_depth=2, random_state=1) # Instantiate ada ada = AdaBoostClassifier( base_estimator=dt, n_estimators=500, random_state=1 ) # Fit ada to the training set ada.fit(X_train, y_train) # Compute the probabilities of obtaining the positive class y_pred_proba = ada.predict_proba(X_test)[:,1] # Evaluate test-set roc_auc_score ada_roc_auc = roc_auc_score(y_test, y_pred_proba) # Print roc_auc_score print(\u0026#39;ROC AUC score: {:.2f}\u0026#39;.format(ada_roc_auc))   ROC AUC score: 0.99 The result shows that the AdaBoostClassifier achieves a ROC-AUC score of about 0.99.\nGradient Boosting (GB) In gradient boosting, each predictor in the ensemble corrects its predecessor\u0026rsquo;s error. In contrast to AdaBoost, the weights of the training samples are not tweaked. Instead, each predictor is trained using the residual errors of its predecessor as labels. We will focus on the technique known as gradient boosted trees where the base learner is a CART.\nTo understand how gradient boosted trees are trained for a regression problem, take a look at the diagram here. The ensemble consists of N trees. Tree1 is trained using the features matrix X and the dataset labels y. The predictions labeled y1hat are used to determine the training set residual errors r1. Tree2 is then trained using the features matrix X and the residual errors r1 of Tree1 as labels. The predicted residuals r1hat are then used to determine the residuals of residuals which are labeled r2. This process is repeated until all of the N trees forming the ensemble are trained.\nAn important parameter used in training gradient boosted trees is shrinkage. Shrinkage refers to the fact that the prediction of each tree in the ensemble is shrinked after it is multiplied by a learning rate $\\eta$ which is a number between 0 and 1. Similarly to AdaBoost, there\u0026rsquo;s a trade-off between $\\eta$ and the number of estimators. Decreasing the learning rate needs to be compensated by increasing the number of estimators in order for the ensemble to reach a certain performance.\nOnce all trees in the ensemble are trained, prediction can be made. When a new sample is available, each tree predicts a label and the final ensemble prediction is given by the formula here.\n$$ y_{\\text {pred }}=y_{1}+\\eta r_{1}+\\ldots+\\eta r_{N} $$\nIn scikit-learn, the class for a gradient boosting regressor is GradientBoostingRegressor. Though not discussed in this blog, a similar algorithm is used for classification problems. The class implementing gradient boosted classification in scikit-learn is GradientBoostingClassifier.\n import GradientBoostingRegressor from sklearn.ensemble. also, import the functions mean_squared_error as MSE. instantiate a GradientBoostingRegressor gbt consisting of 500 CARTs. This can be done by setting the parameters   n_estimators to 500 and     max_depth to 4.   fit gbt to the training set and predict the test set labels. Compute the test set RMSE and print the value.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  # Import from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error as MSE from sklearn import datasets from sklearn.model_selection import train_test_split from time import time W = datasets.fetch_california_housing() X = W.data y = W.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # Instantiate gbt gbt = GradientBoostingRegressor( max_depth=4, n_estimators=500, subsample=0.8, max_features=0.2, random_state=2 ) # Fit gb to the training set t1 = time() gbt.fit(X_train, y_train) t2 = time() print(f\u0026#34;time cost: {t2-t1}s\u0026#34;) # Predict test set labels y_pred = gbt.predict(X_test) # Compute MSE mse_test = MSE(y_test, y_pred) # Compute RMSE rmse_test = mse_test ** 0.5 # Print RMSE print(\u0026#39;Test set RMSE of gb: {:.3f}\u0026#39;.format(rmse_test))   time cost: 14.910254716873169 s Test set RMSE of gbt: 0.478 The result shows that gbt achieves a test set RMSE of 0.478.\nStochastic Gradient Boosting (SGB) Gradient boosting involves an exhaustive search procedure. Each tree in the ensemble is trained to find the best split-points and the best features. This procedure may lead to CARTs that use the same split-points and possibly the same features.\nTo mitigate these effects, you can use an algorithm known as stochastic gradient boosting. In stochastic gradient boosting, each CART is trained on a random subset of the training data. This subset is sampled with put-backs. Furthermore, at the level of each node, features are sampled with put-backs when choosing the best split-points. As a result, this creates further diversity in the ensemble and the net effect is adding more variance to the ensemble of trees and can significantly improve the prediction speed.\nLet\u0026rsquo;s take a closer look at the training procedure used in stochastic gradient boosting by examining the diagram shown below. First, instead of providing all the training samples to a tree, only a fraction of these samples are provided through sampling with put-backs. The sampled data is then used for training a tree. However, not all features are considered when a split is made. Instead, only a certain randomly sampled fraction of these features are used for this purpose. Once a tree is trained, predictions are made and the residual errors can be computed. These residual errors are multiplied by the learning rate $\\eta$ and are fed to the next tree in the ensemble. This procedure is repeated sequentially until all the trees in the ensemble are trained. The prediction procedure for a new sample in stochastic gradient boosting is similar to that of gradient boosting.\nTo put this into practice, define a stochastic gradient boosting regressor consisting of 500 CARTs. This can be done by\n instantiate a GradientBoostingRegressor sgbt, and set the parameters   max_depth=4     n_estimators=500     subsample=0.8 in order for each tree to sample 80% of the data for training.     max_features=0.2 so that each tree uses 20% of available features to perform the best-split.    Once done, fit sgbt to the training set and predict the test set labels. Finally, compute the test set RMSE and print it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  # Import from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error as MSE from sklearn import datasets from sklearn.model_selection import train_test_split from time import time W = datasets.fetch_california_housing() X = W.data y = W.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # Instantiate gbt sgbt = GradientBoostingRegressor( max_depth=4, n_estimators=500, subsample=0.8, max_features=0.5, random_state=2 ) # Fit gb to the training set t1 = time() sgbt.fit(X_train, y_train) t2 = time() print(f\u0026#34;time cost: {t2-t1}s\u0026#34;) # Predict test set labels y_pred = sgbt.predict(X_test) # Compute MSE mse_test = MSE(y_test, y_pred) # Compute RMSE rmse_test = mse_test ** 0.5 # Print RMSE print(\u0026#39;Test set RMSE of sgbt: {:.3f}\u0026#39;.format(rmse_test))   time cost: 6.8205671310424805 s Test set RMSE of sgbt: 0.451 As we can see, SGBT improves the accuracy and prediction speed.\nModel Tuning Tuning a CART\u0026rsquo;s Hyperparameters Machine learning models are characterized by parameters and hyperparameters. Parameters are learned from data through training; examples of parameters include the split-feature and the split-point of a node in a CART. Hyperparameters are not learned from data; they should be set prior to training. Examples of hyperparameters include the maximum-depth and the splitting-criterion of a CART.\nHyperparameter tuning consists of searching for the set of optimal hyperparameters for the learning algorithm. The solution involves finding the set of optimal hyperparameters yielding an optimal model. The optimal model yields an optimal score. The score function measures the agreement between true labels and a model\u0026rsquo;s predictions. In sklearn, it defaults to accuracy for classifiers and r-squared for regressors. A model\u0026rsquo;s generalization performance is evaluated using cross-validation.\nThere are many approaches for hyperparameter tuning including: Grid Search, Random Search, Bayesian Optimization, Genetic Algorithms, and so on. We\u0026rsquo;ll only focus on grid-search.\nIn grid-search cross-validation, first you manually set a grid of discrete hyperparameter values. Then, you pick a metric for scoring model performance and you search exhaustively through the grid. For each set of hyperparameters, you evaluate each model\u0026rsquo;s score. The optimal hyperparameters are those for which the model achieves the best cross-validation score. Note that grid-search suffers from the curse of dimensionality, i.e. the bigger the grid, the longer it takes to find the solution.\nLet\u0026rsquo;s now see how we can inspect the hyperparameters of a CART in scikit-learn. You can first instantiate a DecisionTreeClassifier called dt. call dt\u0026rsquo;s .get_params() method. This prints out a dictionary where the keys are the hyperparameter names and values are the default values fo hyperparameter. In the following, we\u0026rsquo;ll only be optimizing max_depth and min_samples_leaf.\n1 2 3  from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier(random_state=1) dt.get_params()   {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'} Let\u0026rsquo;s now tune dt on the breast cancer dataset:\n import GridSearchCV from sklearn.model_selection. define a dictionary called params_dt containing the names of the hyperparameters to tune as keys and lists of hyperparameter-values as values. instantiate a GridSearchCV object grid_dt by setting   estimator=dt     param_grid=params_dt     scoring='roc_auc' (or \u0026lsquo;accuracy\u0026rsquo; by default)     cv=5    Finally, fit grid_dt to the training set. After training grid_dt, we can obtain\n the best set of hyperparameter-values by grid_dt.best_params_ the best cross validation accuracy by grid_dt.best_score_ the best-model by grid_dt.best_estimator  Note that this model is fitted on the whole training set because the refit parameter of GridSearchCV is set to True by default. Finally, you can evaluate this model\u0026rsquo;s test set accuracy or roc-auc using the score method or roc_auc_score method respectively.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  from sklearn import datasets from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.metrics import roc_auc_score W = datasets.load_breast_cancer() X = W.data y = W.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # Instantiate a DecisionTreeClassifier \u0026#39;dt\u0026#39; dt = DecisionTreeClassifier(random_state=1) # Define params_dt params_dt = { \u0026#39;max_depth\u0026#39;: [2, 3, 4], \u0026#39;min_samples_leaf\u0026#39;: [0.12, 0.14, 0.16, 0.18] } # Instantiate grid_dt grid_dt = GridSearchCV( estimator=dt, param_grid=params_dt, scoring=\u0026#39;roc_auc\u0026#39;, cv=5, n_jobs=-1 ) grid_dt.fit(X_train, y_train) best_param = grid_dt.best_params_ best_rocauc = grid_dt.best_score_ best_model = grid_dt.best_estimator_ y_pred_prob = best_model.predict_proba(X_test)[:,1] roc_auc_score = roc_auc_score(y_test, y_pred_prob) accuracy_score = best_model.score(X_test, y_test) print(f\u0026#39;best parameters: {best_param}\u0026#39;) print(f\u0026#39;best roc-auc score of cv on training set: {best_rocauc}\u0026#39;) print(f\u0026#39;best roc-auc score on test set: {roc_auc_score}\u0026#39;) print(f\u0026#39;accuracy score on test set: {accuracy_score}\u0026#39;)   best parameters: {'max_depth': 3, 'min_samples_leaf': 0.14} best roc-auc score of cv on training set: 0.9483149425287356 best roc-auc score on test set: 0.9642231308411215 accuracy score on test set: 0.9064327485380117  As a note, hyperparameter tuning is computationally expensive and may sometimes lead only to very slight improvement of a model\u0026rsquo;s performance. For this reason, it is desired to weigh the impact of tuning on the pipeline of your data analysis project as a whole in order to understand if it is worth pursuing.\n Tuning a RF\u0026rsquo;s Hyperparameters In addition to the hyperparameters of the CARTs forming random forests, the ensemble itself is characterized by other hyperparameters such as the number of estimators, whether it uses bootstraping or not and so on.\nTo inspect the hyperparameters of a RandomForestRegressor, first, import RandomForestRegressor from sklearn.ensemble and then instantiate a RandomForestRegressor called rf. The hyperparameters of rf along with their default values can be accessed by calling rf\u0026rsquo;s .get_params() method. In the following, we\u0026rsquo;ll be optimizing n_estimators, max_depth, min_samples_leaf and max_features.\nWe\u0026rsquo;ll perform grid-search cross-validation on the auto-dataset:\n import mean_squared_error as MSE from sklearn.metrics and GridSearchCV from sklearn.model_selection. define a dictionary called params_rf containing the grid of hyperparameters. instantiate a GridSearchCV object called grid_rf and set the parameters   estimator=rf,     param_grid=params_rf.     cv=3     scoring='neg_mean_squared_error' in order to use negative mean squared error as a metric.     verbose=1 this parameter controls verbosity; the higher its value, the more messages are printed during fitting. \u0026gt;1 : the computation time for each fold and parameter candidate is displayed; \u0026gt;2 : the score is also displayed; \u0026gt;3 : the fold and candidate parameter indexes are also displayed together with the starting time of the computation.    We can now fit grid_rf to the training set. The output shows messages related to grid fitting as well as the obtained optimal model. We can extract rf\u0026rsquo;s best hyperparameters by getting the attribute best_params_ from grid_rf. We can also extract the best model from rf. This enables us to predict the test set labels and evaluate the test-set MSE.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  from sklearn import datasets from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.metrics import mean_squared_error as MSE W = datasets.fetch_california_housing() X = W.data y = W.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # Instantiate \u0026#39;rf\u0026#39; rf = RandomForestRegressor() params_rf = { \u0026#39;n_estimators\u0026#39;: [300, 400, 500], \u0026#39;max_depth\u0026#39;: [4, 6, 8], \u0026#39;min_samples_leaf\u0026#39;: [0.1, 0.2], \u0026#39;max_features\u0026#39;: [\u0026#39;log2\u0026#39;, \u0026#39;sqrt\u0026#39;] } # Instantiate \u0026#39;grid_rf\u0026#39; grid_rf = GridSearchCV( estimator=rf, param_grid=params_rf, cv=3, scoring=\u0026#39;neg_mean_squared_error\u0026#39;, verbose=1, n_jobs=-1 ) grid_rf.fit(X_train, y_train) best_param = grid_rf.best_params_ best_mse = - grid_rf.best_score_ best_model = grid_rf.best_estimator_ y_pred = best_model.predict(X_test) msescore = MSE(y_test, y_pred) r2score = best_model.score(X_test, y_test) # compute r^2 by default for regression in sklearn print(f\u0026#39;best parameters: {best_param}\u0026#39;) print(f\u0026#39;best MSE score of cv on training set: {best_mse}\u0026#39;) print(f\u0026#39;best MSE score on test set: {msescore}\u0026#39;) print(f\u0026#39;r^2 score on test set: {r2score}\u0026#39;)   Fitting 3 folds for each of 36 candidates, totalling 108 fits best parameters: {'max_depth': 6, 'max_features': 'log2', 'min_samples_leaf': 0.1, 'n_estimators': 400} best MSE score of cv on training set: 0.7203497070079746 best MSE score on test set: 0.758856243156018 r^2 score on test set: 0.4442806821766565 Compendium sklearn.tree sklearn.tree\n DecisionTreeClassifier DecisionTreeRegressor  sklearn.ensemble sklearn.ensemble\n BaggingRegressor BaggingClassifier VotingClassifier RandomForestClassifier AdaBoostClassifier AdaBoostRegressor GradientBoostingClassifier GradientBoostingRegressor   Custom functions used 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323  def plot_decision_regions(X, y, clf, feature_index=None, filler_feature_values=None, filler_feature_ranges=None, ax=None, X_highlight=None, zoom_factor=1., legend=1, hide_spines=True, markers=\u0026#39;s^oxv\u0026lt;\u0026gt;\u0026#39;, colors=(\u0026#39;#1f77b4,#ff7f0e,#3ca02c,#d62728,\u0026#39; \u0026#39;#9467bd,#8c564b,#e377c2,\u0026#39; \u0026#39;#7f7f7f,#bcbd22,#17becf\u0026#39;), scatter_kwargs=None, contourf_kwargs=None, scatter_highlight_kwargs=None): \u0026#34;\u0026#34;\u0026#34;Plot decision regions of a classifier. Please note that this functions assumes that class labels are labeled consecutively, e.g,. 0, 1, 2, 3, 4, and 5. If you have class labels with integer labels \u0026gt; 4, you may want to provide additional colors and/or markers as `colors` and `markers` arguments. See http://matplotlib.org/examples/color/named_colors.html for more information. Parameters ---------- X : array-like, shape = [n_samples, n_features] Feature Matrix. y : array-like, shape = [n_samples] True class labels. clf : Classifier object. Must have a .predict method. feature_index : array-like (default: (0,) for 1D, (0, 1) otherwise) Feature indices to use for plotting. The first index in `feature_index` will be on the x-axis, the second index will be on the y-axis. filler_feature_values : dict (default: None) Only needed for number features \u0026gt; 2. Dictionary of feature index-value pairs for the features not being plotted. filler_feature_ranges : dict (default: None) Only needed for number features \u0026gt; 2. Dictionary of feature index-value pairs for the features not being plotted. Will use the ranges provided to select training samples for plotting. ax : matplotlib.axes.Axes (default: None) An existing matplotlib Axes. Creates one if ax=None. X_highlight : array-like, shape = [n_samples, n_features] (default: None) An array with data points that are used to highlight samples in `X`. zoom_factor : float (default: 1.0) Controls the scale of the x- and y-axis of the decision plot. hide_spines : bool (default: True) Hide axis spines if True. legend : int (default: 1) Integer to specify the legend location. No legend if legend is 0. markers : str (default: \u0026#39;s^oxv\u0026lt;\u0026gt;\u0026#39;) Scatterplot markers. colors : str (default: \u0026#39;red,blue,limegreen,gray,cyan\u0026#39;) Comma separated list of colors. scatter_kwargs : dict (default: None) Keyword arguments for underlying matplotlib scatter function. contourf_kwargs : dict (default: None) Keyword arguments for underlying matplotlib contourf function. scatter_highlight_kwargs : dict (default: None) Keyword arguments for underlying matplotlib scatter function. Returns --------- ax : matplotlib.axes.Axes object Examples ----------- For usage examples, please see http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/ \u0026#34;\u0026#34;\u0026#34; check_Xy(X, y, y_int=True) # Validate X and y arrays dim = X.shape[1] if ax is None: ax = plt.gca() plot_testdata = True if not isinstance(X_highlight, np.ndarray): if X_highlight is not None: raise ValueError(\u0026#39;X_highlight must be a NumPy array or None\u0026#39;) else: plot_testdata = False elif len(X_highlight.shape) \u0026lt; 2: raise ValueError(\u0026#39;X_highlight must be a 2D array\u0026#39;) if feature_index is not None: # Unpack and validate the feature_index values if dim == 1: raise ValueError( \u0026#39;feature_index requires more than one training feature\u0026#39;) try: x_index, y_index = feature_index except ValueError: raise ValueError( \u0026#39;Unable to unpack feature_index. Make sure feature_index \u0026#39; \u0026#39;only has two dimensions.\u0026#39;) try: X[:, x_index], X[:, y_index] except IndexError: raise IndexError( \u0026#39;feature_index values out of range. X.shape is {}, but \u0026#39; \u0026#39;feature_index is {}\u0026#39;.format(X.shape, feature_index)) else: feature_index = (0, 1) x_index, y_index = feature_index # Extra input validation for higher number of training features if dim \u0026gt; 2: if filler_feature_values is None: raise ValueError(\u0026#39;Filler values must be provided when \u0026#39; \u0026#39;X has more than 2 training features.\u0026#39;) if filler_feature_ranges is not None: if not set(filler_feature_values) == set(filler_feature_ranges): raise ValueError( \u0026#39;filler_feature_values and filler_feature_ranges must \u0026#39; \u0026#39;have the same keys\u0026#39;) # Check that all columns in X are accounted for column_check = np.zeros(dim, dtype=bool) for idx in filler_feature_values: column_check[idx] = True for idx in feature_index: column_check[idx] = True if not all(column_check): missing_cols = np.argwhere(~column_check).flatten() raise ValueError( \u0026#39;Column(s) {}need to be accounted for in either \u0026#39; \u0026#39;feature_index or filler_feature_values\u0026#39;.format(missing_cols)) marker_gen = cycle(list(markers)) n_classes = np.unique(y).shape[0] colors = colors.split(\u0026#39;,\u0026#39;) colors_gen = cycle(colors) colors = [next(colors_gen) for c in range(n_classes)] # Get minimum and maximum x_min, x_max = (X[:, x_index].min() - 1./zoom_factor, X[:, x_index].max() + 1./zoom_factor) if dim == 1: y_min, y_max = -1, 1 else: y_min, y_max = (X[:, y_index].min() - 1./zoom_factor, X[:, y_index].max() + 1./zoom_factor) xnum, ynum = plt.gcf().dpi * plt.gcf().get_size_inches() xnum, ynum = floor(xnum), ceil(ynum) xx, yy = np.meshgrid(np.linspace(x_min, x_max, num=xnum), np.linspace(y_min, y_max, num=ynum)) if dim == 1: X_predict = np.array([xx.ravel()]).T else: X_grid = np.array([xx.ravel(), yy.ravel()]).T X_predict = np.zeros((X_grid.shape[0], dim)) X_predict[:, x_index] = X_grid[:, 0] X_predict[:, y_index] = X_grid[:, 1] if dim \u0026gt; 2: for feature_idx in filler_feature_values: X_predict[:, feature_idx] = filler_feature_values[feature_idx] Z = clf.predict(X_predict.astype(X.dtype)) Z = Z.reshape(xx.shape) # Plot decisoin region # Make sure contourf_kwargs has backwards compatible defaults contourf_kwargs_default = {\u0026#39;alpha\u0026#39;: 0.45, \u0026#39;antialiased\u0026#39;: True} contourf_kwargs = format_kwarg_dictionaries( default_kwargs=contourf_kwargs_default, user_kwargs=contourf_kwargs, protected_keys=[\u0026#39;colors\u0026#39;, \u0026#39;levels\u0026#39;]) cset = ax.contourf(xx, yy, Z, colors=colors, levels=np.arange(Z.max() + 2) - 0.5, **contourf_kwargs) ax.contour(xx, yy, Z, cset.levels, colors=\u0026#39;k\u0026#39;, linewidths=0.5, antialiased=True) ax.axis([xx.min(), xx.max(), yy.min(), yy.max()]) # Scatter training data samples # Make sure scatter_kwargs has backwards compatible defaults scatter_kwargs_default = {\u0026#39;alpha\u0026#39;: 0.8, \u0026#39;edgecolor\u0026#39;: \u0026#39;black\u0026#39;} scatter_kwargs = format_kwarg_dictionaries( default_kwargs=scatter_kwargs_default, user_kwargs=scatter_kwargs, protected_keys=[\u0026#39;c\u0026#39;, \u0026#39;marker\u0026#39;, \u0026#39;label\u0026#39;]) for idx, c in enumerate(np.unique(y)): if dim == 1: y_data = [0 for i in X[y == c]] x_data = X[y == c] elif dim == 2: y_data = X[y == c, y_index] x_data = X[y == c, x_index] elif dim \u0026gt; 2 and filler_feature_ranges is not None: class_mask = y == c feature_range_mask = get_feature_range_mask( X, filler_feature_values=filler_feature_values, filler_feature_ranges=filler_feature_ranges) y_data = X[class_mask \u0026amp; feature_range_mask, y_index] x_data = X[class_mask \u0026amp; feature_range_mask, x_index] else: continue ax.scatter(x=x_data, y=y_data, c=colors[idx], marker=next(marker_gen), label=c, **scatter_kwargs) if hide_spines: ax.spines[\u0026#39;right\u0026#39;].set_visible(False) ax.spines[\u0026#39;top\u0026#39;].set_visible(False) ax.spines[\u0026#39;left\u0026#39;].set_visible(False) ax.spines[\u0026#39;bottom\u0026#39;].set_visible(False) ax.yaxis.set_ticks_position(\u0026#39;left\u0026#39;) ax.xaxis.set_ticks_position(\u0026#39;bottom\u0026#39;) if dim == 1: ax.axes.get_yaxis().set_ticks([]) if plot_testdata: if dim == 1: x_data = X_highlight y_data = [0 for i in X_highlight] elif dim == 2: x_data = X_highlight[:, x_index] y_data = X_highlight[:, y_index] else: feature_range_mask = get_feature_range_mask( X_highlight, filler_feature_values=filler_feature_values, filler_feature_ranges=filler_feature_ranges) y_data = X_highlight[feature_range_mask, y_index] x_data = X_highlight[feature_range_mask, x_index] # Make sure scatter_highlight_kwargs backwards compatible defaults scatter_highlight_defaults = {\u0026#39;c\u0026#39;: \u0026#39;none\u0026#39;, \u0026#39;edgecolor\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;alpha\u0026#39;: 1.0, \u0026#39;linewidths\u0026#39;: 1, \u0026#39;marker\u0026#39;: \u0026#39;o\u0026#39;, \u0026#39;s\u0026#39;: 80} scatter_highlight_kwargs = format_kwarg_dictionaries( default_kwargs=scatter_highlight_defaults, user_kwargs=scatter_highlight_kwargs) ax.scatter(x_data, y_data, **scatter_highlight_kwargs) if legend: if dim \u0026gt; 2 and filler_feature_ranges is None: pass else: handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels, framealpha=0.3, scatterpoints=1, loc=legend) return ax def plot_labeled_decision_regions(X,y, models): \u0026#39;\u0026#39;\u0026#39; Function producing a scatter plot of the instances contained in the 2D dataset (X,y) along with the decision regions of two trained classification models contained in the list \u0026#39;models\u0026#39;. Parameters ---------- X: pandas DataFrame corresponding to two numerical features y: pandas Series corresponding the class labels models: list containing two trained classifiers \u0026#39;\u0026#39;\u0026#39; if len(models) != 2: raise Exception(\u0026#39;\u0026#39;\u0026#39; Models should be a list containing only two trained classifiers. \u0026#39;\u0026#39;\u0026#39;) if not isinstance(X, pd.DataFrame): raise Exception(\u0026#39;\u0026#39;\u0026#39; X has to be a pandas DataFrame with two numerical features. \u0026#39;\u0026#39;\u0026#39;) if not isinstance(y, pd.Series): raise Exception(\u0026#39;\u0026#39;\u0026#39; y has to be a pandas Series corresponding to the labels. \u0026#39;\u0026#39;\u0026#39;) fig, ax = plt.subplots(1, 2, figsize=(6.0,2.7), sharey=True) for i, model in enumerate(models): plot_decision_regions(X.values,y.values, model, legend= 2, ax = ax[i]) ax[i].set_title(model.__class__.__name__) ax[i].set_xlabel(X.columns[0]) if i == 0: ax[i].set_ylabel(X.columns[1]) ax[i].set_ylim(X.values[:,1].min(), X.values[:,1].max()) ax[i].set_xlim(X.values[:,0].min(), X.values[:,0].max()) plt.tight_layout() plt.show()   ","date":"2022-02-02T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/machine-learning-iv-tree-based-models-ensemble-learning/","title":"Machine Learning IV (Tree-Based Models \u0026 Ensemble Learning)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Loss Functions Decision boundary A decision boundary tells us what class our classifier will predict for any value of x. The dividing line between the two regions is called the decision boundary.\nThe decision boundary is considered linear because it looks like a line. This definition extends to more than 2 features as well. With 5 features, the space of possible x-values is 5-dimensional. In that case, the boundary would be a higher-dimensional \u0026ldquo;hyperplane\u0026rdquo; cutting the space into two halves. A nonlinear boundary is any other type of boundary. Sometimes this leads to non-contiguous regions of a certain prediction, like in the figure.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79  import numpy as np import matplotlib.pyplot as plt from sklearn import svm, datasets from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC, LinearSVC from sklearn.neighbors import KNeighborsClassifier def make_meshgrid(x, y, h=0.02): \u0026#34;\u0026#34;\u0026#34;Create a mesh of points to plot in Parameters ---------- x: data to base x-axis meshgrid on y: data to base y-axis meshgrid on h: stepsize for meshgrid, optional Returns ------- xx, yy : ndarray \u0026#34;\u0026#34;\u0026#34; x_min, x_max = x.min() - 1, x.max() + 1 y_min, y_max = y.min() - 1, y.max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) return xx, yy def plot_contours(ax, clf, xx, yy, **params): \u0026#34;\u0026#34;\u0026#34;Plot the decision boundaries for a classifier. Parameters ---------- ax: matplotlib axes object clf: a classifier xx: meshgrid ndarray yy: meshgrid ndarray params: dictionary of params to pass to contourf, optional \u0026#34;\u0026#34;\u0026#34; Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) out = ax.contourf(xx, yy, Z, **params) return out # import some data to play with iris = datasets.load_iris() # Take the first two features. We could avoid this by using a two-dim dataset X = iris.data[:, :2] y = iris.target C = 1.0 # SVM regularization parameter models = (LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()) models = (clf.fit(X, y) for clf in models) # title for the plots titles = ( \u0026#34;LogisticRegression\u0026#34;, \u0026#34;LinearSVC\u0026#34;, \u0026#34;SVC\u0026#34;, \u0026#34;KNeighborsClassifier\u0026#34;, ) # Set-up 2x2 grid for plotting. fig, sub = plt.subplots(2, 2) plt.subplots_adjust(wspace=0.4, hspace=0.4) X0, X1 = X[:, 0], X[:, 1] xx, yy = make_meshgrid(X0, X1) for clf, title, ax in zip(models, titles, sub.flatten()): plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8) ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=\u0026#34;k\u0026#34;) ax.set_xlim(xx.min(), xx.max()) ax.set_ylim(yy.min(), yy.max()) ax.set_xlabel(\u0026#34;Sepal length\u0026#34;) ax.set_ylabel(\u0026#34;Sepal width\u0026#34;) ax.set_xticks(()) ax.set_yticks(()) ax.set_title(title) plt.show()   Coefficients Let\u0026rsquo;s create some numpy arrays x and y. To take the dot product between them, we need to multiply them element-wise. The result is 0 (from 0 times 3), 4 (from 1 times 4), and 10 (from 2 times 5). The sum of these numbers, also known as the dot product, is 14. A convenient notation for this in recent Python versions is the \u0026ldquo;at\u0026rdquo; symbol. x@y gives us the same result.\n1 2 3 4 5 6  x=np.array([0, 1, 2]) y=np.array([3, 4, 5]) print(x*y) print(sum(x*y)) print(x@y)   [ 0 4 10] 14 14 Using dot products, we can express how linear classifiers make predictions. First, we compute what we\u0026rsquo;ll call the \u0026ldquo;raw model output\u0026rdquo;, which is the dot product of the coefficients and the features, plus an intercept. We\u0026rsquo;ll then take the sign of this quantity, in other words, we\u0026rsquo;ll check if it\u0026rsquo;s positive or negative. This is a key equation in the course. Crucially, this pattern is the same for both logistic regression and linear SVMs. In scikit-learn terms, we can say logistic regression and linear SVM have different fit functions but the same predict function. The differences in \u0026ldquo;fit\u0026rdquo; relate to loss functions, which are coming later.\nLet\u0026rsquo;s see this equation in action with scikit-learn\u0026rsquo;s breast cancer classification data set. We create a logistic regression object, fit it to the data, and look at the predictions on samples 10 and 20, which are 0 and 1.\nLet\u0026rsquo;s now dig deeper. We can get the learned coefficients and intercept with .coef_ and .intercept_ attributes. Let\u0026rsquo;s compute the raw model output for sample 10. It\u0026rsquo;s negative: that\u0026rsquo;s why we predict the negative class, called \u0026ldquo;0\u0026rdquo; in this data set. On the other hand, for sample 20 the raw model output is positive: so we predict the other class, called \u0026ldquo;1\u0026rdquo; in this data set.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  from sklearn.linear_model import LogisticRegression canc = datasets.load_breast_cancer() X = canc.data y = canc.target lr = LogisticRegression() lr.fit(X, y) y_pred = lr.predict(X) yp10 = y_pred[10] rmo10 = lr.coef_ @ X[10] + lr.intercept_ yp20 = y_pred[20] rmo20 = lr.coef_ @ X[20] + lr.intercept_ print(yp10, rmo10) print(yp20, rmo20)   0 [-6.40822928] 1 [5.20936974] In general, this is what the predict function does for any X: it computes the raw model output, checks if it\u0026rsquo;s positive or negative, and then returns a result based on the names of the classes in your data set, in this case 0 and 1.\nLoss Function minimize We have actually seen loss functions before. For example, least squares linear regression, such as scikit-learn\u0026rsquo;s LinearRegression class, minimizes the sum of squares of the errors (MSE) made on our training set. Here, error is defined as the difference between the true target value and the predicted target value. The loss function is a penalty score that tells us how well the model is doing on the training data. We can think of the \u0026ldquo;fit\u0026rdquo; function as running code that minimizes the loss.\n Note that the score function provided by scikit-learn isn\u0026rsquo;t necessarily the same thing as the loss function.\n The squared error from LinearRegression is not appropriate for classification problems, because our y-values are categories, not numbers. For classification, a natural quantity to think about is the number of errors we\u0026rsquo;ve made. Since we\u0026rsquo;d like to make this as small as possible, the number of errors might be a good loss function. We\u0026rsquo;ll refer to this loss function as the 0-1 loss, because it\u0026rsquo;s defined to be either 0 (if your prediction is correct) or 1 (if your prediction is wrong). By summing this function over all training examples, we get the number of mistakes we\u0026rsquo;ve made on the training set, since we add 1 to the total for each mistake. While the 0-1 loss is important for our conceptual journey, it turns out to be very hard to minimize it directly in practice, which is why logistic regression and SVMs don\u0026rsquo;t use it.\nTo try minimizing a loss function,\n import a Python package called scipy.optimize.minimize, which can minimize all sorts of functions. specify the first argument: the function that be minimized. For example, if we gonna minimize the function y=x^3-3x^2-2x:   specify the second argument: initial guess. Let\u0026rsquo;s try 0.5. refer .x attribute to grab the value that makes the function as small as possible.  1 2 3 4 5  def func(x): y = x ** 3 - 3 * (x ** 2) - 2 * x return y print(minimize(func, 0.5).x)   [2.29099446] We\u0026rsquo;ll train a model on the Boston housing price data set with 0-1 loss function. For simplicity, we won\u0026rsquo;t include an intercept in our regression model. Compare the difference between the coefficients:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # The squared error, summed over training examples def my_loss(w): s = 0 for i in range(y.size): # Get the true and predicted target values for example \u0026#39;i\u0026#39; y_i_true = y[i] y_i_pred = w@X[i] s = s + (y_i_pred-y_i_true)**2 return s # Returns the w that makes my_loss(w) smallest w_fit = minimize(my_loss, X[0]).x print(w_fit) # Compare with scikit-learn\u0026#39;s LinearRegression coefficients lr = LinearRegression(fit_intercept=False).fit(X,y) print(lr.coef_)   [-9.16299112e-02 4.86754828e-02 -3.77698794e-03 2.85635998e+00 -2.88057050e+00 5.92521269e+00 -7.22470732e-03 -9.67992974e-01 1.70448714e-01 -9.38971600e-03 -3.92421893e-01 1.49830571e-02 -4.16973012e-01] [-9.16297843e-02 4.86751203e-02 -3.77930006e-03 2.85636751e+00 -2.88077933e+00 5.92521432e+00 -7.22447929e-03 -9.67995240e-01 1.70443393e-01 -9.38925373e-03 -3.92425680e-01 1.49832102e-02 -4.16972624e-01] loss function diagrams We want to draw loss functions, so let\u0026rsquo;s set up a plot with loss on the vertical axis. On the horizontal axis we\u0026rsquo;ll plot the raw model output. Since we predict using the sign of the raw model output, the plot is divided into two halves: in the left half we predict the one class (call it -1) and in the right half we predict the other class (call it +1). For concreteness, let\u0026rsquo;s focus on a training sample in class +1. Then, the right half represents correct predictions and the left half represents incorrect predictions.\nLet\u0026rsquo;s now draw our 0-1 loss onto the picture. By the definition of the 0-1 loss, incorrect predictions, or mistakes, get a penalty of 1 and correct predictions get no penalty.\nIt\u0026rsquo;s important to distinguish this diagram from the decision boundary plots from earlier: here, the axes aren\u0026rsquo;t two features, but rather the raw model output and the loss, regardless of how many features we have. Keep in mind that this picture is the loss for a particular training example: to get the whole loss, we need to sum up the contribution from all examples.\nUsing this type of diagram, we can draw the loss from least squares linear regression. As the name implies, this is a squared or quadratic function. In linear regression, the raw model output is the prediction. Intuitively, the loss is higher as the prediction is further away from the true target value, which we\u0026rsquo;re assuming is 1 in this case.\nWhile this intuition makes sense for linear regression, it doesn\u0026rsquo;t make sense for a linear classifier: for us, being really close to the true value doesn\u0026rsquo;t matter, as long as we get the sign right. We can see this problem in the picture. The left arm of the curve is OK: the loss is large for wrong answers. But the right arm is problematic: if the raw model output is large and positive, the loss grows large even though we\u0026rsquo;re correctly predicting +1. Since we\u0026rsquo;re fitting a model by minimizing the loss, this means perfectly good models are considered \u0026ldquo;bad\u0026rdquo; by the loss. This is why we need specialized loss functions for classification, and can\u0026rsquo;t just use the squared error from linear regression. Let\u0026rsquo;s now look at the logistic loss used in logistic regression.\nThis is the logistic loss. You can think of it as a smooth version of the 0-1 loss. It has the properties we want: as you move to the right, towards the zone of correct predictions, the loss goes down. We no longer have the interpretation as the number of mistakes, but, unlike the 0-1 loss, we can minimize this easily in practice.\nThis is the hinge loss, used in SVMs. The general shape is the same as for the logistic loss: both penalize incorrect predictions. These loss function diagrams capture the essence of logistic regression and SVMs.\nNow we\u0026rsquo;ll minimize the logistic loss, hinge loss and compare with scikit-learn\u0026rsquo;s LogisticRegression (we\u0026rsquo;ve set C to a large value to disable regularization). The sklearn breast cancer prediction dataset (first 10 features, standardized) is available as the variables X and y.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  # Mathematical functions for logistic and hinge losses def log_loss(raw_model_output): return np.log(1+np.exp(-raw_model_output)) def hinge_loss(raw_model_output): return np.maximum(0,1-raw_model_output) # The logistic loss, summed over training examples def my_loss_l(w): s = 0 for i in range(y.size): raw_model_output = w@X[i] s = s + log_loss(raw_model_output * y[i]) return s # The logistic loss, summed over training examples def my_loss_h(w): s = 0 for i in range(y.size): raw_model_output = w@X[i] s = s + hinge_loss(raw_model_output * y[i]) return s # Returns the w that makes loss smallest w_fit_l = minimize(my_loss_l, X[0]).x w_fit_h = minimize(my_loss_h, X[0]).x # Compare with scikit-learn\u0026#39;s LogisticRegression lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y) print(w_fit_l) print(w_fit_h) print(lr.coef_) # Create a grid of values and plot grid = np.linspace(-2,2,1000) plt.plot(grid, log_loss(grid), label=\u0026#39;logistic\u0026#39;) plt.plot(grid, hinge_loss(grid), label=\u0026#39;hinge\u0026#39;) plt.legend() plt.show()   [ 1.03592182 -1.65378492 4.08331342 -9.40923002 -1.06786489 0.07892114 -0.85110344 -2.44103305 -0.45285671 0.43353448] [ 3.16578173 -1.11720549 3.18675237 -8.64972579 -0.96748775 -0.5534664 -0.62756357 -1.82274064 -0.13455769 0.89649769] [[ 1.03731085 -1.65339037 4.08143924 -9.40788356 -1.06757746 0.07895582 -0.85072003 -2.44079089 -0.45271 0.43334997]] Logistic Regression Regularization regularization reduce the coef Regularization combats overfitting by making the model coefficients smaller. In scikit-learn, the hyperparameter \u0026ldquo;C\u0026rdquo; is the inverse of the regularization strength. In other words, larger C means less regularization and smaller C means more regularization.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Train and validaton errors initialized as empty list train_errs = list() valid_errs = list() # Loop over values of C_value for C_value in [100, 1, 0.01]: # Create LogisticRegression object and fit lr = LogisticRegression(C=C_value) lr.fit(X_train, y_train) plt.plot(lr.coef_.flatten(), label=f\u0026#39;C={C_value}\u0026#39;) plt.ylabel(\u0026#39;value of coef\u0026#39;) plt.xlabel(\u0026#39;coef\u0026#39;) plt.legend() plt.show()   The figure shows the learned coefficients of a logistic regression model with different strength of regularization. It indicates more regularization for our logistic regression model makes the coefficients smaller.\nregularization effect accuracy Let\u0026rsquo;s see how regularization influences training and test accuracy. With the data set already loaded and split into train and valid sets, we instantiate a list of logistic regression models, with various C. We then fit these models. Next, we compute training accuracy and test accuracy. As we can see, regularization will reduce the accuracy on training data set; but will improve the accuracy on valid set within a certain range.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # Train and validaton errors initialized as empty list train_errs = list() valid_errs = list() # Loop over values of C_value for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]: # Create LogisticRegression object and fit lr = LogisticRegression(C=C_value) lr.fit(X_train, y_train) # Evaluate error rates and append to lists train_errs.append( 1.0 - lr.score(X_train, y_train) ) valid_errs.append( 1.0 - lr.score(X_valid, y_valid) ) # Plot results plt.semilogx(C_values, train_errs, C_values, valid_errs) plt.legend((\u0026#34;train\u0026#34;, \u0026#34;validation\u0026#34;)) plt.show()   Now that we\u0026rsquo;ve studied loss functions, we can see why regularization makes the training accuracy go down: Regularization is an extra term that we add to the original loss function, which penalizes large values of the coefficients. Intuitively, without regularization, we are maximizing the training accuracy, so we do well on that metric. When we add regularization, we\u0026rsquo;re modifying the loss function to penalize large coefficients, which distracts from the goal of optimizing accuracy. The larger the regularization penalty (or the smaller we set C), the more we deviate from our goal of maximizing training accuracy. Hence, training accuracy goes down.\nWe discussed why regularization reduces training accuracy, but why does it improve test accuracy? Imagine you did not have access to a particular feature; that\u0026rsquo;s like setting the corresponding coefficient to zero. Regularizing, and thus making your coefficient smaller, is like a compromise between not using the feature at all (setting the coefficient to zero) and fully using it (the un-regularized coefficient value). If using a feature too heavily was causing overfitting, then regularization causes you to \u0026ldquo;fit less\u0026rdquo; and thus overfit less.\nl1 and l2 regularization For linear regression we use the terms Ridge (L2) and Lasso (L1) for two different types of regularization. We can apply ridge (or L2) and lasso (or L1) to logistic regression as well by specifying penalty argument. For example, both help reduce overfitting, and L1 also performs feature selection.\nAs an example, let\u0026rsquo;s train two logistic regression models, with L1, L2 regularization. We can extract the coefficients for both models. As you can see, L1 regularization set many of the coefficients to zero, thus ignoring those features; in other words, it performed feature selection for us. This is analogous to what happens with Lasso and Ridge regression.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  # Specify L1 regularization lr1 = LogisticRegression(penalty=\u0026#39;l1\u0026#39;) lr2 = LogisticRegression() # penalty=\u0026#39;l2\u0026#39; by default # Instantiate the GridSearchCV object and run the search searcher1 = GridSearchCV(lr1, {\u0026#39;C\u0026#39;:[0.001, 0.01, 0.1, 1, 10]}) searcher1.fit(X_train, y_train) searcher2 = GridSearchCV(lr2, {\u0026#39;C\u0026#39;:[0.001, 0.01, 0.1, 1, 10]}) searcher2.fit(X_train, y_train) # Report the best parameters and the number of nonzero coefficients (selected features) print(\u0026#34;For l1: \u0026#34;) print(\u0026#34;Best CV params\u0026#34;, searcher1.best_params_) best_lr1 = searcher1.best_estimator_ coefs1 = best_lr1.coef_ print(\u0026#34;Total number of features:\u0026#34;, coefs1.size) print(\u0026#34;Number of selected features:\u0026#34;, np.count_nonzero(coefs1)) print(\u0026#34;-------------\\nFor l2: \u0026#34;) print(\u0026#34;Best CV params\u0026#34;, searcher2.best_params_) best_lr2 = searcher2.best_estimator_ coefs2 = best_lr2.coef_ print(\u0026#34;Total number of features:\u0026#34;, coefs2.size) print(\u0026#34;Number of selected features:\u0026#34;, np.count_nonzero(coefs2))   For l1: Best CV params {'C': 1} Total number of features: 2500 Number of selected features: 1218 ------------- For l2: Best CV params {'C': 0.1} Total number of features: 2500 Number of selected features: 2500  scaling features is usually good practice, especially when using regularization.\n Probabilities So far we\u0026rsquo;ve been using logistic regression to make hard predictions, meaning we predict either one class or the other. we\u0026rsquo;ll discuss how to interpret the raw model output of the classifier as a probability. The scikit-learn logistic regression object can output probabilities with the \u0026ldquo;predict_proba\u0026rdquo; function.\nIn this figure, the new interpretation of the colors is the predicted probability of the red class. The black line is the old decision boundary, which we can refer to if we need to make definite, or hard, decisions. We can see that this line is where the probabilities cross 0.5. In other words, if we\u0026rsquo;re more than 50% sure it\u0026rsquo;s red, we predict red, and if we\u0026rsquo;re less than 50% sure it\u0026rsquo;s red, we predict blue. We can also see that we get more and more confident as we move away from the decision boundary, which sounds reasonable. In this figure, regularization is effectively disabled because C is very large.\nThis figure shows what happens when we turn on regularization. We see that the coefficients are smaller, as expected. The effect of regularization is that the probabilities are closer to 0.5; we don\u0026rsquo;t get to the very dark red or very dark blue on the figure. In other words, smaller coefficients mean less confident predictions. This fits with our story: regularization is supposed to combat overfitting, and there\u0026rsquo;s a connection between overconfidence and overfitting. With 2 features, we had 2 coefficients even though you only really need one number to represent the slope of a line. We now have a reason for this: the ratio of the coefficients gives us the slope of the line, and the magnitude of the coefficients gives us our confidence level. Finally, as you can see, regularization not only affects the confidence, but also the orientation of the boundary.\nSo how are these probabilities computed? Like the definite class predictions, they come from the raw model output. The raw model output can be any number, but probabilities are numbers between 0 and 1. So we need a way to \u0026ldquo;squash\u0026rdquo; the raw model output to be between 0 and 1. The sigmoid function takes care of that for us. Here\u0026rsquo;s what it looks like.\nwhen the raw model output is zero, the probability is 0.5 - this means we\u0026rsquo;re right on the boundary. When the raw model output is positive, we would have predicted the positive class, and indeed the probability of the positive class approaches 1. When the raw model output is negative, we would have predicted the negative class, and indeed the probability of the positive class approaches 0, which is another way of saying that we\u0026rsquo;re very confident it\u0026rsquo;s the negative class. Since the raw model output grows as we move away from the boundary, we\u0026rsquo;re more confident in our predictions far away from the boundary.\nMulti-class logistic regression Multi-class classification means having more than 2 classes. While we\u0026rsquo;ve used scikit-learn to perform multi-class classification, all of our conceptual discussions have been in the binary, or 2-class, case. Now, we\u0026rsquo;ll discuss how multi-class classification works for linear classifiers.\none-vs-rest We\u0026rsquo;ll cover two popular approaches to multi-class classification. The first is to train a series of binary classifiers for each class. For example, I\u0026rsquo;ve loaded the wine dataset and instantiated 3 logistic regression classifiers. I\u0026rsquo;ll now fit these classifiers on 3 different data sets. The code y==0 returns an array the same size as y that\u0026rsquo;s True when y is 0 and False otherwise, so the classifier learns to predict these true/false values. In other words, it\u0026rsquo;s a binary classifier learning to discriminate between class 0 or not 0. The next one learns y=1 vs. not 1, etc. This is called the one-vs-rest strategy.\n1 2 3 4 5 6 7 8 9 10 11  w = datasets.load_wine() X = w.data y = w.target lr0 = LogisticRegression() lr1 = LogisticRegression() lr2 = LogisticRegression() lr0.fit(X, y==0) lr1.fit(X, y==1) lr2.fit(X, y==2)   In order to make predictions using one-vs-rest, we take the class whose classifier gives the largest raw model output - or decision_function, in scikit-learn terminology. In this case, the largest raw model output comes from classifier 0. This means it\u0026rsquo;s more confident that the class is 0 than any of the other classes, so we predict class 0.\n1 2 3  print(lr0.decision_function(X)[0]) print(lr0.decision_function(X)[0]) print(lr0.decision_function(X)[0])   5.874266145271733 -4.926950694318412 -7.575077057976998 To perform multiclass classification in sklearn, specify multi_class=ovr when instance the estimator:\n1 2 3  lrovr = LogisticRegression(multi_class=\u0026#39;ovr\u0026#39;) lrovr.fit(X,y) print(lrovr.decision_function(X)[0])   [ 5.87426615 -4.92695069 -7.57507706] multinomial logistic regression Another way to achieve multi-class classification with logistic regression is to modify the loss function so that it directly tries to optimize accuracy on the multi-class problem. You may encounter various words related to this, like multinomial logistic regression, softmax, or cross-entropy loss. This figure shows a comparison of the two approaches.\nIn one case you fit separately for each class, whereas in the other you just do it once. The same goes for prediction. An appealing property of the binary approach is that you can reuse your binary classifier implementation rather than needing a new one. On the other hand, you might sometimes get better accuracy with the multinomial classifier since its loss is more directly aligned with accuracy. In the field of neural networks, the multinomial approach is standard. Finally, while both approaches can work for SVMs, one-vs-rest and related strategies tend to be more popular. By the way, both of these methods can output probabilities just like a binary classifier.\nWe can instantiate the multinomial version by setting the argument mutli_class='multinomial'. In scikit-learn, this also requires changing to a non-default solver like \u0026ldquo;lbfgs\u0026rdquo;. The solver hyperparameter specifies the algorithm used to minimize the loss; the default algorithm is for the binary problem, so it can be used for one-vs-rest but not multinomial. (Currently the multinomial option is supported only by the lbfgs, sag, saga and newton-cg solvers.)\n1 2 3  lrmn = LogisticRegression(multi_class=\u0026#39;multinomial\u0026#39;, solver=\u0026#39;lbfgs\u0026#39;) lrmn.fit(X, y) print(lrmn.decision_function(X)[0])   [ 4.39448794 -1.49967454 -2.8948134 ] coefficients We\u0026rsquo;ve talked a lot about the coefficients, so it\u0026rsquo;s natural to ask, what do the coefficients look like for multi-class classification? Continuing with the wine dataset, let\u0026rsquo;s fit a one-vs-rest model and look at the coefficients.\nIn the binary case we had one coefficient per feature and one intercept. For 3 classes we now have 3 entire binary classifiers, so we end up with one coefficient per feature per class, and one intercept per class. Hence, the coefficients of this model are stored in a 3-by-13 array.\n1 2  print(lrovr.coef_.shape, lrovr.intercept_.shape) print(lrmn.coef_.shape, lrmn.intercept_.shape)   (3, 13) (3,) (3, 13) (3,) As we can see, The multinomial classifier has the same number of coefficients and intercepts as one-vs-rest. Although these two approaches work differently, they learn the same number of parameters and, roughly speaking, the parameters have the same interpretations.\nSupport Vector Machines Support vectors In the last chapter we talked about logistic regression, which is a linear classifier learned with the logistic loss function. Linear SVMs are also linear classifiers, but they use the hinge loss instead.\nThe standard definition of an SVM also includes L2 regularization. The logistic and hinge losses look fairly similar. A key difference is in the \u0026ldquo;flat\u0026rdquo; part of the hinge loss, which occurs when the raw model output is greater than 1, meaning you predicted a sample correctly beyond some margin of error. If a training sample falls in this \u0026ldquo;zero loss\u0026rdquo; region, it doesn\u0026rsquo;t contribute to the fit; if I removed that sample, nothing would change. This is a key property of SVMs.\nSupport vectors are defined as samples that are NOT in the flat part of the loss diagram. In the figure, support vectors are shown with yellow circles around them. Another way of defining support vectors is that they include incorrectly classified samples, as well as correctly classified samples that are close to the boundary. If you\u0026rsquo;re wondering how close is considered close enough, this is controlled by the regularization strength.\nSupport vectors are the samples that matter to your fit. If an sample is not a support vector, removing it has no effect on the model, because its loss was already zero. Comparing with logistic regression, there is no flat part of the loss there, and therefore all data points matter to the fit.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Train a linear SVM svm = SVC(kernel=\u0026#34;linear\u0026#34;) svm.fit(X, y) plot_classifier(X, y, svm, lims=(11,15,0,6)) # Make a new data set keeping only the support vectors print(\u0026#34;Number of original examples\u0026#34;, len(X)) print(\u0026#34;Number of support vectors\u0026#34;, len(svm.support_)) X_small = X[svm.support_] y_small = y[svm.support_] # Train a new SVM using only the support vectors svm_small = SVC(kernel=\u0026#34;linear\u0026#34;) svm_small.fit(X_small, y_small) plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))   Number of original examples 178 Number of support vectors 81 The following diagram shows an SVM fit on a linearly separable dataset. As you can see, the learned boundary falls just half way between the two classes. This is an appealing property: in the absence of other information, this boundary makes more sense than a boundary that is much closer to one class than the other.\nThe yellow lines show the distances from the support vectors to the boundary. The length of the yellow lines, which is the same for all 3 cases, is called the margin. If the regularization strength is not too large, SVMs maximize the margin of linearly separable datasets. Unfortunately, most datasets are not linearly separable; in other words, we don\u0026rsquo;t typically expect a training accuracy of 100%.\nKernel SVMs Consider this 2D toy dataset. The two classes are not linearly separable; in other words, there\u0026rsquo;s no linear boundary that perfectly classifies all the points. If you try fitting a linear SVM on these points, you might get back something that predicts blue everywhere.\nHowever, notice that the red points are all close to the point (0,0) in the coordinate system. Let\u0026rsquo;s create two new features, one of which is feature 1 squared and the other of which is feature 2 squared. That means values near zero will become small values, and values far from zero, both positive and negative, will become large. And now they are linearly separable in this transformed universe, because all the red points are near the lower left and all the blue points are above and to the right. We can fit a linear SVM using these new features and the result is a perfect classification.\nBut then we might ask what does this linear boundary look like back in the original space. In other words, if we took these axes and un-squared them, what would happen to the shape of the boundary? In this case, we get an ellipse.\nIt\u0026rsquo;s that fitting a linear model in a transformed space corresponds to fitting a nonlinear model in the original space. In general, the transformation isn\u0026rsquo;t always going to be squaring and the boundary isn\u0026rsquo;t always going to be an ellipse. In fact, the new space often has a different number of dimensions from the original space! But this is the basic idea. Kernels and kernel SVMs implement feature transformations in a computationally efficient way.\nWe\u0026rsquo;ll need to use scikit-learn\u0026rsquo;s SVC class, rather than LinearSVC, to allow for different kernels. To select kernel, specify argument kernel, the available choice has \u0026lsquo;linear\u0026rsquo;, \u0026lsquo;poly\u0026rsquo;, \u0026lsquo;rbf\u0026rsquo;, \u0026lsquo;sigmoid\u0026rsquo;, \u0026lsquo;precomputed\u0026rsquo;. The default behavior is what\u0026rsquo;s called an rbf or Radial Basis Function kernel. Although it\u0026rsquo;s not computed this way, you can think of this as an extremely complicated transformation of the features, followed by fitting a linear boundary in that new space, just like we saw for the simpler squaring transformation.\nWith kernel SVMs, we can call fit and predict in all the usual ways. Let\u0026rsquo;s look at a decision boundary. This is definitely not linear. And, as a result, we\u0026rsquo;ve gotten a higher training accuracy than we could have with a linear boundary. We can control the shape of the boundary using the hyperparameters. As usual we have the C hyperparameter that controls regularization. The RBF kernel also introduces a new hyperparameter, gamma, which controls the smoothness of the boundary. By decreasing gamma, we can make the boundaries smoother.\nWith the right hyperparameters, RBF SVMs are capable of perfectly separating almost any data set, but it leads to overfitting as well.\nTo search for the gamma that maximizes cross-validation accuracy we use can scikit-learn\u0026rsquo;s GridSearchCV:\n1 2 3 4 5 6 7 8 9 10  # Instantiate an RBF SVM svm = SVC() # Instantiate the GridSearchCV object and run the search parameters = {\u0026#39;gamma\u0026#39;:[0.00001, 0.0001, 0.001, 0.01, 0.1]} searcher = GridSearchCV(svm, parameters) # 5-fold cv by default searcher.fit(X, y) # Report the best parameters print(\u0026#34;Best CV params\u0026#34;, searcher.best_params_)   Best CV params {'gamma': 0.001} In the previous example, the best value of gamma was 0.001 using the default value of C, which is 1. Now, we gonna search for the best combination of C and gamma using GridSearchCV. Even though cross-validation already splits the training set into parts, it\u0026rsquo;s often a good idea to hold out a separate test set to make sure the cross-validation results are sensible.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # Instantiate an RBF SVM svm = SVC() # Instantiate the GridSearchCV object and run the search parameters = {\u0026#39;C\u0026#39;:[0.1, 1, 10], \u0026#39;gamma\u0026#39;:[0.00001, 0.0001, 0.001, 0.01, 0.1]} searcher = GridSearchCV(svm, parameters) searcher.fit(X_train, y_train) # Report the best parameters and the corresponding score print(\u0026#34;Best CV params\u0026#34;, searcher.best_params_) print(\u0026#34;Best CV accuracy\u0026#34;, searcher.best_score_) # Report the test accuracy using these best parameters print(\u0026#34;Test accuracy of best grid search hypers:\u0026#34;, searcher.score(X_test, y_test))   Best CV params {'C': 10, 'gamma': 0.0001} Best CV accuracy 0.9988864142538976 Test accuracy of best grid search hypers: 0.9988876529477196 Comparing logistic regression and SVM (and beyond) Let\u0026rsquo;s summarize the points about logistic regression and SVMs we\u0026rsquo;ve covered.\n Both logistic regression and SVM are linear classifiers. Both can be used with kernels, but this is more common with SVMs because the predictions are much faster when the number of support vectors is small. While both can be coerced to output probabilities, this is much more natural with logistic regression. Both can be extended to multi-class with a one-vs-rest scheme or by directly modifying the loss. In logistic regression, like most methods, all data points affect the fit. SVMs have the special property that only a subset of the examples are \u0026ldquo;support vectors\u0026rdquo; and the rest can be removed without affecting the fit. While the term \u0026ldquo;logistic regression\u0026rdquo; doesn\u0026rsquo;t refer to a particular type of regularization, the term \u0026ldquo;SVM\u0026rdquo; typically refers to a linear classifier that uses the hinge loss and L2 regularization.  Let\u0026rsquo;s compare the use of our two methods in scikit-learn. Logistic regression is imported via sklearn.linear_model.LogisticRegression, and its key hyperparameters have\n C, which controls the amount of regularization: smaller C means more regularization and vice versa; penalty which is the type of regularization: scikit-learn supports L2 and L1. multi_class and solver which extend a binary classifier to multi-class.  There are a bunch more hyperparameters that scikit-learn exposes, but these are the ones most fundamental.\nAs for SVMs, they can be instantiated from sklearn.svm using either LinearSVC for a linear SVM or SVC for a kernel SVM. (You can actually also fit linear SVMs via the SVC class by setting the kernel to 'linear', but you may find LinearSVC to be faster.) The key hyperparameters of the SVC class are\n C, just like with logistic regression. kernel, the type of kernel - we only talked about linear and RBF, but scikit learn supports a couple others. gamma, which only applies to the RBF kernel and controls the smoothness. Smaller values of gamma lead to smoother, or simpler, decision boundaries, and larger values of gamma lead to more complex boundaries.  As with LogisticRegression, there are certainly more hyperparameters.\nFinally, I want to introduce scikit-learn\u0026rsquo;s SGDClassifier. SGD stands for stochastic gradient descent. Although we didn\u0026rsquo;t cover SGD, it\u0026rsquo;s worth knowing about SGDClassifier, since it can handle very large datasets much better than the other methods we\u0026rsquo;ve discussed.\nWe\u0026rsquo;ve been talking about how logistic regression and SVM are just two types of linear classifiers, and SGDClassifier really brings this point home. In fact, to switch between logistic regression and a linear SVM, one only has to set the loss hyperparameter of the SGDClassifier. It\u0026rsquo;s just like we said: the model is the same, and only the loss changes. SGDClassifier works pretty much like the other scikit-learn methods we\u0026rsquo;ve seen. One \u0026ldquo;gotcha\u0026rdquo; with SGDClassifier is that the regularization hyperparameter is called alpha instead of C, alpha is the inverse of C.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # We set random_state=0 for reproducibility  linear_classifier = SGDClassifier(random_state=0) # Instantiate the GridSearchCV object and run the search parameters = { \u0026#39;alpha\u0026#39;:[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \u0026#39;loss\u0026#39;:[\u0026#39;hinge\u0026#39;, \u0026#39;log\u0026#39;], \u0026#39;penalty\u0026#39;:[\u0026#39;l1\u0026#39;, \u0026#39;l2\u0026#39;] } searcher = GridSearchCV(linear_classifier, parameters, cv=10) searcher.fit(X_train, y_train) # Report the best parameters and the corresponding score print(\u0026#34;Best CV params\u0026#34;, searcher.best_params_) print(\u0026#34;Best CV accuracy\u0026#34;, searcher.best_score_) print(\u0026#34;Test accuracy of best grid search hypers:\u0026#34;, searcher.score(X_test, y_test))   Best CV params {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'l1'} Best CV accuracy 0.94351630867144 Test accuracy of best grid search hypers: 0.9592592592592593 Compendium LogisticRegression\n1 2 3 4 5 6 7  LogisticRegression( fit_intercept, penalty C, mutli_class, solver )   sklearn.svm sklearn.svm \n LinearSVC SVC  1 2  LinearSVC(), SVC()   ","date":"2022-01-22T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/machine-learning-iii-linear-classifiers/","title":"Machine Learning III (Linear Classifiers)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Clustering Unsupervised learning is a class of machine learning techniques for discovering patterns in data. It learns without labels. It is pure pattern discovery, unguided by a prediction task.\nKmeans k-means finds a specified number of clusters in the samples. To start,\n  import KMeans from sklearn.cluster.\n  instantiate KMeans, specifying the number of clusters we want to find by n_clusters argument.\n  call the .fit() method of the model, passing the array of samples.\n  use the .predict() method of the model on these same samples. This returns a cluster label for each sample, indicating to which cluster a sample belongs. Alternatively, you can use .fit_predict() method to assemble .fit() and .predict().\n  k-means remembers the mean (or average) of the samples in each cluster. These are called the \u0026ldquo;centroids\u0026rdquo;. We can attain these centroids by referring cluster_centers_ attribute on the model.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  from sklearn.cluster import KMeans import matplotlib.pyplot as plt # Create a KMeans instance with 3 clusters: model kms = KMeans(n_clusters=3) # Fit model to points kms.fit(points) # Determine the cluster labels of new_points: labels labels = kms.predict(new_points) # Print cluster labels of new_points print(labels) # Assign the columns of new_points: xs and ys xs = new_points[:,0] ys = new_points[:,1] # Make a scatter plot of xs and ys, using labels to define the colors plt.scatter(xs, ys, c=labels, alpha=0.5) # Assign the cluster centers: centroids centroids = kms.cluster_centers_ # Assign the columns of centroids: centroids_x, centroids_y centroids_x = centroids[:,0] centroids_y = centroids[:,1] # Make a scatter plot of centroids_x and centroids_y plt.scatter(centroids_x, centroids_y, marker=\u0026#39;D\u0026#39;, s=50) plt.show()   [1 2 0 1 2 1 2 2 2 0 1 2 2 0 0 2 0 0 2 2 0 2 1 2 1 0 2 0 0 1 1 2 2 2 0 1 2 2 1 2 0 1 1 0 1 2 0 0 2 2 2 2 0 0 1 1 0 0 0 1 1 2 2 2 1 2 0 2 1 0 1 1 1 2 1 0 0 1 2 0 1 0 1 2 0 2 0 1 2 2 2 1 2 2 1 0 0 0 0 1 2 1 0 0 1 1 2 1 0 0 1 0 0 0 2 2 2 2 0 0 2 1 2 0 2 1 0 2 0 0 2 0 2 0 1 2 1 1 2 0 1 2 1 1 0 2 2 1 0 1 0 2 1 0 0 1 0 2 2 0 2 0 0 2 2 1 2 2 0 1 0 1 1 2 1 2 2 1 1 0 1 1 1 0 2 2 1 0 1 0 0 2 2 2 1 2 2 2 0 0 1 2 1 1 1 0 2 2 2 2 2 2 0 0 2 0 0 0 0 2 0 0 2 2 1 0 1 1 0 1 0 1 0 2 2 0 2 2 2 0 1 1 0 2 2 0 2 0 0 2 0 0 1 0 1 1 1 2 0 0 0 1 2 1 0 1 0 0 2 1 1 1 0 2 2 2 1 2 0 0 2 1 1 0 1 1 0 1 2 1 0 0 0 0 2 0 0 2 2 1] Evaluating inertia_ We need a way to measure the quality of a clustering that uses only the clusters and the samples themselves. A good clustering has tight clusters, meaning that the samples in each cluster are bunched together, not spread out.\nHow spread out the samples within each cluster are can be measured by the inertia. The inertia of a kmeans model is measured automatically when fit methods are called, and is available afterwards as the inertia_ attribute. Intuitively, inertia measures how far samples are from their centroids. We want clusters that are not spread out, so lower values of the inertia are better.\nIntuitively, inertia decreases monotonically with the number of classes, Thus a good rule of thumb is to choose an elbow in the inertia plot, that is, a point where the inertia begins to decrease more slowly.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  ks = range(1, 6) inertias = [] for k in ks: # Create a KMeans instance with k clusters: model model = KMeans(n_clusters=k) # Fit model to samples model.fit(samples) # Append the inertia to the list of inertias inertias.append(model.inertia_) # Plot ks vs inertias plt.plot(ks, inertias, \u0026#39;-o\u0026#39;) plt.xlabel(\u0026#39;number of clusters, k\u0026#39;) plt.ylabel(\u0026#39;inertia\u0026#39;) plt.xticks(ks) plt.show()   In this case, n_clusters=3 would be a good choice.\ncrosstab If the dataset is labeled before, we can use crosstab to evaluate the clustering.\n Import pandas, create a two-column DataFrame, where the first column is cluster labels and the second column is the real label use the pandas crosstab function to build the cross tabulation, passing the two columns of the DataFrame.  Cross tabulations like these provide great insights into which sort of samples are in which cluster. For example, the grain variety for each sample varieties is given:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  # Create a KMeans model with 3 clusters: model model2 = KMeans(n_clusters=2) model3 = KMeans(n_clusters=3) model4 = KMeans(n_clusters=4) # Use fit_predict to fit model and obtain cluster labels: labels labels2 = model2.fit_predict(samples) labels3 = model3.fit_predict(samples) labels4 = model4.fit_predict(samples) # Create a DataFrame with labels and varieties as columns: df df2 = pd.DataFrame({\u0026#39;labels\u0026#39;: labels2, \u0026#39;varieties\u0026#39;: varieties}) df3 = pd.DataFrame({\u0026#39;labels\u0026#39;: labels3, \u0026#39;varieties\u0026#39;: varieties}) df4 = pd.DataFrame({\u0026#39;labels\u0026#39;: labels4, \u0026#39;varieties\u0026#39;: varieties}) # Create crosstab: ct ct2 = pd.crosstab(df2[\u0026#39;labels\u0026#39;], df2[\u0026#39;varieties\u0026#39;]) ct3 = pd.crosstab(df3[\u0026#39;labels\u0026#39;], df3[\u0026#39;varieties\u0026#39;]) ct4 = pd.crosstab(df4[\u0026#39;labels\u0026#39;], df4[\u0026#39;varieties\u0026#39;]) # Display inertia_ print(model2.inertia_) print(model3.inertia_) print(model4.inertia_) # Display ct print(ct2) print(ct3) print(ct4)   1011.7123453151189 587.318611594043 471.49830938958627 varieties Canadian wheat Kama wheat Rosa wheat labels 0 70 55 1 1 0 15 69 varieties Canadian wheat Kama wheat Rosa wheat labels 0 0 1 60 1 68 9 0 2 2 60 10 varieties Canadian wheat Kama wheat Rosa wheat labels 0 64 8 0 1 0 0 48 2 6 53 0 3 0 9 22 If we assume that each cluster has similar size, then we can see n_cluster=3 is indeed the best choice, just like we concluded from the inertia plot.\nStandardization In sometimes, the features of the dataset have very different variances. In KMeans clustering, the variance of a feature corresponds to its influence on the clustering algorithm. To give every feature a chance, the data needs to be transformed so that features have equal variance. This can be achieved with the StandardScaler from scikit-learn.\n import StandardScaler from sklearn.preprocessing. create a StandardScaler object fit it to the samples. The transform method can now be used to standardize any samples, either the same ones, or completely new ones.  The APIs of StandardScaler and KMeans are similar, but there is an important difference. StandardScaler transforms data, and so has a transform method. KMeans, in contrast, assigns cluster labels to samples, and this done using the predict method.\nSo we need to perform two steps. Firstly, to standardize the data using StandardScaler, and secondly to take the standardized data and cluster it using KMeans. This can be conveniently achieved by combining the two steps using a scikit-learn pipeline. Data then flows from one step into the next, automatically.\n creating a StandardScaler and a KMeans object. import the make_pipeline function from sklearn.pipeline. Apply the make_pipeline function to the steps that we want to compose, in this case, the scaler and the kmeans objects. use the fit method of the pipeline to fit both the scaler and kmeans, and use its predict method to obtain the cluster labels.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  # Perform the necessary imports from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans import pandas as pd # Create scaler: scaler scaler = StandardScaler() # Create KMeans instance: kmeans kmeans = KMeans(n_clusters=4) # Create pipeline: pipeline pipeline = make_pipeline(scaler, kmeans) # Fit the pipeline to samples pipeline.fit(samples) # Calculate the cluster labels: labels labels = pipeline.predict(samples) # Create a DataFrame with labels and species as columns: df df = pd.DataFrame( { \u0026#39;labels\u0026#39; : labels, \u0026#39;species\u0026#39; : species } ) # Create crosstab: ct ct = pd.crosstab(df[\u0026#39;labels\u0026#39;], df[\u0026#39;species\u0026#39;]) # Display ct print(ct) # Un-standardized Data kmeans2 = KMeans(n_clusters=4) labels2 = kmeans2.fit_predict(samples) df2 = pd.DataFrame( { \u0026#39;labels\u0026#39; : labels2, \u0026#39;species\u0026#39; : species } ) ct2 = pd.crosstab(df2[\u0026#39;labels\u0026#39;], df2[\u0026#39;species\u0026#39;]) print(ct2)   species Bream Pike Roach Smelt labels 0 0 17 0 0 1 33 0 1 0 2 0 0 0 13 3 1 0 19 1 species Bream Pike Roach Smelt labels 0 17 10 3 0 1 0 4 0 0 2 1 1 17 14 3 16 2 0 0 We can see that standardization improves the clustering ability.\nThere are others kinds of data transformation methods in scikit-learn, for example MaxAbsScaler and Normalizer:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # Import Normalizer from sklearn.preprocessing import Normalizer from sklearn.cluster import KMeans import pandas as pd # Create a normalizer: normalizer normalizer = Normalizer() # Create a KMeans model with 10 clusters: kmeans kmeans = KMeans(n_clusters=10) # Make a pipeline chaining normalizer and kmeans: pipeline pipeline = make_pipeline(normalizer, kmeans) # Fit pipeline to the daily price movements pipeline.fit(movements) # Predict the cluster labels: labels labels = pipeline.predict(movements) # Create a DataFrame aligning labels and companies: df df = pd.DataFrame({\u0026#39;labels\u0026#39;: labels, \u0026#39;companies\u0026#39;: companies}) # Display df sorted by cluster label print(df.sample(10))    labels companies 3 1 American express 7 5 Canon 47 4 Symantec 41 8 Philip Morris 14 4 Dell 1 1 AIG 34 5 Mitsubishi 9 2 Colgate-Palmolive 4 3 Boeing 10 7 ConocoPhillips Hierarchical Clustering and t-SNE Hierarchical clustering visualization Hierarchical clustering can organize any sort of data into a hierarchy. Let\u0026rsquo;s consider a new type of dataset, describing how countries scored performances at the Eurovision 2016 song contest. The result of applying hierarchical clustering to the Eurovision scores can be visualized as a tree-like diagram called a \u0026ldquo;dendrogram\u0026rdquo;. This single picture reveals a great deal of information about the voting behavior of countries at the Eurovision.\nHierarchical clustering proceeds in steps. In the beginning, every country is its own cluster - so there are as many clusters as there are countries. At each step, the two closest clusters are merged. This decreases the number of clusters, and eventually, there is only one cluster left, and it contains all the countries. The entire process of the hierarchical clustering is encoded in the dendrogram. To understand better, let\u0026rsquo;s zoom in and look at just one part of this dendrogram.\nIn the beginning, there are six clusters, each containing only one country. The first merging is the node where the Cyprus and Greece are merged together. Later on, the new cluster is merged with the cluster containing Bulgaria. Shortly after that, the clusters containing Moldova and Russia are merged, which later is in turn merged with the cluster containing Armenia. Later still, the two big composite clusters are merged together. This process continues until there is only one cluster left, and it contains all the countries.\nTo perform a hierarchical clustering:\n import the linkage and dendrogram functions from scipy.cluster.hierarchy. Use the linkage function to obtain a hierarchical clustering of the samples. Notice there is an extra method argument method='complete', we\u0026rsquo;ll cover this later. pass the output of linkage to the dendrogram function to visualize the result. For the dendrogram, we\u0026rsquo;ll need a list of varieties of each observation to specify the labels argument.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # Perform the necessary imports from scipy.cluster.hierarchy import linkage, dendrogram import matplotlib.pyplot as plt # Calculate the linkage: mergings mergings = linkage(samples, method=\u0026#39;complete\u0026#39;) # Plot the dendrogram, using varieties as labels dendrogram(mergings, labels=varieties, leaf_rotation=90, leaf_font_size=6, ) print(samples.shape, len(varieties)) plt.show()   (42, 7) 42 cluster labels An intermediate stage in the hierarchical clustering is specified by choosing a height on the dendrogram. For example, choosing a height of 15 defines a clustering in which Bulgaria, Cyprus and Greece are in one cluster, Russia and Moldova are in another, and Armenia is in a cluster on its own.\nThe y-axis of the dendrogram encodes the distance between merging clusters. For example, the distance between the cluster containing Cyprus and the cluster containing Greece was approximately 6 when they were merged into a single cluster. When this new cluster was merged with the cluster containing Bulgaria, the distance between them was 12. So the height that specifies an intermediate clustering corresponds to a distance. This specifies that the hierarchical clustering should stop merging clusters when all clusters are at least this far apart.\nThe distance between two clusters is measured using a \u0026ldquo;linkage method\u0026rdquo;. In our example, we used \u0026ldquo;complete\u0026rdquo; linkage, where the distance between two clusters is the maximum of the distances between their samples. This was specified via the \u0026ldquo;method\u0026rdquo; parameter. There are many other linkage methods, like \u0026ldquo;single\u0026rdquo; linkage, the distance between two clusters is the minimum of the distances between their samples. See more method here.\nThe cluster labels for any intermediate stage of the hierarchical clustering can be extracted using the fcluster function, specifying the linkage returned, height and criterion='distance'. This returns a numpy array containing the cluster labels for all the countries.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  # Perform the necessary imports import matplotlib.pyplot as plt from scipy.cluster.hierarchy import linkage, dendrogram, fcluster import pandas as pd # Calculate the linkage: mergings mergings = linkage(samples, method=\u0026#39;single\u0026#39;) # Plot the dendrogram dendrogram( mergings, labels=country_names, leaf_rotation=90, leaf_font_size=6, ) plt.show() # Use fcluster to extract labels: labels labels = fcluster(mergings, 6, criterion=\u0026#39;distance\u0026#39;) # Create a DataFrame with labels and varieties as columns: df df = pd.DataFrame({\u0026#39;labels\u0026#39;: labels, \u0026#39;varieties\u0026#39;: varieties}) # Create crosstab: ct ct = pd.crosstab(df[\u0026#39;labels\u0026#39;], df[\u0026#39;varieties\u0026#39;]) # Display ct print(ct)   varieties Canadian wheat Kama wheat Rosa wheat labels 1 14 3 0 2 0 0 14 3 0 11 0 t-SNE t-SNE stands for \u0026ldquo;t-distributed stochastic neighbor embedding\u0026rdquo;. It maps samples from their high-dimensional space into a 2 or 3 dimensional space so they can visualized.\nt-SNE is available in scikit-learn, but it works a little differently to the fit/transform components we\u0026rsquo;ve already met. The samples are in a 2-dimensional numpy array samples, and there is a list variety_numbers giving the species of each sample.\n import TSNE from sklearn.manifold create a TSNE object, with argument learning_rate. apply the fit_transform() method to the samples, and then make a scatter plot of the result, coloring the points using the species.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # Import TSNE from sklearn.manifold import TSNE # Create a TSNE instance: model tsne = TSNE(learning_rate=200) # Apply fit_transform to samples: tsne_features tsne_features = tsne.fit_transform(samples) # Select the 0th feature: xs xs = tsne_features[:,0] # Select the 1st feature: ys ys = tsne_features[:,1] # Scatter plot, coloring by variety_numbers plt.scatter(xs, ys, c=variety_numbers) plt.show()   There are three aspects that deserve special attention:\n the fit_transform method t-SNE only has a fit_transform method which simultaneously fits the model and transforms the data. However, t-SNE does not have separate fit and transform methods. This means that you can\u0026rsquo;t extend a t-SNE map to include new samples. Instead, you have to start over each time. and the learning_rate. The learning_rate makes the use of t-SNE more complicated than some other techniques. You may need to try different learning rates for different datasets. Normally it\u0026rsquo;s enough to try a few values between 50 and 200. A final thing to be aware of is that the axes of a t-SNE plot do not have any interpretable meaning. In fact, they are different every time t-SNE is applied, even on the same data.  t-SNE provides great visualizations when the individual samples can be labeled. Here we apply t-SNE to the company stock price data. The stock price movements for each company are in the array normalized_movements (which have been normalized). The list companies gives the name of each company.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # Import TSNE from sklearn.manifold import TSNE # Create a TSNE instance: model tsne = TSNE(learning_rate=50) # Apply fit_transform to normalized_movements: tsne_features tsne_features = tsne.fit_transform(normalized_movements) # Select the 0th feature: xs xs = tsne_features[:,0] # Select the 1th feature: ys ys = tsne_features[:,1] # Scatter plot plt.scatter(xs, ys, alpha=0.5) # Annotate the points for x, y, company in zip(xs, ys, companies): plt.annotate(company, (x, y), fontsize=5, alpha=0.75) plt.show()   Dimension Reduction Dimension reduction finds patterns in data, and uses these patterns to re-express it in a compressed form. This makes subsequent computation with the data much more efficient, and this can be a big deal in a world of big datasets. However, the most important function of dimension reduction is to reduce a dataset to its \u0026ldquo;bare bones\u0026rdquo;, discarding noisy features that cause big problems for supervised learning tasks like regression and classification. In many real-world applications, it\u0026rsquo;s dimension reduction that makes prediction possible.\nPCA The most fundamental of dimension reduction techniques is called \u0026ldquo;Principal Component Analysis\u0026rdquo;, or \u0026ldquo;PCA\u0026rdquo; for short. PCA performs dimension reduction in two steps, and the first one, called \u0026ldquo;de-correlation\u0026rdquo;, doesn\u0026rsquo;t change the dimension of the data at all. It\u0026rsquo;s this first step that we\u0026rsquo;ll focus on in this section.\nIn this first step, PCA rotates the samples so that they are aligned with the coordinate axes; Furthermore PCA also shifts the samples so that they have mean zero. These scatter plots show the effect of PCA applied to two features of the wine dataset. Notice that no information is lost - this is true no matter how many features the dataset has.\nscikit-learn has an implementation of PCA de-correlation, and it has fit and transform methods just like StandardScaler. The fit method learns how to shift and how to rotate the samples, but doesn\u0026rsquo;t actually change them. The transform method, on the other hand, applies the transformation that fit learned. In particular, the transform method can be applied to new, unseen samples.\n import PCA from sklearn.decomposition create a PCA object, and fit it to the samples. use the fit PCA object to transform the samples.  This returns a new array of transformed samples. This new array has the same number of rows and columns as the original sample array, each row and column represents a transformed sample and feature respectively.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  # Perform the necessary imports import matplotlib.pyplot as plt from scipy.stats import pearsonr from sklearn.decomposition import PCA # Create PCA instance: model pca = PCA() # Apply the fit_transform method of model to grains: pca_features pca_features = pca.fit_transform(grains) # Assign 0th column of pca_features: xs xs = pca_features[:,0] # Assign 1st column of pca_features: ys ys = pca_features[:,1] # Assign the 0th column of grains: width width = grains[:, 0] # Assign the 1st column of grains: length length = grains[:, 1] # Calculate the Pearson correlation correlationo, pvalueo = pearsonr(width, length) correlationt, pvaluet = pearsonr(xs, ys) # Display the correlation print(\u0026#39;origin corr: \u0026#39;, correlationo) print(\u0026#39;transformed corr: \u0026#39;, correlationt) # Scatter plot xs vs ys plt.scatter(width, length, label=\u0026#39;origin\u0026#39;) plt.scatter(xs, ys, label=\u0026#39;transform\u0026#39;) plt.axis(\u0026#39;equal\u0026#39;) plt.legend() plt.show()   Finally, PCA is called \u0026ldquo;principal component analysis\u0026rdquo; because it learns the \u0026ldquo;principal components\u0026rdquo; of the data. These are the directions in which the samples vary the most, depicted here in red. It is the principal components that PCA aligns with the coordinate axes. After a PCA model has been fit, the principal components are available as the components_ attribute. This is numpy array with one row for each principal component.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  # Make a scatter plot of the untransformed points plt.scatter(grains[:,0], grains[:,1]) # Create a PCA instance: model pca = PCA() # Fit model to points pca.fit(grains) # Get the mean of the grain samples: mean mean = pca.mean_ # Get the first principal component: first_pc first_pc0 = pca.components_[0,:] first_pc1 = pca.components_[1,:] print(pca.components_) # Plot first_pc as an arrow, starting at mean plt.arrow(mean[0], mean[1], first_pc0[0], first_pc0[1], color=\u0026#39;red\u0026#39;, width=0.01) plt.arrow(mean[0], mean[1], first_pc1[0], first_pc1[1], color=\u0026#39;red\u0026#39;, width=0.01) # Keep axes on same scale plt.axis(\u0026#39;equal\u0026#39;) plt.show()   [[ 0.63910027 0.76912343] [-0.76912343 0.63910027]] Intrinsic dimension The intrinsic dimension of a dataset is the number of features required to approximate it. The intrinsic dimension informs dimension reduction, because it tells us how much a dataset can be compressed.\nTo better illustrate the intrinsic dimension, let\u0026rsquo;s consider an example dataset containing only some of the samples from the iris dataset. Specifically, let\u0026rsquo;s take three measurements from the iris versicolor samples: sepal length, sepal width, and petal width. So each sample is represented as a point in 3-dimensional space. However, if we make a 3d scatter plot of the samples, we see that they all lie very close to a flat, 2-dimensional sheet. This means that the data can be approximated by using only two coordinates, without losing much information. So this dataset has intrinsic dimension 2.\nBut scatter plots are only possible if there are 3 features or less. So how can the intrinsic dimension be identified, even if there are many features? This is where PCA is really helpful. The intrinsic dimension can be identified by counting the PCA features that have high variance.\nTo see how, let\u0026rsquo;s see what happens when PCA is applied to the dataset of versicolor samples. PCA rotates and shifts the samples to align them with the coordinate axes. This expresses the samples using three PCA features. The PCA features are in a special order. Here is a bar graph showing the variance of each of the PCA features. As you can see, each PCA feature has less variance than the last, and in this case the last PCA feature has very low variance. This agrees with the scatter plot of the PCA features, where the samples don\u0026rsquo;t vary much in the vertical direction. In the other two directions, however, the variance is apparent.\nThe intrinsic dimension is the number of PCA features that have significant variance. In our example, only the first two PCA features have significant variance. So this dataset has intrinsic dimension 2, which agrees with what we observed when inspecting the scatter plot.\nTo plot the variances of the PCA features in practice:\n imports PCA from sklearn.decomposition. create a PCA model pca, and fit it to the samples. create a range enumerating the number of PCA features which is available as the .n_components_ attribute of pca make a bar plot of the variances; the variances are available as the explained_variance attribute of the pca  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # Perform the necessary imports from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline import matplotlib.pyplot as plt # Create scaler: scaler scaler = StandardScaler() # Create a PCA instance: pca pca = PCA() # Create pipeline: pipeline pipeline = make_pipeline(scaler, pca) # Fit the pipeline to \u0026#39;samples\u0026#39; pipeline.fit(samples) # Plot the explained variances features = range(pca.n_components_) plt.bar(features, pca.explained_variance_) plt.xlabel(\u0026#39;PCA feature\u0026#39;) plt.ylabel(\u0026#39;variance\u0026#39;) plt.xticks(features) plt.show()   Dimension reduction with PCA Dimension reduction represents the same data using less features and is vital for building machine learning pipelines using real-world data. We\u0026rsquo;ve seen already that the PCA features are in decreasing order of variance. PCA performs dimension reduction by discarding the PCA features with lower variance, which it assumes to be noise, and retaining the higher variance PCA features, which it assumes to be informative.\nTo use PCA for dimension reduction, you need to specify how many PCA features to keep. A good choice is the intrinsic dimension of the dataset, if you know it:\n import PCA as usual. create a PCA model specifying n_components=2, fit the model and transform the samples as usual.  Printing the shape of the transformed samples, we see that there are only two features.\n1 2 3 4 5 6 7 8 9 10 11 12 13  # Import PCA from sklearn.decomposition import PCA # Create a PCA model with 2 components: pca pca = PCA(n_components=2) # Fit and transform the PCA instance to the scaled samples pca.fit(scaled_samples) pca_features = pca.transform(scaled_samples) # Print the shape of pca_features print(scaled_samples.shape) print(pca_features.shape)   (85, 6) (85, 2) PCA discards the low variance features, and assumes that the higher variance features are informative. Like all assumptions, there are cases where this doesn\u0026rsquo;t hold.\nTruncatedSVD In some cases, an alternative implementation of PCA needs to be used. Word frequency arrays are a great example. In a word-frequency array, each row corresponds to a document, and each column corresponds to a word from a fixed vocabulary. The entries of the word-frequency array measure how often each word appears in each document. Only some of the words from the vocabulary appear in any one document, so most entries of the word frequency array are zero.\nArrays like this are said to be \u0026ldquo;sparse\u0026rdquo;, and are often represented using a special type of array called a \u0026ldquo;scipy.sparse.csr_matrix\u0026rdquo;. csr_matrices save space by remembering only the non-zero entries of the array.\nFor example, TfidfVectorizer transforms a list of documents into a word frequency array, which it outputs as a csr_matrix. It has fit() and transform() methods like other sklearn objects. documents is a list of toy documents about pets:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # Import TfidfVectorizer from sklearn.feature_extraction.text import TfidfVectorizer # Create a TfidfVectorizer: tfidf tfidf = TfidfVectorizer() # Apply fit_transform to document: csr_mat csr_mat = tfidf.fit_transform(documents) # Print result of toarray() method print(csr_mat.toarray()) # Get the words: words words = tfidf.get_feature_names() # Print words print(words)   [[0.51785612 0. 0. 0.68091856 0.51785612 0. ] [0. 0. 0.51785612 0. 0.51785612 0.68091856] [0.51785612 0.68091856 0.51785612 0. 0. 0. ]] ['cats', 'chase', 'dogs', 'meow', 'say', 'woof'] Scikit-learn\u0026rsquo;s PCA doesn\u0026rsquo;t support csr_matrices, and we\u0026rsquo;ll need to use sklearn.decomposition.TruncatedSVD instead. TruncatedSVD performs the same transformation as PCA, but accepts csr_matrices as input. Other than that, you interact with TruncatedSVD and PCA in exactly the same way.\nNow we will combine our knowledge of TruncatedSVD and kmeans to cluster some popular pages from Wikipedia.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  # Perform the necessary imports from sklearn.decomposition import TruncatedSVD from sklearn.cluster import KMeans from sklearn.pipeline import make_pipeline import pandas as pd # Create a TruncatedSVD instance: svd svd = TruncatedSVD(n_components=50) # Create a KMeans instance: kmeans kmeans = KMeans(n_clusters=6) # Create a pipeline: pipeline pipeline = make_pipeline(svd, kmeans) # Fit the pipeline to articles pipeline.fit(articles) # Calculate the cluster labels: labels labels = pipeline.predict(articles) # Create a DataFrame aligning labels and titles: df df = pd.DataFrame({\u0026#39;label\u0026#39;: labels, \u0026#39;article\u0026#39;: titles}) # Display df sorted by cluster label print(df.sort_values(\u0026#39;label\u0026#39;))    label article 59 0 Adam Levine 57 0 Red Hot Chili Peppers 56 0 Skrillex 55 0 Black Sabbath 54 0 Arctic Monkeys 53 0 Stevie Nicks 52 0 The Wanted 51 0 Nate Ruess 50 0 Chad Kroeger 58 0 Sepsis 30 1 France national football team 31 1 Cristiano Ronaldo 32 1 Arsenal F.C. 33 1 Radamel Falcao 37 1 Football 35 1 Colombia national football team 36 1 2014 FIFA World Cup qualification 38 1 Neymar 39 1 Franck Ribry 34 1 Zlatan Ibrahimovi 26 2 Mila Kunis 28 2 Anne Hathaway 27 2 Dakota Fanning 25 2 Russell Crowe 29 2 Jennifer Aniston 23 2 Catherine Zeta-Jones 22 2 Denzel Washington 21 2 Michael Fassbender 20 2 Angelina Jolie 24 2 Jessica Biel 10 3 Global warming 11 3 Nationally Appropriate Mitigation Action 13 3 Connie Hedegaard 14 3 Climate change 12 3 Nigel Lawson 16 3 350.org 17 3 Greenhouse gas emissions by the United States 18 3 2010 United Nations Climate Change Conference 19 3 2007 United Nations Climate Change Conference 15 3 Kyoto Protocol 8 4 Firefox 1 4 Alexa Internet 2 4 Internet Explorer 3 4 HTTP cookie 4 4 Google Search 5 4 Tumblr 6 4 Hypertext Transfer Protocol 7 4 Social search 49 4 Lymphoma 42 4 Doxycycline 47 4 Fever 46 4 Prednisone 44 4 Gout 43 4 Leukemia 9 4 LinkedIn 48 4 Gabapentin 0 4 HTTP 404 45 5 Hepatitis C 41 5 Hepatitis B 40 5 Tonsillitis Non-negative Matrix Factorization (NMF) Basics of NMF NMF stands for \u0026ldquo;non-negative matrix factorization\u0026rdquo;. NMF, like PCA, is a dimension reduction technique. In constract to PCA, however, NMF models are interpretable. This means an NMF models are easier to understand yourself, and much easier for you to explain to others. NMF can not be applied to every dataset, however. It is required that the sample features be \u0026ldquo;non-negative\u0026rdquo;, so greater than or equal to 0. NMF achieves its interpretability by decomposing samples as sums of their parts. For example, NMF decomposes documents as combinations of common themes, and images as combinations of common patterns.\nNMF is available in scikit learn, and follows the same fit/transform pattern as PCA. However, unlike PCA, the argument n_components must always be specified. NMF works both with numpy arrays and sparse arrays in the csr_matrix format.\nLet\u0026rsquo;s see an application of NMF to a toy example of a word-frequency array articles. Each row represents a document, and each column represents a word. And the entries of the array measure the frequency of each word in each document using what\u0026rsquo;s known as \u0026ldquo;tf-idf\u0026rdquo;. \u0026ldquo;tf\u0026rdquo; is the frequency of the word in the document. \u0026ldquo;idf\u0026rdquo; is a weighting scheme that reduces the influence of frequent words like \u0026ldquo;the\u0026rdquo;. Let\u0026rsquo;s now see how to use NMF in Python.\n import NMF from sklearn.decomposition. create a model, specifying the desired number of components n_components=2. fit the model to the samples, then transform to obtain the dimension-reducted features.  As we saw with PCA, our transformed data in this example will have 6 columns, corresponding to our 6 new features.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  # Import NMF and pandas from sklearn.decomposition import NMF import pandas as pd # Create an NMF instance: model nmf = NMF(n_components=6) # Fit the model to articles nmf.fit(articles) # Transform the articles: nmf_features nmf_features = nmf.transform(articles) # Print the NMF features print(articles.shape) print(nmf_features.shape) print(nmf_features.round(2)[1:10,:]) print(\u0026#39;--------------------------------\u0026#39;) # Create a pandas DataFrame: df df = pd.DataFrame(nmf_features, index=titles) # Print the row for \u0026#39;Anne Hathaway\u0026#39; print(df.loc[\u0026#39;Anne Hathaway\u0026#39;]) # Print the row for \u0026#39;Denzel Washington\u0026#39; print(df.loc[\u0026#39;Denzel Washington\u0026#39;])   (60, 13125) (60, 6) [[0. 0. 0. 0. 0. 0.57] [0. 0. 0. 0. 0. 0.4 ] [0. 0. 0. 0. 0. 0.38] [0. 0. 0. 0. 0. 0.49] [0.01 0.01 0.01 0.03 0. 0.33] [0. 0. 0.02 0. 0.01 0.36] [0. 0. 0. 0. 0. 0.49] [0.02 0.01 0. 0.02 0.03 0.48] [0.01 0.03 0.03 0.07 0.02 0.34]] -------------------------------- 0 0.004 1 0.000 2 0.000 3 0.576 4 0.000 5 0.000 Name: Anne Hathaway, dtype: float64 0 0.000 1 0.006 2 0.000 3 0.422 4 0.000 5 0.000 Name: Denzel Washington, dtype: float64 The NMF feature values are always non-negative. The entries of the NMF components are non-negative, as well. Just as PCA has principal components, NMF has components which it learns from the samples, and as with PCA, the dimension of the components is the same as the dimension of the samples. These components are avaliable as components_ attribute, each row represents one component and each column represents the projection of a component on each feature dimension, just like PCA.\n1 2  print(nmf.components_.round(2)) print(nmf.components_.shape)   [[0.01 0. 0. ... 0. 0. 0. ] [0. 0. 0.01 ... 0. 0. 0. ] [0. 0. 0. ... 0. 0. 0. ] [0. 0. 0. ... 0. 0.01 0. ] [0. 0. 0. ... 0. 0. 0. ] [0. 0. 0.01 ... 0. 0. 0. ]] (6, 13125) The features and the components of an NMF model can be combined to approximately reconstruct the original data samples: if we multiply each NMF components by the corresponding NMF feature value, and add up each column, we get something very close to the original sample. So a sample can be reconstructed by multiplying the NMF components by the NMF feature values of the sample, and adding up. This calculation also can be expressed as a product of matrice, that\u0026rsquo;s where the \u0026ldquo;matrix factorization\u0026rdquo;, or \u0026ldquo;MF\u0026rdquo;, in NMF comes from.\n1 2 3  print(nmf_features.shape) print(nmf.components_.shape) print(articles.shape)   (60, 6) (6, 13125) (60, 13125) So the feature matrix $\\cdot$ the components matrix is almost the original sample matrix.\nNMF learns interpretable parts Text analysis The components of NMF represent patterns that frequently occur in the samples. Let\u0026rsquo;s consider a concrete example, where scientific articles are represented by their word frequencies. There are 20000 articles, and 800 words. So the array has 800 columns.\nLet\u0026rsquo;s fit an NMF model with 10 components to the articles. The 10 components are stored as the 10 rows of a 2-dimensional numpy array. The rows, or components, live in an 800-dimensional space - there is one dimension for each of the words. Aligning the words of our vocabulary with the columns of the NMF components allows them to be interpreted. Choosing a component, and looking at which words have the highest values,\nwe see that they fit a theme: the words are \u0026lsquo;species\u0026rsquo;, \u0026lsquo;plant\u0026rsquo;, \u0026lsquo;plants\u0026rsquo;, \u0026lsquo;genetic\u0026rsquo;, \u0026lsquo;evolution\u0026rsquo; and \u0026lsquo;life\u0026rsquo;. So if NMF is applied to documents, then the components correspond to topics, and the NMF features reconstruct the documents from the topics.\nwords is a list of the words that label the columns of the word-frequency array:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # Import pandas import pandas as pd # Create a DataFrame: components_df components_df = pd.DataFrame(nmf.components_, columns=words) # Print the shape of the DataFrame print(components_df.shape) # Select row 3: component component = components_df.iloc[3] # Print result of nlargest print(component.nlargest())   (6, 13125) film 0.628 award 0.253 starred 0.245 role 0.211 actress 0.186 Name: 3, dtype: float64 Grayscale image An image in which all the pixels are shades of gray ranging from black to white is called a \u0026ldquo;grayscale image\u0026rdquo;. Since there are only shades of grey, a grayscale image can be encoded by the brightness of every pixel. Representing the brightness as a number between 0 and 1, where 0 is totally black and 1 is totally white, the image can be represented as 2-dimensional array of numbers. These 2-dimensional arrays of numbers can then be flattened by enumerating the entries. For instance, we could read-off the values row-by-row, from left-to-right and top to bottom. The grayscale image is now represented by a flat array of non-negative numbers.\nA collection of grayscale images of the same size can thus be encoded as a 2-dimensional array, in which each row represents an image as a flattened array, and each column represents a pixel, and the pixels as features, we see that the data is arranged similarly to the word frequency array. Indeed, the entries of this array are non-negative, so NMF can be used to learn the parts of the images. To recover the image, use the reshape method of the sample, specifying the dimensions of the original image as a tuple. This yields the 2-dimensional array of pixel brightnesses. To display the corresponding image, import pyplot, and pass the 2-dimensional array to the plt.imshow function.\nIf NMF is applied to a collection of images, then the NMF components represent patterns that frequently occur in the images. For instance, NMF decomposes images from an LED display into the individual cells of the display.\nSuppose we are given 100 images as a 2D array samples, where each row represents a single 13x8 image. The images in the dataset are pictures of a LED digital display:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  # Import pyplot from matplotlib import pyplot as plt # Select the 0th row: digit digit = samples[0,:] print(samples.shape) # Print digit print(digit) print(digit.shape) print(\u0026#39;-----------------------\u0026#39;) # Reshape digit to a 13x8 array: bitmap bitmap = digit.reshape(13,-1) # Print bitmap print(bitmap) print(bitmap.shape) print(\u0026#39;-----------------------\u0026#39;) # Use plt.imshow to display bitmap plt.imshow(bitmap, cmap=\u0026#39;gray\u0026#39;, interpolation=\u0026#39;nearest\u0026#39;) plt.colorbar() plt.show()   (100, 104) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (104,) ----------------------- [[0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 1. 1. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.]] (13, 8) ----------------------- 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  # Import NMF from sklearn.decomposition import NMF # Create an NMF model: model model = NMF(n_components=7) # Apply fit_transform to samples: features features = model.fit_transform(samples) # Call show_as_image on each component def show_as_image(sample): bitmap = sample.reshape((13, 8)) ax[i].imshow( bitmap, cmap=\u0026#39;gray\u0026#39;, interpolation=\u0026#39;nearest\u0026#39; ) fig, ax = plt.subplots(1, 7) i=0 for component in model.components_: show_as_image(component) i+=1 plt.show() # Select the 0th row of features: digit_features digit_features = features[0,:] # Print digit_features print(digit_features)   [4.76823559e-01 0.00000000e+00 0.00000000e+00 5.90605054e-01 4.81559442e-01 0.00000000e+00 7.37562716e-16] Thus we can see that NMF transform the feature form Pixel (100,104) to Cell (100,7). And each of components (7,104) represents the Pixel of the Cell.\nUnlike NMF, PCA doesn\u0026rsquo;t learn the parts of things. Its components do not correspond to topics (in the case of documents) or to parts of images, when trained on images. Notice that the components of PCA do not represent meaningful parts of images of LED digits!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # Import PCA from sklearn.decomposition import PCA # Create a PCA instance: model model = PCA(n_components=7) # Apply fit_transform to samples: features features = model.fit_transform(samples) # Call show_as_image on each component def show_as_image(sample): bitmap = sample.reshape((13, 8)) ax[i].imshow( bitmap, cmap=\u0026#39;gray\u0026#39;, interpolation=\u0026#39;nearest\u0026#39; ) fig, ax = plt.subplots(1, 7) i=0 for component in model.components_: show_as_image(component) i+=1 plt.show()   Although NMF has the ability to learn patterns, the effect of learning depends on the parameters n_components. For example, when studying LED images, the best value for this parameter is 7, because LEDs have only 7 beads:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  models = [NMF(n_components=k) for k in range(5,10)] features = [model.fit_transform(samples) for model in models] def show_as_image(sample): bitmap = sample.reshape((13, 8)) ax[i,j].imshow( bitmap, cmap=\u0026#39;gray\u0026#39;, interpolation=\u0026#39;nearest\u0026#39; ) fig, ax = plt.subplots(5, 9) i, j = 0, 0 for m in models: mc = m.components_ for c in mc: show_as_image(c) j += 1 j = 0 ax[i,0].set_ylabel(f\u0026#39;n={i+5}\u0026#39;) i += 1 plt.show()   Building recommender systems using NMF Given an article, how can we find articles that have similar topics? Our strategy for solving this problem is to apply NMF to the word-frequency array of the articles, and to use the resulting NMF features. We have known these NMF features describe the topic mixture of an article. So similar articles will have similar NMF features.\nSuppose that we are given a word frequency array articles corresponding to the collection of newspaper articles in question. Import NMF, create the model, and use the fit_transform method to obtain the transformed articles. Now we\u0026rsquo;ve got NMF features for every article, given by the columns of the new array. Now we need to define how to compare articles using their NMF features. Similar documents have similar topics, but it isn\u0026rsquo;t always the case that the NMF feature values are exactly the same. For instance, one version of a document might use very direct language, whereas other versions might interleave the same content with meaningless chatter. Meaningless chatter reduces the frequency of the topic words overall, which reduces the values of the NMF features representing the topics. However, on a scatter plot of the NMF features, all these versions lie on a single line passing through the origin.\nFor this reason, when comparing two documents, it\u0026rsquo;s a good idea to compare these lines. We\u0026rsquo;ll compare them using what is known as the cosine similarity, which uses the angle between the two lines. Higher values indicate greater similarity.\nWith the help of a pandas DataFrame, we can label the similarities with the article titles. To compute the cosine similarity.\n import the normalize function from sklearn.preprocessing, apply it to the array of all NMF features. create a DataFrame whose rows are the normalized features, using the titles as an index. use the loc method of the DataFrame to select the normalized feature values for the current article calculate the cosine similarities using the dot method of the DataFrame. use the nlargest method of the resulting pandas Series to find the articles with the highest cosine similarity.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  # Perform the necessary imports from sklearn.decomposition import NMF import pandas as pd from sklearn.preprocessing import normalize # Create an NMF instance: model nmf = NMF(n_components=6) # Fit the model to articles nmf.fit(articles) # Transform the articles: nmf_features nmf_features = nmf.transform(articles) # Normalize the NMF features: norm_features norm_features = normalize(nmf_features) # Create a DataFrame: df df = pd.DataFrame(norm_features, index=titles) # Select the row corresponding to \u0026#39;Cristiano Ronaldo\u0026#39;: article article = df.loc[\u0026#39;Cristiano Ronaldo\u0026#39;] # Compute the dot products: similarities similarities = df.dot(article) # Display those with the largest cosine similarity print(similarities.nlargest(10))   Cristiano Ronaldo 1.000 Franck Ribry 1.000 Radamel Falcao 1.000 Zlatan Ibrahimovi 1.000 France national football team 1.000 Colombia national football team 1.000 Neymar 0.999 2014 FIFA World Cup qualification 0.998 Arsenal F.C. 0.998 Football 0.975 dtype: float64 Suppose that we are given a sparse array artists whose rows correspond to artists and whose columns correspond to users. The entries give the number of times each artist was listened to by each user. The names of the musical artists are available as the list artist_names.\nWe will build a pipeline and transform the array into normalized NMF features. MaxAbsScaler, transforms the data so that all users have the same influence on the model, regardless of how many different artists they\u0026rsquo;ve listened to. After fit_transform was called on the pipeline , norm_features is an array containing the normalized NMF features as rows.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # Perform the necessary imports from sklearn.decomposition import NMF from sklearn.preprocessing import Normalizer, MaxAbsScaler from sklearn.pipeline import make_pipeline import pandas as pd # Create a MaxAbsScaler: scaler scaler = MaxAbsScaler() # Create an NMF model: nmf nmf = NMF(n_components=20) # Create a Normalizer: normalizer normalizer = Normalizer() # Create a pipeline: pipeline pipeline = make_pipeline(scaler, nmf, normalizer) # Apply fit_transform to artists: norm_features norm_features = pipeline.fit_transform(artists) # Create a DataFrame: df df = pd.DataFrame(norm_features, index=artist_names) # Select row of \u0026#39;Bruce Springsteen\u0026#39;: artist artist = df.loc[\u0026#39;Bruce Springsteen\u0026#39;] # Compute cosine similarities: similarities similarities = df.dot(artist) # Display those with highest cosine similarity print(similarities.nlargest())   Bruce Springsteen 1.000 Neil Young 0.956 Van Morrison 0.872 Leonard Cohen 0.865 Bob Dylan 0.859 dtype: float64 Compendium sklearn.cluster\n1 2 3 4 5 6 7  sklearn.cluster KMeans(n_clusters=6) .fit() .predict() .fit_predict() .cluster_centers_ .inertia_   sklearn.manifold\n1 2 3 4  sklearn.manifold TSNE() .learning_rate .fit_transform()   sklearn.decomposition\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  sklearn.decomposition PCA(n_components=5) .fit() .transform() .fit_transform() .components_ .n_components_ .explained_variance_ TruncatedSVD(n_components=5) NMF(n_components=6) .fit() .transform() .components_   scipy.cluster.hierarchy\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  scipy.cluster.hierarchy linkage(samples, method=\u0026#39;single\u0026#39;) dendrogram( linkage(samples, method=\u0026#39;single\u0026#39;), labels=, leaf_rotation=90, leaf_font_size=6, ) fcluster( linkage(samples, method=\u0026#39;single\u0026#39;), 6, criterion=\u0026#39;distance\u0026#39; )   ","date":"2022-01-15T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/machine-learning-ii-unsupervised-learning/","title":"Machine Learning II (Unsupervised Learning)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Classification k-Nearest Neighbors We have a set of labeled data and we want to build a classifier that takes unlabeled data as input and outputs a label. We first need choose a type of classifier and it needs to learn from the already labeled data. For this reason, we call the already labeled data the training data.\nThere is a simple algorithm called K-nearest neighbors (KNN). The basic idea of KNN, is to predict the label of any data point by looking at the K, for example, 3, closest labeled data points and getting them to vote on what label the unlabeled point should have.\nIn this image, there\u0026rsquo;s an example of KNN in two dimensions: if k equals 3, you would classify it as red\nif k equals 5, as green.\nAll machine learning models in scikit-learn are implemented as python classes. These classes serve two purposes: they implement the algorithms for learning a model, and predicting, while also storing all the information that is learned from the data.\n Training a model on the data is also called fitting the model to the data. In scikit-learn, we use the .fit() method to do this. The .predict() method is what we use to predict the label of an, unlabeled data point.  To fit our first classifier using scikit-learn,\n We first need to import KNeighborsClassifier from sklearn.neighbors. We then instantiate our KNeighborsClassifier, set the number of neighbors k to argument n_neighbors, and assign it to the the instance (the classifier). Then we can fit this classifier to our training set, the labeled data. To do so, we apply the method .fit() to the classifier and pass it two arguments: the features as a NumPy array and the labels (or target) as a NumPy array. When we fit the classifier, it returns the classifier itself and modifies it to fit it to the data. And then we can use it to predict on some unlabeled data. We use the .predict() method on the classifier and pass it the data to be predicted. And it return a array with a prediction for each observation or row in these data.   The scikit-learn API requires firstly that you have the data as a NumPy array or pandas DataFrame, and the data with features in columns and observations in rows; It also requires that the features take on continuous values, such as the price of a house, as opposed to categories, such as \u0026lsquo;male\u0026rsquo; or \u0026lsquo;female\u0026rsquo;. It also requires that there are no missing values in the data.\n Now take an overview of our data:\n1 2 3 4 5 6 7  print(\u0026#39;Data To Be Trained: \u0026#39;) print(df.info()) print(\u0026#39;------------------------------------------\u0026#39;) print(df[\u0026#39;party\u0026#39;].head()) # target print(\u0026#39;------------------------------------------\u0026#39;) print(\u0026#39;Data To Be Predicted: \u0026#39;) print(X_new.info())   Data To Be Trained: \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 435 entries, 0 to 434 ### \u0026lt;-- 435 observations (samples) Data columns (total 17 columns): ### \u0026lt;-- 16 feature + 1 target party 435 non-null object infants 435 non-null int64 water 435 non-null int64 budget 435 non-null int64 physician 435 non-null int64 salvador 435 non-null int64 religious 435 non-null int64 satellite 435 non-null int64 aid 435 non-null int64 missile 435 non-null int64 immigration 435 non-null int64 synfuels 435 non-null int64 education 435 non-null int64 superfund 435 non-null int64 crime 435 non-null int64 duty_free_exports 435 non-null int64 eaa_rsa 435 non-null int64 dtypes: int64(16), object(1) memory usage: 57.9+ KB None ------------------------------------------ 0 republican 1 republican 2 democrat 3 democrat 4 democrat Name: party, dtype: object ------------------------------------------ Data To Be Predicted: \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 1 entries, 0 to 0 Data columns (total 16 columns): 0 1 non-null float64 1 1 non-null float64 2 1 non-null float64 3 1 non-null float64 4 1 non-null float64 5 1 non-null float64 6 1 non-null float64 7 1 non-null float64 8 1 non-null float64 9 1 non-null float64 10 1 non-null float64 11 1 non-null float64 12 1 non-null float64 13 1 non-null float64 14 1 non-null float64 15 1 non-null float64 dtypes: float64(16) memory usage: 208.0 bytes None The first column of df, df['party'] is the target, the following 16 column df.iloc[:, 1:17] is the feature.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # Import KNeighborsClassifier from sklearn.neighbors from sklearn.neighbors import KNeighborsClassifier # Create arrays for the features and the response variable y = df[\u0026#39;party\u0026#39;] X = df.iloc[:, 1:17] # Create a k-NN classifier with 6 neighbors: knn knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the data knn.fit(X, y) # Predict and print the label for the new data point X_new new_prediction = knn.predict(X_new) print(\u0026#34;Prediction: {}\u0026#34;.format(new_prediction))   Prediction: ['democrat'] Measuring model performance Now that we know how to fit a classifier and use it to predict the labels of previously unseen data, we need to figure out how to measure its performance. In classification problems, accuracy is a commonly-used metric. The accuracy of a classifier is defined as the number of correct predictions divided by the total number of data points.\nTo created data to compute accuracy, a common practice is to split the data into two sets, a training set and a test set. You train or fit the classifier on the training set. Then you make predictions on the labeled test set and compare these predictions with the known labels. You then compute the accuracy of your predictions.\nTo do this, we first import train_test_split from sklearn.model_selection. We then use the train_test_split function to randomly split our data.\n The first argument will be the feature data X, the second the targets or labels y. The test_size argument specifies what proportion of the original data is used for the test set. The random_state kwarg sets a seed for the random number generator that splits the data into train and test. Setting the seed with the same argument later will allow you to reproduce the exact split and your downstream results. It is also best practice to perform our split so that the split reflects the labels on the data. That is, we want the labels to be distributed in train and test sets as they are in the original dataset. To achieve this, we use the keyword argument stratify equals y, where y the list or array containing the labels.  train_test_split returns four arrays: X train, X test, y train, and y test in that order, respectively. By default, train test split splits the data into 75% training data and 25% test data, which is a good rule of thumb.\nWe then instantiate our K-nearest neighbors classifier, fit it to the training data using the .fit() method. To check out the accuracy of our model, we use the .score() method of the classifier and pass it X_test and y_test (note that we can even do not call .predict()).\nGenerally, complex models (in these case, small k) run the risk of being sensitive to noise in the specific data that you have, rather than reflecting general trends in the data. This is know as overfitting. But if you increase k even more and make the model even simpler, then the model will perform less well on both test and training sets, as indicated in this schematic figure, known as a model complexity curve. This is called underfitting.\nIn the following context, we\u0026rsquo;ll work with the MNIST digits recognition dataset, which is one of scikit-learn\u0026rsquo;s included datasets. The dataset contains 1797 samples, each sample in this scikit-learn dataset is an 8x8 image representing a handwritten digit. Each pixel is represented by an integer in the range 0 to 16, indicating varying levels of black.\n1 2 3 4 5 6 7 8 9 10  from sklearn import datasets import matplotlib.pyplot as plt # Load the digits dataset: digits digits = datasets.load_digits() print(digits.keys()) print(digits.images.shape) print(digits.data.shape) print(digits.images[0])   dict_keys(['data', 'target', 'target_names', 'images', 'DESCR']) (1797, 8, 8) (1797, 64) [[ 0. 0. 5. 13. 9. 1. 0. 0.] [ 0. 0. 13. 15. 10. 15. 5. 0.] [ 0. 3. 15. 2. 0. 11. 8. 0.] [ 0. 4. 12. 0. 0. 8. 8. 0.] [ 0. 5. 8. 0. 0. 9. 8. 0.] [ 0. 4. 11. 0. 1. 12. 7. 0.] [ 0. 2. 14. 5. 10. 12. 0. 0.] [ 0. 0. 6. 13. 10. 0. 0. 0.]] Recall that scikit-learn\u0026rsquo;s built-in datasets are of type Bunch, which are dictionary-like objects. Helpfully for the MNIST dataset, scikit-learn provides an \u0026lsquo;images\u0026rsquo; key in addition to the \u0026lsquo;data\u0026rsquo; and \u0026lsquo;target\u0026rsquo; keys. Because it is a 2D array of the images corresponding to each sample, this \u0026lsquo;images\u0026rsquo; key is useful for visualizing the images. On the other hand, the \u0026lsquo;data\u0026rsquo; key contains the feature array - that is, the \u0026lsquo;images\u0026rsquo; as a flattened array of 64 pixels.\nNotice that you can access the keys of these Bunch objects in two different ways: By using the . notation, as in digits.images, or the [] notation, as in digits['images'].\n1 2 3 4 5 6 7 8 9 10 11 12  # # Display some digits fig, ax = plt.subplots(5, 5) for i in range(5): for j in range(5): ax[i, j].imshow( digits.images[i + j], cmap=plt.cm.gray_r, interpolation=\u0026#39;nearest\u0026#39; ) plt.show()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  # Import necessary modules from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split # Create feature and target arrays X = digits.data y = digits.target # Split into training and test set X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.2, random_state=42, stratify=y ) # Create a k-NN classifier with 7 neighbors: knn knn = KNeighborsClassifier(n_neighbors=7) # Fit the classifier to the training data knn.fit(X_train, y_train) # Print the accuracy print(knn.score(X_test, y_test))   0.9833333333333333 We will compute and plot the training and testing accuracy scores for a variety of different neighbor values. By observing how the accuracy scores differ for the training and testing sets with different values of k, we will develop the intuition for overfitting and underfitting.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  # Setup arrays to store different k neighbors = np.arange(1, 9) # Setup arrays to store train and test accuracies train_accuracy = np.empty(len(neighbors)) test_accuracy = np.empty(len(neighbors)) # Loop over different values of k for i, k in enumerate(neighbors): # Setup a k-NN Classifier with k neighbors: knn knn = KNeighborsClassifier(n_neighbors=k) # Fit the classifier to the training data knn.fit(X_train, y_train) #Compute accuracy on the training set train_accuracy[i] = knn.score(X_train, y_train) #Compute accuracy on the testing set test_accuracy[i] = knn.score(X_test, y_test) # Generate plot plt.title(\u0026#39;k-NN: Varying Number of Neighbors\u0026#39;) plt.plot(neighbors, test_accuracy, label = \u0026#39;Testing Accuracy\u0026#39;) plt.plot(neighbors, train_accuracy, label = \u0026#39;Training Accuracy\u0026#39;) plt.legend() plt.xlabel(\u0026#39;Number of Neighbors\u0026#39;) plt.ylabel(\u0026#39;Accuracy\u0026#39;) plt.show()   Regression Checking the type of X and y, we see that both are NumPy arrays. To turn them into NumPy arrays of the desired shape (2D array), we apply the reshape method to keep the first dimension, but add another dimension of size one to X and y.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # Import numpy and pandas import numpy as np import pandas as pd # Read the CSV file into a DataFrame: df df = pd.read_csv(\u0026#39;gapminder.csv\u0026#39;) # Create arrays for features and target variable y = df.life X = df.fertility # Print the dimensions of y and X before reshaping print(\u0026#34;Dimensions of y before reshaping: \u0026#34;, y.shape) print(\u0026#34;Dimensions of X before reshaping: \u0026#34;, X.shape) # Reshape X and y y_reshaped = y.reshape(-1,1) X_reshaped = X.reshape(-1,1) # Print the dimensions of y_reshaped and X_reshaped print(\u0026#34;Dimensions of y after reshaping: \u0026#34;, y_reshaped.shape) print(\u0026#34;Dimensions of X after reshaping: \u0026#34;, X_reshaped.shape)   Dimensions of y before reshaping: (139,) Dimensions of X before reshaping: (139,) Dimensions of y after reshaping: (139, 1) Dimensions of X after reshaping: (139, 1) OLS The API of regression is almost same as classification.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # Import LinearRegression from sklearn.linear_model import LinearRegression # Create the regressor: reg reg = LinearRegression() # Fit the model to the data reg.fit(X_fertility, y) # Print R^2  print(reg.score(X_fertility, y)) # Create the regression line prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1) y_pred = reg.predict(prediction_space) # Plot scatter plot and regression line plt.scatter(X_fertility, y) # scatter plot plt.plot(prediction_space, y_pred, color=\u0026#39;black\u0026#39;, linewidth=3) # regression line plt.show()   0.6192442167740035 Multi-variable regression is totally like single-variable regression:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  # Import necessary modules from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split # Create training and test sets X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.3, random_state=42 ) print(f\u0026#34;Shape of X: {X.shape},\\nShape of y: {y.shape}\u0026#34;) # Create the regressor: reg_all reg_all = LinearRegression() # Fit the regressor to the training data reg_all.fit(X_train, y_train) # Predict on the test data: y_pred y_pred = reg_all.predict(X_test) # Compute and print R^2 and RMSE print(\u0026#34;R^2: {}\u0026#34;.format(reg_all.score(X_test, y_test))) rmse = np.sqrt(mean_squared_error(y_test, y_pred)) print(\u0026#34;Root Mean Squared Error: {}\u0026#34;.format(rmse))   Shape of X: (139, 8), Shape of y: (139,) R^2: 0.838046873142936 Root Mean Squared Error: 3.2476010800377213 Cross-validation We have talked about train test split and computing model performance metrics on test set. But there is a potential pitfall of this process: if you\u0026rsquo;re computing R^2 on the test set, the R^2 returned is dependent on the way that you split up the data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size = 0.3, random_state=41) X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size = 0.3, random_state=42) X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size = 0.3, random_state=43) reg1 = LinearRegression() reg2 = LinearRegression() reg3 = LinearRegression() reg1.fit(X_train1, y_train1) reg2.fit(X_train2, y_train2) reg3.fit(X_train3, y_train3) print(\u0026#34;R^2 for reg1: {}\u0026#34;.format(reg1.score(X_test1, y_test1))) print(\u0026#34;R^2 for reg2: {}\u0026#34;.format(reg2.score(X_test2, y_test2))) print(\u0026#34;R^2 for reg3: {}\u0026#34;.format(reg3.score(X_test3, y_test3)))   R^2 for reg1: 0.8785550178323325 R^2 for reg2: 0.838046873142936 R^2 for reg3: 0.8852965321157533 You can see that 0.83 and 0.88 is a big distinction. The data points in the test set may have some peculiarities that mean the R^2 computed on it is not representative of the model\u0026rsquo;s ability to generalize to unseen data.\nTo combat this dependence on what is essentially an arbitrary split, we use a technique called cross-validation. We begin by splitting the dataset into five folds. Then we hold out the first fold as a test set, fit our model on the remaining four folds, predict on the test set, and compute the metric of interest. Next, we hold out the second fold as our test set, fit on the remaining data, predict on the test set, and compute the metric of interest. Then similarly with the third, fourth, and fifth fold.\nAs a result we get five values of R^2 from which we can compute statistics of interest, such as the mean and median and 95% confidence intervals. As we split the dataset into five folds, we call this process 5-fold cross validation. More generally, if you use k folds, it is called k-fold cross validation or k-fold CV.\nTo perform k-fold CV in scikit-learn, we first import cross_val_score from sklearn.model_selection. Note that cross_val_score is a function:\n1  print(type(cross_val_score))   \u0026lt;class 'function'\u0026gt; Then, we instantiate our model. We then call cross_val_score with the regressor reg, the feature data X, and the target data y as the first three positional arguments. (Note that we don\u0026rsquo;t need to split X, y into train and test here). We also specify the number of folds with the keyword argument, cv:\n1  cv_results = cross_val_score(reg, X, y, cv=5)    Note that cross_val_score is just a function that performs splitting and fits model on splitted data repeatedly, thus you can pass others model in it, like the instance of Rigid or Lasso.\n This returns an array of cross-validation scores, which we assign to cv_results. The length of the array is the number of folds utilized. Note that the score reported is R^2, as this is the default score for linear regression.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # Import the necessary modules from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score # Create a linear regression object: reg reg = LinearRegression() # Compute 5-fold cross-validation scores: cv_scores5 cv_scores5 = cross_val_score(reg, X, y, cv=5) print(cv_scores5) print(\u0026#34;Average 5-Fold CV Score: {}\u0026#34;.format(np.mean(cv_scores5))) # Compute 7-fold cross-validation scores: cv_scores7 cv_scores7 = cross_val_score(reg, X, y, cv=7) print(cv_scores7) print(\u0026#34;Average 7-Fold CV Score: {}\u0026#34;.format(np.mean(cv_scores7)))   [0.81720569 0.82917058 0.90214134 0.80633989 0.94495637] Average 5-Fold CV Score: 0.8599627722793232 [0.66129291 0.90449445 0.8350383 0.90814522 0.7705185 0.88507953 0.96175008] Average 7-Fold CV Score: 0.8466169980423074 Regularized regression Recall that what fitting a linear regression does is minimize a loss function to choose a coefficient $a_i$ for each feature variable. If we allow these coefficients or parameters to be super large, we can get over-fitting. If your data sit in a high-dimensional space with large coefficients, it gets easy to predict nearly anything. For this reason, it is common practice to alter the loss function so that it penalizes for large coefficients. This is called regularization. We will introduce two class Ridge and Lasso, which are similar with LinearRegression.\nRidge regression The first type of regularized regression that we\u0026rsquo;ll look at is called ridge regression in which our loss function is the standard OLS loss function plus the squared value of each coefficient multiplied by some constant alpha. $$ Loss_{rigid} = Loss_{OLS} + \\alpha\\cdot \\sum_{i=1}^{n}a_{i}^{2} $$\nThus, when minimizing the loss function to fit to our data, models are penalized for coefficients with a large magnitude: large positive and large negative coefficients. Note that $\\alpha$ is a parameter we need to choose in order to fit and predict. Essentially, we can select the $\\alpha$ for which our model performs best. Picking $\\alpha$ for ridge regression is similar to picking k in KNN. This is called hyperparameter tuning. This $\\alpha$ can be thought of as a parameter that controls model complexity (recall that more complex the model is, the more likely it is to over-fitting).\nNotice that when $\\alpha$ is equal to zero, we get back OLS. Large coefficients in this case are not penalized and the over-fitting problem is not accounted for. A very high alpha means that large coefficients are significantly penalized, which can lead to a model that is too simple and ends up under-fitting the data.\nThe method of performing ridge regression with scikit-learn mirrors the other models that we have seen.\n Import Ridge from sklearn.linear_model, Instantiate class Ridge, with argument alpha and normalize. Setting normalize=True ensures that all our variables are on the same scale and we will talk about this in more depth later. Split our data into test and train, .fit() on the training, and .predict() on the test.  Recall that Rigid is a class, you can specify argument by\n1  rigid = Rigid(alpha=0.1, argument=True)   or\n1 2 3  rigid = Rigid() rigid.alpha=0.1 rigid.argument=True   Now, we will practice fitting ridge regression models over a range of different alphas, and plot cross-validated scores for each.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  # Import necessary modules from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # Setup the array of alphas and lists to store scores alpha_space = np.logspace(-4, 0, 50) ridge_scores = [] ridge_scores_std = [] # Create a ridge regressor: ridge ridge = Ridge(normalize=True) # Compute scores over range of alphas for alpha in alpha_space: # Specify the alpha value to use: ridge.alpha ridge.alpha = alpha # Perform 10-fold CV: ridge_cv_scores ridge_cv_scores = cross_val_score(ridge, X, y, cv=10) # Append the mean of ridge_cv_scores to ridge_scores ridge_scores.append(np.mean(ridge_cv_scores)) # Append the std of ridge_cv_scores to ridge_scores_std ridge_scores_std.append(np.std(ridge_cv_scores)) # Plots the score as well as standard error for each alpha: def display_plot(cv_scores, cv_scores_std): fig = plt.figure() ax = fig.add_subplot(1,1,1) ax.plot(alpha_space, cv_scores) std_error = cv_scores_std / np.sqrt(10) ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2) ax.set_ylabel(\u0026#39;CV Score +/- Std Error\u0026#39;) ax.set_xlabel(\u0026#39;Alpha\u0026#39;) ax.axhline(np.max(cv_scores), linestyle=\u0026#39;--\u0026#39;, color=\u0026#39;.5\u0026#39;) ax.set_xlim([alpha_space[0], alpha_space[-1]]) ax.set_xscale(\u0026#39;log\u0026#39;) plt.show() display_plot(ridge_scores, ridge_scores_std)   Lasso regression There is another type of regularized regression called lasso regression, in which our loss function is the standard OLS loss function plus the absolute value of each coefficient multiplied by some constant alpha.\n$$ Loss_{lasso} = Loss_{OLS} + \\alpha\\cdot \\sum_{i=1}^{n}|a_{i}| $$\nThe method of performing lasso regression in scikit-learn mirrors ridge regression: we import Lasso from sklearn.linear_model, and the others steps are totally same.\nOne of the really cool aspects of lasso regression is that it can be used to select important features of a dataset. This is because it tends to shrink the coefficients of less important features to be exactly zero. The features whose coefficients are not shrunk to zero are \u0026lsquo;selected\u0026rsquo; by the LASSO algorithm. Thus we can extract the coefficients of variables (features) to investigate the importance of each one.\nTo show this, we import Lasso as before. We then instantiate our regressor, .fit() it to the data as always. Then we can extract the coef_ attribute and plot the features and their coef_.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # Import Lasso from sklearn.linear_model import Lasso # Instantiate a lasso regressor: lasso lasso = Lasso(alpha=0.4, normalize=True) # Fit the regressor to the data lasso.fit(X,y) # Compute and print the coefficients lasso_coef = lasso.coef_ print(lasso_coef) # Plot the coefficients plt.plot(range(len(df_columns)), lasso_coef) plt.xticks(range(len(df_columns)), df_columns.values, rotation=60) plt.margins(0.02) plt.show()   [-0. -0. -0. 0. 0. 0. -0. -0.07087587] Fine-tuning Metrics for classification In classification, we\u0026rsquo;ve used accuracy, the fraction of correctly classified samples, to measure model performance. However, accuracy is not always a useful metric. Consider a spam classification problem in which 99% of emails are real and only 1% are spam. I could build a model that classifies all emails as real; this model would be correct 99% of the time and thus have an accuracy of 99%, which is great. However, this naive classifier does a horrible job of predicting spam: it never predicts spam at all, so it completely fails at its original purpose.\nThe situation when one class is more frequent is called class imbalance because the class of real emails contains way more instances than the class of spam. This is a very common situation in practice and requires a more nuanced metric to assess the performance of our model.\nGiven a binary classifier, such as our spam email example, we can draw up a 2-by-2 matrix that summarizes predictive performance called a confusion matrix: across the top are the predicted labels, down the side the actual labels:\nGiven any model, we can fill in the confusion matrix according to its predictions. In the top left square, we have the number of spam emails correctly labeled; in the bottom right square, we have the number of real emails correctly labeled; in the top right, the number of spam emails incorrectly labeled; and in the bottom left, the number of real emails incorrectly labeled. Note that correctly labeled spam emails are referred to as true positives and correctly labeled real emails as true negatives. While incorrectly labeled spam will be referred to as false negatives and incorrectly labeled real emails as false positives.\nUsually, the \u0026ldquo;class of interest\u0026rdquo; is called the positive class. As we are trying to detect spam, this makes spam the positive class. Notice that we can retrieve accuracy from the confusion matrix: it\u0026rsquo;s the sum of the diagonal divided by the total sum of the matrix.\n$$ accuracy = \\frac{tp+tn}{tp+fp+tn+fn} =\\frac{t}{t+f} . $$\nThere are several other important metrics we can easily calculate from the confusion matrix. Precision, which is the number of true positives divided by the total number of true positives and false positives. It is also called the positive predictive value or PPV.\n$$ precision = \\frac{tp}{tp+fp} = \\frac{tp}{p} $$\nIn our case, this is the number of correctly labeled spam emails divided by the total number of emails classified as spam.\nRecall, which is the number of true positives divided by the total number of true positives and false negatives. This is also called sensitivity, hit rate, or true positive rate.\n$$ recall = \\frac{tp}{tp+fn} $$\nThe F1-score is defined as two times the product of the precision and recall divided by the sum of the precision and recall, in other words, it\u0026rsquo;s the harmonic mean of precision and recall.\n$$ f1=2\\times \\frac{prs \\cdot rcl}{prs+rcl} $$\nTo put it in plain language,\n high precision means that our classifier had a low false positive rate, that is, not many real emails were predicted as being spam. high recall means that our classifier predicted most positive or spam emails correctly.  To compute the confusion matrix, along with the metrics for the classifier,\n we import classification_report and confusion_matrix from sklearn.metrics. As always, we instantiate our classifier, split the data into train and test, .fit() the training data, and .predict() the labels of the test set. To compute the confusion matrix, we pass the test set labels (y_test) and the predicted labels (y_pred) to the function confusion_matrix. To compute the resulting metrics, we pass the same arguments to classification_report, which outputs a string containing all the relevant metrics.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # Import necessary modules from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix # Create training and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Instantiate a k-NN classifier: knn knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the training data knn.fit(X_train, y_train) # Predict the labels of the test data: y_pred y_pred = knn.predict(X_test) # Generate the confusion matrix and classification report print(confusion_matrix(y_test, y_pred)) print(\u0026#34;------------------------------\u0026#34;) print(classification_report(y_test, y_pred))   [[176 30] [ 52 50]] ------------------------------ precision recall f1-score support 0 0.77 0.85 0.81 206 1 0.62 0.49 0.55 102 avg / total 0.72 0.73 0.72 308 Note that the definition of positive and negative are entirely subjective. Thus conventionally, for all metrics in scikit-learn, the first argument is always the true label (y_test) and the prediction (y_label) is always the second argument. Then the first line (0) of the output of classification_report is what we need. The second line (1) is the inverse of positive and negative of (0), for example, precision of (0) is tp/(tp+fp), but precision of (1) if tn/(tn+fn); and recall of (0) is tp/(tp+fn), but recall of (1) if tn/(tn+fp).\nLogistic regression and the ROC curve Building a logistic regression model Logistic regression, despite its name, is used in classification problems, not regression problems. When we have two possible labels for the target variable.\nUsing logistic regression in scikit-learn follows exactly the same formula that we have known so well:\n perform the necessary imports, instantiate the classifier, split data into training and test sets, .fit() the model on the training data, and .predict() on the test set.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # Import the necessary modules from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, classification_report # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42) # Create the classifier: logreg logreg = LogisticRegression() # Fit the classifier to the training data logreg.fit(X_train, y_train) # Predict the labels of the test set: y_pred y_pred = logreg.predict(X_test) # Compute and print the confusion matrix and classification report print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred))   [[176 30] [ 35 67]] precision recall f1-score support 0 0.83 0.85 0.84 206 1 0.69 0.66 0.67 102 avg / total 0.79 0.79 0.79 308 Notice that in defining logistic regression, we have specified a threshold of 0.5 for the probability, a threshold that defines our model. This is not particular for log reg but also could be used for KNN. When we call .predict(), given one feature, log reg will calculate a probability, p which is independent with the threshold, with respect to the target variable. If p is greater than 0.5, label will be predicted as \u0026lsquo;1\u0026rsquo;; and if less than 0.5, label will be \u0026lsquo;0\u0026rsquo;.\nTo extract the probability which is calculated by log reg, we call predict_proba method and pass the test data (X_test), predict_proba returns an array with two columns which contain the probabilities for the 0 and 1 respectively.\n1  print(logreg.predict_proba(X_test)[0:5,:])   [[0.60409835 0.39590165] [0.76042394 0.23957606] [0.79670177 0.20329823] [0.77236009 0.22763991] [0.57194882 0.42805118]] plot ROC When the threshold equals zero, the model predicts \u0026lsquo;1\u0026rsquo; for all the data, which means the true positive rate (tp/(tp+fn)) is equal to the false positive rate (fp/(fp+tn)) is equal to one. When the threshold equals \u0026lsquo;1\u0026rsquo;, the model predicts \u0026lsquo;0\u0026rsquo; for all data, which means that both true and false positive rates are 0.\nIf we vary the threshold between these two extremes, we get a series of different false positive and true positive rates. The set of points we get when trying all possible thresholds is called the receiver operating characteristic curve or ROC curve.\nTo plot the ROC curve, after instantiating LogisticRegression, fitting the model:\n import roc_curve from sklearn.metrics; call predict_proba(X_text) on the model; then call the function roc_curve; the first argument is given by the actual labels (y_test), the second by the predicted probabilities for 1.  Here we used the predicted probabilities for \u0026ldquo;1\u0026rdquo; (positive), which is the second column of the outcome of predict_prob. Note that these probabilities don\u0026rsquo;t rely on any threshold. Given a threshold t, the observations in X_test whose probabilities for \u0026ldquo;1\u0026rdquo; are greater than t will be labeled by 1 (w.r.t. threshold t), and the others will be labeled by 0 (w.r.t. threshold t). And then, combining with y_test, we can calculate the tpr and fpr (w.r.t. threshold t), which performs a point on ROC.\n We unpack the result into three variables: false positive rate, FPR; true positive rate, TPR; and the thresholds. We can then plot the FPR and TPR using plt.plot function.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # Import necessary modules from sklearn.metrics import roc_curve # Compute predicted probabilities: y_pred_prob y_pred_prob = logreg.predict_proba(X_test)[:,1] # Generate ROC curve values: fpr, tpr, thresholds fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob) # Plot ROC curve plt.plot([0, 1], [0, 1], \u0026#39;k--\u0026#39;) plt.plot(fpr, tpr) plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) plt.title(\u0026#39;ROC Curve\u0026#39;) plt.show()   Calculate AUC Given the ROC curve, we can extract a metric of interest. Because the larger the area under the ROC curve, the better our model is. The way to think about this is the following: if we had a model which produced an ROC curve that had a single point at (1,0), the upper left corner, representing a true positive rate of one and a false positive rate of zero, this would be a great model. For this reason, the area under the ROC, commonly denoted as AUC, is another popular metric for classification models.\nTo compute the AUC,\n import roc_auc_score from sklearn.metrics, instantiate the classifier, split our data into train and test sets, and fit the model to the training set, compute the predicted probabilities by call ing predict_proba on logreg, pass the true labels (y_test) and the predicted probabilities to roc_auc_score.  We can also compute the AUC using cross-validation. To do so,\n import and use the function cross_val_score as before, passing it the estimator logreg, the features X, and the target y, and cv. We then additionally pass it the keyword argument scoring=\u0026quot;roc auc\u0026quot;.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Import necessary modules from sklearn.metrics import roc_auc_score from sklearn.model_selection import cross_val_score # Compute predicted probabilities: y_pred_prob y_pred_prob = logreg.predict_proba(X_test)[:,1] # Compute and print AUC score print(\u0026#34;AUC: {}\u0026#34;.format(roc_auc_score(y_test, y_pred_prob))) # Compute cross-validated AUC scores: cv_auc cv_auc = cross_val_score(logreg, X, y, cv=5, scoring=\u0026#34;roc_auc\u0026#34;) # Print list of AUC scores print(\u0026#34;AUC scores computed using 5-fold cross-validation: {}\u0026#34;.format(cv_auc))   AUC: 0.8254806777079764 AUC scores computed using 5-fold cross-validation: [0.80148148 0.8062963 0.81481481 0.86245283 0.8554717 ] Hyperparameter tuning We have seen that when fitting a linear regression, what we are really doing is choosing parameters for the model that fit the data the best. We also saw that we had to choose a value for the alpha in ridge and lasso regression before fitting it. Analogously, before fitting and predicting K-nearest neighbors, we need to choose n_neighbors. Such parameters, ones that need to be specified before fitting a model, are called hyperparameters. In other words, these are parameters that cannot be explicitly learned by fitting the model.\nHerein lies a fundamental key for building a successful model: choosing the correct hyperparameter. The basic idea is to try a whole bunch of different values, fit all of them separately, see how well each performs, and choose the best one. This is called hyperparameter tuning. Now, when fitting different values of a hyperparameter, it is essential to use cross-validation as using train test split alone would risk overfitting the hyperparameter to the test set.\nThe basic idea is as follows: we choose a grid of possible values we want to try for the hyperparameter or hyperparameters. We then perform k-fold cross-validation for each point in the grid, that is, for each pair of hyperparameters. We then choose for our model the choice of hyperparameters that performed the best.\nThis is called a grid search and in scikit-learn we implement it using the class GridSearchCV.\n  import GridSearchCV from sklearn.model_selection.\n  then specify the hyperparameter as a dictionary in which the keys are the hyperparameter names, such as n_neighbors in KNN or alpha in lasso regression. The values in the grid dictionary are lists containing the values we wish to tune the relevant hyperparameter or hyperparameters over. If we specify multiple parameters, all possible combinations will be tried.\n  instantiate classifier.\n  then use GridSearchCV and pass it\n our model (instance), the grid we wish to tune over the number of folds cv that we wish to use.  This returns a GridSearch object that you can then call .fit() with data (X and y) on it to performs the actual grid search and cross validation.\n  We can then apply the attributes best_params_ and best_score_, respectively, to retrieve the hyperparameters that perform the best along with the mean cross-validation score over that fold.\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # Import necessary modules from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Setup the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {\u0026#39;C\u0026#39;: c_space} # Instantiate a logistic regression classifier: logreg logreg = LogisticRegression() # Instantiate the GridSearchCV object: logreg_cv logreg_cv = GridSearchCV(logreg, param_grid, cv=5) # Fit it to the data logreg_cv.fit(X,y) # Print the tuned parameters and score print(\u0026#34;Tuned Logistic Regression Parameters: {}\u0026#34;.format(logreg_cv.best_params_)) print(\u0026#34;Best score is {}\u0026#34;.format(logreg_cv.best_score_))   Tuned Logistic Regression Parameters: {'C': 3.727593720314938} Best score is 0.7708333333333334 GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV, in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions.\nFor example, Decision trees have many parameters that can be tuned, such as max_features, max_depth, and min_samples_leaf: This makes it an ideal use case for RandomizedSearchCV.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  # Import necessary modules from scipy.stats import randint from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import RandomizedSearchCV # Setup the parameters and distributions to sample from: param_dist param_dist = {\u0026#34;max_depth\u0026#34;: [3, None], \u0026#34;max_features\u0026#34;: randint(1, 9), \u0026#34;min_samples_leaf\u0026#34;: randint(1, 9), \u0026#34;criterion\u0026#34;: [\u0026#34;gini\u0026#34;, \u0026#34;entropy\u0026#34;]} # Instantiate a Decision Tree classifier: tree tree = DecisionTreeClassifier() # Instantiate the RandomizedSearchCV object: tree_cv tree_cv = RandomizedSearchCV(tree, param_dist, cv=5) # Fit it to the data tree_cv.fit(X, y) # Print the tuned parameters and score print(\u0026#34;Tuned Decision Tree Parameters (RGS): {}\u0026#34;.format(tree_cv.best_params_)) print(\u0026#34;Best score is (RGS): {}\u0026#34;.format(tree_cv.best_score_))   Tuned Decision Tree Parameters (RGS): {'criterion': 'gini', 'max_depth': 3, 'max_features': 5, 'min_samples_leaf': 2} Best score is (RGS): 0.7395833333333334 Hold-out set After using K-fold cross-validation to tune my model\u0026rsquo;s hyperparameters, I may want to report how well my model can be expected to perform on a dataset that it has never seen before, given my scoring function of choice. So, I want to use my model to predict on some labeled data, compare my prediction to the actual labels, and compute the scoring function. However, if I have used all of my data for cross-validation, estimating my model performance on any of it may not provide an accurate picture of how it will perform on unseen data.\nFor this reason, it is important to split all of my data at the very beginning into a training set and hold-out set, then perform cross-validation on the training set to tune my model\u0026rsquo;s hyperparameters. After this, I can select the best hyperparameters and use the hold-out set, which has not been used at all, to test how well the model can be expected to perform on a dataset that it has never seen before.\nClassification Logistic regression has a \u0026lsquo;penalty\u0026rsquo; hyperparameter which specifies whether to use \u0026lsquo;l1\u0026rsquo; or \u0026lsquo;l2\u0026rsquo; regularization. Here we create a hold-out set, tune the \u0026lsquo;C\u0026rsquo; and \u0026lsquo;penalty\u0026rsquo; hyperparameters of a logistic regression classifier using GridSearchCV on the training set.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  # Import necessary modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Create the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {\u0026#39;C\u0026#39;: c_space, \u0026#34;penalty\u0026#34;: [\u0026#39;l1\u0026#39;, \u0026#39;l2\u0026#39;]} # Instantiate the logistic regression classifier: logreg logreg = LogisticRegression() # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Instantiate the GridSearchCV object: logreg_cv logreg_cv = GridSearchCV(logreg, param_grid, cv=5) # Fit it to the training data logreg_cv.fit(X_train, y_train) # Print the optimal parameters and best score print(\u0026#34;Tuned Logistic Regression Parameter: {}\u0026#34;.format(logreg_cv.best_params_)) print(\u0026#34;Tuned Logistic Regression Accuracy: {}\u0026#34;.format(logreg_cv.best_score_))   Tuned Logistic Regression Parameter: {'C': 0.4393970560760795, 'penalty': 'l1'} Tuned Logistic Regression Accuracy: 0.7652173913043478 Regression Recall that Lasso used the L1 penalty to regularize, while ridge used the L2 penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties: $$ a\\cdot L_1 + (1-a)\\cdot L_2 $$\nIn scikit-learn, $a$ is represented by the \u0026lsquo;l1_ratio\u0026rsquo; parameter of ElasticNet class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  # Import necessary modules from sklearn.linear_model import ElasticNet from sklearn.metrics import mean_squared_error from sklearn.model_selection import GridSearchCV, train_test_split # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create the hyperparameter grid l1_space = np.linspace(0, 1, 30) param_grid = {\u0026#34;l1_ratio\u0026#34;: l1_space} # Instantiate the ElasticNet regressor: elastic_net elastic_net = ElasticNet() # Setup the GridSearchCV object: gm_cv gm_cv = GridSearchCV(elastic_net, param_grid, cv=5) # Fit it to the training data gm_cv.fit(X_train, y_train) # Predict on the test set and compute metrics y_pred = gm_cv.predict(X_test) r2 = gm_cv.score(X_test, y_test) # \u0026lt;-- Notice here !! mse = mean_squared_error(y_test, y_pred) # \u0026lt;-- Notice here !! print(\u0026#34;Tuned ElasticNet l1 ratio: {}\u0026#34;.format(gm_cv.best_params_)) print(\u0026#34;Tuned ElasticNet R squared: {}\u0026#34;.format(r2)) print(\u0026#34;Tuned ElasticNet MSE: {}\u0026#34;.format(mse))   Tuned ElasticNet l1 ratio: {'l1_ratio': 0.20689655172413793} Tuned ElasticNet R squared: 0.8668305372460283 Tuned ElasticNet MSE: 10.05791413339844 Preprocessing and pipelines Dummies variable Suppose we are dealing with a dataset that has categorical features, such as \u0026lsquo;red\u0026rsquo; or \u0026lsquo;blue\u0026rsquo;, or \u0026lsquo;male\u0026rsquo; or \u0026lsquo;female\u0026rsquo;. As these are not numerical values, the scikit-learn API will not accept them and we will have to preprocess these features into the correct format. Our goal is to convert these features so that they are numerical. The way we achieve this by splitting the feature into a number of binary features called \u0026lsquo;dummy variables\u0026rsquo;, one for each category: \u0026lsquo;0\u0026rsquo; means the observation was not that category, while \u0026lsquo;1\u0026rsquo; means it was.\nFor example, if we are dealing with a car dataset like this\n1 2  auto = pd.read_csv(\u0026#34;/Users/wanghaoming/Downloads/auto.csv\u0026#34;) print(auto.head())    mpg displ hp weight accel origin size 0 18.0 250.0 88 3139 14.5 US 15.0 1 9.0 304.0 193 4732 18.5 US 20.0 2 36.1 91.0 60 1800 16.4 Asia 10.0 3 18.5 250.0 98 3525 19.0 US 15.0 4 34.3 97.0 78 2188 15.8 Europe 10.0 The target variable here is miles per gallon or mpg, and the dataset has a \u0026lsquo;origin\u0026rsquo; feature with three different possible values: \u0026lsquo;US\u0026rsquo;, \u0026lsquo;Asia\u0026rsquo;, and \u0026lsquo;Europe\u0026rsquo;. We create binary features for each of the origins, as each car is made in exactly one country, each row in the dataset will have a one in exactly one of the three columns and zeros in the other two. Notice that in this case, if a car is from neither US nor Asia, then implicitly, it is from Europe. That means that we do not actually need three separate features, but only two, so we can delete the \u0026lsquo;Europe\u0026rsquo; column. If we do not do this, we are duplicating information, which might be an issue for some models.\nThere are several ways to create dummy variables in Python. In scikit-learn, we can use OneHotEncoder. Or we can use pandas get_dummies function. Here, we will use get_dummies. So there is one categorical feature, origin, with three possible values: \u0026lsquo;US\u0026rsquo;, \u0026lsquo;Asia\u0026rsquo;, and \u0026lsquo;Europe\u0026rsquo;. We import pandas, read in the DataFrame, and then apply the get_dummies function.\n1 2  auto_dum = pd.get_dummies(auto) print(auto_dum.head())    mpg displ hp weight accel size origin_Asia origin_Europe origin_US 0 18.0 250.0 88 3139 14.5 15.0 0 0 1 1 9.0 304.0 193 4732 18.5 20.0 0 0 1 2 36.1 91.0 60 1800 16.4 10.0 1 0 0 3 18.5 250.0 98 3525 19.0 15.0 0 0 1 4 34.3 97.0 78 2188 15.8 10.0 0 1 0 Notice, pandas deletes the original categories column origin. In the third row, origin_USA and origin_Europe have zeroes, while origin_Asia has a one, indicating that the car is of Asian origin. But if origin_USA and origin_Europe are zero, then we already know that the car is Asian. So, we drop the origin_Asia column. Alternatively, we can set attribute \u0026ldquo;drop_first=True\u0026rdquo; to get_dummies.\n1 2 3 4  auto_dum1 = auto_dum.drop(\u0026#34;origin_Asia\u0026#34;, axis=1) print(auto_dum1.head()) auto_dum2 = pd.get_dummies(auto, drop_first=True) print(auto_dum2.head())    mpg displ hp weight accel size origin_Europe origin_US 0 18.0 250.0 88 3139 14.5 15.0 0 1 1 9.0 304.0 193 4732 18.5 20.0 0 1 2 36.1 91.0 60 1800 16.4 10.0 0 0 3 18.5 250.0 98 3525 19.0 15.0 0 1 4 34.3 97.0 78 2188 15.8 10.0 1 0 mpg displ hp weight accel size origin_Europe origin_US 0 18.0 250.0 88 3139 14.5 15.0 0 1 1 9.0 304.0 193 4732 18.5 20.0 0 1 2 36.1 91.0 60 1800 16.4 10.0 0 0 3 18.5 250.0 98 3525 19.0 15.0 0 1 4 34.3 97.0 78 2188 15.8 10.0 1 0 Notice that the new column names have the following structure: \u0026ldquo;original column name\u0026rdquo; + _ + \u0026ldquo;value name\u0026rdquo;. Once we have created our dummy variables, we can fit models as before.\nMissing data drop We say that data is missing when there is no value for a given feature in a particular row. Missing values can be encoded in a number of different ways, such as by zeroes, or question marks, or negative ones.\nChecking out the dataset, it looks as though there are observations where insulin is zero. And triceps, which is the thickness of the skin, is zero. These are not possible and, as we have no indication of the real values, the data is, for all intents and purposes, missing.\n1 2  dia = pd.read_csv(\u0026#34;~/Downloads/diab.csv\u0026#34;) print(dia.head())    pregnancies glucose diastolic triceps insulin bmi dpf age diabetes 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 Before we go any further, let\u0026rsquo;s make all these entries NaN using the replace method on the relevant columns.\n1 2 3 4  dia.insulin.replace(0, np.nan, inplace=True) dia.triceps.replace(0, np.nan, inplace=True) dia.bmi.replace(0, np.nan, inplace=True) dia.info()   \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 pregnancies 768 non-null int64 1 glucose 768 non-null int64 2 diastolic 768 non-null int64 3 triceps 541 non-null float64 4 insulin 394 non-null float64 5 bmi 757 non-null float64 6 dpf 768 non-null float64 7 age 768 non-null int64 8 diabetes 768 non-null int64 dtypes: float64(4), int64(5) memory usage: 54.1 KB So, how do we deal with missing data? One way is to drop all rows containing missing data. We can do so using the pandas DataFrame method dropna with attribute axis=0. (Notice that dropna(axis=0) will drop the rows that contain missing values)\n1 2  dia1 = dia.dropna(axis=0) dia1.info()   \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 393 entries, 3 to 765 Data columns (total 9 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 pregnancies 393 non-null int64 1 glucose 393 non-null int64 2 diastolic 393 non-null int64 3 triceps 393 non-null float64 4 insulin 393 non-null float64 5 bmi 393 non-null float64 6 dpf 393 non-null float64 7 age 393 non-null int64 8 diabetes 393 non-null int64 dtypes: float64(4), int64(5) memory usage: 30.7 KB Checking out the shape of the resulting data frame, we see that we now have only approximately half the rows left, this is unacceptable. Thus we need a more robust method. It is generally an equally bad idea to remove columns that contain NaNs.\nfill Another option is to impute missing data. A common strategy is, in any given column with missing values, to compute the mean of all the non-missing entries and to replace all missing values with the mean.\n import SimpleImputer from sklearn.impute instantiate an instance of the SimpleImputer: imp.  argument missing_values='NaN' here specifies that missing values are represented by NaN; argument strategy='mean' specifies that we will use the mean as described above;   call .fit() method on imp to calculate the relative values of the each column, which can be used when filling the missing data later. call .transform() on imp to fill the missing data of each column and return the filled data, which is a np.ndarray.   Alternatively, you can call .fit_transform() method to combine fit and transform methods together.\n 1 2 3 4 5  from sklearn.impute import SimpleImputer imp = SimpleImputer(missing_values=np.nan, strategy=\u0026#39;mean\u0026#39;) imp.fit(dia) dia = imp.transform(dia) print(dia[0:5,:])   [[6.00000000e+00 1.48000000e+02 7.20000000e+01 3.50000000e+01 1.55548223e+02 3.36000000e+01 6.27000000e-01 5.00000000e+01 1.00000000e+00] [1.00000000e+00 8.50000000e+01 6.60000000e+01 2.90000000e+01 1.55548223e+02 2.66000000e+01 3.51000000e-01 3.10000000e+01 0.00000000e+00] [8.00000000e+00 1.83000000e+02 6.40000000e+01 2.91534196e+01 1.55548223e+02 2.33000000e+01 6.72000000e-01 3.20000000e+01 1.00000000e+00] [1.00000000e+00 8.90000000e+01 6.60000000e+01 2.30000000e+01 9.40000000e+01 2.81000000e+01 1.67000000e-01 2.10000000e+01 0.00000000e+00] [0.00000000e+00 1.37000000e+02 4.00000000e+01 3.50000000e+01 1.68000000e+02 4.31000000e+01 2.28800000e+00 3.30000000e+01 1.00000000e+00]] Due to their ability to transform our data as such, imputers are known as transformers, and any model that can transform data this way, using the transform method, is called a transformer.\npipeline After transforming the data, we could then fit our supervised learning model to it. But we can use the scikit-learn pipeline object to do all these at once\n import Pipeline from sklearn.pipeline and SimpleImputer from sklearn.impute. instantiate a SimpleImputer and a estimator, such as LogisticRegression. construct a list of steps in the pipeline, where each step is a 2-tuple containing the name we wish to give the relevant step and the instance (in this case, the transformer and estimator). instantiate Pipeline and pass the list.  So far, we have spliced the pipeline, and now we need to inject water into the pipeline. Note that, in a pipeline, each step but the last must be a transformer and the last must be an estimator, such as, a classifier or a regressor.\n use train_test_split method to split the data into training and test sets call .fit() method on Pipeline instance and pass the training data to train the estimator. call .predict() method on Pipeline instance and pass the test data. call .score() method on Pipeline instance and pass the test data to compute accuracy.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import pandas as pd import numpy as np from sklearn.impute import SimpleImputer from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline # instantiate transformer and estimator  imp = SimpleImputer(missing_values=np.nan, strategy=\u0026#39;mean\u0026#39;) logreg = LinearRegression() # create steps list and instance pipeline step = [ (\u0026#39;transformer\u0026#39;, imp), (\u0026#39;estimator\u0026#39;, logreg) ] pip = Pipeline(step) # handle data dia = pd.read_csv(\u0026#34;~/Downloads/diab.csv\u0026#34;) dia.insulin.replace(0, np.nan, inplace=True) dia.triceps.replace(0, np.nan, inplace=True) dia.bmi.replace(0, np.nan, inplace=True) y = dia.diabetes X = dia.drop(columns=\u0026#39;diabetes\u0026#39;) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # train and evaluate model pip.fit(X_train, y_train) score = pip.score(X_test, y_test) print(score)   0.2790835169520589 Centering and scaling standardization Data imputation is one of several important preprocessing steps for machine learning. Let\u0026rsquo;s use describe method to check out the ranges of the feature variables in the red wine quality dataset. The features are chemical properties such as acidity, pH, and alcohol content. The target value is good or bad, encoded as \u0026lsquo;1\u0026rsquo; and \u0026lsquo;0\u0026rsquo;, respectively. We see that the ranges vary widely: \u0026lsquo;density\u0026rsquo; varies from (point) 99 to to 1 and \u0026lsquo;total sulfur dioxide\u0026rsquo; from 6 to 289!\n1 2  rw = pd.read_csv(\u0026#34;~/Downloads/redw.csv\u0026#34;) print(rw.describe())    fixed acidity volatile acidity citric acid residual sugar \\ count 1599.000000 1599.000000 1599.000000 1599.000000 mean 8.319637 0.527821 0.270976 2.538806 std 1.741096 0.179060 0.194801 1.409928 min 4.600000 0.120000 0.000000 0.900000 25% 7.100000 0.390000 0.090000 1.900000 50% 7.900000 0.520000 0.260000 2.200000 75% 9.200000 0.640000 0.420000 2.600000 max 15.900000 1.580000 1.000000 15.500000 chlorides free sulfur dioxide total sulfur dioxide density \\ count 1599.000000 1599.000000 1599.000000 1599.000000 mean 0.087467 15.874922 46.467792 0.996747 std 0.047065 10.460157 32.895324 0.001887 min 0.012000 1.000000 6.000000 0.990070 25% 0.070000 7.000000 22.000000 0.995600 50% 0.079000 14.000000 38.000000 0.996750 75% 0.090000 21.000000 62.000000 0.997835 max 0.611000 72.000000 289.000000 1.003690 pH sulphates alcohol quality count 1599.000000 1599.000000 1599.000000 1599.000000 mean 3.311113 0.658149 10.422983 5.636023 std 0.154386 0.169507 1.065668 0.807569 min 2.740000 0.330000 8.400000 3.000000 25% 3.210000 0.550000 9.500000 5.000000 50% 3.310000 0.620000 10.200000 6.000000 75% 3.400000 0.730000 11.100000 6.000000 max 4.010000 2.000000 14.900000 8.000000 Many machine learning models use some form of distance to inform them so if you have features on far larger scales, they can unduly influence your model. For example, K-nearest neighbors uses distance explicitly when making predictions. For this reason, we actually want features to be on a similar scale. To achieve this, we do what is called normalizing or scaling and centering.\nThere are several ways to normalize the data: given any column,\n you can subtract the mean and divide by the std so that all features are centred around zero and have variance one. This is called standardization. you can also subtract the minimum and divide by the range of the data so the normalized dataset has minimum zero and maximum one. you can also normalize so that data ranges from -1 to 1 instead.  Here we will perform standardization. To do this\n import scale from sklearn.preprocessing. pass the feature data to scale and this returns our scaled data. (a np.ndarray)  Looking at the mean and standard deviation of the columns of both the original and scaled data verifies this.\n1 2 3 4 5  from sklearn.preprocessing import scale tsdo = rw[\u0026#34;total sulfur dioxide\u0026#34;] tsds = scale(tsdo) print(np.mean(tsdo), np.std(tsdo)) print(np.mean(tsds), np.std(tsds))   46.46779237023139 32.88503665178367 4.443669391870545e-17 0.9999999999999999 pipeline We can also put a scalar in a pipeline object. To do so, we\n import StandardScaler from sklearn.preprocessing instantiate the scaler, estimator and pipline. Here we\u0026rsquo;ll use a K-nearest neighbors algorithm. split the dataset in training and test sets, fit the pipeline to training set, and predict on test set. computing the accuracy (score)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline scl = StandardScaler() knn = KNeighborsClassifier() steps = [(\u0026#39;scale\u0026#39;, scl), (\u0026#39;knn\u0026#39;, knn)] pip = Pipeline(steps) rw = pd.read_csv(\u0026#34;~/Downloads/redw.csv\u0026#34;) y = rw[\u0026#34;quality\u0026#34;] X = rw.drop(columns=\u0026#39;quality\u0026#39;) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) pip.fit(X_train, y_train) print(\u0026#39;scaled: \u0026#39;, pip.score(X_test, y_test)) knno = KNeighborsClassifier() knno.fit(X_train, y_train) print(\u0026#39;original: \u0026#39;, knno.score(X_test, y_test))   scaled: 0.5729166666666666 original: 0.44166666666666665 Scaling did improve our model performance.\nuse GridSearch in pipeline we can use cross-validation with a supervised learning pipeline:\n build our pipeline at first. then specify our hyperparameter space by creating a dictionary.  Note that the keys must follows the format: \u0026lt;pipeline step name\u0026gt;__\u0026lt;hyperparameter name\u0026gt;; the corresponding value is a list or an array of the values to try for that particular hyperparameter. In this case, we are tuning only the n_neighbors in the KNN model.\n split our data into cross-validation and hold-out sets. perform a GridSearch over the parameters in the pipeline by instantiating the GridSearchCV object fit GridSearchCV to training data. call predict method will the best found parameters on the hold-out set.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV # Setup the pipeline steps = [(\u0026#39;scaler\u0026#39;, StandardScaler()), (\u0026#39;SVM\u0026#39;, SVC())] pipeline = Pipeline(steps) # Specify the hyperparameter space parameters = {\u0026#39;SVM__C\u0026#39;:[1, 10, 100], \u0026#39;SVM__gamma\u0026#39;:[0.1, 0.01]} # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21) # Instantiate the GridSearchCV object: cv cv = GridSearchCV(pipeline, parameters, cv=3) # Fit to the training set cv.fit(X_train, y_train) # Predict the labels of the test set: y_pred y_pred = cv.predict(X_test) # Compute and print metrics print(\u0026#34;Accuracy: {}\u0026#34;.format(cv.score(X_test, y_test))) print(classification_report(y_test, y_pred)) print(\u0026#34;Tuned Model Parameters: {}\u0026#34;.format(cv.best_params_))   Accuracy: 0.7795918367346939 precision recall f1-score support False 0.83 0.85 0.84 662 True 0.67 0.63 0.65 318 avg / total 0.78 0.78 0.78 980 Tuned Model Parameters: {'SVM__C': 10, 'SVM__gamma': 0.1} Another comprehensive example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.linear_model import ElasticNet from sklearn.model_selection import GridSearchCV # Setup the pipeline steps: steps steps = [(\u0026#39;imputation\u0026#39;, SimpleImputer(missing_values=\u0026#39;NaN\u0026#39;, strategy=\u0026#39;mean\u0026#39;)), (\u0026#39;scaler\u0026#39;, StandardScaler()), (\u0026#39;elasticnet\u0026#39;, ElasticNet())] # Create the pipeline: pipeline  pipeline = Pipeline(steps) # Specify the hyperparameter space parameters = {\u0026#39;elasticnet__l1_ratio\u0026#39;:np.linspace(0,1,30)} # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create the GridSearchCV object: gm_cv gm_cv = GridSearchCV(pipeline, parameters, cv=3) # Fit to the training set gm_cv.fit(X_train, y_train) # Compute and print the metrics r2 = gm_cv.score(X_test, y_test) print(\u0026#34;Tuned ElasticNet Alpha: {}\u0026#34;.format(gm_cv.best_params_)) print(\u0026#34;Tuned ElasticNet R squared: {}\u0026#34;.format(r2))   Tuned ElasticNet Alpha: {'elasticnet__l1_ratio': 1.0} Tuned ElasticNet R squared: 0.8862016570888217 Compendium sklearn.neighbors 1 2 3 4  sklearn.neighbors KNeighborsClassifier(n_neighbors) .fit() .predict()   sklearn.linear_model sklearn.linear_model\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  sklearn.linear_model LinearRegression() .fit() .predict() .score(X_test, y_test) Lasso() .fit() .predict() .alpha .normalize .coef_ Ridge() .fit() .predict() .alpha .normalize ElasticNet() .fit() .predict() .score(X_test, y_test)\t.l1_ratio LogisticRegression() .fit() # method .predict() .predict_proba() fit_intercept, # argument penalty C, mutli_class, solver   sklearn.metrics 1 2 3 4 5 6 7 8 9 10  sklearn.metrics mean_squared_error(y_test, y_pred) confusion_matrix(y_test, y_pred), classification_report(y_test, y_pred), roc_curve(y_test, y_pred_prob), roc_auc_score(y_test, y_pred_prob),   sklearn.model_selection sklearn.model_selection\n train_test_split cross_val_score GridSearchCV RandomizedSearchCV  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  sklearn.model_selection train_test_split(X, y, test_size, random_state, stratify=y) cross_val_score(model , X, y, cv) GridSearchCV(model, grid, cv) .fit(X, y) .best_params_ .best_score_ .best_estimator_ RandomizedSearchCV(model, grid, cv) .fit(X, y) .best_params_ .best_score_\t  sklearn.impute 1 2 3 4  sklearn.impute SimpleImputer() .missing_values .strategy   sklearn.pipeline sklearn.pipeline\n Pipeline make_pipeline  1 2 3 4 5 6 7  sklearn.pipeline Pipeline(steps) .fit() .predict() .score() make_pipeline()   1 2 3 4  sklearn.preprocessing scale() StandardScaler()   ","date":"2022-01-08T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/machine-learning-i-supervised-learning-by-scikit-learn/","title":"Machine Learning I (Supervised Learning by Scikit-Learn)"},{"content":" ASHRAEAmerican Society of Heating, Refrigerating and Air-Conditioning Engineers1894HVAC54,000ASHRAE\n\n\n1000\nnotebooknotebook\n Step 1:  Step 2:  Step 3:  Step 4: LightGBM Step 5:   \n  1:   2:   3: LightGBMCatBoost  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # import pandas as pd import numpy as np import os import gc import copy import warnings import lightgbm as lgb from lightgbm import LGBMRegressor from sklearn.metrics import mean_squared_log_error from sklearn.model_selection import StratifiedKFold, KFold from tqdm.notebook import tqdm import matplotlib.pyplot as plt import seaborn as sns warnings.filterwarnings(\u0026#39;ignore\u0026#39;) pd.set_option(\u0026#34;max_columns\u0026#34;, 500) %matplotlib inline   1.  ASHRAE6csv51\n[train/test].csv\n building_id meter : id, {0:  , 1: , 2: , 3: } timestamp meter_reading, site0  building_meta.csv\n site_id:  building_id: training.csv primary_use: EnergyStar property type definitionseducation, office) square_feet:  year_built:  floor_count:   weather_[train/test].csv,\n site_id:  air_temperature:  cloud_coverage: oktas dew_temperature:  precip_depth_1_hr:  sea_level_pressure: / wind_direction: 0-360 wind_speed:   1 2 3 4 5  train = pd.read_csv(\u0026#34;../input/ashrae-energy-prediction/train.csv\u0026#34;, parse_dates=[\u0026#34;timestamp\u0026#34;]) test = pd.read_csv(\u0026#34;../input/ashrae-energy-prediction/test.csv\u0026#34;, parse_dates=[\u0026#34;timestamp\u0026#34;]) building = pd.read_csv(\u0026#39;../input/ashrae-energy-prediction/building_metadata.csv\u0026#39;) weather_train = pd.read_csv(\u0026#39;../input/ashrae-energy-prediction/weather_train.csv\u0026#39;, parse_dates=[\u0026#34;timestamp\u0026#34;]) weather_test = pd.read_csv(\u0026#34;../input/ashrae-energy-prediction/weather_test.csv\u0026#34;, parse_dates=[\u0026#34;timestamp\u0026#34;])   1 2 3 4  df = pd.DataFrame( weather_train[\u0026#39;site_id\u0026#39;].value_counts() ) df   2.   \u0026ldquo;meter\u0026quot; {0:  , 1: , 2: , 3: } 0, site,meter_type,primary_use  trainbuilding\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  train_plot = train.merge(building, on=\u0026#39;building_id\u0026#39;, how=\u0026#39;left\u0026#39;) site = 0 # meter_type = 1 # primary_use = \u0026#39;Education\u0026#39; # r = int( np.ceil( len( train_plot[ (train_plot[\u0026#39;site_id\u0026#39;] == site) \u0026amp; (train_plot[\u0026#39;primary_use\u0026#39;] == primary_use) \u0026amp; (train_plot[\u0026#39;meter\u0026#39;] == meter_type) ][\u0026#39;building_id\u0026#39;].value_counts(dropna=False).index.to_list() ) / 2 ) )   1 2 3 4 5 6 7  s = enumerate( train_plot[ (train_plot[\u0026#39;site_id\u0026#39;] == site) \u0026amp; (train_plot[\u0026#39;primary_use\u0026#39;] == primary_use) \u0026amp; (train_plot[\u0026#39;meter\u0026#39;] == meter_type) ][\u0026#39;building_id\u0026#39;].value_counts(dropna=False).index.to_list() )   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  fig, axes = plt.subplots(r,2,figsize=(14, 36), dpi=100) for i, building_id in s: train_plot[ (train_plot[\u0026#39;site_id\u0026#39;] == site) \u0026amp; (train_plot[\u0026#39;primary_use\u0026#39;] == primary_use) \u0026amp; (train_plot[\u0026#39;meter\u0026#39;] == meter_type) \u0026amp; (train_plot[\u0026#39;building_id\u0026#39;] == building_id) ][ [\u0026#39;timestamp\u0026#39;, \u0026#39;meter_reading\u0026#39;] ].set_index(\u0026#39;timestamp\u0026#39;).resample(\u0026#39;H\u0026#39;).mean()[\u0026#39;meter_reading\u0026#39;].plot( ax=axes[i%r][i//r], alpha=0.8, label=\u0026#39;By hour\u0026#39;, color=\u0026#39;tab:blue\u0026#39; ).set_ylabel(\u0026#39;Mean meter reading\u0026#39;, fontsize=13); train_plot[ (train_plot[\u0026#39;site_id\u0026#39;] == site) \u0026amp; (train_plot[\u0026#39;primary_use\u0026#39;] == primary_use) \u0026amp; (train_plot[\u0026#39;meter\u0026#39;] == meter_type) \u0026amp; (train_plot[\u0026#39;building_id\u0026#39;] == building_id ) ][ [\u0026#39;timestamp\u0026#39;, \u0026#39;meter_reading\u0026#39;] ].set_index(\u0026#39;timestamp\u0026#39;).resample(\u0026#39;D\u0026#39;).mean()[\u0026#39;meter_reading\u0026#39;].plot( ax=axes[i%r][i//r], alpha=1, label=\u0026#39;By day\u0026#39;, color=\u0026#39;tab:orange\u0026#39; ).set_xlabel(\u0026#39;\u0026#39;); axes[i%r][i//r].legend(); axes[i%r][i//r].set_title(\u0026#39;building_id: \u0026#39; + str(building_id ), fontsize=13); plt.subplots_adjust(hspace=0.45) del train_plot,fig,axes,r gc.collect();   3.   1: \n:\n3.1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def compress_dataframe(df): \u0026#39;\u0026#39;\u0026#39;\u0026#39;\u0026#39;\u0026#39; result = df.copy() for col in result.columns: col_data = result[col] dn = col_data.dtype.name if dn == \u0026#34;object\u0026#34;: result[col] = pd.to_numeric(col_data.astype(\u0026#34;category\u0026#34;).cat.codes, downcast=\u0026#34;integer\u0026#34;) elif dn == \u0026#34;bool\u0026#34;: result[col] = col_data.astype(\u0026#34;int8\u0026#34;) elif dn.startswith(\u0026#34;int\u0026#34;) or (col_data.round() == col_data).all(): result[col] = pd.to_numeric(col_data, downcast=\u0026#34;integer\u0026#34;) else: result[col] = pd.to_numeric(col_data, downcast=\u0026#39;float\u0026#39;) return result   3.2  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  def set_time(df): df.timestamp = (df.timestamp - pd.to_datetime(\u0026#34;2016-01-01\u0026#34;)).dt.total_seconds() // 3600 #timestamp16110// return df # site # https://www.kaggle.com/patrick0302/locate-cities-according-weather-temperature site_GMT_offsets = [-5, 0, -7, -5, -8, 0, -5, -5, -5, -6, -7, -5, 0, -6, -5, -5] #, def weather_set_time(df,time_zone): df.timestamp = (df.timestamp - pd.to_datetime(\u0026#34;2016-01-01\u0026#34;)).dt.total_seconds() // 3600 GMT_offset_map = {site: offset for site, offset in enumerate(site_GMT_offsets)} df.timestamp = df.timestamp + df.site_id.map(GMT_offset_map) # site_dfs = [] for site_id in df.site_id.unique(): #  site_df = df[df.site_id == site_id].set_index(\u0026#34;timestamp\u0026#34;).reindex(time_zone) site_df.site_id = site_id for col in [c for c in site_df.columns if c != \u0026#34;site_id\u0026#34;]: site_df[f\u0026#34;had_{col}\u0026#34;] = ~site_df[col].isna() site_df[col] = site_df[col].interpolate(limit_direction=\u0026#39;both\u0026#39;, method=\u0026#39;linear\u0026#39;) #  site_df[col] = site_df[col].fillna(df[col].median()) site_dfs.append(site_df) df = pd.concat(site_dfs).reset_index() # make timestamp back into a regular column for col in df.columns: if df[col].isna().any(): df[f\u0026#34;had_{col}\u0026#34;] = ~df[col].isna() #had_xxx xxx return df # def _add_time_features(X): return X.assign(tm_day_of_week=((X.timestamp // 24) % 7), tm_hour_of_day=(X.timestamp % 24)) building = compress_dataframe(building.fillna(-1)).set_index(\u0026#34;building_id\u0026#34;) train = compress_dataframe(set_time(train)) test = compress_dataframe(set_time(test)).set_index(\u0026#34;row_id\u0026#34;) weather_train = compress_dataframe(weather_set_time(weather_train,range(8784))).set_index([\u0026#34;site_id\u0026#34;, \u0026#34;timestamp\u0026#34;]) weather_test = compress_dataframe(weather_set_time(weather_test,range(8784,26304))).set_index([\u0026#34;site_id\u0026#34;, \u0026#34;timestamp\u0026#34;])   3.3  1 2 3 4  def combined_data(df,weather): df = compress_dataframe(df.join(building, on=\u0026#34;building_id\u0026#34;).join(weather, on=[\u0026#34;site_id\u0026#34;, \u0026#34;timestamp\u0026#34;]).fillna(-1)) return df.drop(columns=[\u0026#34;meter_reading\u0026#34;]),df.meter_reading   3.4  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  def make_is_bad_zero(Xy_subset, min_interval=48, summer_start=3000, summer_end=7500): #3000/24=1257500/24=312.5,125312.5 meter = Xy_subset.meter_id.iloc[0] is_zero = Xy_subset.meter_reading == 0 #0indices if meter == 0: #0meter00training dataframedrop return is_zero transitions = (is_zero != is_zero.shift(1))#00 all_sequence_ids = transitions.cumsum()#pd.Seires ids = all_sequence_ids[is_zero].rename(\u0026#34;ids\u0026#34;)#0 if meter in [2, 3]: #  keep = set(ids[(Xy_subset.timestamp \u0026lt; summer_start) | (Xy_subset.timestamp \u0026gt; summer_end)].unique())#indices is_bad = ids.isin(keep) \u0026amp; (ids.map(ids.value_counts()) \u0026gt;= min_interval) #48 elif meter == 1: time_ids = ids.to_frame().join(Xy_subset.timestamp).set_index(\u0026#34;timestamp\u0026#34;).ids#idstimestamp is_bad = ids.map(ids.value_counts()) \u0026gt;= min_interval#48 #  jan_id = time_ids.get(0, False)#id dec_id = time_ids.get(8283, False)#id if (jan_id and dec_id and jan_id == time_ids.get(500, False) and dec_id == time_ids.get(8783, False)): #5005000 is_bad = is_bad \u0026amp; (~(ids.isin(set([jan_id, dec_id])))) #is_bad else: raise Exception(f\u0026#34;Unexpected meter type: {meter}\u0026#34;) result = is_zero.copy() result.update(is_bad) return result def find_bad_zeros(X, y): \u0026#34;\u0026#34;\u0026#34;Index\u0026#34;\u0026#34;\u0026#34; Xy = X.assign(meter_reading=y, meter_id=X.meter) is_bad_zero = Xy.groupby([\u0026#34;building_id\u0026#34;, \u0026#34;meter\u0026#34;]).apply(make_is_bad_zero) return is_bad_zero[is_bad_zero].index.droplevel([0, 1]) def find_bad_sitezero(X): \u0026#34;\u0026#34;\u0026#34;Site 0 index.\u0026#34;\u0026#34;\u0026#34; return X[(X.timestamp \u0026lt; 3378) \u0026amp; (X.site_id == 0) \u0026amp; (X.meter == 0)].index def find_bad_building1099(X, y): \u0026#34;\u0026#34;\u0026#34;1099index .\u0026#34;\u0026#34;\u0026#34; return X[(X.building_id == 1099) \u0026amp; (X.meter == 2) \u0026amp; (y \u0026gt; 3e4)].index def find_bad_rows(X, y): return find_bad_zeros(X, y).union(find_bad_sitezero(X)).union(find_bad_building1099(X, y))   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  X, y = combined_data(train,weather_train) bad_rows = find_bad_rows(X, y) #index pd.Series(bad_rows.sort_values()).to_csv(\u0026#34;rows_to_drop.csv\u0026#34;, header=False, index=False) X = X.drop(index=bad_rows) y = y.reindex_like(X) X = _add_time_features(X) X = compress_dataframe(X) X = X.drop(columns=\u0026#34;timestamp\u0026#34;) # droptimestamp del bad_rows,train,weather_train gc.collect();   3.5  Kaggle RMSLERoot Mean Squared Logarithmic Error, RMSLE)RMSLERMSLE\n$${\\rm RMSLE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 }$$\n\n $n$public/private, $p_i$  $a_i$ i. $\\log(x)$   $y = \\log(y+1)$RMSEnumpylog1p\n$y = e^y-1$ y = np.exp1m(y)\n1 2  # y = np.log1p(y)    2: \n:\n4. LightGBM 4.1  LightGBM \n learning_rate, num_leavesLightGBMleaf-wisenum_leaves subsample0-1 lambda_l2L2 num_trees  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  params = { \u0026#39;task\u0026#39;: \u0026#39;train\u0026#39;, \u0026#39;boosting_type\u0026#39;: \u0026#39;gbdt\u0026#39;, \u0026#39;objective\u0026#39;: \u0026#39;regression\u0026#39;, \u0026#39;metric\u0026#39;: \u0026#39;rmse\u0026#39;, \u0026#39;num_leaves\u0026#39;: 40, \u0026#39;subsample\u0026#39;:0.8, \u0026#39;learning_rate\u0026#39;: 0.03, \u0026#39;verbose\u0026#39;: 1, \u0026#39;lambda_l2\u0026#39;:3 } num_trees = 1000 # categorical_features=[\u0026#39;building_id\u0026#39;, \u0026#39;site_id\u0026#39;, \u0026#39;primary_use\u0026#39;, \u0026#39;had_air_temperature\u0026#39;, \u0026#39;had_cloud_coverage\u0026#39;, \u0026#39;had_dew_temperature\u0026#39;, \u0026#39;had_precip_depth_1_hr\u0026#39;,\u0026#39;had_sea_level_pressure\u0026#39;, \u0026#39;had_wind_direction\u0026#39;, \u0026#39;had_wind_speed\u0026#39;, \u0026#39;tm_day_of_week\u0026#39;, \u0026#39;tm_hour_of_day\u0026#39;]   4.2  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  n_splits = 3 for val in X[\u0026#39;meter\u0026#39;].unique(): X1 = X[X[\u0026#39;meter\u0026#39;] == val].drop(columns=[\u0026#39;meter\u0026#39;]) kf = StratifiedKFold(n_splits=n_splits,random_state=42) #StratifiedKFoldfold3fold t = 0 for train_index, test_index in kf.split(X1, X1[\u0026#39;tm_hour_of_day\u0026#39;]): #fold[\u0026#39;tm_hour_of_day\u0026#39;] train_features = X1.iloc[train_index] train_target = y[X1.iloc[train_index].index] test_features = X1.iloc[test_index] test_target = y[X1.iloc[test_index].index] d_train = lgb.Dataset(train_features, train_target, categorical_feature=categorical_features) d_eval = lgb.Dataset(test_features,test_target, categorical_feature=categorical_features) print(\u0026#34;Building model meter :\u0026#34;,val,\u0026#39;fold:\u0026#39;,t) md = lgb.train(params, d_train, num_boost_round=num_trees, valid_sets=(d_train, d_eval), early_stopping_rounds=200,verbose_eval=20) md.save_model(\u0026#39;lgb_val{}_fold{}.bin\u0026#39;.format(val,t)) t += 1 del X1 del d_train, d_eval, train_features, test_features, md gc.collect();    3: LightGBMCatBoost\n:\n5.  1 2 3 4 5 6  X = compress_dataframe(test.join(building, on=\u0026#34;building_id\u0026#34;).join(weather_test, on=[\u0026#34;site_id\u0026#34;, \u0026#34;timestamp\u0026#34;]).fillna(-1)) X = compress_dataframe(_add_time_features(X)) X = X.drop(columns=\u0026#34;timestamp\u0026#34;) # droptimestamp del test, weather_test gc.collect();   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # result = np.zeros(len(X)) for val in X[\u0026#39;meter\u0026#39;].unique(): ix = np.nonzero((X[\u0026#39;meter\u0026#39;] == val).to_numpy()) for i in tqdm(range(n_splits)): # model = lgb.Booster(model_file=\u0026#39;lgb_val{}_fold{}.bin\u0026#39;.format(val, i)) result[ix] += model.predict(X.iloc[ix].drop(columns=[\u0026#39;meter\u0026#39;]), num_iteration=model.best_iteration)/n_splits del model gc.collect(); predictions = pd.DataFrame({ \u0026#34;row_id\u0026#34;: X.index, \u0026#34;meter_reading\u0026#34;: np.clip(np.expm1(result), 0, None) }) # float_format predictions.to_csv(\u0026#34;submission.csv\u0026#34;, index=False, float_format=\u0026#34;%.4f\u0026#34;)   1    ","date":"2021-07-23T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/kaggle-ashrae-great-energy-predictor-iii/","title":"Kaggle - ASHRAE, Great Energy Predictor III"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\nAll operations in this series of blogs (Linux Operation System) are done in AWS Elastic Compute Cloud (EC2).\n passwd, shadow and group file /etc/passwd The /etc/passwd file contains basic information about each user account on the system. Each line of the file represents a single account of type, normal user or system user and there are 7 fields on each line.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ cat /etc/passwd | tail -n 5 pollinate:x:111:1::/var/cache/pollinate:/bin/false ec2-instance-connect:x:112:65534::/nonexistent:/usr/sbin/nologin systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false   Let\u0026rsquo;s take these fields one by one.\n The first field is the user\u0026rsquo;s login name. The second field, at the beginning of the Unix era, used to contain an encrypted password. Nowadays, it contains the letter x to denote that a password has been assigned but was saved in /etc/shadow file. If this second field is blank, the user does not need to enter a password to log in. The third field is the user ID, a positive integer number assigned to the user, followed by the primary group ID in the fourth field. The fifth field is a comment. Sometimes it is left blank. The next field is the user\u0026rsquo;s home directory. The last field is the default shell usually set to /bin/bash. If instead of bash you see there nologin of false it means there is a system user thats not allowed to log into the system.  That is the format of etc password.\n1 2 3 4 5 6 7  (1)ubuntu: user login name (2)x: password was saved in `/etc/shadow` (3)1000: user ID (4)1000: group ID (5)Ubuntu: comment (6)/home/ubuntu: user home directory (7)/bin/bash   /etc/shadow /etc/shadow stores the actual passwords of the users in an encrypted format. The /etc/passwd file is world-readable and that means that any user can read it but the /etc/shadow file is only readable by the root account.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ sudo cat /etc/shadow | head -n 5 root:$6$3L2TtFLTJR5Nz1sD$x5UX/leqXMB07ALQ/RAHGCGqXeZ9y23.UqNeygwp/gA.R9mh5CLDA6bMVXYBl/tqfXGvFzPuG9570cQ7xwjIV/:19100:0:99999:7::: daemon:*:18960:0:99999:7::: bin:*:18960:0:99999:7::: sys:*:18960:0:99999:7::: sync:*:18960:0:99999:7:::   Each line of the file contains nine comma separated fields:\n The first field is the user name, and this is how a line in the shadow file is connected to the corresponding line in the password file. The second field represents the password. If the password field contains an asterisk * or an exclamation ! point the user will not be able to login to the system using the password authentication. Other login methods like key-based authentication or switching to the user are still allowed. Then come 7 fields related to password expiration date like last password change, minimum and maximum passport age and so on. you can find a full description of these fields running man shadow in which has a full description of each field.  Lets take a deeper look at the password field.\n1 2 3  $type 6 $salt 3L2TtFLTJR5Nz1sD $hash x5UX/leqXMB07ALQ/RAHGCGqXeZ9y23.UqNeygwp/gA.R9mh5CLDA6bMVXYBl/tqfXGvFzPuG9570cQ7xwjIV/   Usually the password format is set to $type$salt$hash.\nThe type, 6 in this case, is the cryptographic hash algorithm used and can have the following values:\n 1: MD5; 2a: Blowfish; 2y: Eksblowfish; 5: SHA-256; 6: SHA-512.  The salt is randomly generated for each password, but its not secret like the password is. Then the salt combined with the password is added to the hashing process to enforce the uniqueness of the outhash, or in other words, the same password will give different hashes because of this random salt.\nIf through having the hash an attacker can find the clear text password, all the other similar passwords of other users are still secure because their hash is different.\nWe now create two user accounts user1, user2 which has the same password test123:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  ubuntu@ip-172-31-87-26:~$ sudo useradd user1 ubuntu@ip-172-31-87-26:~$ sudo passwd user1 New password: Retype new password: passwd: password updated successfully ubuntu@ip-172-31-87-26:~$ sudo useradd user2 ubuntu@ip-172-31-87-26:~$ sudo passwd user2 New password: Retype new password: passwd: password updated successfully ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ cat /etc/passwd | tail -n5 systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false user1:x:1001:1001::/home/user1:/bin/sh user2:x:1002:1002::/home/user2:/bin/sh   Now let\u0026rsquo;s check the hashes in the shadow file.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ sudo cat /etc/shadow | tail -n 5 systemd-coredump:!!:19096:::::: ubuntu:!:19096:0:99999:7::: lxd:!:19096:::::: user1:$6$OU/fVA4GQsPufXdI$wsJFgBDT0hnH4Fbm5xRI0RpaxBCZpQBqoOQT3RxWmZep/yUehuyG25M.3.ZYcfrh/90wRWnqA9UWZqeSdpJmt1:19110:0:99999:7::: user2:$6$sUJt5NJm8RSMpllV$9NYKlDsht5mZBvuFy5Dhrsi8R/098JdVgJS0FqnlHurMtmj9l6KcNjnWx3SfL8pekIazuQy7qyIBuhn1nCT411:19110:0:99999:7:::   And we notice that even though both users have the same password, the saved hash is different. That\u0026rsquo;s because it has computed the hash using both the common password and the salt, which is unique for each user. Now, if a hacker manages to find the password of the first user having the hash, the hacker doesn\u0026rsquo;t know that the second user has the same password because there is another hash in the file.\n password and Shadow file, should not be edited by hand unless you know what you are doing. Always use a command that is designed for this purpose.\n /etc/group The main purpose of the groups is to define a set of privileges or permissions like read, write or execute for a given file that can be shared among the users within the group. You have to add the user to a group if you want to have the privileges of the group.\nIn Linux, there are two types of groups that the user can belong to.\nPrimary or login group Primary group is the group that is assigned to the files that are created by the user. The group id of the primary group is stored in /etc/passwd(4th field) and the group name in /etc/group(1st field).\nWhen a user creates a file, the user will be the owner of the file and file\u0026rsquo;s primary group will be the group owner of the file. Remember that in Linux a file is owned by both a user and a group. I am creating a new file using touch:\n1 2 3  ubuntu@ip-172-31-87-26:~$ touch a.txt ubuntu@ip-172-31-87-26:~$ ls -l a.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 28 05:55 a.txt   The user that runs touch command is the owner and its primary group (having usually the same name as the user) is the group owner. Note that each user must belong exactly to one primary group.\nSecondary or supplementary groups A user can be a member of none or more secondary groups. The secondary groups are stored in /etc/group.\nLet\u0026rsquo;s take the current user and see its entry in /etc/passwd:\n1 2  ubuntu@ip-172-31-87-26:~$ grep ubuntu /etc/passwd ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash   so from the 4th field, we know that the primary group of this user has the ID 1000. Let\u0026rsquo;s search the /etc/group file for that ID.\n1 2  ubuntu@ip-172-31-87-26:~$ grep 1000 /etc/group ubuntu:x:1000:   So the user\u0026rsquo;s primary group is a group with the same name, ubuntu in this case; typically no other user will belong to this group. Now let\u0026rsquo;s find out the secondary groups of the user; to do that, I\u0026rsquo;ll search for the user name in the /etc/group file.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ grep ubuntu /etc/group adm:x:4:syslog,ubuntu dialout:x:20:ubuntu cdrom:x:24:ubuntu floppy:x:25:ubuntu sudo:x:27:ubuntu audio:x:29:ubuntu dip:x:30:ubuntu video:x:44:ubuntu plugdev:x:46:ubuntu netdev:x:117:ubuntu lxd:x:118:ubuntu ubuntu:x:1000:   The last field of each line indicates the users that belong to that group as a secondary group. If there are more users belonging to the same group, then they are separated by a comma.\nTo summarize:\n find the primary group of the user:  grep user\u0026rsquo;s name in passwd file to acquire the primary group id grep primary group id in group file to acquire the primary group name if the primary group name is equal to the user\u0026rsquo;s name, then you can grep user\u0026rsquo;s name in group file.   find the secondary group of the user:  grep the user\u0026rsquo;s name in group file.    the first field of group file is the names of groups; the 3rd field of the file is the group\u0026rsquo;s id; the last field is the users belong to the group.\ngroups, id command Normally, you don\u0026rsquo;t search through these files to find out to which groups a user belongs. There\u0026rsquo;s the groups command that does the job for you.\nWhen executed without any arguments, the command will print out a list of all groups the current user belongs to. And the first group is the primary one.\nTo get a list of all groups a specific user belongs to provide the user name to the groups command as argument.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ groups ubuntu adm dialout cdrom floppy sudo audio dip video plugdev netdev lxd ubuntu@ip-172-31-87-26:~$ groups ubuntu ubuntu : ubuntu adm dialout cdrom floppy sudo audio dip video plugdev netdev lxd   Another useful command is id. It prints information about the specified user and its groups and as the groups command, if the username is omitted, it shows information for the current user.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm),20(dialout),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),117(netdev),118(lxd) ubuntu@ip-172-31-87-26:~$ id ubuntu uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm),20(dialout),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),117(netdev),118(lxd)   The command is showing the user ID 1000, the user\u0026rsquo;s primary group and group id and the user\u0026rsquo;s secondary groups and their id.\nUser Create user useradd command In Linux you need root privileges to create, change or delete an account. To create a new account you run the useradd command. Run man useradd and youll see lots of options.\nTo create a new user without specifying any options, just run useradd and the new username:\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ sudo useradd u1 ubuntu@ip-172-31-87-26:~$ tail -n 5 /etc/passwd ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false user1:x:1001:1001::/home/user1:/bin/sh user2:x:1002:1002::/home/user2:/bin/sh u1:x:1003:1003::/home/u1:/bin/sh   And the last user is u1, the user that have just created. By default, for the new user a group with the same name as the user name was also created.\n1 2  ubuntu@ip-172-31-87-26:~$ groups u1 u1 : u1   And it belongs to only one group called u1; this is its primary group. Note that the default behavior of useradd can differ from distribution to distribution;\n On some distributions, especially those based on Ubuntu, you\u0026rsquo;ll see another command called adduser. useradd is a native binary compiled with any Linux system. But, adduser is just a perl script which uses useradd binary as a back-end. adduser is more user friendly and interactive than its back-end useradd. There is no actual difference in features provided.\n passwd command useradd reads the contents of the file /etc/login.def which contains options for the shadow password, such as password expiration policy, ranges of user IDs used when creating system and regular users and more. To be able to login , we need to set a password to the newly created user; to do that passwd command followed by the username.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ sudo passwd u1 New password: Retype new password: passwd: password updated successfully   Now the user has a password and can login.\n-m If a user is created without any option, its home directory is not created as well. If you want to create the user\u0026rsquo;s home directory, use the -m option, and the directory name will be the same as the username and will be created in /home.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo useradd -m u2 ubuntu@ip-172-31-87-26:~$ ls ~/.. u2 ubuntu   -d If you want to give it another name or create it in another location add the -d option to -m.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo useradd -md /home/uu3 u3 ubuntu@ip-172-31-87-26:~$ ls ~/.. u2 ubuntu uu3   -c If you want to add a comment to the user entry in /etc/ passwd file, add the -c option and a comment between quotes. This comment is typically a short description, like the user\u0026rsquo;s full name or the contact information.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ sudo useradd -c \u0026#34;comment for u4\u0026#34; u4 ubuntu@ip-172-31-87-26:~$ tail -n 5 /etc/passwd user2:x:1002:1002::/home/user2:/bin/sh u1:x:1003:1003::/home/u1:/bin/sh u2:x:1004:1004::/home/u2:/bin/sh u3:x:1005:1005::/home/uu3:/bin/sh u4:x:1006:1006:comment for u4:/home/u4:/bin/sh   -g, -G By default, the primary group of the user will be a group with the same name as the user. If you want to add this user to another primary group, use the -g option and the name of the group. However, this is not common. What is common is to add the user to some secondary groups as well. To do that, use the capital -G option and the secondary groups with a comma between them.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo useradd -G sudo,adm,video u6 ubuntu@ip-172-31-87-26:~$ groups u6 u6 : u6 adm sudo video   Note that the groups should already exist or you\u0026rsquo;ll get an error.\n-e If you want to create a temporary user, then you should specify an expiration date, the date on which the user account will be disabled. Use the -e option and the date in the format yyyy-mm-dd.\n1  ubuntu@ip-172-31-87-26:~$ sudo useradd -e 2021-12-31 u7   If you want to change the user\u0026rsquo;s password expire policy of an existing user run the chage command and the username.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ sudo chage u7 Changing the aging information for u7 Enter the new value, or press ENTER for the default Minimum Password Age [0]: Maximum Password Age [99999]: Last Password Change (YYYY-MM-DD) [2021-04-29]: Password Expiration Warning [7]: Password Inactive [-1]: Account Expiration Date (YYYY-MM-DD) [2021-12-31]:   And if you want to list the expiration information only, add -l option to chage:\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ sudo chage -l u7 Last password change\t: Apr 29, 2021 Password expires\t: never Password inactive\t: never Account expires\t: Dec 31, 2021 Minimum number of days between password change\t: 0 Maximum number of days between password change\t: 99999 Number of days of warning before password expires\t: 7   -s In computing, a shell is a computer program (an executable file in Linux) which exposes an operating system\u0026rsquo;s services to a human user or other programs. In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), depending on a computer\u0026rsquo;s role and particular operation. It is named a shell because it is the outermost layer around the operating system.\nTo create the user with a specific login shell at the -s option and the absolute path to that shell. That shell will be automatically launched when the user logs in or opens a terminal. We usually use /bin/bash shell.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ sudo useradd -s /bin/bash u5 ubuntu@ip-172-31-87-26:~$ tail -n 5 /etc/passwd u1:x:1003:1003::/home/u1:/bin/sh u2:x:1004:1004::/home/u2:/bin/sh u3:x:1005:1005::/home/uu3:/bin/sh u4:x:1006:1006:comment for u4:/home/u4:/bin/sh u5:x:1007:1007::/home/u5:/bin/bash   If you take a look in /etc/passwd youll see that some accounts have as shell something like /usr/bin/nologin (or /usr/sbin/nologin) or /bin/false.\nSometimes you need a system account to be used only to run a service such as a Web server. In Linux every daemon or process should be run as a specific user and its recommended not to be root. This is an important security consideration. To have such a limited user account, that never logs in, you set its shell as being false or nologin. If nologin is set as the user\u0026rsquo;s shell, when the user tries to login, it will get a message saying this account is currently not available. And false, is just a binary that immediately exists, returning false when called. That\u0026rsquo;s why when someone that has false as shell logs in is immediately logged out. There isn\u0026rsquo;t actually such a big difference between nologin and false.\nBy the way, /bin/bash, /usr/sbin/nologin, /bin/false and the default shell when user is created /bin/sh and all executable files:\n1 2 3 4 5  ubuntu@ip-172-31-87-26:~$ ls -lF /bin/bash /usr/sbin/nologin /bin/false /bin/sh -rwxr-xr-x 1 root root 1183448 Apr 18 09:14 /bin/bash* -rwxr-xr-x 1 root root 39256 Sep 5 2019 /bin/false* lrwxrwxrwx 1 root root 4 Nov 29 23:30 /bin/sh -\u0026gt; dash* -rwxr-xr-x 1 root root 14640 Jul 14 2021 /usr/sbin/nologin*   Change property Let\u0026rsquo;s see how to change the properties of an existing user.\nAll of the information about a user is stored in the following files: /etc/passwd, /etc/shadow that can be displayed only by root, /etc/group and /etc/gshadow; there\u0026rsquo;s also /etc/login.defs which contains the default options of a new user (if they are not specified when creating that user). Such an example is the fact that it\u0026rsquo;s not explicitly indicated if the home directory is created or not. The shell commands that create, change or remove a user or a group are in fact only modifying these files.\nusermod is the shell command used to change the properties of an existing user. It uses the same options as useradd just so that instead of creating a user with the specified options, it changes those options.\nFor example change or add a comment for an existing user.\n1 2 3 4 5  ubuntu@ip-172-31-87-26:~$ sudo usermod -c \u0026#34;cmt for user 7\u0026#34; u7 ubuntu@ip-172-31-87-26:~$ tail -n 3 /etc/passwd u5:x:1007:1007::/home/u5:/bin/bash u6:x:1008:1008::/home/u6:/bin/sh u7:x:1009:1009:cmt for user 7:/home/u7:/bin/sh   To change the primary group of user use, the -g option and the name. Note that the group name, in this case u6, must already exist.\n1 2 3 4 5  ubuntu@ip-172-31-87-26:~$ groups u7 u7 : u7 ubuntu@ip-172-31-87-26:~$ sudo usermod -g u6 u7 ubuntu@ip-172-31-87-26:~$ groups u7 u7 : u6   usermod does not delete the group u7, and you can set it back.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo usermod -g u7 u7 ubuntu@ip-172-31-87-26:~$ groups u7 u7 : u7   If you want to add the user to some secondary groups use the -G option. Each group is separated from the next one by a comma, with no intervening whitespace.\n1 2 3 4 5  ubuntu@ip-172-31-87-26:~$ groups u6 u6 : u6 adm sudo video ubuntu@ip-172-31-87-26:~$ sudo usermod -G dip,lxd u6 ubuntu@ip-172-31-87-26:~$ groups u6 u6 : u6 dip lxd   Note that the primary group was not modified and the user belongs only to those 2 groups as secondary groups. If the user is currently a member of a group, which is not specified as an option of -G, then the user will be removed from that group. Or in other words, after running the command, the user will be a member of those groups only, in this case dip and lxd groups. The user is not anymore a member of adm, video or sudo. This behavior can be changed via -a option, which appends the user to the current secondary group list.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo usermod -aG plugdev,audio u6 ubuntu@ip-172-31-87-26:~$ groups u6 u6 : u6 audio dip plugdev lxd   Delete user In Linux you can delete a user account and all its associated files by using the userdel command. It also deletes the group with the same name as the user, only if no other user is a member of that group. Let\u0026rsquo;s delete an account.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ ls /home u2 ubuntu uu3 ubuntu@ip-172-31-87-26:~$ sudo userdel u2 ubuntu@ip-172-31-87-26:~$ ls /home u2 ubuntu uu3 ubuntu@ip-172-31-87-26:~$ grep u2 /etc/passwd ubuntu@ip-172-31-87-26:~$   The account u2 was removed, but its home directory is still there. To remove a user along with its home directory and mail spool use the -r option.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ sudo usermod -g u4 u3 ubuntu@ip-172-31-87-26:~$ groups u3 u3 : u4 ubuntu@ip-172-31-87-26:~$ ls ~/.. u2 ubuntu uu3 ubuntu@ip-172-31-87-26:~$ sudo userdel -r u3 userdel: group u3 not removed because it is not the primary group of user u3. userdel: u3 mail spool (/var/mail/u3) not found ubuntu@ip-172-31-87-26:~$ ls /home u2 ubuntu   The user was removed. It\u0026rsquo;s home directory was removed, but the group with the same name called u3 was not removed because it was not the primary group of the user u3; and the user u3\u0026rsquo;s primary group u4 was also not removed, because it is the primary group of user u4.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ cat /etc/group | grep u[1-9] audio:x:29:ubuntu,u6 dip:x:30:ubuntu,u6 plugdev:x:46:ubuntu,u6 lxd:x:118:ubuntu,u6 u1:x:1003: u3:x:1005: u4:x:1006: u5:x:1007: u6:x:1008: u7:x:1009: ubuntu@ip-172-31-87-26:~$ groups u4 u4 : u4   Note that the command didn\u0026rsquo;t delete any other files that belong to the user and are located in other places; you have to search for and delete them manually.\n As with useradd, userdel is a low level utility, but Debian and Ubuntu distributions have a friendlier command called deluser. It just uses userdel as a backend.\n Create admin user A common administrative task is to give the user the possibility to run administrative commands as root. By default only the user created at system installation time can run commands as root by prefixing them with sudo.\nThere are 2 predefined group names: sudo on debian and ubuntu based distributions (BTW in older versions of ubuntu this group\u0026rsquo;s name was admin.) and wheel on redhat and centos based distributions. If the user is part of this group then it can run commands as root.\nFor example, Im creating a new user called u8 and then check if it can run commands as root:\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ sudo useradd -m u8 ubuntu@ip-172-31-87-26:~$ sudo passwd u8 New password: Retype new password: passwd: password updated successfully ubuntu@ip-172-31-87-26:~$ su u8 Password: $ id uid=1010(u8) gid=1011(u8) groups=1011(u8)   And now I\u0026rsquo;m u8, Let\u0026rsquo;s see if it can run administrative commands:\n1 2 3 4 5  $ cat /etc/shadow cat: /etc/shadow: Permission denied $ sudo cat /etc/shadow [sudo] password for u8: u8 is not in the sudoers file. This incident will be reported.   And it\u0026rsquo;s not allowed. I\u0026rsquo;m logging out by exit. You can see that user u8 only has a $ as prompt, that because we haven\u0026rsquo;t set shell for u8. Now set it\u0026rsquo;s shell as /bin/bash and add u8 in group sudo:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  ubuntu@ip-172-31-87-26:~$ sudo usermod -s /bin/bash -aG sudo u8 ubuntu@ip-172-31-87-26:~$ su u8 Password: To run a command as administrator (user \u0026#34;root\u0026#34;), use \u0026#34;sudo \u0026lt;command\u0026gt;\u0026#34;. See \u0026#34;man sudo_root\u0026#34; for details. u8@ip-172-31-87-26:/home/ubuntu$ id uid=1010(u8) gid=1011(u8) groups=1011(u8),27(sudo) u8@ip-172-31-87-26:/home/ubuntu$ sudo cat /etc/shadow | tail -n 3 [sudo] password for u8: u6:!:19111:0:99999:7::: u7:!:19111:0:99999:7::19357: u8:$6$EEryHMMe70k6tYzh$jgHahiEp89iwwX/ngdU0WNHg1Vjci.6lHoqY05omT.5s/VIEob2gpxSlo41KKrDmnpanvIwL11uxZpyc8zgvU1:19111:0:99999:7::: u8@ip-172-31-87-26:/home/ubuntu$ exit exit ubuntu@ip-172-31-87-26:~$ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm),20(dialout),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),117(netdev),118(lxd)   And It was allowed to run the command as root. That\u0026rsquo;s because the user belongs to the sudo group.\nGroup To create a new group use the groupadd command. In its simplest form you just specify the group name as argument.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ sudo addgroup g1 Adding group `g1\u0026#39; (GID 1004) ... Done. ubuntu@ip-172-31-87-26:~$ sudo tail -n 3 /etc/group u3:x:1010: u8:x:1011: g1:x:1004:   And the group was added to the group file. Once the group is created, you can start adding users to the group; use the useradd or usermod command with the -G option.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo useradd -G g1 u9 ubuntu@ip-172-31-87-26:~$ groups u9 u9 : u9 g1   To change the name of the group, used the group groupmod command with -n option and the new name.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo groupmod -n g2 g1 ubuntu@ip-172-31-87-26:~$ groups u9 u9 : u9 g2   To delete a group run the groupdel command followed by the groupname.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo groupdel g2 ubuntu@ip-172-31-87-26:~$ groups u9 u9 : u9   On success, the groupdel command does not print any output. if the group plan to remove doesn\u0026rsquo;t exist, the system will print an error message saying that the group does not exist.\nNote that it\u0026rsquo;s not possible to move the primary group of an existing user without first removing the user.\nUser Account Monitoring User account monitoring, which means finding out who\u0026rsquo;s logged into the system and what the users do. Because in Linux you can log in as a user and become another one by running the su command that there are two concepts: the real user ID and the effective user ID.\nThe real user ID or are RUID is the id of the user who initially logs in and never changes. On the other hand, EUID or effective user ID is the id of the user who executes a command in the shell. For example, I log in as ubuntu. This is the real user ID, but I can become root by running sudo su. Now, the effective user ID is root.\nwhoami, id command the whoami command, prints out the user name of the effective user ID who is root in this case. Its equivalent to id -un\n1 2 3 4 5 6 7 8 9 10 11  ubuntu@ip-172-31-87-26:~$ whoami ubuntu ubuntu@ip-172-31-87-26:~$ sudo su root@ip-172-31-87-26:/home/ubuntu# whoami root root@ip-172-31-87-26:/home/ubuntu# id -un root root@ip-172-31-87-26:/home/ubuntu# exit exit ubuntu@ip-172-31-87-26:~$ id -un ubuntu   Another useful command is id, which prints the effective user and its groups.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo su root@ip-172-31-87-26:/home/ubuntu# id uid=0(root) gid=0(root) groups=0(root)   who command To display the name of the real user id run who. The name of the user who initially logged in is ubuntu. If you want to find out who\u0026rsquo;s logged into your system run who.\n What the who command does is parsing and displaying the contents of /var/run/utmp. This is the file which logs the current users on the system. There is also another file /var/log/wtmp which is like history for the utmp file, it maintains the logs of all logged in and logged out users in the past. Each time a user logs into the system, a record for that session is written to a file /var/log/wtmp.\n 1 2  root@ip-172-31-87-26:/home/ubuntu# who ubuntu pts/0 2021-04-29 06:51 (174.139.202.93)   At this moment, there is only one user logged in. We will discuss this again when we reach the \u0026ldquo;Network Management in Linux\u0026rdquo; topic.\nTo display the IP address of the logged in user and the time when it logged in, add -H option.\n1 2 3  root@ip-172-31-87-26:/home/ubuntu# who -H NAME LINE TIME COMMENT ubuntu pts/0 2021-04-29 06:51 (174.139.202.93)   You can also add the -a option to display more information.\n1 2 3 4 5 6 7  root@ip-172-31-87-26:/home/ubuntu# who -a system boot 2021-04-14 01:53 LOGIN ttyS0 2021-04-14 01:54 578 id=tyS0 LOGIN tty1 2021-04-14 01:54 616 id=tty1 run-level 5 2021-04-14 01:54 ubuntu + pts/0 2021-04-29 06:51 . 138433 (174.139.202.93) pts/1 2021-04-22 09:38 103062 id=ts/1 term=0 exit=0   w command For a little more context about users, the simple w command provides a list of who\u0026rsquo;s logged in and what they are doing, what\u0026rsquo;s their current process.\n1 2 3 4  root@ip-172-31-87-26:/home/ubuntu# w 09:32:12 up 15 days, 7:38, 1 user, load average: 0.00, 0.00, 0.00 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT ubuntu pts/0 174.139.202.93 06:51 0.00s 0.24s 0.01s sshd: ubun   The header shows:\n the current time, how long the system has been running, 15 days 7 hours and 38 minutes. how many users are currently logged in, the system load average for the past 1, and 15 minutes. Notice that the values for these load averages should be below 1 or there\u0026rsquo;s a problem.  The same information can be displayed running the uptime command.\n1 2  root@ip-172-31-87-26:/home/ubuntu# uptime 09:34:20 up 15 days, 7:40, 1 user, load average: 0.00, 0.00, 0.00   last last is very useful when you need to track users activity or investigate a possible security breach. It shows a listing of the last logged in users.\n1 2 3 4 5 6  root@ip-172-31-87-26:/home/ubuntu# last | head -n 5 ubuntu pts/0 174.139.202.93 Fri Apr 29 06:51 still logged in ubuntu pts/0 174.139.202.93 Thu Apr 28 03:37 - 06:16 (02:39) ubuntu pts/0 174.139.202.93 Wed Apr 27 02:52 - 05:56 (03:03) ubuntu pts/0 174.139.202.93 Tue Apr 26 02:13 - 05:09 (02:56) ubuntu pts/0 174.139.202.93 Tue Apr 26 02:09 - 02:12 (00:02)   last reads the /var/log/wtmp file, and prints information about the logins and logouts of the users. Records are printed in reverse order, starting from the most recent ones. And if you want to restrict the output to a specific user give that username as argument to last, for example,\n1 2  root@ip-172-31-87-26:/home/ubuntu# last reboot reboot system boot 5.11.0-1022-aws Thu Apr 14 01:53 still running   The command has some options that could prove useful sometimes that can be find more information about them in the man page of the command.\nCompendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  ########################## ## Account Management ########################## ## IMPORTANT FILES # /etc/passwd # =\u0026gt; users and info: username:x:uid:gid:comment:home_directory:login_shell # /etc/shadow # =\u0026gt; users\u0026#39; passwords # /etc/group # =\u0026gt; groups # creating a user account useradd [OPTIONS] username # OPTIONS: # -m =\u0026gt; create home directory # -d directory =\u0026gt; specify another home directory # -c \u0026#34;comment\u0026#34; # -s shell # -G =\u0026gt; specify the secondary groups (must exist) # -g =\u0026gt; specify the primary group (must exist) Exemple: useradd -m -d /home/john -c \u0026#34;C++ Developer\u0026#34; -s /bin/bash -G sudo,adm,mail john # changing a user account usermod [OPTIONS] username # =\u0026gt; uses the same options as useradd Example: usermod -aG developers,managers john # =\u0026gt; adding the user to two secondary groups # deleting a user account userdel -r username # =\u0026gt; -r removes user\u0026#39;s home directory as well # creating a group groupadd group_name # deleting a group groupdel group_name # displaying all groups cat /etc/groups # displaying the groups a user belongs to groups # creating admin users # add the user to sudo group in Ubuntu and wheel group in CentOS usermod -aG sudo john ## Monitoring Users ## who -H # =\u0026gt; displays logged in users id # =\u0026gt; displays the current user and its groups whoami # =\u0026gt; displays EUID # listing whos logged in and whats their current process. w uptime # printing information about the logins and logouts of the users last last -u username   ","date":"2021-05-15T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/linux-operation-system-iv-user-account-management/","title":"Linux Operation System IV (User Account Management)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\nAll operations in this series of blogs (Linux Operation System) are done in AWS Elastic Compute Cloud (EC2).\nThis blog contains lots of gif figures (especially in chapter \u0026ldquo;Vim\u0026rdquo;), please wait a moment to load them.\n Pipe and Redirection | pipe Every command or program we run in Linux has three data streams connected to it:\n STDIN (0) or Standard input: which represents data fed into the program or the data the program is working with; STDOUT (1) or Standard output: which is data printed by the program, by default to the terminal; STDERR (2) or Standard error: for error messages and also defaults to the terminal.  Piping and redirection are the means by which we may connect these streams, input output and error among programs and files to direct data in interesting and useful ways.\nUsing the pipe symbol |, we can connect two or more commands at the time. So a pipe lets us send the output of one command as input to another one. For example, If i want to filter out the the 5th largest directory or file in /var:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  ubuntu@ip-172-31-87-26:~$ ls -lSh /var total 44K drwxr-xr-x 2 root root 4.0K Apr 22 06:25 backups drwxr-xr-x 13 root root 4.0K Apr 15 02:41 cache drwxrwxrwt 2 root root 4.0K Nov 29 23:33 crash drwxr-xr-x 41 root root 4.0K Apr 15 03:17 lib drwxrwsr-x 2 root staff 4.0K Apr 15 2020 local drwxrwxr-x 9 root syslog 4.0K Apr 22 00:00 log drwxrwsr-x 2 root mail 4.0K Nov 29 23:30 mail drwxr-xr-x 2 root root 4.0K Nov 29 23:30 opt drwxr-xr-x 7 root root 4.0K Nov 29 23:36 snap drwxr-xr-x 4 root root 4.0K Nov 29 23:31 spool drwxrwxrwt 6 root root 4.0K Apr 22 02:01 tmp lrwxrwxrwx 1 root root 9 Nov 29 23:30 lock -\u0026gt; /run/lock lrwxrwxrwx 1 root root 4 Nov 29 23:30 run -\u0026gt; /run ubuntu@ip-172-31-87-26:~$ ls -lSh /var | head -n 6 | tail -n 1 drwxrwsr-x 2 root staff 4.0K Apr 15 2020 local   Notice that the first line of the STDOUT of ls is \u0026ldquo;total 44K\u0026rdquo; thus we need specify -n 6 for the head command instead of -n 5.\nHere is another example, I want to find out how many authentication failures took place.\n First, using cat which will return the entire file /var/log/auth.log, this is the file where authentications are saved. Then the output of the cat command will be given as input to another comment called grep, which searches for a string inside a file or a text and displays the lines containing the string. And finally, use wc to count the lines of the output of the grep command.  1 2  ubuntu@ip-172-31-87-26:~$ cat -n /var/log/auth.log | grep -a \u0026#34;failure\u0026#34; | wc -l 48   So there were 48 authentication failures saved in the log file.\nHere, wc is short for word count. It prints line, word, and byte counts for each file.\n1 2  ubuntu@ip-172-31-87-26:~$ wc /etc/passwd 34 48 1825 /etc/passwd   So there are 34 lines, 48 words and 1825 characters in that file. If you only want to see the line or word or character counts, then use option -l, -w or -c respectively.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ wc -l /etc/passwd 34 /etc/passwd ubuntu@ip-172-31-87-26:~$ wc -w /etc/passwd 48 /etc/passwd ubuntu@ip-172-31-87-26:~$ wc -c /etc/passwd 1825 /etc/passwd   Redirection Every command you run has three data streams connected to it: STDIN (0) or Standard input, STDOUT (1) or or Standard Output and STDERR (2) or Standard error.\n\u0026gt; Normally, when we run a command, we want to see the output on the screen, in the terminal, but sometimes we may wish to save it to a file, to keep it for the record, feed it into another program or send it to someone else. To redirect the output of a command to a file there\u0026rsquo;s the output redirection operator \u0026gt;; it indicates to the command that we wish the output to be saved in a file instead of being printed on the screen.\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ ls -l total 12 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 12:40 dir1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 11:40 dir2 -rw-r--r-- 1 ubuntu ubuntu 4096 Apr 21 12:47 passwd ubuntu@ip-172-31-87-26:~$ ls -l \u0026gt; ls.txt ubuntu@ip-172-31-87-26:~$ cat ls.txt total 12 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 12:40 dir1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 11:40 dir2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 22 09:18 ls.txt -rw-r--r-- 1 ubuntu ubuntu 4096 Apr 21 12:47 passwd   So you can see that when we run ls -l, the outcome, as expected, is printed out the output on the screen. But when we use \u0026gt; to redirect the STDOUT of ls, the contents will be redirected to the file instead of printing it on the screen.\nWhen using the output redirection will have two case scenarios.\n the file is created if it doesn\u0026rsquo;t exist it is overwritten if it already exists.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  ubuntu@ip-172-31-87-26:~$ ifconfig \u0026gt; ls.txt ubuntu@ip-172-31-87-26:~$ cat ls.txt eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 9001 inet 172.31.87.26 netmask 255.255.240.0 broadcast 172.31.95.255 inet6 fe80::10b8:5eff:fe3f:aa45 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 12:b8:5e:3f:aa:45 txqueuelen 1000 (Ethernet) RX packets 504542 bytes 481777304 (481.7 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 278091 bytes 50826569 (50.8 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 844 bytes 84372 (84.3 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 844 bytes 84372 (84.3 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0   \u0026gt;\u0026gt; If we don\u0026rsquo;t want to override the file, we can instead append or add to the end of the file by using the double greater than operator (\u0026gt;\u0026gt;). the \u0026gt;\u0026gt; operator creates the file if it doesn\u0026rsquo;t exist or appends to it if it already exists.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  ubuntu@ip-172-31-87-26:~$ ifconfig \u0026gt;\u0026gt; icg.txt ubuntu@ip-172-31-87-26:~$ ls -l \u0026gt;\u0026gt; icg.txt ubuntu@ip-172-31-87-26:~$ cat icg.txt eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 9001 inet 172.31.87.26 netmask 255.255.240.0 broadcast 172.31.95.255 inet6 fe80::10b8:5eff:fe3f:aa45 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 12:b8:5e:3f:aa:45 txqueuelen 1000 (Ethernet) RX packets 504662 bytes 481786312 (481.7 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 278179 bytes 50839123 (50.8 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 844 bytes 84372 (84.3 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 844 bytes 84372 (84.3 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 total 20 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 12:40 dir1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 11:40 dir2 -rw-rw-r-- 1 ubuntu ubuntu 894 Apr 22 09:24 icg.txt -rw-rw-r-- 1 ubuntu ubuntu 894 Apr 22 09:21 ls.txt -rw-r--r-- 1 ubuntu ubuntu 4096 Apr 21 12:47 passwd   We see the output of both commands. The output of the second one was appended to the file.\nRecall that everything is a file in Linux or is treated as a file. There is an example which epitomizes this concept. In Linux, even the terminal is treated as a file. To see the file behind a terminal run the tty command.\n1 2  ubuntu@ip-172-31-87-26:~$ tty /dev/pts/0   It\u0026rsquo;s called 0 in pts directory, which resides in dev. Here we open another terminal and redirect the output of a command to the file that represents the other terminal.\nAs expected, we see the outputs of the command ifconfig, ls and echo were written to the file that represents the terminal and appeared in the first terminal.\n2\u0026gt;, 2\u0026gt;\u0026gt; The three streams STDIN, STDOUT and STDERR, attached to any Linux command, are associated with the number 0, 1, 2 respectively. We use these numbers to identify the streams.\nIf we place a number before the \u0026gt; or \u0026gt;\u0026gt; operator, like 2\u0026gt;(Notice that there can not exist space between them), then it will redirect that stream. If we don\u0026rsquo;t use a number, like we have been doing so far, then it defaults to stream 1, which is STDOUT.\nLet\u0026rsquo;s see some examples. I\u0026rsquo;m running a command which will return an error.\n1 2  ubuntu@ip-172-31-87-26:~$ tail -n 3 /etc/shadow tail: cannot open \u0026#39;/etc/shadow\u0026#39; for reading: Permission denied   A non-privileged user cannot see that file. To redirect the error message to a file:\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ tail -n 3 /etc/shadow \u0026gt; err.txt tail: cannot open \u0026#39;/etc/shadow\u0026#39; for reading: Permission denied ubuntu@ip-172-31-87-26:~$ tail -n 3 /etc/shadow 2\u0026gt; err.txt ubuntu@ip-172-31-87-26:~$ cat err.txt tail: cannot open \u0026#39;/etc/shadow\u0026#39; for reading: Permission denied ubuntu@ip-172-31-87-26:~$ ls 2\u0026gt; ls.txt dir1 dir2 err.txt icg.txt ls.txt passwd   You can see that barely \u0026gt; operator can not redirect STDERR stream, and correspondingly 2\u0026gt; can not redirect STDOUT stream. To redirect both the normal output and the error messages of the same command, you should specify \u0026gt; and 2\u0026gt; respectively.\nFor example, the first half of the command I am running will return the expected contents, while the second half will lead an permission error.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ tail -n 3 /etc/passwd /etc/shadow ==\u0026gt; /etc/passwd \u0026lt;== systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false tail: cannot open \u0026#39;/etc/shadow\u0026#39; for reading: Permission denied   And we can redirect them respectively:\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ tail -n 3 /etc/passwd /etc/shadow \u0026gt; pswd.txt 2\u0026gt; error.txt ubuntu@ip-172-31-87-26:~$ cat pswd.txt error.txt ==\u0026gt; /etc/passwd \u0026lt;== systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false tail: cannot open \u0026#39;/etc/shadow\u0026#39; for reading: Permission denied   2\u0026gt;\u0026amp;1 You can also redirect both standard output and standard error to the same file. Here we redirect standard output to a file and the standard error to standard output \u0026amp;1.\nNote that the file contains both the output and the errors. One remark here: we redirect to a stream by placing an \u0026amp; in front of a stream 1. Otherwise it would have redirected to a file called 1.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ tail -n 3 /etc/passwd /etc/shadow \u0026gt; pswd1.txt 2\u0026gt;\u0026amp;1 ubuntu@ip-172-31-87-26:~$ cat pswd1.txt ==\u0026gt; /etc/passwd \u0026lt;== systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false tail: cannot open \u0026#39;/etc/shadow\u0026#39; for reading: Permission denied   You can also attain this result by this:\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ tail -n 3 /etc/passwd /etc/shadow \u0026gt; pswd3.txt 2\u0026gt;\u0026gt;pswd3.txt ubuntu@ip-172-31-87-26:~$ cat pswd3.txt ==\u0026gt; /etc/passwd \u0026lt;== systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false tail: cannot open \u0026#39;/etc/shadow\u0026#39; for reading: Permission denied   redirection and pipe You can also combine command reduction with pipes. Let\u0026rsquo;s filter the line that contains the Mac address using a pipe:\n1 2 3  ubuntu@ip-172-31-87-26:~$ ifconfig | grep \u0026#34;ether\u0026#34; \u0026gt; mac.txt ubuntu@ip-172-31-87-26:~$ cat mac.txt ether 12:b8:5e:3f:aa:45 txqueuelen 1000 (Ethernet)   By the way, if you just want to get the Mac address without extra strings, you could try cut command. It cuts out sections from each line, you can specify the delimiter and field number by -d, -f options.\n1 2  ubuntu@ip-172-31-87-26:~$ ifconfig | grep \u0026#34;ether\u0026#34; | cut -d\u0026#34; \u0026#34; -f 10 12:b8:5e:3f:aa:45   Another example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  ubuntu@ip-172-31-87-26:~$ cat /etc/passwd | tail -n 10 tss:x:106:111:TPM software stack,,,:/var/lib/tpm:/bin/false uuidd:x:107:112::/run/uuidd:/usr/sbin/nologin tcpdump:x:108:113::/nonexistent:/usr/sbin/nologin sshd:x:109:65534::/run/sshd:/usr/sbin/nologin landscape:x:110:115::/var/lib/landscape:/usr/sbin/nologin pollinate:x:111:1::/var/cache/pollinate:/bin/false ec2-instance-connect:x:112:65534::/nonexistent:/usr/sbin/nologin systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false ubuntu@ip-172-31-87-26:~$ cat /etc/passwd | tail -n 10 | cut -d: -f 1 tss uuidd tcpdump sshd landscape pollinate ec2-instance-connect systemd-coredump ubuntu lxd ubuntu@ip-172-31-87-26:~$ cat /etc/passwd | tail -n 10 | cut -d: -f 6 /var/lib/tpm /run/uuidd /nonexistent /run/sshd /var/lib/landscape /var/cache/pollinate /nonexistent / /home/ubuntu /var/snap/lxd/common/lxd   tee command We can store and view the output of the command at the same time by tee command. Its name comes from the t-splitter used in plumbing. It basically breaks the output of a program so that it can be both displayed and saved in a file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  ubuntu@ip-172-31-87-26:~$ cat /etc/passwd | tail -n 10 | cut -d: -f 6 | tee path.txt /var/lib/tpm /run/uuidd /nonexistent /run/sshd /var/lib/landscape /var/cache/pollinate /nonexistent / /home/ubuntu /var/snap/lxd/common/lxd ubuntu@ip-172-31-87-26:~$ cat path.txt /var/lib/tpm /run/uuidd /nonexistent /run/sshd /var/lib/landscape /var/cache/pollinate /nonexistent / /home/ubuntu /var/snap/lxd/common/lxd   By default, tee command overwrites the file if the file is existed. You can instruct tee to append to the end of the file using the option a\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  ubuntu@ip-172-31-87-26:~$ ls | tee -a path.txt dir1 dir2 err.txt error.txt icg.txt ls.txt mac.txt passwd path.txt pswd.txt pswd1.txt pswd2 pswd2.txt pswd3.txt ubuntu@ip-172-31-87-26:~$ cat path.txt /var/lib/tpm /run/uuidd /nonexistent /run/sshd /var/lib/landscape /var/cache/pollinate /nonexistent / /home/ubuntu /var/snap/lxd/common/lxd dir1 dir2 err.txt error.txt icg.txt ls.txt mac.txt passwd path.txt pswd.txt pswd1.txt pswd2 pswd2.txt pswd3.txt   Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  ########################## ## Piping and Command Redirection ########################## ## Piping Examples: ls -lSh /etc/ | head # see the first 10 files by size ps -ef | grep sshd # checking if sshd is running ps aux --sort=-%mem | head -n 3 # showing the first 3 process by memory consumption ## Command Redirection # output redirection ps aux \u0026gt; running_processes.txt who -H \u0026gt; loggedin_users.txt # appending to a file id \u0026gt;\u0026gt; loggedin_users.txt # output and error redirection tail -n 10 /var/log/*.log \u0026gt; output.txt 2\u0026gt; errors.txt # redirecting both the output and errors to the same file tail -n 2 /etc/passwd /etc/shadow \u0026gt; output_errors.txt 2\u0026gt;\u0026amp;1 cat -n /var/log/auth.log | grep -ai \u0026#34;authentication failure\u0026#34; | wc -l cat -n /var/log/auth.log | grep -ai \u0026#34;authentication failure\u0026#34; \u0026gt; auth.txt # =\u0026gt; piping and redirection   Find locate command To install locate on Ubuntu just run:\n1  so sudo apt install mlocate    By the way mlocate is the new implementation of locate. So at least on the latest versions of Ubuntu when you run locate it actually runs mlocate.\n locate is faster than find because it doesn\u0026rsquo;t search in the file system in real time. It uses a previously built database, whereas the find command searches in the real system, through all the actual directories and files, and the find command has more options available.\nThe locate command searches by name and returns a list of path names containing that name in their path. The database used by locate will be updated periodically. It can be done automatically on a daily basis from Chron or you do it manually by running sudo updatedb . The database created by updatedb that will be searched by the locate command is /var/lib/mlocate/mlocate.db To see its path together with some statistics, like how many files and directories are indexed, run locate -S:\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ locate -S Database /var/lib/mlocate/mlocate.db: 23815 directories 178437 files 11104634 bytes in file names 4151686 bytes used to store database   -b The basic usage of the command is locate \u0026lt;filename or dirname\u0026gt;, and it has returned a list with all path names that contain the string. Pay attention that it searches for the string in the entire absolute path name. For example:\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ locate dir1 /home/ubuntu/dir1 /home/ubuntu/dir11 /home/ubuntu/dir1/a1 /home/ubuntu/dir1/a3 /home/ubuntu/dir1/a4 /home/ubuntu/dir1/a5   It will find all the files and directories that have that string in their absolute path name. To search only the base name against the specified pattern, use the -b option. This is the opposite of the default option, which is -w or --wholename.\n1 2 3  ubuntu@ip-172-31-87-26:~$ locate -b dir1 /home/ubuntu/dir1 /home/ubuntu/dir11   When you search for a name, it will replace that name with star name star implicitly. So if you search for dir1, it will replace that name with *dir1* and will also find the files called dir11.\nTo disable this behavior and search for a file by the exact name use a \\ before the name and enclose them in quotation marks. Backslash is a globing character that disables the implicit replacement of of string by *string*.\n1 2  ubuntu@ip-172-31-87-26:~$ locate -b \u0026#34;\\dir1\u0026#34; /home/ubuntu/dir1   updatedb command As said earlier, the locate command does not search for a file in the file system, but in its own database, which should be updated periodically. The locate command can not find anything that was just created, because the new file does not exist in its database.\n1 2  ubuntu@ip-172-31-87-26:~$ mkdir dir1234 ubuntu@ip-172-31-87-26:~$ locate dir1234   We can update the database by sudo updatedb to reflect the latest changes in the file system.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo updatedb ubuntu@ip-172-31-87-26:~$ locate dir1234 /home/ubuntu/dir1234   And this time it found the file.\n-e If we remove a file, locate can still find it, because that change has not been updated into the database. You can update the database as root everytime you use it, but a more efficient way is specifying the -e or --existing option. It will check if the file really exists.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ locate dir1234 /home/ubuntu/dir1234 ubuntu@ip-172-31-87-26:~$ rm -r dir1234 ubuntu@ip-172-31-87-26:~$ locate dir1234 /home/ubuntu/dir1234 ubuntu@ip-172-31-87-26:~$ locate -e dir1234   Note that when you update the mlocate database, it keeps timestamp information in the database. This allows mlocate to know if the contents of a directory changed without reading the contents again and makes updates to the database faster.\n-i Another useful option is -i which will make locate ignore the case. Linux is a case sensitive OS and locate makes no distinction.\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ locate Dir1 ubuntu@ip-172-31-87-26:~$ locate -ie Dir1 /home/ubuntu/dir1 /home/ubuntu/dir11 /home/ubuntu/dir1/a1 /home/ubuntu/dir1/a3 /home/ubuntu/dir1/a4 /home/ubuntu/dir1/a5   -r The last option will discuss is -r, which allows you to search by using simple regular expressions. We will talk about this topic later.\nwhich command The which command locates a command and returns the absolute path of the executable that is called when the command is run. It does this by searching the path for executable files. This can be useful in finding the location of an executable command when you intend to create a shortcut or run it using its absolute path.\nTo search for more executables at once all you have to do is to give the executables as arguments\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ which pwd ifconfig find grep rm /usr/bin/pwd /usr/sbin/ifconfig /usr/bin/find /usr/bin/grep /usr/bin/rm   find command Other Linux command used to search for files is find. It\u0026rsquo;s a complex command line utility that searches for files in a directory and recursively throw subdirectories as well.\nUsing find, you can search for files by name, owner, group type, permissions , date and other criteria. The syntax of the fine command is\n1  find [path] [criteria]   And the form of the criteria is like -option expression. Let\u0026rsquo;s create a new directory with two files. This is the directory tree.\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ mkdir projects ubuntu@ip-172-31-87-26:~$ touch projects/report.txt projects/todo.txt ubuntu@ip-172-31-87-26:~$ tree .  dir2  ls.txt  mac.txt  projects  report.txt  todo.txt 2 directories, 4 files   -name, -iname To search by name, specify -name option. The following command line will search the file whose name is todo.txt in the current directory (.):\n1 2 3  ubuntu@ip-172-31-87-26:~$ find . -name todo.txt ./projects/todo.txt ubuntu@ip-172-31-87-26:~$ find . -name todo.txT   As anything else in Linux, the name of the file is case sensitive. Another option is -iname, which is similar to name, but the match is case insensitive.\n1 2  ubuntu@ip-172-31-87-26:~$ find . -iname todo.tXt ./projects/todo.txt   Notice that find will search by that exact name. If you want to specify only a part of the filename, you should use the wildcards:\n1 2  ubuntu@ip-172-31-87-26:~$ find . -iname *oDo* ./projects/todo.txt   * here means any character, it\u0026rsquo;s recommended to enclose them in double quotes. So it\u0026rsquo;s better to specify the searched string like this\n1 2  ubuntu@ip-172-31-87-26:~$ find . -iname \u0026#34;*oDo*\u0026#34; ./projects/todo.txt   If you search a location where your user is not allowed to see, you\u0026rsquo;ll get the permission denied error. To search in those directories as well, you need to run the command as root.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ find /etc -name passwd find: /etc/polkit-1/localauthority: Permission denied /etc/passwd find: /etc/ssl/private: Permission denied find: /etc/sudoers.d: Permission denied /etc/pam.d/passwd ubuntu@ip-172-31-87-26:~$ sudo find /etc -name passwd /etc/passwd /etc/pam.d/passwd   -delete To find and delete a file pass the -delete option to find. This will delete the file with no possibility of recovering it. So be very careful.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ find . -name todo.txt -delete ubuntu@ip-172-31-87-26:~$ tree .  dir2  ls.txt  mac.txt  projects  report.txt 2 directories, 3 files   -ls Another option we can pass to find is -ls and it will execute the ls command on each file it will find.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  ubuntu@ip-172-31-87-26:~$ tree .  dir1  dir2  dir3  dir1  dir2  ls.txt  mac.txt  projects  report.txt 6 directories, 3 files ubuntu@ip-172-31-87-26:~$ find . -name \u0026#34;dir*\u0026#34; -ls 256056 4 drwxrwxr-x 3 ubuntu ubuntu 4096 Apr 22 12:47 ./dir1 256335 4 drwxrwxr-x 3 ubuntu ubuntu 4096 Apr 22 12:47 ./dir1/dir2 256337 4 drwxrwxr-x 3 ubuntu ubuntu 4096 Apr 22 12:47 ./dir1/dir2/dir3 256338 4 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 22 12:47 ./dir1/dir2/dir3/dir1 307840 4 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 11:40 ./dir2    Notice that -ls option must be placed at the end of the line, because it lists what comes before it. For example, if you set find . -ls -name xxx, the all contents in the current directory will be listed in a long format.\n -type, -maxdepth To search files by type, you can use -type option. -type d means directory, -type f means regular file.\n1 2  ubuntu@ip-172-31-87-26:~$ sudo find /var -type f | wc 3365 3365 150961   For example, I am counting all files from /var and its subdirectories recursively. This is its default behavior. However, you can specify the depth of a directory traversal using -maxdepth option. This will control the maximum searching levels deep.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ sudo find /var -maxdepth 2 -type f | wc 49 49 1356 ubuntu@ip-172-31-87-26:~$ sudo find /var -maxdepth 3 -type f | wc 131 131 3815 ubuntu@ip-172-31-87-26:~$ sudo find /var -maxdepth 4 -type f | wc 3143 3143 134502   -perm -perm option will search by those exact permissions.\n1 2  ubuntu@ip-172-31-87-26:~$ sudo find /etc -perm 755 -type d,f |wc 306 306 7623   Notice that we separate multiple arguments to -type using: \u0026lsquo;,\u0026rsquo;\nNotice that when you specify more options or search criteria, those criteria should be true at the same time, that is when you set multi options, find will search in their intersection, even though you set the same options multi-times.\n1 2  ubuntu@ip-172-31-87-26:~$ sudo find /etc -perm 755 -type d -type f |wc 0 0 0   -size To search by size, use the -size option and the number. it will search the directories recursively for files with the size of one kilobytes rounded up to the next unit.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ sudo find /etc -size 10k -ls 698 12 -rw-r--r-- 1 root root 10037 Feb 7 2020 /etc/nanorc 44 12 -rw-r--r-- 1 root root 9440 Apr 15 06:07 /etc/locale.gen 802 12 -rwxr-xr-x 1 root root 9802 Mar 9 2020 /etc/vmware-tools/vm-support 800 12 -rw-r--r-- 1 root root 10144 Mar 9 2020 /etc/vmware-tools/vgauth/schemas/xmldsig-core-schema.xsd 788 12 -rw-r--r-- 1 root root 9967 Mar 9 2020 /etc/vmware-tools/tools.conf.example   You can also use the + and - prefixes to signify greater than and less than the size of n units.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ sudo find /var -size +10M -size -30M -ls 65439 26408 -rw-r--r-- 1 root root 27037832 Apr 23 2020 /var/lib/apt/lists/us-east-1.ec2.archive.ubuntu.com_ubuntu_dists_focal_universe_i18n_Translation-en 2092 25576 -rw------- 2 root root 26189824 Nov 29 23:36 /var/lib/snapd/seed/snaps/amazon-ssm-agent_4046.snap 65370 27276 -rw------- 2 root root 27930624 Apr 14 09:44 /var/lib/snapd/snaps/amazon-ssm-agent_5163.snap 2092 25576 -rw------- 2 root root 26189824 Nov 29 23:36 /var/lib/snapd/snaps/amazon-ssm-agent_4046.snap 65370 27276 -rw------- 2 root root 27930624 Apr 14 09:44 /var/lib/snapd/cache/a1e309752e4db2726ed7837713e0bcd1c54aa9929fab90cee57b6650f7d0540af959382a5337e7e0f925fa490d593344 256002 24580 -rw-r----- 1 root systemd-journal 25165824 Apr 22 13:17 /var/log/journal/d0b8b5a439b34eb9b32ecd640e8efcec/system.journal   Notice, since you set -size twice times, find will search in the intersection, that is the files have the size greater than 10M and smaller that 25M.\n-atime, -mtime, -ctime To search by timestamps recorded on the files, used the -atime, -mtime or -ctime options and the number that represents the number of days. For example -mtime 0 filters the files modified in the last 24 hours. And if you use -mtime 1, it means files modify to between 1 and 2 days ago.\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ sudo find /etc -atime 2 /etc/e2scrub.conf /etc/magic ubuntu@ip-172-31-87-26:~$ stat /etc/magic File: /etc/magic Size: 111 Blocks: 8 IO Block: 4096 regular file Device: ca01h/51713d\tInode: 635 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2021-04-20 13:11:46.772302006 +0000 Modify: 2020-01-16 20:39:11.000000000 +0000 Change: 2021-11-29 23:34:34.304638987 +0000 Birth: -   You can also use + and - for -atime, -mtime or -ctime to represents \u0026ldquo;greater than\u0026rdquo; and \u0026ldquo;small than\u0026rdquo;.\n1 2  ubuntu@ip-172-31-87-26:~$ sudo find /var -atime -2 -atime +0 | wc 63 63 3554   Notice that + means \u0026ldquo;greater than\u0026rdquo; instead of \u0026ldquo;greater than or equal to\u0026rdquo;, and the same to -. So when we specify -atime -2 -atime +0 what we mean is the files accessed one day ago but within two days. So if you specify -atime -1 -atime +0, you will find nothing, because it means one day ago but within one day. It is an emptyset.\n1 2  ubuntu@ip-172-31-87-26:~$ sudo find /var -atime -1 -atime +0 | wc 0 0 0   -amin, -mmin, -cmin If you want to search by minute instead of day use the -amin, mmin or cmin options. For example, -mmin -60 will find the files that are modified within an hour.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ find . -mmin -120 . ./dir1 ./dir1/dir2 ./dir1/dir2/dir3 ./dir1/dir2/dir3/dir1 ./projects ./projects/report.txt ./.bash_history   So these files (directory is also file) has just been modified within 2 hours.\n-user, -group To search by owner use the -user option and to search by group, use the -group option.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  ubuntu@ip-172-31-87-26:~$ sudo cp -rp /var/log ~ ubuntu@ip-172-31-87-26:~$ ls -l total 20 drwxrwxr-x 3 ubuntu ubuntu 4096 Apr 22 12:47 dir1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 11:40 dir2 drwxrwxr-x 9 root syslog 4096 Apr 22 00:00 log -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 22 09:51 ls.txt -rw-rw-r-- 1 ubuntu ubuntu 61 Apr 22 10:04 mac.txt drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 22 12:40 projects ubuntu@ip-172-31-87-26:~$ sudo find . -user root -type d ./log ./log/unattended-upgrades ./log/dist-upgrade ./log/journal ./log/journal/d0b8b5a439b34eb9b32ecd640e8efcec ./log/apt ./log/private ./log/amazon ./log/amazon/ssm ./log/amazon/ssm/audits ubuntu@ip-172-31-87-26:~$ sudo find . -group syslog -type d ./log   -not To negate an option, use -not before the option.\n1 2 3 4 5 6 7 8 9 10 11  ubuntu@ip-172-31-87-26:~$ sudo find . -not -user root -type d . ./dir1 ./dir1/dir2 ./dir1/dir2/dir3 ./dir1/dir2/dir3/dir1 ./.ssh ./.cache ./projects ./dir2 ./log/landscape   1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ sudo find -not -type d -not -group syslog | wc 44 44 1062 ubuntu@ip-172-31-87-26:~$ sudo find -not -perm 755 |wc 60 60 1315 ubuntu@ip-172-31-87-26:~$ ls -l ./ls.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 22 09:51 ./ls.txt   -exec, -ok Sometimes you want to operate or execute a command on the files that are found. Theres the -exec option, which comes from execute, that does that. This option will run the following command on each file found by find.\nFor example, I copy if I want to see the last 3 lines of of the files in the ~ that are modified in the last 24 hours. I can run:\n1  sudo find ~ -type f -mtime 0 -exec tail -n3 {} \\;    sudo: There are protected files among the founded files, so we need switch to root user; find ~ -type f -mtime 0: just as we talked before, find the files in ~ that are modified in 24h. -exec: run tail on each file found by find. tail -n3 {}: the pair of curly braces will expand to the file (\u0026rsquo;s absolute path) found by the find command. \\;: The command after -exec ends when backslash semicolon appears; that\u0026rsquo;s always there at the end, the syntax of this construction.  Let\u0026rsquo;s do another example. I want to back up or make a copy of all configuration files from /etc that were modified in the last week.\nFor each file found by the find command, the cp command is executed. cp simply requires at least 2 arguments: a source and the destination; the source will be the file found by find (which is represented by the pair of curly braces) and the destination, the /root/backup directory.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ sudo mkdir /root/backup ubuntu@ip-172-31-87-26:~$ sudo find /etc -mtime -7 -type f -exec cp {} /root/backup \\; ubuntu@ip-172-31-87-26:~$ sudo ls /root/backup 01autoremove-kernels shadow snap-snapd-15534.mount ubuntu@ip-172-31-87-26:~$ sudo ls -l /root/backup total 12 -r--r--r-- 1 root root 1874 Apr 25 02:42 01autoremove-kernels -rw-r----- 1 root root 1067 Apr 25 02:42 shadow -rw-r--r-- 1 root root 285 Apr 25 02:42 snap-snapd-15534.mount   Note that there is also the -ok option, which does the same is -exec, but it will ask the user first. If the user agrees, it runs the command that follows. Otherwise it just returns false.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ sudo find /etc -mtime -7 -type f -ok cp {} /root/backup \\;\u0026lt; cp ... /etc/apt/apt.conf.d/01autoremove-kernels \u0026gt; ? y \u0026lt; cp ... /etc/systemd/system/snap-snapd-15534.mount \u0026gt; ? y \u0026lt; cp ... /etc/shadow \u0026gt; ? y ubuntu@ip-172-31-87-26:~$   grep command The grep command searches in a text file or output of another command for a pattern referenced to as a regular expression and prints out all lines that contain the pattern. The syntax of grep is\n1  grep [OPTION...] PATTERNS [FILE...]   So to search for a pattern in a file you write grep, You need specify some desired options optionally, the pattern to search for and the name of the file as a correct absolute or relative path.\nFor example, Im searching in the configuration file of the ssh client, /etc/ssh/ssh_config, for the string \u0026ldquo;user\u0026rdquo;. And it displayed all the lines that contained the string user.\n1 2 3  ubuntu@ip-172-31-87-26:~$ grep user /etc/ssh/ssh_config # users, and the values can be changed in per-user configuration files # 2. user-specific file   If the pattern contains spaces, you should enclose it in quotes.\n1 2 3  ubuntu@ip-172-31-87-26:~$ grep \u0026#34;command line\u0026#34; /etc/ssh/ssh_config # or on the command line. # 1. command line options   -i To ignore the latter case, use the -i option.\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ grep SSH /etc/ssh/ssh_config ubuntu@ip-172-31-87-26:~$ grep -i SSH /etc/ssh/ssh_config # This is the ssh client system-wide configuration file. See # ssh_config(5) for more information. This file provides defaults for # ssh_config(5) man page. Include /etc/ssh/ssh_config.d/*.conf # IdentityFile ~/.ssh/id_rsa # IdentityFile ~/.ssh/id_dsa # IdentityFile ~/.ssh/id_ecdsa # IdentityFile ~/.ssh/id_ed25519 # MACs hmac-md5,hmac-sha1,umac-64@openssh.com # ProxyCommand ssh -q -W %h:%p gateway.example.com   -n To print the line numbers, where the pattern was found in the file, use the -n option.\n1 2 3 4 5 6 7 8 9 10 11  ubuntu@ip-172-31-87-26:~$ grep -in SSH /etc/ssh/ssh_config 2:# This is the ssh client system-wide configuration file. See 3:# ssh_config(5) for more information. This file provides defaults for 17:# ssh_config(5) man page. 19:Include /etc/ssh/ssh_config.d/*.conf 36:# IdentityFile ~/.ssh/id_rsa 37:# IdentityFile ~/.ssh/id_dsa 38:# IdentityFile ~/.ssh/id_ecdsa 39:# IdentityFile ~/.ssh/id_ed25519 42:# MACs hmac-md5,hmac-sha1,umac-64@openssh.com 48:# ProxyCommand ssh -q -W %h:%p gateway.example.com   -w To search for whole words, use the -w option.\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ grep -inw SSH /etc/ssh/ssh_config 2:# This is the ssh client system-wide configuration file. See 19:Include /etc/ssh/ssh_config.d/*.conf 36:# IdentityFile ~/.ssh/id_rsa 37:# IdentityFile ~/.ssh/id_dsa 38:# IdentityFile ~/.ssh/id_ecdsa 39:# IdentityFile ~/.ssh/id_ed25519 48:# ProxyCommand ssh -q -W %h:%p gateway.example.com   You can see the 42 line which contains \u0026ldquo;openssh\u0026rdquo; has been ignored.\n-v To invert the sense of matching, so to select and non matching lines, use the -v option; v comes from inverse. So the lines that do not contain the letter \u0026ldquo;a\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  ubuntu@ip-172-31-87-26:~$ grep -inv a /etc/ssh/ssh_config 1: 6: 9:# 2. user-specific file 10:# 3. system-wide file 14: 18: 19:Include /etc/ssh/ssh_config.d/*.conf 20: 21:Host * 32:# CheckHostIP yes 34:# ConnectTimeout 0 39:# IdentityFile ~/.ssh/id_ed25519 40:# Port 22 44:# Tunnel no 49:# RekeyLimit 1G 1h   -a In a binary file as if it were a text file, use the -a option. For example, the executable file is in binary:\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ which ls /usr/bin/ls ubuntu@ip-172-31-87-26:~$ grep help /usr/bin/ls Binary file /usr/bin/ls matches ubuntu@ip-172-31-87-26:~$ grep -a help /usr/bin/ls | tail -n 1 General help using GNU software: \u0026lt;https://www.gnu.org/gethelp/\u0026gt;   -R To search recursively in the directory and search for the pattern in every text file in that directory and all its subdirectories, use the -R option. And it displayed all files that contain that string.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  ubuntu@ip-172-31-87-26:~$ sudo grep -R 127.0.0.1 /etc Binary file /etc/alternatives/rlogin matches Binary file /etc/alternatives/rsh matches /etc/security/access.conf:#+:root:127.0.0.1 /etc/hosts:127.0.0.1 localhost /etc/dhcp/dhclient.conf:#prepend domain-name-servers 127.0.0.1; /etc/dhcp/dhclient.conf:# option domain-name-servers 127.0.0.1; /etc/cloud/templates/ntp.conf.debian.tmpl:restrict 127.0.0.1 /etc/cloud/templates/ntp.conf.sles.tmpl:restrict 127.0.0.1 /etc/cloud/templates/hosts.alpine.tmpl:127.0.0.1 localhost localhost.localdomain /etc/cloud/templates/hosts.alpine.tmpl:127.0.0.1 localhost4 localhost4.localdomain4 /etc/cloud/templates/ntp.conf.fedora.tmpl:restrict 127.0.0.1 /etc/cloud/templates/hosts.suse.tmpl:127.0.0.1 localhost.localdomain localhost /etc/cloud/templates/hosts.suse.tmpl:127.0.0.1 localhost4.localdomain4 localhost4 /etc/cloud/templates/ntp.conf.rhel.tmpl:restrict 127.0.0.1 /etc/cloud/templates/ntp.conf.opensuse.tmpl:restrict 127.0.0.1 /etc/cloud/templates/hosts.photon.tmpl:127.0.0.1 {{fqdn}} {{hostname}} /etc/cloud/templates/hosts.photon.tmpl:127.0.0.1 localhost.localdomain localhost /etc/cloud/templates/hosts.photon.tmpl:127.0.0.1 localhost4.localdomain4 localhost4 /etc/cloud/templates/hosts.debian.tmpl:127.0.0.1 localhost /etc/cloud/templates/ntp.conf.ubuntu.tmpl:restrict 127.0.0.1 /etc/cloud/templates/hosts.redhat.tmpl:127.0.0.1 {{fqdn}} {{hostname}} /etc/cloud/templates/hosts.redhat.tmpl:127.0.0.1 localhost.localdomain localhost /etc/cloud/templates/hosts.redhat.tmpl:127.0.0.1 localhost4.localdomain4 localhost4 /etc/cloud/templates/ntp.conf.photon.tmpl:restrict 127.0.0.1 /etc/cloud/templates/hosts.freebsd.tmpl:127.0.0.1 {{fqdn}} {{hostname}} /etc/cloud/templates/hosts.freebsd.tmpl:127.0.0.1 localhost.localdomain localhost /etc/cloud/templates/hosts.freebsd.tmpl:127.0.0.1 localhost4.localdomain4 localhost4 ubuntu@ip-172-31-87-26:~$   -s, -c To suppress error messages about the nonexisting or unreadable files, use -s option. If you just want to see the number of the matches use the -c option. Of course, You could get the same result by using a pipe and a wc -l command.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ grep -invc a /etc/ssh/ssh_config 15 ubuntu@ip-172-31-87-26:~$ grep -inv a /etc/ssh/ssh_config | wc -l 15   -A, -B, -C The grep command can be used to search in the output of another command as well. We will talk about this in the following subsections.\nFor example, I want to search for \u0026ldquo;ether\u0026rdquo; in the outcome of ifconfig:\n1 2  ubuntu@ip-172-31-87-26:~$ ifconfig | grep ether ether 12:b8:5e:3f:aa:45 txqueuelen 1000 (Ethernet)   And there\u0026rsquo;s a single line. To see a context, which means a few after before and before the match, use -A and -B with a number respectively:\n1 2 3 4 5  ubuntu@ip-172-31-87-26:~$ ifconfig | grep -A 1 -B 2 ether inet 172.31.87.26 netmask 255.255.240.0 broadcast 172.31.95.255 inet6 fe80::10b8:5eff:fe3f:aa45 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 12:b8:5e:3f:aa:45 txqueuelen 1000 (Ethernet) RX packets 617506 bytes 506492068 (506.4 MB)   You can also use -C from context and the number and it will print out the same number of lines before and after the match.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ ifconfig | grep -C 2 ether inet 172.31.87.26 netmask 255.255.240.0 broadcast 172.31.95.255 inet6 fe80::10b8:5eff:fe3f:aa45 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 12:b8:5e:3f:aa:45 txqueuelen 1000 (Ethernet) RX packets 617594 bytes 506500386 (506.5 MB) RX errors 0 dropped 0 overruns 0 frame 0   Another common example is to check what program is listening on a specific port. To display all ports run: netstat with some options:\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ sudo netstat -tupan Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.53:53 0.0.0.0:* LISTEN 27691/systemd-resol tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 648/sshd: /usr/sbin tcp 0 0 172.31.87.26:22 61.177.172.142:19913 SYN_RECV - tcp 0 0 172.31.87.26:22 61.177.172.142:30233 SYN_RECV - tcp 0 0 172.31.87.26:22 61.177.172.142:32886 SYN_RECV - tcp 0 340 172.31.87.26:22 174.139.202.93:40218 ESTABLISHED 119654/sshd: ubuntu tcp6 0 0 :::22 :::* LISTEN 648/sshd: /usr/sbin udp 0 0 127.0.0.53:53 0.0.0.0:* 27691/systemd-resol udp 0 0 172.31.87.26:68 0.0.0.0:* 27686/systemd-netwo   It has displayed all open ports; and to check if there is something that\u0026rsquo;s listening on port 53 and port 80, filter the output using grep like this:\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ sudo netstat -tupan | grep 53 tcp 0 0 127.0.0.53:53 0.0.0.0:* LISTEN 27691/systemd-resol tcp 0 0 172.31.87.26:22 61.177.172.142:53550 SYN_RECV - udp 0 0 127.0.0.53:53 0.0.0.0:* 27691/systemd-resol ubuntu@ip-172-31-87-26:~$ sudo netstat -tupan | grep 80 ubuntu@ip-172-31-87-26:~$   So there are some processes that listen on port 53, and there is no process listening on port 80.\nLet\u0026rsquo;s see the last example: store all files' name (exclude the directories') inside a directory into a file.\n ls -RF: -R means list the contents recrusively, -F adds the type symbol at the end of item\u0026rsquo;s name, for example, the item\u0026rsquo;s end with / is a directory. grep -v /: ignore the directories in /var/log grep -v \u0026quot;^$\u0026quot;: ignore the white space line sort: sort by file name in alphabeta ascending order.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  ubuntu@ip-172-31-87-26:~$ sudo ls -RF /var/log | grep -v / | grep -v \u0026#34;^$\u0026#34; | sort \u0026gt; files ubuntu@ip-172-31-87-26:~$ cat files alternatives.log amazon-ssm-agent-audit-2022-04-14 amazon-ssm-agent.log auth.log auth.log.1 auth.log.2.gz btmp cloud-init-output.log cloud-init.log dmesg dpkg.log eipp.log.xz hibernate.log history.log kern.log kern.log.1 kern.log.2.gz lastlog sysinfo.log syslog syslog.1 syslog.2.gz syslog.3.gz syslog.4.gz syslog.5.gz syslog.6.gz syslog.7.gz system.journal term.log ubuntu-advantage-timer.log unattended-upgrades-dpkg.log unattended-upgrades-shutdown.log unattended-upgrades.log user-1000.journal wtmp   strings command The grep command searches for strings in text files. However, not only text files contain ASCII characters. Binary files such as executables may contain strings in human-readable text. In the compilation process ascii characters are embedded into the binary files and sometimes, when an application is deployed, some ASCII text is packed into the binary file by the developer.\nThis contains useful information about the binary, like a short help or copyright information. There are also binary files used by different applications that embed sensitive information as credentials. The strings command extracts printable characters from binary files. Before run springs, we need install it on ubuntu:\n1  sudo apt install binutils   Let\u0026rsquo;s see the printable characters from the ls command.\nThese are the ASCII characters embedded into ls. And then we can use pipe and grep or less to find some specific strings:\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ strings /usr/bin/ls | grep help # Configuration file for dircolors, a utility to help you set the %s online help: \u0026lt;%s\u0026gt; help Try \u0026#39;%s --help\u0026#39; for more information. --help display this help and exit General help using GNU software: \u0026lt;https://www.gnu.org/gethelp/\u0026gt;   or\nCompendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66  ########################## ## Finding Files (find, locate) ########################## ## LOCATE ## # updating the locate db sudo updatedb # displaying statistics locate -S # finding file by name locate filename # =\u0026gt; filename is expanded to *filename* locate -i filename # =\u0026gt; the filename is case insensitive locate -b \u0026#39;\\filename\u0026#39; # =\u0026gt; finding by exact name # finding using the basename locate -b filename # finding using regular expressions locate -r \u0026#39;regex\u0026#39; # checking that the file exists locate -e filename # showing command path which command which -a command ## FIND ## find PATH OPTIONS # Example: find ~ -type f -size +1M # =\u0026gt; finding all files in ~ bigger than 1 MB ## Options: # -type f, d, l, s, p # -name filename # -iname filename # =\u0026gt; case-insensitive # -size n, +n, -n # -perm permissions # -links n, +n, -n # -atime n, -mtime n, ctime n # -user owner # -group group_owner ########################## ## Searching for text patterns (grep) ########################## grep [OPTIONS] pattern file Options: -n # =\u0026gt; print line number -i # =\u0026gt; case insensitive -v # inverse the match -w # search for whole words -a # search in binary files -R # search in directory recursively -c # display only the no. of matches -C n # display a context (n lines before and after the match) # printing ASCII chars from a binary file strings binary_file   Compare cmp command This is especially useful when you want to compare either the output of the same command at different moments or the differences between a back up configuration file and the one currently used by the system.\nTo see if there was any difference between the two files, you can run the cmp command:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  ubuntu@ip-172-31-87-26:~$ ifconfig \u0026gt; a ubuntu@ip-172-31-87-26:~$ ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=50 time=1.01 ms 64 bytes from 8.8.8.8: icmp_seq=2 ttl=50 time=0.848 ms 64 bytes from 8.8.8.8: icmp_seq=3 ttl=50 time=0.900 ms 64 bytes from 8.8.8.8: icmp_seq=4 ttl=50 time=0.879 ms 64 bytes from 8.8.8.8: icmp_seq=5 ttl=50 time=0.981 ms ^C --- 8.8.8.8 ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 4005ms rtt min/avg/max/mdev = 0.848/0.924/1.012/0.062 ms ubuntu@ip-172-31-87-26:~$ ifconfig \u0026gt; b ubuntu@ip-172-31-87-26:~$ cmp a b a b differ: byte 291, line 5   cmp compares the files byte by byte and displays the location of the first mismatch. It doesn\u0026rsquo;t see how the files differ and if they are identical, no message is displayed. Note that cmp can be used to compare binary files as well.\n1 2 3 4 5  ubuntu@ip-172-31-87-26:~$ cp /usr/bin/ls . ubuntu@ip-172-31-87-26:~$ cmp ls .usr/bin/ls cmp: .usr/bin/ls: No such file or directory ubuntu@ip-172-31-87-26:~$ cmp ls /usr/bin/ls ubuntu@ip-172-31-87-26:~$   So, they\u0026rsquo;re identical, because nothing was displayed.\nsha256sum command Another way to compare 2 binary files to see if they are identical is to calculate and compare their hash. The hash is a unique cryptographic id for each file. To calculate the hashes of the ls files, use sha256sum\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ sha256sum ls /usr/bin/ls 1e39354a6e481dac48375bfebb126fd96aed4e23bab3c53ed6ecf1c5e4d5736d ls 1e39354a6e481dac48375bfebb126fd96aed4e23bab3c53ed6ecf1c5e4d5736d /usr/bin/ls ubuntu@ip-172-31-87-26:~$ echo \u0026#34;0\u0026#34; \u0026gt;\u0026gt; ls ubuntu@ip-172-31-87-26:~$ sha256sum ls /usr/bin/ls d70a063927fb3446e28b4001da6486250e1760fdf77aa7873ebfdf91cee5bf69 ls 1e39354a6e481dac48375bfebb126fd96aed4e23bab3c53ed6ecf1c5e4d5736d /usr/bin/ls   Even a single bit were different, the hashes would be different. And they are not identical anymore:\n1 2  ubuntu@ip-172-31-87-26:~$ cmp ls /usr/bin/ls cmp: EOF on /usr/bin/ls after byte 142144, in line 448   diff command Now, if you want to see exactly how they differ, you should run the diff command. It\u0026rsquo;s used only for text files. Diff stands for difference and is used to display the differences in the files by comparing them line by line.\nWe now create two files c and d, and modify d at 3 places: change, delete and add respectively:\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ diff c d 1c1 \u0026lt; eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 9001 --- \u0026gt; et10: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 9001 14d13 \u0026lt; RX packets 1406 bytes 136780 (136.7 KB) 18a18 \u0026gt; ee   You can see diff return 3 lines that be modified: 1c1, 14d13 and 18a18\nthe number is showing us the line that differs. and the character c, d and a indicate that the line has been changed, deleted and added respectively. Lines preceded by a less than sign (\u0026lt;) are lines from the first file, and \u0026gt; are from the second.\nYou can see that 14d13 only has \u0026lt;, because we delete the corresponging line in the second file; and 18a18 only has \u0026gt; for the similar reason.\nThere are some useful options of the diff command like -b to ignore blank lines; -w to ignore white spaces; And -i do ignore case differences in file contents. And -c for a more detailed comparison. And -y to display the entire file\u0026rsquo;s contents on 2 column side, the change, delete, add lines will be signed by |, \u0026gt; and \u0026lt;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  ubuntu@ip-172-31-87-26:~$ diff -y c d eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 9001 |\tet10: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 9001 inet 172.31.87.26 netmask 255.255.240.0 broadcast 1\tinet 172.31.87.26 netmask 255.255.240.0 broadcast 1 inet6 fe80::10b8:5eff:fe3f:aa45 prefixlen 64 scopei\tinet6 fe80::10b8:5eff:fe3f:aa45 prefixlen 64 scopei ether 12:b8:5e:3f:aa:45 txqueuelen 1000 (Ethernet)\tether 12:b8:5e:3f:aa:45 txqueuelen 1000 (Ethernet) RX packets 626456 bytes 509593982 (509.5 MB)\tRX packets 626456 bytes 509593982 (509.5 MB) RX errors 0 dropped 0 overruns 0 frame 0\tRX errors 0 dropped 0 overruns 0 frame 0 TX packets 406778 bytes 108993151 (108.9 MB)\tTX packets 406778 bytes 108993151 (108.9 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisi\tTX errors 0 dropped 0 overruns 0 carrier 0 collisi lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536\tlo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0\tinet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt;\tinet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback)\tloop txqueuelen 1000 (Local Loopback) RX packets 1406 bytes 136780 (136.7 KB)\t\u0026lt; RX errors 0 dropped 0 overruns 0 frame 0\tRX errors 0 dropped 0 overruns 0 frame 0 TX packets 1406 bytes 136780 (136.7 KB)\tTX packets 1406 bytes 136780 (136.7 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisi\tTX errors 0 dropped 0 overruns 0 carrier 0 collisi \u0026gt;\tee ubuntu@ip-172-31-87-26:~$   Vim Intro vim is a very powerful text editor and it offers extensive text editing options. To use vim on a file you type in vim and the name of the file using a correct, absolute or relative path. If the file already exists, wim will open it for editing. But if it doesn\u0026rsquo;t exist, then wim will create the file.\n1 2  ubuntu@ip-172-31-87-26:~$ ifconfig \u0026gt; a ubuntu@ip-172-31-87-26:~$ vim a   Vim is a model editor, it has three basic modes of operation:\n command mode insert mode last line modes.  Command mode When you open a file in vim, you are in the command mode. In this mode, characters you type get interpreted as commands.\nFor example, x will delete the character currently under the cursor.\nr will replace the character under the cursor.\nTo get back to command mode from any other mode, just press ESC one or more times.\n Tips: when you are in a mode and get stuck press a few times the escape key and it will take you to the command mode.\n Insert mode To start writing text, as in a normal text editor, you need to enter into insert mode. To enter into insert mode from command mode press lowercase i and it will insert the text before the cursor. If you press the uppercase I it will start inserting the text at the beginning of the current line.\nIf you press a, vim will append the text after the cursor; an upper case A will make it append the text to the end of the current line.\nIf you press o, vim write the text on ** a new line below** the current line, and O to write on ** a new line above** the current line.\n So in a nutshell, to enter the insert mood from command mode type i, a and o, both lower and upper case.\n Last line mode The 3rd vim mode is the last line mode; you can only get to the last line mode from command mode and to get into last line, you press the colon key : (shift+;), then colon character will appear at the beginning of the last line of the vim editor window and the cursor will be moved to that position.\nFrom vim last line mode, you can do all sorts of things like saving the file (w!+enter), quitting the file without saving it (q!+enter) and quit with saving (wq!+enter).\nOne of most useful shortcuts while in the command mode is to save and quit the file quickly and easily by typing shift+z+shift+z (ZZ).\nMore on vim run shell command You can run a shell command from within your vim editor session at last line mode by:\n1  !\u0026lt;command\u0026gt;   the outcome returned will be print in bash terminal.\nsearch To search for a string forward in the command mode type:\n1  /\u0026lt;string\u0026gt; + enter   and backword type:\n1  ?\u0026lt;string\u0026gt; + enter   Any matches in the current window will be highlighted. You can then navigate next matches with n and last matches with N. Go to the end of the file by typing capital G, and go to the start of the file by gg. These work exactly the same as for the less command.\njump to word Another quick way to find (and jump to) the next occurrence of the word under cursor in command mode is to press an asterisk *; to find (and jump to) the last occurrence of the word under cursor, press hash #.\njump to line If you want to move to a specific line in the last line mode, type the line number and press enter. This is useful when you get an error in the shell script or configuration file and it says that the error is on a specific line.\nTo move to the end of the file type capital G and to move to the beginning of the file type gg in the command mode. Or you can type 1 in the last line mode to jump the the start.\nreplace To search for a pattern and replace all occurrences in the last line mode, write\n1  %s/\u0026lt;str1\u0026gt;/\u0026lt;str2\u0026gt;/g + enter   in the last line. For example, to replace all \u0026ldquo;err\u0026rdquo; to \u0026ldquo;ERR\u0026rdquo;, write %s/err/ERR/g:\nundo Now, I think that was a mistake and I can undo till the last saved version of the document by typing e! + enter in the last line mode.\nNote that it didn\u0026rsquo;t undo only your last operation, but up till the moment you\u0026rsquo;ve last saved the file.\nTo undo the last operation only in command mode type u. (This is equivalent to ctrl+z in a graphical text editor). And to redo the previous operation type control + r.\ncut, copy and paste In command mode, press dd to cut the line. Press capital P to paste before the cursor or small p to paste after.\nTo cut more than one line, typed the number and then dd. For example, if you wan to cut 10 lines below the cursor then you type 10dd + enter in command mode.\nTo copy a text, the first step is to position the cursor where you want to begin copying, press\n lowercase v to select characters (letter), uppercase V to select whole line control + v to select rectangular blocks.  press v again to unselect. Then use of the arrow keys to define the selection. After selecting the text press y to copy. y stands for Young in recent which in other editors is usually called Copy.\nconfig In the last line mode, you can run a many vim configuration commands. For example, to show line numbers in the current editor window, write\n1  set nu + enter   in the last line. And it has displayed the line numbers. if you don\u0026rsquo;t want to display the line number (for example, when you want to copy and paste from vim into another editor and don\u0026rsquo;t want to take the line number as well), write\n1  set nonu   Another useful feature is syntax highlighting. To enable or disable syntax colors in the last line mode run\n1 2 3  syntax on # or syntax off   Note that these changes are not permanent if you exit vim and then open it again. To make the changes permanent in the user\u0026rsquo;s home directory create or edit a file called .vimrc, and write these commands in it. So run\n1  ubuntu@ip-172-31-87-26:~$ vim .vimrc   and add the commands.\nmulti files Let\u0026rsquo;s see how to work with multiple files at once, this is useful when you want to copy and paste between files or to visually compare the file contents. You can open more files like\n1  vim \u0026lt;file1\u0026gt; \u0026lt;file2\u0026gt;   vim will dislplay the first file, but you can move to the next one by typing n and get back by N in the last line mode.\nAnother way to work with multiple files at the same time is to use the -o option, which will split the window and open a window for each file. And you can move between windows by pressing control + w in command mode.\nSometimes it\u0026rsquo;s useful to view to files side by side and see the differences between them. This is especially useful when you change some configuration options of a service. To do that, you can use the -d option or use vimdiff command. And you can move between files by using control + w.\n1 2 3  vim -d \u0026lt;file1\u0026gt; \u0026lt;file2\u0026gt; # is same as vimdiff \u0026lt;file1\u0026gt; \u0026lt;file2\u0026gt;   swap file If you exit vim without properly closing it, so without using a command or a shortcut, like closing the terminal or pressing control +z, the next time when you open the same file, you\u0026rsquo;ll be presented with a message like\nYou can try to recover the file by pressing R and then saving it normally or you can simply delete the swap file that was created:\nCompendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55  ########################## ## VIM ########################## Modes of operation: Command, Insert, and Last Line Modes. VIM Config File: ~/.vimrc # Entering the Insert Mode from the Command Mode i =\u0026gt; insert before the cursor I =\u0026gt; insert at the beginning of the line a =\u0026gt; insert after the cursor A =\u0026gt; insert at the end of the line o =\u0026gt; insert on the next line # Entering the Last Line Mode from the Command Mode : # Returning to Command Mode from Insert or Last Line Mode  ESC # Shortcuts in Last Line Mode w! =\u0026gt; write/save the file q! =\u0026gt; quit the file without saving wq! =\u0026gt; save/write and quit e! =\u0026gt; undo to the last saved version of the file set nu =\u0026gt; set line numbers set nonu =\u0026gt; unset line numbers syntax on|off %s/search_string/replace_string/g # Shortcuts in Command Mode x =\u0026gt; remove char under the cursor dd =\u0026gt; cut the current line 5dd =\u0026gt; cut 5 lines ZZ =\u0026gt; save and quit u =\u0026gt; undo G =\u0026gt; move to the end of file $ =\u0026gt; move to the end of line 0 or ^ =\u0026gt; move to the beginning of file :n (Ex :10) =\u0026gt; move to line n Shift+v =\u0026gt; select the current line y =\u0026gt; yank/copy to clipboard p =\u0026gt; paste after the cursor P =\u0026gt; paste before the cursor /string =\u0026gt; search for string forward ?string =\u0026gt; search for string backward n =\u0026gt; next occurrence N =\u0026gt; previous occurrence # Opening more files in stacked windows vim -o file1 file2 # Opening more files and highlighting the differences vim -d file1 file2 Ctrl+w =\u0026gt; move between files   tar command tar is the most widely used command to create compressed archive files that can be moved easily from one disk or machine to another. The syntax of tar is\n1  tar [option]... \u0026lt;destinate file\u0026gt; \u0026lt;compressed files\u0026gt;...   tar is a complex tool used to both archive and compress files as well as extract the resulting archives. Before continuing, I want to make a distinction between two terms: archiving and compressing. Archiving means that if you take 10 files of 100 kilobytes and archive them, the resulting single file is 1000 kilobytes. On the other hand, if you compress those 10 files, you might find that the resulting file has a wide range from only a few kilobytes to close to the original size, depending on the original file type. In general, text files have a higher compression ratio.\ntar alone only archives the files, it does not compress them as well. But using specific options tar can pass the files to the compression algorithm and the final result will be compressed, without using another dedicated command for that.\n-c, -z, -j, -v, -f  -c: creates an archive. (By the way, the archive file are called tarballs.) -z: also compress the archive using gzip or gnuzip compression.  so if you want to compress a file/dir you need specify -cz at the same time.\n -v: means verbose, prints the process of the command in the terminal. -f: allows you to specify the filename of the archive.  1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ mkdir -p d1/d2/d3/d4 ubuntu@ip-172-31-87-26:~$ tar -czvf d1.tar.gz d1 d1/ d1/d2/ d1/d2/d3/ d1/d2/d3/d4/ ubuntu@ip-172-31-87-26:~$ ls a b d d1.tar.gz dir2 ls mac.txt projects auth.log c d1 dir1 files ls.txt passwd   You can see tar will compress every other directory inside the specified directory itself. In other words, it works recursively.\nNote that the type of the file is not given by its extension. It\u0026rsquo;s only helpful to visually find out what kind a specific file is. In this case it\u0026rsquo;s an archive that was further compressed with gzip.\nIn other Linux commands, it doesn\u0026rsquo;t matter the order of the options, but in the case of tar it\u0026rsquo;s mandatory to specify the archive file name after the option -f. If I write the -f option bat the beginning Ill get an error like this.\n1 2 3  ubuntu@ip-172-31-87-26:~$ tar -fczv d1.tar.gz d1 tar: You must specify one of the \u0026#39;-Acdtrux\u0026#39;, \u0026#39;--delete\u0026#39; or \u0026#39;--test-label\u0026#39; options Try \u0026#39;tar --help\u0026#39; or \u0026#39;tar --usage\u0026#39; for more information.   And another useful compression algorithm is bzip2 which creates smaller files, but at the cost of speed. However, nowadays computers are so fast that most users won\u0026rsquo;t notice much of a difference between the between the times taken by gzip or bzip2 to compress a group of files.\nTo bzip2 instead of gzip, simply use the -j option instead of -z.\n1 2 3 4 5 6 7 8 9 10 11  ubuntu@ip-172-31-87-26:~$ tar -cjvf d2/d1.tar.bzip2 d1 d1/ d1/d2/ d1/d2/d3/ d1/d2/d3/d4/ ubuntu@ip-172-31-87-26:~$ tree d2 d2  d1.tar.bzip2  d1.tar.gz 0 directories, 2 files   tar is frequently used to compress a single directory, but you could also use it to compress multiple directories, multiple individual files or both; just provide a list of files or directories instead of a single one, like this:\n1 2 3 4 5 6 7 8 9 10 11  ubuntu@ip-172-31-87-26:~$ ls a b d d1.tar.gz dir1 files ls.txt passwd auth.log c d1 d2 dir2 ls mac.txt projects ubuntu@ip-172-31-87-26:~$ tar -czf d2/multi.tar.gz d1 ls.txt mac.txt ubuntu@ip-172-31-87-26:~$ tree d2 d2  d1.tar.bzip2  d1.tar.gz  multi.tar.gz 0 directories, 3 files   --exclude In some cases, it\u0026rsquo;s useful to exclude some files from the archive. Imagine that you make a back up of a directory and want to exclude the music, video or other non important files.\nTo do that, use one or more --exclude options like this:\n1 2 3  ubuntu@ip-172-31-87-26:~$ tar --exclude=\u0026#34;*.mkv\u0026#34; --exclude=\u0026#34;*.conf\u0026#34; -czf myhome.tar.gz ~ tar: Removing leading `/\u0026#39; from member names tar: /home/ubuntu: file changed as we read it   -x To extract the contents of an archive in the current working directory (instead of the dir where the compressed file is), use -x option, which comes from extract, and the same options -zvf and the name of the archive, so just replace -czvf (or -cjvf) with -xzvf (or -xjvf)\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ sudo tar -czf d2/etc.tar.gz /etc ubuntu@ip-172-31-87-26:~$ ls d2 d1.tar.bzip2 d1.tar.gz etc.tar.gz home.tar.gz multi.tar.gz ubuntu@ip-172-31-87-26:~$ sudo tar -xzf d2/etc.tar.gz ubuntu@ip-172-31-87-26:~$ ls d2 d1.tar.bzip2 d1.tar.gz etc.tar.gz home.tar.gz multi.tar.gz ubuntu@ip-172-31-87-26:~$ ls a d d11.tar.gz dir2 ls myhome.tar.gz b d1 d2 etc ls.txt passwd c d1.tar.gz dir1 files mac.txt projects   You can see the extracted file etc/ is in the ~.\nIf you want to extract an archive into another directory use -C option and the destination directory.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ mkdir d3 ubuntu@ip-172-31-87-26:~$ tar -xzf d2/etc.tar.gz -C d3/ ubuntu@ip-172-31-87-26:~$ ls d3 etc   Note that these two compression algorithms are incompatible; if you want to extract a gzip archive using the -j option, which is the option for archives of type bzip2, youll get an error. Specify the correct option, which is -z for gzip and -j for bzip, or allow tar to auto-detect the compression type by specifying no option for the compression algorithm.\n1 2 3 4 5  ubuntu@ip-172-31-87-26:~$ tar -xjf d2/etc.tar.gz -C d3/ bzip2: (stdin) is not a bzip2 file. tar: Child returned status 2 tar: Error is not recoverable: exiting now ubuntu@ip-172-31-87-26:~$ tar -xf d2/etc.tar.gz -C d3/   By the way, There are more and more files with the .xz extension. These are files compressed with the LZMA algorithm and tar will autodetect the compression type and extract the archive.\nIn most cases, we\u0026rsquo;ll tar to compress files using different compression protocols. However, note that there are also dedicated commands that only compress or extract files and directories using gzip and bzip2. gzip is used to compress a file or directory. gunzip is used to extract a gzip archive. Then there\u0026rsquo;s bzip2 and bunzip2.\n-t To see the contents of an archive use the -t option, which comes from the table of contents. When you want to check if a specific file is in the archive pipe the output of the previous command to grep. For example,\n1 2 3 4 5  ubuntu@ip-172-31-87-26:~$ tar -tf d2/etc.tar.gz | grep sshd_config etc/ssh/sshd_config etc/ssh/sshd_config.d/ ubuntu@ip-172-31-87-26:~$ tar -tf d2/etc.tar.gz | grep apache2 etc/apparmor.d/abstractions/apache2-common   name file by date If you want to have versions, which are backups made in different days, then you need to insert the date into the back up filename, like this\n1 2  abcd-$(date +%F).tar.bz2 # note that there is a white space between date and +   For example,\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ sudo tar -czf d2/etc-$(date +%F).tar.gz /etc ubuntu@ip-172-31-87-26:~$ ls d2 d1.tar.bzip2 etc-2022-04-27.tar.gz home.tar.gz d1.tar.gz etc.tar.gz multi.tar.gz   Link hard link inode In this section well discuss inodes and hard links. Each file on the disk has a data structure called index node or inode associated with it which is fixed in length.\nThis structure stores metadata information about the file such as the type, files permission, files owner and group, timestamp information, file size and so on. You can see most of the information stored in a files inode in the output of ls -l\nThe inode structure contains all file information except the file contents and the name. Each inode is uniquely identified by an integer number called inode number. To see it run ls with -i option. You can combine it with other options as well.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ ls -i 306837 a 306643 d1.tar.gz 307840 dir2 285491 mac.txt 285460 b 285475 d11.tar.gz 306845 etc 306839 myhome.tar.gz 306637 c 306834 d2 306633 files 285474 passwd 306638 d 520974 d3 306636 ls 256333 projects 256339 d1 256056 dir1 256381 ls.txt   The file name and the file contents are not part of the Inode. In the inode structure there are only some pointers to the data blocks where the file contents are saved. But where\u0026rsquo;s the filename if it\u0026rsquo;s not in the file\u0026rsquo;s inode structure? A filename is an entry in a directory. Basically directories are two column tables where the first column is the filename and the second column is the Inode number.\nThis is a simplified approach, I\u0026rsquo;m creating a new file using the touch command. When I referred to a file like, cat qq.txt it will search in the current directory for the corresponding Inode number of a.txt, that is 307689.\n1 2 3  ubuntu@ip-172-31-87-26:~$ touch qq.txt ubuntu@ip-172-31-87-26:~$ ls -li qq.txt 307689 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 27 04:36 qq.txt   Having that number, the kernel can identify the file and all of the information about it. So as a conclusion, the file name doesn\u0026rsquo;t belong to the file structure. Its just a string, and the pointing between the file structure and the filename happens through the data structure of the directory that contains the filename. An association between a file structure and a file name is called hard link.\nIn fact, this is a hardlink. In the current directory qq.txt was associated with the number 307689. By the way, that\u0026rsquo;s why the 3rd column of the output of ls -li qq.txt is 1. It means 1 hardlink. To see the number of hardlinks run ls -l or stat.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ stat qq.txt File: qq.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 307689 Links: 1 # \u0026lt;-- The number of hard link Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2021-04-27 04:36:48.010032608 +0000 Modify: 2021-04-27 04:36:48.010032608 +0000 Change: 2021-04-27 04:36:48.010032608 +0000 Birth: -   create hard link One useful application of the hard links is to allow files, programs and scripts to be easily accessed in a different directory without making copies of the same file. You can create a new hardlink by running the ln command, which comes from link.\nAt this moment, the file called qq.txt has only one hard link, because there is one name that points to the inode structure. To create a new link to the same file structure run\n1  ln \u0026lt;existing name\u0026gt; \u0026lt;new name\u0026gt;   which creates a new file name for the same file structure. We can see that both files have the same inode number and the number of hardlinks is now 2.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ ln qq.txt qq1.txt ubuntu@ip-172-31-87-26:~$ ls -li qq.txt qq1.txt 307689 -rw-rw-r-- 2 ubuntu ubuntu 0 Apr 27 04:36 qq.txt 307689 -rw-rw-r-- 2 ubuntu ubuntu 0 Apr 27 04:36 qq1.txt   Note that qq1.txt is not a copy of qq.txt. The information on the disk is saved only once and both files point to the same data. If I change one of them, the other will be changed as well.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  ubuntu@ip-172-31-87-26:~$ ifconfig \u0026gt; qq.txt ubuntu@ip-172-31-87-26:~$ cat qq1.txt eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 9001 inet 172.31.87.26 netmask 255.255.240.0 broadcast 172.31.95.255 inet6 fe80::10b8:5eff:fe3f:aa45 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 12:b8:5e:3f:aa:45 txqueuelen 1000 (Ethernet) RX packets 672426 bytes 521515910 (521.5 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 457130 bytes 114725986 (114.7 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 1542 bytes 150034 (150.0 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1542 bytes 150034 (150.0 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0   Note that you cannot create a hard link to a directory or a hard link to a file on a different partition or disk. A hardlink cannot cross the filesystem boundaries.\n1 2  ubuntu@ip-172-31-87-26:~$ ln d1 d111 ln: d1: hard link not allowed for directory   However, if you run the ls -l command on a directory, you notice the number of links is not 1.\n1 2  ubuntu@ip-172-31-87-26:~$ ls -ldi /etc 43 drwxr-xr-x 94 root root 4096 Apr 25 04:32 /etc   The /etc directory has 94 hardlinks. That because there are 94 names to the same structure, the /etc directory. One is etc, another one is the ., which belongs to each directory and its reference to itself. You can see that etc and /etc/. has the same inode number 43.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ ls -lai /etc | head -n 5 total 820 43 drwxr-xr-x 94 root root 4096 Apr 25 04:32 . 2 drwxr-xr-x 19 root root 4096 Apr 14 07:31 .. 119 -rw------- 1 root root 0 Nov 29 23:30 .pwd.lock 739 drwxr-xr-x 3 root root 4096 Nov 29 23:32 NetworkManager   And then there are 92 subdirectories, and each of them contains the .., which is a reference to its parent. You can see that the inode number of /etc/NetworkManager/.. is 43.\n1 2 3 4 5  ubuntu@ip-172-31-87-26:~$ ls -lai /etc/NetworkManager | head -n 5 total 12 739 drwxr-xr-x 3 root root 4096 Nov 29 23:32 . 43 drwxr-xr-x 94 root root 4096 Apr 25 04:32 .. 740 drwxr-xr-x 2 root root 4096 Nov 29 23:32 dispatcher.d   and there are 92 subdirectories in /etc:\n1 2  ubuntu@ip-172-31-87-26:~$ ls -l /etc | grep -c \u0026#34;^d\u0026#34; 92   delete hard link If you delete a file using rm then only the entry from the directory that contains the file is removed and the link count of the inode is decremented. Only when it reaches zero, the disk space associated with the inode and the inode itself is freed for further reuse.\n1 2 3  ubuntu@ip-172-31-87-26:~$ rm qq1.txt ubuntu@ip-172-31-87-26:~$ ls -li qq.txt 307689 -rw-rw-r-- 1 ubuntu ubuntu 902 Apr 27 04:45 qq.txt   find hard link Consider that you know that there are 2 hard links for the file to which qq.txt points and want to find out the other one.\n1 2 3  ubuntu@ip-172-31-87-26:~$ ln qq.txt d1/qq2.txt ubuntu@ip-172-31-87-26:~$ ls -li qq.txt 307689 -rw-rw-r-- 2 ubuntu ubuntu 902 Apr 27 04:45 qq.txt   To do this, use the find command with the -inum option and the inode number, like\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo find . -inum 307689 ./qq.txt ./d1/qq2.txt   it found all the links. And if you want to find files by the number of hardlinks, use the find command with -links option. Let\u0026rsquo;s find all the files in /usr with more than one link:\n1 2  ubuntu@ip-172-31-87-26:~$ find /usr -type f -links +1 | wc -l 217   All these files have more than one link.\nsoft link Soft links is also called symbolic links or symlinks. They are similar to shortcuts in Windows and behave differently compared to hard links.\nIf a hard link is an association between an inode structure and a filename in a directory, a symlink is just a special filetype that points to or contains a reference to another file or directory. So a hard link references the files inode and a symlink references another file.\nIf you delete a hard link, the remaining files point to the same space or inode in the file system and they will still contain the data stored on the disk. But if you delete the file, the symlink points to the symlink will be useless or broken.\nTo create a symlink we use the same command as for creating hard links, which is ln, but with the -s option.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ touch ppp.txt ubuntu@ip-172-31-87-26:~$ ln -s ppp.txt ppps.txt ubuntu@ip-172-31-87-26:~$ ln ppp.txt ppph.txt ubuntu@ip-172-31-87-26:~$ ls -li ppp* 307690 -rw-rw-r-- 2 ubuntu ubuntu 0 Apr 27 05:42 ppp.txt 307690 -rw-rw-r-- 2 ubuntu ubuntu 0 Apr 27 05:42 ppph.txt 307691 lrwxrwxrwx 1 ubuntu ubuntu 7 Apr 27 05:42 ppps.txt -\u0026gt; ppp.txt   See ls -l is indicating that ppps.txt is a symlink and its also displaying the file it points to. The symlink has its own inode structure. However, all hard links have the same inode structure and inode number. I can access the contents of ppp.txt using the symlink.\n1 2 3  ubuntu@ip-172-31-87-26:~$ echo \u0026#34;test\u0026#34; \u0026gt; ppph.txt ubuntu@ip-172-31-87-26:~$ cat ppps.txt test   A symlink contains only the path to the original file or directory, not its contents. Thus if I move ppp.txt to another directory, ppph.txt, the hardlink will not be affected, but the symlink will be broken.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ mkdir dd ubuntu@ip-172-31-87-26:~$ mv ppp.txt dd/ ubuntu@ip-172-31-87-26:~$ cat ppph.txt test ubuntu@ip-172-31-87-26:~$ cat ppps.txt cat: ppps.txt: No such file or directory   Symlinks are used more than hardinks and we can say that many services like the Web server Appache2 use symlinks extensively in their configuration. Other properties of symlinks are:\n you can create a symlink to a directory. A symlink can cross the filesystem. The permissions of symlinks dont matter because the permissions of the target file will be checked.  ","date":"2021-05-01T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/linux-operation-system-iii-file-system-2/","title":"Linux Operation System III (File System 2)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\nAll operations in this series of blogs (Linux Operation System) are done in AWS Elastic Compute Cloud (EC2).\n File System Intro A file system controls how data is stored and retrieved. Without a file system, data placed on a storage medium, like a hard disk drive or USB stick, would be one large body of data with no way of telling where one piece of it stops and where another one begins. By separating the data into pieces and giving each piece a name, the data is easily isolated and identified.\nEach group of data is called a file and the structure and the logic is used to manage the files and their names are called file systems. In other words, a file system is a logical collection of files on a partition or disk. The hard drive can have multiple partitions, which usually contain only one file system. On a Linux system, everything is considered to be a file, including physical devices like a USB stick. Even a directory is also a special kind of file and, if something is not a file, then it\u0026rsquo;s a process.\nOn Linux the structure of the file system is hierarchical, much like an upside down three, the root directory, which is a /, lies at the base of the file system and all other directories are spreading from there.\nFilesystem Hierarchy Standard (FHS) defines the directory structure and contents for Linux distributions. For example, this is the contents of the root directly:\n1 2 3  ubuntu@ip-172-31-87-26:~$ ls / bin dev home lib32 libx32 media opt root sbin srv tmp var boot etc lib lib64 lost+found mnt proc run snap sys usr   In the FHS all files and directories appear under the root directory, even if they are stored on different physical devices like on different disks or partitions. When you want to access an additional file system, like the one on a USB stick, you need to mount it or logically attach it to an existing directory of the unique file system. That directory is called Mount Point.\n Mount is the process by which the operating system makes the file system on one storage device (such as a hard disk, CD-ROM, or shared resource) accessible to users through the other (computer\u0026rsquo;s) file system.\n To see the mount point, you can run df -h, i will make it on my laptop: You can see that my mobile HDD space has been automatically mounted in /Volumes/space, and we can access its contents in that directory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  (base) wanghaoming@localhost ~ % df -h Filesystem Size Used Avail Capacity iused ifree %iused Mounted on /dev/disk3s1s1 228Gi 15Gi 45Gi 25% 575614 474126600 0% / devfs 203Ki 203Ki 0Bi 100% 702 0 100% /dev /dev/disk3s6 228Gi 3.0Gi 45Gi 7% 3 474126600 0% /System/Volumes/VM /dev/disk3s2 228Gi 755Mi 45Gi 2% 2972 474126600 0% /System/Volumes/Preboot /dev/disk3s4 228Gi 22Mi 45Gi 1% 67 474126600 0% /System/Volumes/Update /dev/disk1s2 500Mi 6.0Mi 481Mi 2% 3 4926240 0% /System/Volumes/xarts /dev/disk1s1 500Mi 7.5Mi 481Mi 2% 91 4926240 0% /System/Volumes/iSCPreboot /dev/disk1s3 500Mi 636Ki 481Mi 1% 39 4926240 0% /System/Volumes/Hardware /dev/disk3s5 228Gi 163Gi 45Gi 79% 1911877 474126600 0% /System/Volumes/Data map auto_home 0Bi 0Bi 0Bi 100% 0 0 100% /System/Volumes/Data/home /dev/disk4s1 466Gi 175Gi 290Gi 38% 1437326 2378046 38% /Volumes/space /Applications/Mi Home.app/Wrapper 228Gi 164Gi 46Gi 78% 1919113 487182760 0% /private/var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/X/81C72F62-6D4D-5490-B104-2ECBB5B21B26 (base) wanghaoming@localhost ~ % (base) wanghaoming@localhost ~ % (base) wanghaoming@localhost ~ % ls /Volumes/space $RECYCLE.BIN 01.9 5.14 ASUS  Kg MATLAB MATLAB_R2017a.app System Volume Information media old pro ssd flie ~$2021 .docx ~$.docx    FHS This is the contents of the root directory. Most Linux distributions will show the same or a very similar layout to this output.\n1 2 3  ubuntu@ip-172-31-87-26:~$ ls / bin dev home lib32 libx32 media opt root sbin srv tmp var boot etc lib lib64 lost+found mnt proc run snap sys usr   /bin, /sbin The /bin directory contains binaries or user executable files which are available to all users. Recall that ls is an executable file, as you see, ls resides in this directory.\n1 2  ubuntu@ip-172-31-87-26:~$ ls -l /bin/ls -rwxr-xr-x 1 root root 142144 Sep 5 2019 /bin/ls   Theres another directory called /sbin which is similar to /bin, but it contains applications that only the superuser (hence the initial s) will need. These executables are used for system administration. For example, ifconfig used to configure the network adapter or fdisk used to manipulate disk partitions.\n1 2 3  ubuntu@ip-172-31-87-26:~$ ls -l /sbin/ifconfig /sbin/fdisk -rwxr-xr-x 1 root root 153880 Feb 7 13:33 /sbin/fdisk -rwxr-xr-x 1 root root 87152 Feb 1 2019 /sbin/ifconfig   /boot, /home and /root The boot directory contains files required for starting the system. You normally do not touch it.\n/home stores the users home directories. Under this directory there is another directory for each user, if that particular user has a home directory. It\u0026rsquo;s common for the name of the home directory to be the same as the user\u0026rsquo;s name.\n1 2 3  ubuntu@ip-172-31-87-26:~$ ls /home ubuntu   Note that ~ (tilde) represents the current user\u0026rsquo;s home directory. So if you run ls ~ , it\u0026rsquo;s the same as is running ls /home/ubuntu. But when you switch to the root user, ~ will denote to /root, which is the home directory of root user.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ ls ~ d1 d2 d3 d4 f1 f2 f3 f4 ubuntu@ip-172-31-87-26:~$ sudo ls ~ d1 d2\td3 d4\tf1 f2\tf3 f4 ubuntu@ip-172-31-87-26:~$ sudo su root@ip-172-31-87-26:/home/ubuntu# ls ~ dir snap t0   /usr /usr was the initial users home directory in the early days of Unix. However these days /home took its place and /usr contains many other subdirectories with binary files, shared libraries and so on.\nOn some distributions like CentOS many commands are saved in /usr/bin or in /usr/sbin.\n/dev /dev contains device files; many of these are generated at boot time or even on the fly. This directory contains the device files for every hardware device attached to the system, like a hard disk, partition, keyboard, stereo and printer.\nThese are not device drivers rather they are files that represent each device on the computer and facilitate the access to those devices. Remember that in Linux everything is a file and the disk partition is logically represented as a special file as well.\n/media, /mnt The /media directory is where the external storage will be automatically mounted when you plug it and try to access it. When you insert an external hard disk or USB stick they will be automatically mounted and made available for you under /media.\n On mac OS, this directory is like to /Volumes.\n /mnt is like /media but its not very often used these days. It was the mount point for the cdroms or the floppy disks. However, when you need to quickly mount a partition and don\u0026rsquo;t know where you always have /mnt available.\n/etc /etc gets its name from the earliest Unix operating systems and it was literally etc. because it was the dumping ground for system files, the system administrators were not sure where to put. On a modern Linux distribution, it contains not all system wide configuration files.\nFor example, the files that contain the users and their passwords, the groups, how the network is configured, or how a network service like a web or an e-mail server is configured are all of there.\n It\u0026rsquo;s always a good idea to make a copy or a backup of this directory.\n /lib, /tmp /lib contains shared library files used by different applications. You don\u0026rsquo;t talk to this directly. The package manager can update it accordingly when you install, remove or update an application.\n/tmp contains temporary files, usually saved there by applications that are running. Non-privileged users may also store files there temporarily. Note that files stored in this directory may be deleted at any time without prior notice.\n/proc /proc is a virtual directory. It contains information about your computer hardware, such as information about CPU, RAM memory or Kernel. The files and directories are generated when the computer starts or on the fly as the system is running and things change.\nFor example if I want to see information about the cpu, I can run cat /proc/cpuinfo :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  ubuntu@ip-172-31-87-26:~$ cat /proc/cpuinfo processor\t: 0 vendor_id\t: GenuineIntel cpu family\t: 6 model\t: 63 model name\t: Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz stepping\t: 2 microcode\t: 0x46 cpu MHz\t: 2399.816 cache size\t: 30720 KB physical id\t: 0 siblings\t: 1 core id\t: 0 cpu cores\t: 1 apicid\t: 0 initial apicid\t: 0 fpu\t: yes fpu_exception\t: yes cpuid level\t: 13 wp\t: yes flags\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm cpuid_fault invpcid_single pti fsgsbase bmi1 avx2 smep bmi2 erms invpcid xsaveopt bugs\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit bogomips\t: 4800.01 clflush size\t: 64 cache_alignment\t: 64 address sizes\t: 46 bits physical, 48 bits virtual power management:   And it has displayed information about the cpu. Or if I want to see information about the memory, then i can run cat /proc/meminfo:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  ubuntu@ip-172-31-87-26:~$ cat /proc/meminfo MemTotal: 992204 kB MemFree: 220400 kB MemAvailable: 644932 kB Buffers: 62684 kB Cached: 462020 kB SwapCached: 0 kB Active: 336272 kB Inactive: 255416 kB Active(anon): 1128 kB Inactive(anon): 78276 kB Active(file): 335144 kB Inactive(file): 177140 kB Unevictable: 23064 kB Mlocked: 18528 kB SwapTotal: 0 kB SwapFree: 0 kB Dirty: 0 kB Writeback: 0 kB AnonPages: 90084 kB Mapped: 65292 kB Shmem: 832 kB KReclaimable: 79780 kB Slab: 119504 kB SReclaimable: 79780 kB SUnreclaim: 39724 kB KernelStack: 2576 kB PageTables: 2388 kB NFS_Unstable: 0 kB Bounce: 0 kB WritebackTmp: 0 kB CommitLimit: 496100 kB Committed_AS: 767944 kB VmallocTotal: 34359738367 kB VmallocUsed: 17472 kB VmallocChunk: 0 kB Percpu: 13632 kB HardwareCorrupted: 0 kB AnonHugePages: 0 kB ShmemHugePages: 0 kB ShmemPmdMapped: 0 kB FileHugePages: 0 kB FilePmdMapped: 0 kB HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB Hugetlb: 0 kB DirectMap4k: 104448 kB DirectMap2M: 944128 kB   /sys, /srv, /run, /var /sys contains information about devices, drivers and some Kernel features and /srv contains data for servers; We use neither /sys nor /srv directly.\n/run directory is fairly new and different distributions use it in different ways. Its a temporary filesystem which runs in RAM so its contents will vanish when the system reboots. This directory is used only by processes.\n/var contains variable-length files such as log which are files that register events that happen on the system.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ ls /var/log alternatives.log cloud-init-output.log kern.log syslog.1 amazon cloud-init.log kern.log.1 syslog.2.gz apt dist-upgrade landscape syslog.3.gz auth.log dmesg lastlog ubuntu-advantage-timer.log auth.log.1 dpkg.log private unattended-upgrades btmp journal syslog wtmp   Path A path is a unique location to a file or a directory in the file system of an operating system. In Linux there are two types of path names: absolute and relative path names.\nAbsolute path An absolute path is defined by specifying the location of a file or directly from the root directory /. It\u0026rsquo;s a complete path and always starts with /.\nTo write an absolute path, start at the root directory / and go deep into the filesystem, writing a slash / after every directory name. Remember that ~ means the user\u0026rsquo;s home directory, that is /home/student in this case. So you can add the directory name behind ~.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ ls /home/ubuntu/ d1 d2 d3 d4 f1 f2 f3 f4 ubuntu@ip-172-31-87-26:~$ ls ~ d1 d2 d3 d4 f1 f2 f3 f4   Relative path A relative path is defined as the path related to the current working directly. It begins at the current directory and never starts with a forward slash. The absolute path to the directory always remains the same, but the relative path will be changed, according to the current working directory.\nA single dot (.) represents the directory you are in and two successive dots or double dot (..) represents the parent directory. On a Linux system these items are automatically created in every directory, and can be seen by using the ls command with the -a option, which shows all hidden files. (any file or directories that starts with a dot is hidden)\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ ls -a . .bash_logout .lesshst .sudo_as_admin_successful d3 f2 .. .bashrc .profile d1 d4 f3 .bash_history .cache .ssh d2 f1 f4 ubuntu@ip-172-31-87-26:~$ ls d1 d2 d3 d4 f1 f2 f3 f4   When you use relative path, you can just start from the file\u0026rsquo;s or directory\u0026rsquo;s name in the currently working directory, it is just the same as ./balbal:\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ ls -a ./d1 . .. ubuntu@ip-172-31-87-26:~$ ls -a d1 . ..   A more useful symbol is .., you can string multi .. together, (separate by slash /), or you can even string .. with the absolute path together. So you can access to any directory or file in Linux File System by .., theoretically.\n1 2 3 4 5 6 7 8 9 10 11  ubuntu@ip-172-31-87-26:~$ ls -a .. . .. ubuntu ubuntu@ip-172-31-87-26:~$ ls -a ../.. . bin dev home lib32 libx32 media opt root sbin srv tmp var .. boot etc lib lib64 lost+found mnt proc run snap sys usr ubuntu@ip-172-31-87-26:~$ ls ~/../.. bin dev home lib32 libx32 media opt root sbin srv tmp var boot etc lib lib64 lost+found mnt proc run snap sys usr ubuntu@ip-172-31-87-26:~$ ls /home/.. bin dev home lib32 libx32 media opt root sbin srv tmp var boot etc lib lib64 lost+found mnt proc run snap sys usr   Tree Another nice command we can use to get used to paths is tree. Tree is a recursive directory listing tool that helps us see easier and clearer the parent and children directories. We need to install it before using:\n1  sudo apt install tree   If you run it with no arguments it will display the contents of the current working directory recursively. showing sub-directories and files in tree-like format.\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ tree .  d1  d2  d3  d4  f1  f2  f3  f4 4 directories, 4 files   When a directory is given as argument, tree lists all the files and directories found in the given directory.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ tree ~/.. /home/ubuntu/..  ubuntu  d1  d2  d3  d4  f1  f2  f3  f4 5 directories, 4 files   Some useful options -d to print out only the directories and -f to print out the absolute path for each file and directory.\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ tree -df ~ /home/ubuntu  /home/ubuntu/d1  /home/ubuntu/d2  /home/ubuntu/d3  /home/ubuntu/d4 4 directories   Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  ########################## ## Linux Paths ########################## . # =\u0026gt; the current working directory .. # =\u0026gt; the parent directory ~ # =\u0026gt; the user\u0026#39;s home directory cd # =\u0026gt; changing the current directory to user\u0026#39;s home directory cd ~ # =\u0026gt; changing the current directory to user\u0026#39;s home directory cd - # =\u0026gt; changing the current directory to the last directory cd /path_to_dir # =\u0026gt; changing the current directory to path_to_dir  pwd # =\u0026gt; printing the current working directory # installing tree sudo apt install tree tree directory/ # =\u0026gt; Ex: tree . tree -d . # =\u0026gt; prints only directories tree -f . # =\u0026gt; prints absolute paths   ls command The syntax of the ls command is as follows:\n1  ls [OPTION]... [FILE]...   option and file are optional, and can be repeated. If you simply ran a ls with no options or arguments, it will print the contents of the current working directory at the terminal. The files are listed in alphabetical order. To list the files in a specific directory, pass the path to that directory as argument to the ls command.\nYou can specify as many arguments you want. For example, ls will display the contents of the root directory /; the /home directory, the home directory of the user ~; and the current working directory .\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ ls / /home ~ . .: d1 d2 d3 d4 f1 f2 f3 f4 /: bin dev home lib32 libx32 media opt root sbin srv tmp var boot etc lib lib64 lost+found mnt proc run snap sys usr /home: ubuntu /home/ubuntu: d1 d2 d3 d4 f1 f2 f3 f4   -1 option The real power of the ls command comes from its options. If you want to have your listing produced in a single column, use the -1 option; 1 means one file per line.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ ls -1 d1 d2 d3 d4 f1 f2 f3 f4   -l, -d option -l will display the output in a long listing format which means lots of information about files and directories.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ ls -l total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4   So let\u0026rsquo;s clear out the meaning of each columns.\n The first character indicates the file type: A hyphen (-) as the 1st character indicates a normal file, d indicates a directory and l indicates a symbolic link which is sort of a shortcut. The nine characters that follow indicate the file permissions for the owner, the group owner and the others. The second column indicates the number of hardlinks. The third and the fourth columns indicates the owner, and the group owner of the file; The fifth is the size and then the modification time and finally the file name.  If you want to see information about the directory, instead of its contents, then use the -d option. It\u0026rsquo;s displaying information about the directory that you specify, and not about its contents;\n1 2  ubuntu@ip-172-31-87-26:~$ ls -ld ~ drwxr-xr-x 8 ubuntu ubuntu 4096 Apr 16 11:19 /home/ubuntu   -h, -a option ls with -lh option will show the file size in human readable format.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  ubuntu@ip-172-31-87-26:~$ ls -lh /var total 44K drwxr-xr-x 2 root root 4.0K Apr 20 06:25 backups drwxr-xr-x 13 root root 4.0K Apr 15 02:41 cache drwxrwxrwt 2 root root 4.0K Nov 29 23:33 crash drwxr-xr-x 41 root root 4.0K Apr 15 03:17 lib drwxrwsr-x 2 root staff 4.0K Apr 15 2020 local lrwxrwxrwx 1 root root 9 Nov 29 23:30 lock -\u0026gt; /run/lock drwxrwxr-x 9 root syslog 4.0K Apr 20 00:00 log drwxrwsr-x 2 root mail 4.0K Nov 29 23:30 mail drwxr-xr-x 2 root root 4.0K Nov 29 23:30 opt lrwxrwxrwx 1 root root 4 Nov 29 23:30 run -\u0026gt; /run drwxr-xr-x 7 root root 4.0K Nov 29 23:36 snap drwxr-xr-x 4 root root 4.0K Nov 29 23:31 spool drwxrwxrwt 6 root root 4.0K Apr 20 09:53 tmp   By default, the ls command will not show hidden files. In Linux a hidden file or directory is any file or director whose name begins with a dot. To display all files, including the hidden ones, use the -a option.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ ls -a . .bash_logout .lesshst .sudo_as_admin_successful d3 f2 .. .bashrc .profile d1 d4 f3 .bash_history .cache .ssh d2 f1 f4   -S, -X, --hide option Another useful option is capital -S which sorts the files by size and the largest file will come first.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  ubuntu@ip-172-31-87-26:~$ ls -lhS /var total 44K drwxr-xr-x 2 root root 4.0K Apr 20 06:25 backups drwxr-xr-x 13 root root 4.0K Apr 15 02:41 cache drwxrwxrwt 2 root root 4.0K Nov 29 23:33 crash drwxr-xr-x 41 root root 4.0K Apr 15 03:17 lib drwxrwsr-x 2 root staff 4.0K Apr 15 2020 local drwxrwxr-x 9 root syslog 4.0K Apr 20 00:00 log drwxrwsr-x 2 root mail 4.0K Nov 29 23:30 mail drwxr-xr-x 2 root root 4.0K Nov 29 23:30 opt drwxr-xr-x 7 root root 4.0K Nov 29 23:36 snap drwxr-xr-x 4 root root 4.0K Nov 29 23:31 spool drwxrwxrwt 6 root root 4.0K Apr 20 09:53 tmp lrwxrwxrwx 1 root root 9 Nov 29 23:30 lock -\u0026gt; /run/lock lrwxrwxrwx 1 root root 4 Nov 29 23:30 run -\u0026gt; /run   So backups is the largest file in /var. Note that ls does not display the real size of a directory as we expect. It displays only the size of the inode structure. To see the size of a directory, use the du command instead. du comes from disk usage.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo du -sh /var 867M\t/var   By the way, in /var, there are some files that cannot be read by an unprivileged user, so we add sudp prefix to du command.\nTo sort by extension, use the -X option, written in uppercase.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  ubuntu@ip-172-31-87-26:~$ ls -X /etc NetworkManager networks gai.conf PackageKit newt hdparm.conf X11 opt host.conf gshadow- terminfo pam.d gss timezone profile.d hostname ubuntu-advantage rc0.d hosts udev rc1.d initramfs-tools udisks2 rc2.d inputrc ufw rc3.d iproute2 update-manager rc4.d iscsi update-notifier rc5.d issue vim rc6.d kernel vmware-tools rcS.d landscape vtrgb rsyslog.d ldap wgetrc sudoers.d legal xdg sysctl.d libblockdev zsh_command_not_found tmpfiles.d localtime python3.8 update-motd.d logcheck locale.alias cron.daily lsb-release hosts.allow login.defs lvm bash.bashrc at.deny machine-id bindresvport.blacklist hosts.deny   (Delete some content for easy display)\nIf you want to omit certain files from the listing, use the --hide option.\nFor example, if you don\u0026rsquo;t want to see the files which end in that .conf and .d in the listing, you can run this command: ls --hide=*.conf --hide=*.d /etc/, here * is wildcards.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  ubuntu@ip-172-31-87-26:~$ ls --hide=*.conf --hide=*.d /etc/ NetworkManager hibinit-config.cfg perl PackageKit hostname pki calendar legal shadow cloud libblockdev shadow- console-setup locale.alias shells cron.daily locale.gen skel cron.hourly localtime sos cron.monthly logcheck ssh cron.weekly login.defs ssl crontab lsb-release subgid cryptsetup-initramfs lvm subgid- crypttab machine-id subuid dbus-1 magic subuid- dconf magic.mime sudoers   (Delete some content for easy display)\n-R, --color option, , \\\u0026lt;char\u0026gt; With -R option you can list directories recursively. Recursively means that ls works its way through the entire directory tree below the starting directory, and lists the files in each subdirectory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  ubuntu@ip-172-31-87-26:~$ ls -R /home /home: ubuntu /home/ubuntu: d1 d2 d3 d4 f1 f2 f3 f4 /home/ubuntu/d1: /home/ubuntu/d2: /home/ubuntu/d3: /home/ubuntu/d4:   If you run type ls, you\u0026rsquo;ll see that there\u0026rsquo;s an alias called ls to ls --color=auto\n1 2  ubuntu@ip-172-31-87-26:~$ type ls ls is aliased to `ls --color=auto\u0026#39;   The --color=auto is the option that provides the different colors for the different file types in the listing. Thats why when you run ls -l you see directories in blue, regular files in black, symlinks in cyan, or device files in yellow on a black background.\nIf you want to run the original ls command without adding --color=auto, run \\ls and youll see everything displayed in black. The \\char cancels the alias.\n1 2  ubuntu@ip-172-31-87-26:~$ \\ls d1 d2\td3 d4\tf1 f2\tf3 f4   Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  ########################## ## The ls Command ## ls [OPTIONS] [FILES] ########################## # listing the current directory # ~ =\u0026gt; user\u0026#39;s home directory # . =\u0026gt; current directory # .. =\u0026gt; parent directory ls ls . # listing more directories ls ~ /var / # -l =\u0026gt; long listing ls -l ~ # -a =\u0026gt; listing all files and directories including hidden ones ls -la ~ # -1 =\u0026gt; listing on a single column ls -1 /etc # -d =\u0026gt; displaying information about the directory, not about its contents ls -ld /etc # -h =\u0026gt; displaying the size in human readable format ls -h /etc # -S =\u0026gt; displaying sorting by size ls -Sh /var/log # Note: ls does not display the size of a directory and all its contents. Use du instead du -sh ~ # -X =\u0026gt; displaying sorting by extension ls -lX /etc # --hide =\u0026gt; hiding some files ls --hide=*.log /var/log # -R =\u0026gt; displaying a directory recursively ls -lR ~ # -i =\u0026gt; displaying the inode number ls -li /etc   Timestamps Every file on Linux has three timestamps:\n The access time or atime is the last time the file was read. This means the time when someone used a program to display the contents of the file, it doesn\u0026rsquo;t matter if the file contains were changed or not; the modification timestamp or mtime signifies the last time the contents of the file was modified. If someone used a text editor and changed something inside, the file mtime was updated. the change time or ctime is the time when some metadata related to the file, not to the file contents, was changed.   Linux timestamps hold an integer number rather than a date and time. This number is the number of seconds since the unique epoch, which was midnight on January 1st, 1970 UTC. When Linux needs to display the date, it translates that number of seconds into a date and time. This makes it easier for humans to understand.\n To view these timestamps, use the stat or ls with options.\nstat command The stat command is showing statistics about the file including the timestamps. You can see the access time, the modification time and the changed time.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ stat ~ File: /home/ubuntu Size: 4096 Blocks: 8 IO Block: 4096 directory Device: ca01h/51713d\tInode: 256109 Links: 8 Access: (0755/drwxr-xr-x) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2021-04-20 07:53:14.829663366 +0000 Modify: 2021-04-16 11:19:59.303335040 +0000 Change: 2021-04-16 11:19:59.303335040 +0000 Birth: -   But in most cases, this has not been implemented yet. In this example, file creation or birth hasn\u0026rsquo;t been implemented yet. That\u0026rsquo;s there is a hyphen instead.\nls with options -u To see the timestamps with the ls command run ls -lu to see the access time:\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ ls -lu total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:38 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4   -t ls -l, or ls -lt to see the modification time:\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ ls -lt total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3   -c and ls -lc to see the changed time.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ ls -lc total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4   --full-time Sometimes the ls-l is not displaying the timestamp with a maximum precision. In above example, we only know the month, the day, the hour and the minute of the timestamps. To see the entire timestamp that was saved, which includs the year and the second, use the --full-time option like this: ls -l --full-time\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  ubuntu@ip-172-31-87-26:~$ ls -l --full-time total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:29.215918658 +0000 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:29.215918658 +0000 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:29.215918658 +0000 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:38.811846868 +0000 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:36:14.736027002 +0000 f4 ubuntu@ip-172-31-87-26:~$ ls -lu --full-time total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-20 09:38:09.745767696 +0000 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-20 09:50:48.340039341 +0000 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-20 09:50:48.344039311 +0000 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-20 09:50:48.348039281 +0000 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:36:14.736027002 +0000 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:36:14.736027002 +0000 f4 ubuntu@ip-172-31-87-26:~$ ls -lc --full-time total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:29.215918658 +0000 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:29.215918658 +0000 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:29.215918658 +0000 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:38.811846868 +0000 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:36:14.736027002 +0000 f4   And we are seeing the entire timestamp.\n-t, -u We can sort the output of the ls command by timestamp. By default, the ls -l command is showing the modification time, this is the modification time for each file, and sorting the output by name and the ls command is listing the files in alphabetical order.\nIf you want to sort by modification time at the -lt option, it will display the output sorted by modification time, newest files first.\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ ls -lt total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 linux.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 ubuntu.txt   -ltu option will sort and show the outcome by access time.\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ ls -ltu total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d4 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:38 d1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 linux.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 ubuntu.txt   -r, --reverse If you want to reverse the order of sorting add the -r or the --reverse option like:\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ ls -ltur total 16 -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 ubuntu.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 linux.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:38 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d4   -r option can be grouped with the other option like -S, -X, ot just -l (reverse order in alphabeta order)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  ubuntu@ip-172-31-87-26:~$ ls -l total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 linux.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 ubuntu.txt ubuntu@ip-172-31-87-26:~$ ls -lr total 16 -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 ubuntu.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 linux.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1   touch command This is useful when you want a back up program to include or exclude some files or when you simply do not want other users to know that you\u0026rsquo;ve read or modified the file; you can change the timestamp on a file using the touch command.\nThe touch command is also used to create an empty file. So if the file, which is the argument of the touch command, does not exist, touch will create it. But if it exists, touch will update the file timestamps to the computer\u0026rsquo;s current time.\nSo if we run touch linux.txt it will create the file since the file does not already exist.\n1 2 3 4 5  ubuntu@ip-172-31-87-26:~$ ls d1 d2 d3 d4 f1 f2 f3 f4 ubuntu@ip-172-31-87-26:~$ touch linux.txt ubuntu@ip-172-31-87-26:~$ ls d1 d2 d3 d4 f1 f2 f3 f4 linux.txt   Run stat linux.txt; these are the timestamps of the file.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2021-04-20 12:06:57.257973914 +0000 Modify: 2021-04-20 12:06:57.257973914 +0000 Change: 2021-04-20 12:06:57.257973914 +0000 Birth: -    By the way, in Linux, it\u0026rsquo;s not recommended to use spaces or other special characters in filenames; separate the multiple words with an underscore or a hyphen. But if for some reason you want to include a whitespace in the filename, then enclose the file name in a pair of double quotes.\n Now, if we run touch linux.txt again, it will update all three timestamps of the file to the current time:\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ touch linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2021-04-20 12:11:07.872070244 +0000 Modify: 2021-04-20 12:11:07.872070244 +0000 Change: 2021-04-20 12:11:07.872070244 +0000 Birth: -   -a If you want to change only the access time to the current time, use the -a option.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ touch -a linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2021-04-20 12:12:36.715395475 +0000 Modify: 2021-04-20 12:11:07.872070244 +0000 Change: 2021-04-20 12:12:36.715395475 +0000 Birth: -   Note that the access time was updated to the current time, the change time was also updated because metadata information of the file like atime was changed.\n But if you run cat linux.txt, then only atime is changed, why ???\n -m To change the modification timestamp, you can use the same -m option :\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ touch -m linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2021-04-20 12:12:36.715395475 +0000 Modify: 2021-04-20 12:13:53.902809262 +0000 Change: 2021-04-20 12:13:53.902809262 +0000 Birth: -   The mtime and chnge time were modified.\n-t If you want to set the atime or mtime or both to a specific time, not the system current time, you can use -t yyyymmddhhmm.ss option.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  ubuntu@ip-172-31-87-26:~$ touch -mt 199903050708.09 linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2021-04-20 12:12:36.715395475 +0000 Modify: 1999-03-05 07:08:09.000000000 +0000 Change: 2021-04-20 12:23:05.730619023 +0000 Birth: - ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ touch -at 197903050708.09 linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 1979-03-05 07:08:09.000000000 +0000 Modify: 1999-03-05 07:08:09.000000000 +0000 Change: 2021-04-20 12:23:48.706292726 +0000 Birth: - ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ touch -amt 200803050708.09 linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2008-03-05 07:08:09.000000000 +0000 Modify: 2008-03-05 07:08:09.000000000 +0000 Change: 2021-04-20 12:25:26.953546776 +0000 Birth: -   -d If you want to change simultaneously both atime and mtime, you can also use the -d \u0026quot;yyyy-mm-dd hh:mm:ss\u0026quot; option.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ touch -d \u0026#34;2020-02-20 12:12:12\u0026#34; linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2020-02-20 12:12:12.000000000 +0000 Modify: 2020-02-20 12:12:12.000000000 +0000 Change: 2021-04-20 12:29:26.847378845 +0000 Birth: -   Note that -t and -d options accept different data formats.\n-r You can use the file1 -r file2 option to set the timestamps of file1 equal to file2\u0026rsquo;s. For example, we create a new file ubuntu.txt:\n1 2 3 4 5 6 7 8 9 10 11  ubuntu@ip-172-31-87-26:~$ touch ubuntu.txt ubuntu@ip-172-31-87-26:~$ touch ubuntu.txt -r linux.txt ubuntu@ip-172-31-87-26:~$ stat ubuntu.txt File: ubuntu.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 285473 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2020-02-20 12:12:12.000000000 +0000 Modify: 2020-02-20 12:12:12.000000000 +0000 Change: 2021-04-20 12:33:27.865283951 +0000 Birth: -    Note that it\u0026rsquo;s not possible to modify only the change time to a specific timestamp, but here is a trick. If you want to modify only the change time to a specific time, then you set the system date and time to the desired change time run the command touch and the name of the file to change all the timestamps to the system current time and then change back the access and modification times to the initial values. Change time will remain at the desired value.\n Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67  ########################## ## File Timestamps and Date ########################## # displaying atime ls -lu # displaying mtime ls -l ls -lt # displaying ctime ls -lc # displaying all timestamps stat file.txt # displaying the full timestamp ls -l --full-time /etc/ # creating an empty file if it does not exist, update the timestamps if the file exists touch file.txt # changing only the access time to current time touch -a file # changing only the modification time to current time touch -m file # changing the modification time to a specific date and time touch -m -t 201812301530.45 a.txt # changing both atime and mtime to a specific date and time touch -d \u0026#34;2010-10-31 15:45:30\u0026#34; a.txt # changing the timestamp of a.txt to those of b.txt touch a.txt -r b.txt # displaying the date and time date # showing this month\u0026#39;s calendar cal # showing the calendar of a specific year cal 2021 # showing the calendar of a specific month and year cal 7 2021 # showing the calendar of previous, current and next month cal -3 # setting the date and time date --set=\u0026#34;2 OCT 2020 18:00:00\u0026#34; # displaying the modification time and sorting the output by name. ls -l # displaying the output sorted by modification time, newest files first ls -lt # displaying and sorting by atime ls -ltu # reversing the sorting order ls -ltu --reverse   File Types Linux determines the type of a file via a code in the file header. It doesn\u0026rsquo;t depend on the file extension. In fact, it\u0026rsquo;s quite common in Linux for files to not have any extension at all. For example, the passwd, group do not have extension. Also all executable files or commands do not have extension. (In Windows, they would be .exe files.)\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ ls -l /bin/ls -rwxr-xr-x 1 root root 142144 Sep 5 2019 /bin/ls ubuntu@ip-172-31-87-26:~$ file /bin/ls /bin/ls: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=2f15ad836be3339dec0e2e6a3c637e08e48aacbd, for GNU/Linux 3.2.0, stripped   This is one of the differences between Linux and Windows. In Linux file extensions do not matter so much. They are mostly for humans to find out visually what the specific file could represent. When you see a file name which ends in .log, then it\u0026rsquo;s probably a log file or a file which ends in .conf it\u0026rsquo;s probably a configuration file.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ file /var/log/auth.log /var/log/auth.log: ASCII text ubuntu@ip-172-31-87-26:~$ file /etc/host.conf /etc/host.conf: ASCII text   But they have the same type in Linux\u0026rsquo;s perspective. There is another example:\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ echo \u0026#34;.\u0026#34; \u0026gt; dog.txt ubuntu@ip-172-31-87-26:~$ file dog.txt dog.txt: UTF-8 Unicode text ubuntu@ip-172-31-87-26:~$ mv dog.txt dog.pdf ubuntu@ip-172-31-87-26:~$ ls d1 d2 d3 d4 dog.pdf f1 f2 f3 f4 linux.txt ubuntu.txt ubuntu@ip-172-31-87-26:~$ file dog.pdf dog.pdf: UTF-8 Unicode text ubuntu@ip-172-31-87-26:~$ cat dog.pdf .   So you can see the extension doesn\u0026rsquo;t matter at all. However, graphical applications might require the file to have a specific extension in order to open it. If you have a .txt file and change its extension to .pdf, then you cannot open that file any more by double clicking with the mouse on it. So extensions are useful only for graphical applications.\nls -l again Let\u0026rsquo;s take a closer look at the output of ls-l command, which is one way of finding what type of specific fileis; this is indicated by the first character.\nIf you see a hyphen (-), it means that\u0026rsquo;s a regular file; d indicates a directory, which is a special type of file.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ ls -l total 20 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 -rw-rw-r-- 1 ubuntu ubuntu 46 Apr 20 13:17 dog.pdf -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 linux.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 ubuntu.txt   l indicates a symbolic link, which is like a shortcut; b means a block device; and c indicates a char device.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ ls -l /dev/rtc /dev/xvda /dev/vcsu lrwxrwxrwx 1 root root 4 Apr 14 01:53 /dev/rtc -\u0026gt; rtc0 crw-rw---- 1 root tty 7, 64 Apr 14 01:53 /dev/vcsu brw-rw---- 1 root disk 202, 0 Apr 14 01:53 /dev/xvda   Device files represent logical hardware devices like a disk, a partition or a serial console port. If you see an s the file is a type of socket.\n1 2  ubuntu@ip-172-31-87-26:~$ ls -l /run/snapd-snap.socket srw-rw-rw- 1 root root 0 Apr 15 06:08 /run/snapd-snap.socket   This is used by processes to communicate and has nothing to do with the socket term used in TCP/IP Networking.\nAnd the last possible option for the first character is a p which indicates a named pipe.\n1 2  ubuntu@ip-172-31-87-26:~$ ls -l /run/initctl prw------- 1 root root 0 Apr 14 01:53 /run/initctl   A named pipe, like a socket, provides an easy way for the processes to communicate. Using named pipes, you could send the output from one command to another and end up with only the data you want to see without selecting and formatting the entire output.\nls -F Another way to classify a file is to run ls with the capital -F option. This will add the symbol after the filename according to its type.\nA / indicates a directory; no symbol added indicates a regular non-executable file; an @ sign indicates a symlink; a * means executable file.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ ls -F /etc/vtrgb /etc/xdg /etc/pam.conf /etc/pam.conf /etc/vtrgb@ /etc/xdg: autostart/ systemd/ user-dirs.conf user-dirs.defaults ubuntu@ip-172-31-87-26:~$ ls -F /bin/ls /bin/ls*   and the = sign indicates a socket; And a pipe |, a vertical bar, indicates a named pipe.\n1 2 3  ubuntu@ip-172-31-87-26:~$ ls -lF /run/snapd-snap.socket /run/initctl prw------- 1 root root 0 Apr 14 01:53 /run/initctl| srw-rw-rw- 1 root root 0 Apr 15 06:08 /run/snapd-snap.socket=   file file command show type information about file or directory specified:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  ubuntu@ip-172-31-87-26:~$ file ~ /home/ubuntu: directory ubuntu@ip-172-31-87-26:~$ file ~/ /home/ubuntu/: directory ubuntu@ip-172-31-87-26:~$ file ~/* /home/ubuntu/d1: directory /home/ubuntu/d2: directory /home/ubuntu/d3: directory /home/ubuntu/d4: directory /home/ubuntu/dog.pdf: UTF-8 Unicode text /home/ubuntu/f1: empty /home/ubuntu/f2: empty /home/ubuntu/f3: empty /home/ubuntu/f4: empty /home/ubuntu/linux.txt: empty /home/ubuntu/ubuntu.txt: empty   View Files The entire configuration of the Linux system is saved in text files. User accounts and their passwords groups, the network configuration, the security policy, the configuration of any network service like a Web, FTP or an email server are all saved in text files. So read text files on a Linux system is essential.\ncat command The cat command displays the contents of the file in the terminal window.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ cat /etc/hosts 127.0.0.1 localhost # The following lines are desirable for IPv6 capable hosts ::1 ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts   The syntax of the cat command is cat \u0026lt;filename\u0026gt;; if you want, you can give more files as arguments and cat will display all of them in the terminal.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ cat /etc/hosts /etc/host.conf 127.0.0.1 localhost # The following lines are desirable for IPv6 capable hosts ::1 ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts # The \u0026#34;order\u0026#34; line is only used by old versions of the C library. order hosts,bind multi on   Using option -n to print out the number of the line:\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ cat /etc/hosts /etc/host.conf -n 1\t127.0.0.1 localhost 2\t3\t# The following lines are desirable for IPv6 capable hosts 4\t::1 ip6-localhost ip6-loopback 5\tfe00::0 ip6-localnet 6\tff00::0 ip6-mcastprefix 7\tff02::1 ip6-allnodes 8\tff02::2 ip6-allrouters 9\tff02::3 ip6-allhosts 10\t# The \u0026#34;order\u0026#34; line is only used by old versions of the C library. 11\torder hosts,bind 12\tmulti on    note that cat is not interactive and if the file is too long, the contents scroll off or exceed the screen. So cat is the tool of choice when you want to display small files whose entire contents fit to the screen.\n cat\u0026rsquo;s short for word concatenated, so besides displaying the contents on the screen, it\u0026rsquo;s also concatenating files into a single one.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  ubuntu@ip-172-31-87-26:~$ cat /etc/hosts /etc/host.conf \u0026gt; cathost ubuntu@ip-172-31-87-26:~$ ls cathost d1 d2 d3 d4 dog.pdf f1 f2 f3 f4 linux.txt ubuntu.txt ubuntu@ip-172-31-87-26:~$ cat -n cathost 1\t127.0.0.1 localhost 2\t3\t# The following lines are desirable for IPv6 capable hosts 4\t::1 ip6-localhost ip6-loopback 5\tfe00::0 ip6-localnet 6\tff00::0 ip6-mcastprefix 7\tff02::1 ip6-allnodes 8\tff02::2 ip6-allrouters 9\tff02::3 ip6-allhosts 10\t# The \u0026#34;order\u0026#34; line is only used by old versions of the C library. 11\torder hosts,bind 12\tmulti on   less command If the output of a text file exceeds the terminal window you probably should use less commands instead of cat. less is probably the most advanced and used reading tool that Linux provides. To view the contents of a file with less, you write less \u0026lt;filename\u0026gt;:\n1  ubuntu@ip-172-31-87-26:~$ less /var/log/dmesg   This is the contents of the file. We\u0026rsquo;ve already discussed some rules and options when we\u0026rsquo;ve talked about man pages in the previous blog. But now we will recall them: When you are in a less window:\n To get additional help by typing the letter h. To exit the file type q. To move a line up or down use the arrow keys. To forward one window press on control + f or on the space key. To move backwards a screen or a window, you press control + b. To go to the very beginning of the file typed lowercase g. To go to the very end type uppercase G. To search in the current file from the current line to the end of file using /\u0026lt;string\u0026gt; and then typing the enter. To search in the current file from the current line to the begining of file using ?\u0026lt;string\u0026gt; and then typing the enter. To navigate between matches with n which means next occurrence; navigate to the previous occurrence by N.  tail, head, watch tail -n Another useful command used for viewing files is tail. Using tail you can view the last lines of a file and by default it shows the last 10 lines. If you want to read a custom number of lines of a file, use -n and the number.\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ tail -n 4 /etc/passwd ec2-instance-connect:x:112:65534::/nonexistent:/usr/sbin/nologin systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false ubuntu@ip-172-31-87-26:~$ tail -n 2 /etc/group ubuntu:x:1000: mlocate:x:119:   If you want to read the last lines, starting with a specific line number, for example 7 line, then you can use\n1 2 3  tail -6 # or tail -n +7   for example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  ubuntu@ip-172-31-87-26:~$ cat -n cathost 1\t127.0.0.1 localhost 2\t3\t# The following lines are desirable for IPv6 capable hosts 4\t::1 ip6-localhost ip6-loopback 5\tfe00::0 ip6-localnet 6\tff00::0 ip6-mcastprefix 7\tff02::1 ip6-allnodes 8\tff02::2 ip6-allrouters 9\tff02::3 ip6-allhosts 10\t# The \u0026#34;order\u0026#34; line is only used by old versions of the C library. 11\torder hosts,bind 12\tmulti on ubuntu@ip-172-31-87-26:~$ tail -6 cathost ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts # The \u0026#34;order\u0026#34; line is only used by old versions of the C library. order hosts,bind multi on ubuntu@ip-172-31-87-26:~$ tail -n +7 cathost ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts # The \u0026#34;order\u0026#34; line is only used by old versions of the C library. order hosts,bind multi on   So tail will display the contents from line 7 to the end.\ntail -f tail is really useful for reading a file in real time, like when you want to view the last n lines of a log file for important system messages. Using the -f option, tail automatically prints new messages from an open file to the screen in real time (it will not return the prompt untill you press control + c)\nFor example, let\u0026rsquo;s open two AWS EC2 terminals, and run:\n1  tail -n 5 -f /var/log/auth.log   in one of the terminal, and then run sudo su in the other terminal to switch to the root user, this exectuation will be record in the /var/log/auth.log file, and the tail window will update the contents automatically.\n Remember that log file generally be stored in /var/log directory. Amd the authentication tries are saved in /var/log/auth.log\n the head command, its the complementary of tail command, prints the top n number of lines from a given file.\nwatch command If you want to run a command repeatedly displaying its output, you use the watch command. It allows you to watch the program output change over time. What watch does is implement the following command every 2 seconds, and print the outcome to the window. The process will go on untill you press control + c.\nFor example, it can accomplish the above task of monitoring logs in real time (actually 2 seconds) in a way if you run\n1  watch -n 1 tail -n 5 /var/log/auth.log   Here the first -n 1 is the option of watch command, it control the update time interval; the second -n 5 is tail command\u0026rsquo;s.\nAnother interesting option is -d to highlight the differences between successive updates. For example, I want to see how packets are sent and received by executing the ifconfig repeatedly. So run\n1  watch -n 1 -d ifconfig   Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  ########################## ## Viewing files (cat, less, more, head, tail, watch) ########################## # displaying the contents of a file cat filename # displaying more files cat filename1 filename2 # displaying the line numbers can -n filename # concatenating 2 files cat filename1 filename2 \u0026gt; filename3 # viewing a file using less less filename # less shortcuts: # h =\u0026gt; getting help # q =\u0026gt; quit # enter =\u0026gt; show next line # space =\u0026gt; show next screen # /string =\u0026gt; search forward for a string # ?string =\u0026gt; search backwards for a string # n / N =\u0026gt; next/previous appearance # showing the last 10 lines of a file tail filename # showing the last 15 lines of a file tail -n 15 filename # showing the last lines of a file starting with line no. 5 tail -n +5 filename # showing the last 10 lines of the file in real-time tail -f filename # showing the first 10 lines of a file head filename # showing the first 15 lines of a file head -n 15 filename # running repeatedly a command with refresh of 3 seconds watch -n 3 ls -l   Create, Copy and Move (Remove) touch, mkdir command To create a new file in Linux, you have to use the touch command. We\u0026rsquo;ve already discussed it in a previous section. So now we\u0026rsquo;ll just recap it quickly.\nThe touch command takes as argument one or more files. If those files already exist touch will update the files timestamps to the current system time. But if the files do not exist, then touch will create them.\nIt\u0026rsquo;s important to note that the user executing this command must have enough permissions to create the file in the parent directory or he will get a permission denied error. For example, We are now a non privileged user, and if I try touch /abc. I will get a permission denied error, because non-privileged user is not allowed to create the file called abc in /.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm),20(dialout),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),117(netdev),118(lxd) ubuntu@ip-172-31-87-26:~$ touch /abc touch: cannot touch \u0026#39;/abc\u0026#39;: Permission denied   To create a new directories using the mkdir command. mkdir comes from make directory. It\u0026rsquo;s syntax is mkdir [option]... [directory].... In case you want mkdir to display some details about the operation is performing, then add the -v option, it displays this message and created the directory:\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ mkdir -v ddd mkdir: created directory \u0026#39;ddd\u0026#39; ubuntu@ip-172-31-87-26:~$ ls cathost d1 d2 d3 d4 d5 d6 d7 ddd dog.pdf f1 f2 f3 f4 linux.txt ubuntu.txt   And if you want to create more directories in one command, then give all directories, you want to create as arguments. In this example, We create three directories at once.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ mkdir -v ddd1 ddd2 ddd3 mkdir: created directory \u0026#39;ddd1\u0026#39; mkdir: created directory \u0026#39;ddd2\u0026#39; mkdir: created directory \u0026#39;ddd3\u0026#39;   Now, let\u0026rsquo;s suppose I want to create a complete directory structure: ~/first/second/third. Note that first/, second/, third do not exist.\n1 2  ubuntu@ip-172-31-87-26:~$ mkdir first/second/third mkdir: cannot create directory first/second/third: No such file or directory   We get an error. That because mkdir wanted to create the directories from right to left. So when it wanted to create a directory called third/, it noticed that its parent directory called second/ doesn\u0026rsquo;t exist. To solve this issue just at the -p option, which comes from parents. And it will make the parent directories as needed.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ mkdir -p first/second/third ubuntu@ip-172-31-87-26:~$ tree first/ first/  second  third 2 directories, 0 files   -p option is also good when the directory already exists and you don\u0026rsquo;t want an error. For example. we have just created first directory. If you mkdir first again, you will get an error.\n1 2  ubuntu@ip-172-31-87-26:~$ mkdir first mkdir: cannot create directory first: File exists   Remember that a directory is a special type of file. But if I add the -p option, there will be no error. Notice that mkdir -p just closet the error, instead of overwritting the first directoty.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ mkdir -p first ubuntu@ip-172-31-87-26:~$ tree first/ first/  second  third 2 directories, 0 files   cp command To copy files and directories, you use the cp command, which stands for copy. cp has three principal modes of operation, depending on the number and the type of arguments passed.\ncp \u0026lt;file\u0026gt; \u0026lt;file\u0026gt; If the command has two arguments of type files, it copies the contents of the first file to the second file. If the second file does not exist, cp will create it. But if it already exists, cp will overwrite the destination file.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ cp /etc/passwd user.txt ubuntu@ip-172-31-87-26:~$ ls cathost d2 d4 d6 ddd ddd2 dog.pdf f2 f4 linux.txt user.txt d1 d3 d5 d7 ddd1 ddd3 f1 f3 first ubuntu.txt ubuntu@ip-172-31-87-26:~$ tail -n 5 user.txt pollinate:x:111:1::/var/cache/pollinate:/bin/false ec2-instance-connect:x:112:65534::/nonexistent:/usr/sbin/nologin systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false   To show files that are being copied pass the -v option to the cp command. This prints the file and folders that are being copied to standard output. And if you wanted to prompt before overwriting add the -i option.\n1 2 3  ubuntu@ip-172-31-87-26:~$ cp -vi /etc/passwd user.txt cp: overwrite \u0026#39;user.txt\u0026#39;? y \u0026#39;/etc/passwd\u0026#39; -\u0026gt; \u0026#39;user.txt\u0026#39;   cp \u0026lt;file\u0026gt;... \u0026lt;dir\u0026gt; The second mode of operation is when cp has one or more arguments of type file, which is the source and the last argument of typed directory, which is the destination. In this case, it copies the source files to the destination directory, creating any files that do not already exist in the destination. And you can use -i and -v here.\n1 2 3 4 5 6 7 8 9 10 11  ubuntu@ip-172-31-87-26:~$ cp -vi linux.txt user.txt ubuntu.txt d1 \u0026#39;linux.txt\u0026#39; -\u0026gt; \u0026#39;d1/linux.txt\u0026#39; \u0026#39;user.txt\u0026#39; -\u0026gt; \u0026#39;d1/user.txt\u0026#39; \u0026#39;ubuntu.txt\u0026#39; -\u0026gt; \u0026#39;d1/ubuntu.txt\u0026#39; ubuntu@ip-172-31-87-26:~$ tree d1 d1  linux.txt  ubuntu.txt  user.txt 0 directories, 3 files   Notice, the destation directory must exist before, otherwise cp will view the directory name as a filename.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ cp /etc/passwd user.txt ubuntu@ip-172-31-87-26:~$ cp /etc/passwd user ubuntu@ip-172-31-87-26:~$ ls -l total 8 -rw-r--r-- 1 ubuntu ubuntu 1825 Apr 21 11:22 user -rw-r--r-- 1 ubuntu ubuntu 1825 Apr 21 11:22 user.txt   cp -r \u0026lt;dir\u0026gt;... And the third and the last mode of operation is when all arguments are of type directory. In this case, cp copies all files in the source directory to the destination directoy creating any files or directories needed. The destination directory must exist before. This requires -r, an option that indicates the recursive copying of directories. In these mode, option -i is invalid.\nThe user that runs the cp command becomes the files owner and that\u0026rsquo;s not good in some cases. To preserve file attributes, timestamps, permissions, group and user ownership pass the -p option, which stands for a preserve.\nTimestamp:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  ubuntu@ip-172-31-87-26:~$ stat d1 File: d1 Size: 4096 Blocks: 8 IO Block: 4096 directory Device: ca01h/51713d\tInode: 256334 Links: 3 Access: (0775/drwxrwxr-x) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2022-04-21 11:26:00.967959343 +0000 Modify: 2022-04-21 11:23:42.937078490 +0000 Change: 2022-04-21 11:23:42.937078490 +0000 Birth: - ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ cp -r d1 d6 ubuntu@ip-172-31-87-26:~$ cp -rp d1 d7 ubuntu@ip-172-31-87-26:~$ stat d6/d1 File: d6/d1 Size: 4096 Blocks: 8 IO Block: 4096 directory Device: ca01h/51713d\tInode: 307832 Links: 3 Access: (0775/drwxrwxr-x) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2022-04-21 11:30:52.237590828 +0000 Modify: 2022-04-21 11:30:52.237590828 +0000 Change: 2022-04-21 11:30:52.237590828 +0000 Birth: - ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ stat d7/d1 File: d7/d1 Size: 4096 Blocks: 8 IO Block: 4096 directory Device: ca01h/51713d\tInode: 307836 Links: 3 Access: (0775/drwxrwxr-x) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2022-04-21 11:26:00.967959343 +0000 Modify: 2022-04-21 11:23:42.937078490 +0000 Change: 2022-04-21 11:31:06.249476726 +0000 Birth: -   User and Group:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  ubuntu@ip-172-31-87-26:~$ sudo cp -r /var/log ~ ubuntu@ip-172-31-87-26:~$ ls -l total 20 drwxrwxr-x 3 ubuntu ubuntu 4096 Apr 22 12:47 dir1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 11:40 dir2 drwxrwxr-x 9 ubuntu ubuntu 4096 Apr 22 13:42 log -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 22 09:51 ls.txt -rw-rw-r-- 1 ubuntu ubuntu 61 Apr 22 10:04 mac.txt drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 22 12:40 projects ubuntu@ip-172-31-87-26:~$ sudo rm -rf log ubuntu@ip-172-31-87-26:~$ sudo cp -rp /var/log ~ ubuntu@ip-172-31-87-26:~$ ls -l total 20 drwxrwxr-x 3 ubuntu ubuntu 4096 Apr 22 12:47 dir1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 11:40 dir2 drwxrwxr-x 9 root syslog 4096 Apr 22 00:00 log -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 22 09:51 ls.txt -rw-rw-r-- 1 ubuntu ubuntu 61 Apr 22 10:04 mac.txt drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 22 12:40 projects   mv command move The mv command is a command line utility that moves one or more file directories from one place to another. This command has two distinct functions. It moves files and directories or it renames files and directories.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  ubuntu@ip-172-31-87-26:~$ mkdir -p dir1/dir2 ubuntu@ip-172-31-87-26:~$ touch dir1/a.txt dir1/dir2/b.txt ubuntu@ip-172-31-87-26:~$ tree dir1 dir1  a.txt  dir2  b.txt 1 directory, 2 files ubuntu@ip-172-31-87-26:~$ mv dir1/dir2/b.txt dir1/ ubuntu@ip-172-31-87-26:~$ tree dir1 dir1  a.txt  b.txt  dir2 1 directory, 2 files   The file was moved. You can move multiple files into a directory as well: all the files you want to move as arguments and the last argument will be the destination directory.\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ mv dir1/a.txt dir1/b.txt dir1/dir2/ ubuntu@ip-172-31-87-26:~$ tree dir1 dir1  dir2  a.txt  b.txt 1 directory, 2 files   You can also use a pattern matching using the wildcard; And if you want to move a directory, you move it as you would any other file.\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ mv dir1/dir2/*.txt dir1 ubuntu@ip-172-31-87-26:~$ mv dir1/dir2 ~ ubuntu@ip-172-31-87-26:~$ tree .  dir1  a.txt  b.txt  dir2  user  user.txt 2 directories, 4 files   -i If the destination file already exists, the destination file will be overwritten without prompting; to prompt before overwriting a file, the -i option can be used; -i cmes from interactive.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  ubuntu@ip-172-31-87-26:~$ touch a.txt b.txt ubuntu@ip-172-31-87-26:~$ mv a.txt dir1 ubuntu@ip-172-31-87-26:~$ mv -i b.txt dir1 mv: overwrite \u0026#39;dir1/b.txt\u0026#39;? y ubuntu@ip-172-31-87-26:~$ tree .  dir1  a.txt  b.txt  dir2  user  user.txt 2 directories, 4 files   -n To prevent the existing file being overwritten, use the -n option. You can see that the a.txt, b.txt in ~ does not move because we set the opetion -n.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  ubuntu@ip-172-31-87-26:~$ touch a.txt b.txt ubuntu@ip-172-31-87-26:~$ mv -n a.txt b.txt dir1/ ubuntu@ip-172-31-87-26:~$ tree .  a.txt  b.txt  dir1  a.txt  b.txt  dir2  user  user.txt 2 directories, 6 files   -u Another useful option is -u or --update When using this option, it will move only when the source file is newer than the destination file or when the destination file is missing. This guarantees that only newer files will be moved.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ mv -u dir1/a.txt ~ ubuntu@ip-172-31-87-26:~$ mv -u b.txt dir1/ ubuntu@ip-172-31-87-26:~$ tree .  a.txt  dir1  a.txt  b.txt  dir2  user  user.txt 2 directories, 5 files   You can see that ~/dir1/a.txt does not move to the ~ be cause ~/a.txt is newer than ~/dir1/a.txt; and ~/b.txt moves in to dir1, because it is newer than ~/dir1/b.txt, and ~/dir1/b.txt has been overwritten.\nrename mv can rename the file and directory.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ ls a.txt dir1 dir2 user user.txt ubuntu@ip-172-31-87-26:~$ mv a.txt aa.txt ubuntu@ip-172-31-87-26:~$ mv dir1 dir11 ubuntu@ip-172-31-87-26:~$ ls aa.txt dir11 dir2 user user.txt   You can even move the file to a different directory and rename it at the same time.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  ubuntu@ip-172-31-87-26:~$ tree .  aa.txt  dir11  a.txt  b.txt  dir2  user  user.txt 2 directories, 5 files ubuntu@ip-172-31-87-26:~$ mv user dir11/c.txt ubuntu@ip-172-31-87-26:~$ tree .  aa.txt  dir11  a.txt  b.txt  c.txt  dir2  user.txt 2 directories, 5 files   rm, shred command In Linux to remove files or directories from the command line use the rm command. Be extra careful when removing files or directories, because once the file is deleted with the rm command it cannot be easily recovered. There isn\u0026rsquo;t such a concept as the recycle bin or trash for the files that were removed from the command line.\nTo remove one or more files, give them as arguments to the rm command.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  ubuntu@ip-172-31-87-26:~$ tree .  aa.txt  dir11  a.txt  b.txt  c.txt  dir2  user.txt 2 directories, 5 files ubuntu@ip-172-31-87-26:~$ rm aa.txt dir11/b.txt ubuntu@ip-172-31-87-26:~$ tree .  dir11  a.txt  c.txt  dir2  user.txt 2 directories, 3 files   Note that the files were removed immediately without prompting. To prompt for confirmation when removing a file use the -i option; it is the same option we\u0026rsquo;ve seen at cp and mv comments and comes from interactive. It will cause rm to ask for confirmation.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ rm -i dir11/a.txt rm: remove regular empty file \u0026#39;dir11/a.txt\u0026#39;? y ubuntu@ip-172-31-87-26:~$ tree .  dir11  c.txt  dir2  user.txt 2 directories, 2 files   To show more information when removing a file add the -v option, which comes from verbose:\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ rm -v dir11/c.txt user.txt removed \u0026#39;dir11/c.txt\u0026#39; removed \u0026#39;user.txt\u0026#39; ubuntu@ip-172-31-87-26:~$ tree .  dir11  dir2 2 directories, 0 files   To remove the directorie, use the -r or --recursive option to make removal recursive.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ rm dir11 rm: cannot remove \u0026#39;dir11\u0026#39;: Is a directory ubuntu@ip-172-31-87-26:~$ rm -rv dir11 removed directory \u0026#39;dir11\u0026#39; ubuntu@ip-172-31-87-26:~$ tree .  dir2 1 directory, 0 files   protect You can protect a file by creating it as root. And if you you try to remove a protected file with rm by default, it will prompt for your confirmation.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ sudo mkdir dir1 ubuntu@ip-172-31-87-26:~$ sudo touch dir1/a.txt ubuntu@ip-172-31-87-26:~$ ls -l ~ dir1/a.txt -rw-r--r-- 1 root root 0 Apr 21 12:29 dir1/a.txt /home/ubuntu: total 8 drwxr-xr-x 2 root root 4096 Apr 21 12:29 dir1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 11:40 dir2   you can see that dir1 and dir1/a.txt are belong to root, which means thay are under protection. And when you remove them, rm will ask for confirmation.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ rm dir1/a.txt rm: remove write-protected regular empty file \u0026#39;dir1/a.txt\u0026#39;? Y rm: cannot remove \u0026#39;dir1/a.txt\u0026#39;: Permission denied ubuntu@ip-172-31-87-26:~$ rm -r dir1 rm: descend into write-protected directory \u0026#39;dir1\u0026#39;? y rm: remove write-protected regular empty file \u0026#39;dir1/a.txt\u0026#39;? y rm: cannot remove \u0026#39;dir1/a.txt\u0026#39;: Permission denied   -f, wildcards Just imagine that you have to remove hundreds of files from a directory and you have to confirm each removal. To force removal and not receive a prompt pass the -f option. -f stands for force.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  ubuntu@ip-172-31-87-26:~$ tree .  dir1  a1  a2  a3  a4  a5  dir2  dir3  dir2 4 directories, 5 files ubuntu@ip-172-31-87-26:~$ rm -rf dir1/*2 ubuntu@ip-172-31-87-26:~$ tree .  dir1  a1  a3  a4  a5  dir2 2 directories, 4 files    Notice, when using pattern matches and wildcards like asterisks, dots or question marks, you should be careful. Before running a command like rm *.txt execute echo and the pattern, to see exactly what are the files that will be matched. Sometimes these patterns expand in names you don\u0026rsquo;t expect to. So before executing this command execute echo and the pattern to see which files will be matched. This is also applicable to other possible destructive commands, not only rm.\n shred command The rm command does not actually delete a file. Instead, it unlinks it, meaning the data is still on disk. The space on disk occupied by the deleted file is marked as being free and can be used by other file or process. Now depending on different factors that are professional solutions that make the recovery of the deleted files possible. This is available to any other operating system, including Windows, Mac or Android. So if you remove a file, you can\u0026rsquo;t be 100 percent sure that no one will ever recover it.\nWhat you can do is use the shred command instead. shred overwrites a file to hide its contents many times before removing. Let\u0026rsquo;s see an example. We copy the password file from /etc to the current directory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  ubuntu@ip-172-31-87-26:~$ cp /etc/passwd passwd ubuntu@ip-172-31-87-26:~$ ls dir1 dir2 passwd ubuntu@ip-172-31-87-26:~$ shred -vu -n 10 passwd shred: passwd: pass 1/10 (random)... shred: passwd: pass 2/10 (b6db6d)... shred: passwd: pass 3/10 (ffffff)... shred: passwd: pass 4/10 (db6db6)... shred: passwd: pass 5/10 (000000)... shred: passwd: pass 6/10 (random)... shred: passwd: pass 7/10 (555555)... shred: passwd: pass 8/10 (492492)... shred: passwd: pass 9/10 (aaaaaa)... shred: passwd: pass 10/10 (random)... shred: passwd: removing shred: passwd: renamed to 000000 shred: 000000: renamed to 00000 shred: 00000: renamed to 0000 shred: 0000: renamed to 000 shred: 000: renamed to 00 shred: 00: renamed to 0 shred: passwd: removed ubuntu@ip-172-31-87-26:~$ ls dir1 dir2   here:\n -v means verbose or showing the process -u means removing the files after overwriting. -n and an integer number N, overwrites the file N times instead of the default, which is 3.  You can see what shred did to the target file by this\n Even though shred is good enough in the secure removal of files, if you want to keep your confidential data secure, you have to encrypt it. You cannot rely on simply deleting a file and suppose that no one could ever recover and access it.\n Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82  ########################## ## Working with files and directory (touch, mkdir, cp, mv, rm, shred) ########################## # creating a new file or updating the timestamps if the file already exists touch filename # creating a new directory mkdir dir1 # creating a directory and its parents as well mkdir -p mydir1/mydir2/mydir3 ###################### ### The cp command ### ###################### # copying file1 to file2 in the current directory cp file1 file2 # copying file1 to dir1 as another name (file2) cp file1 dir1/file2 # copying a file prompting the user if it overwrites the destination cp -i file1 file2 # preserving the file permissions, group and ownership when copying cp -p file1 file2 # being verbose cp -v file1 file2 # recursively copying dir1 to dir2 in the current directory cp -r dir1 dir2/ # copy more source files and directories to a destination directory cp -r file1 file2 dir1 dir2 destination_directory/ ###################### ### The mv command ### ###################### # renaming file1 to file2 mv file1 file2 # moving file1 to dir1  mv file1 dir1/ # moving a file prompting the user if it overwrites the destination file mv -i file1 dir1/ # preventing a existing file from being overwritten mv -n file1 dir1/ # moving only if the source file is newer than the destination file or when the destination file is missing mv -u file1 dir1/ # moving file1 to dir1 as file2 mv file1 dir1/file2 # moving more source files and directories to a destination directory mv file1 file2 dir1/ dir2/ destination_directory/ ###################### ### The rm command ### ###################### # removing a file rm file1 # being verbose when removing a file rm -v file1 # removing a directory rm -r dir1/ # removing a directory without prompting rm -rf dir1/ # removing a file and a directory prompting the user for confirmation rm -ri fil1 dir1/ # secure removal of a file (verbose with 100 rounds of overwriting) shred -vu -n 100 file1   ","date":"2021-04-21T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/linux-operation-system-ii-file-system-1/","title":"Linux Operation System II (File System 1)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\nAll operations in this series of blogs (Linux Operation System) are done in AWS Elastic Compute Cloud (EC2).\n Getting Help There are two common kinds of commands' kind in Linux, the executable file, or build-in command. You can use type command to check out. For example, cp is an executable file:\n1 2  ubuntu@ip-172-31-87-26:~$ type cp cp is /usr/bin/cp   while cd is a build-in command:\n1 2  ubuntu@ip-172-31-87-26:~$ type cd cd is a shell builtin   For executable file, You can use man command to fetch ones documentation.\n by the way, man is also an executable file, which means you can use man man command to get its documentation.\n Executable file Structure of man page man is short for a manual and a man page is simply the documentation of a program or command. You can use the man command to read the documentation of a specific command. To open the man page of the ls command. use\n1  ubuntu@ip-172-31-87-26:~$ man ls   By default, man uses a program called \u0026ldquo;less\u0026rdquo; to display the output. While you are in the man page, you can get additional help by typing the letter h. Then the page will return to \u0026ldquo;summary of less commands\u0026rdquo;. You can get back to man page by typing the letter q. You can also quit the man page by q.\nYou\u0026rsquo;ll find the same structure for all man pages.\nThe first line contains the name of the command you are reading about, along with a very short description. Then comes the synopsis of the command, and a formal description with all its available options. At the end of the man page, there are the author, the copyright and see also sections. For example:\nIn a man page everything that appears directly before ..., which are also called ellipsis, can be repeated, and everything with in a square of bracket [] is optional. This means that you can specify more than one option or file to ls, or you can can run it without options and arguments.\nWhen you see an option in short format with single minus -, a comma and an option in long format with double minus --, it means that you can use any form you wish.\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ ls d1 d2 d3 d4 f1 f2 f3 f4 ubuntu@ip-172-31-87-26:~$ ls -a . .bash_history .bashrc .lesshst .ssh d1 d3 f1 f3 .. .bash_logout .cache .profile .sudo_as_admin_successful d2 d4 f2 f4 ubuntu@ip-172-31-87-26:~$ ls --all . .bash_history .bashrc .lesshst .ssh d1 d3 f1 f3 .. .bash_logout .cache .profile .sudo_as_admin_successful d2 d4 f2 f4   Navigation of man page Use the arrow keys to move a line up or down. To forward a screen, press on control and F or on the space key. To move backwards a screen, you press control and B.\nTo go to the very beginning of the man page type lower case g, and to go to the very end type upper case G.\nSometimes you want to search in the current man page for a specific option; To search in the below pages, you could type:\n1  \\\u0026lt;string\u0026gt;   and then press enter. Any matches in the current man page will be highlighted. And then you can go to the next match by typing n or go to the prior match by typing N.\nTo search in the above pages, you could type:\n1  ?\u0026lt;string\u0026gt;   and press enter. Also, you can use n or N to navigate.\nBuild-in command Most of the time shell built in commands don\u0026rsquo;t have a dedicated man page. To see the help of such a command you can use help command:\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ help cd cd: cd [-L|[-P [-e]] [-@]] [dir] Change the shell working directory. Change the current directory to DIR. The default DIR is the value of the HOME shell variable. ...   Note that you can use the help command only for a shell built in commands and not for commands that are executable files on the disk. But another way to get some help is the option --help, this is available for both shell builtin and executable files commands.\nTo search for a command, feature keyword in all man pages, you can use man -k and the keyword you are searching for. For example:\n1 2 3  ubuntu@ip-172-31-87-26:~$ man -k ifconfig ifcfg (8) - simplistic script which replaces ifconfig IP management ifconfig (8) - configure a network interface   You can also pass a string enclosed in double quotes as argument:\n1 2 3 4 5  ubuntu@ip-172-31-87-26:~$ man -k \u0026#34;copy files\u0026#34;. cp (1) - copy files and directories cpio (1) - copy files to and from archives git-checkout-index (1) - Copy files from the index to the working tree install (1) - copy files and set attributes   It will search for the string copy files in all man pages. Note that man -k is similar to the apropos command:\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ apropos ifconfig ifcfg (8) - simplistic script which replaces ifconfig IP management ifconfig (8) - configure a network interface ubuntu@ip-172-31-87-26:~$ apropos \u0026#39;copy files\u0026#39; cp (1) - copy files and directories cpio (1) - copy files to and from archives git-checkout-index (1) - Copy files from the index to the working tree install (1) - copy files and set attributes   Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  ########################## ## Getting Help in Linux ########################## # MAN Pages man command # For executable file =\u0026gt; Ex: man ls # The man page is displayed with the less command # SHORTCUTS: # h =\u0026gt; getting help # q =\u0026gt; quit # enter =\u0026gt; show next line # space =\u0026gt; show next screen # control + f =\u0026gt; show next screen # control + b =\u0026gt; show last screen # /string =\u0026gt; search forward for a string # ?string =\u0026gt; search backwards for a string # n / N =\u0026gt; next/previous appearance # checking if a command is shell built-in or executable file type rm # =\u0026gt; rm is /usr/bin/rm type cd # =\u0026gt; cd is a shell builtin # getting help for shell built-in commands help command # For build-in command =\u0026gt; Ex: help cd command --help # For both executable file and build-in command =\u0026gt; Ex: rm --help # searching for a command, feature or keyword in all man Pages man -k uname man -k \u0026#34;copy files\u0026#34; apropos passwd   Keyboard Shoutcuts Autocomplete The TAB key for auto-completion will not only save you much time at the Linux command line, but it will keep you from making mistakes as well.\nTAB is a feature supported by Bash and other shells as well. When typing at the Linux command line, just use TAB to autocomplete commands, filenames or folder names.\nIf a single match is found, Bash will automatically complete the filename; if multiple matches are found, you have to press the TAB key twice and it will show you all results and you should continue typing until there is only one match.\n1 2  ubuntu@ip-172-31-87-26:~$ ls f f1 f2 f3 f4   If TAB does not complete automatically a command or a filename when it should, then there is a mistake. Do not continue writing the command or the file name, but stop and analyze what you have done up to that point.\nFor example, let\u0026rsquo;s suppose that by mistake we write a whitespace before the file path.\n1  ubuntu@ip-172-31-87-26:~$ rm /home/ubuntu/ f   Then when you press TAB, there is nothing happens, and you should notice it. Otherwise rm command will delete your home directory and leads to the system corrupt. Such a mistake is avoided if you always use tab completion.\nCompendium There are also some important shortcuts:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  ########################## ## Keyboard Shortcuts ########################## TAB # autocompletes the command or the filename if its unique TAB TAB (press twice) # displays all commands or filenames that start with those letters # clearing the terminal CTRL + L # closing the shell (you can do this by command `exit`) CTRL + D # cutting (removing) the current line  CTRL + U # moving the cursor to the start of the line CTRL + A # moving the cursor to the end of the line Ctrl + E # stopping the current command CTRL + C # sleeping a the running program CTRL + Z # opening a terminal  CTRL + ALT + T   History command history $HISTFILESIZE and $HISTSIZE The bash shell stores historical commands in the ~/.bash_history by default.\n1 2 3  ubuntu@ip-172-31-87-26:~$ ls -a ~/ . .bash_history .bashrc .lesshst .ssh d1 d3 f1 f3 .. .bash_logout .cache .profile .sudo_as_admin_successful d2 d4 f2 f4    Every user has his history file in his home directory.\n The number of commands that will be stored in the this file is controlled by an environment variable called HISTFILESIZE . To print the value of a variable run:\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ echo $HISTFILESIZE 2000 ubuntu@ip-172-31-87-26:~$ cat .bash_history | tail -n 5 locate \u0026#39;f2\u0026#39; find / \u0026#39;f2\u0026#39; cd ~ ls exit   We can see that Linux system saves 2000 commands into the history file. Remember that these commands are stored in disk, which means these commands will not update until you log out. So to recall the commands you just used, you can use history command, it will return the commands that stored in the memory:\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ history | tail -n 5 149 ls ~/ -a 150 cat .bash_history | tail -n 5 151 cd .. 152 cd ~ 153 history | tail -n 5   Note that these running the history commands doesnt mean listing the entire contents of the history file. There is another variable called HISTSIZE that controls how many commands from your bash history will be stored in the memory:\n1 2  ubuntu@ip-172-31-87-26:~$ echo $HISTSIZE 1000   Thus bash will save maximum 2000 commands in the ~/.bash_history file, and maximum 1000 commands in the memory.\n! To run a specific command from the history, you can use an exclamation mark ! and the command number, without any space. For example:\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ history | tail -n 5 154 echo $HISTSIZE 155 history | tail -n 5 156 ! 154 157 history 158 history | tail -n 5 ubuntu@ip-172-31-87-26:~$ !154 echo $HISTSIZE 1000   !! And to rerun the last command type two exclamation marks. It has the same effect as pressing the Up arrow once to view the previous command and then pressing Enter.\n1 2 3  ubuntu@ip-172-31-87-26:~$ !! echo $HISTSIZE 1000   !- You can also refer to a command from a certain number of lines, back in the bash history. For example, !-7 would run the last 7th command.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ history | tail -n5 156 ! 154 157 history 158 history | tail -n 5 159 echo $HISTSIZE 160 history | tail -n5 ubuntu@ip-172-31-87-26:~$ !-2 echo $HISTSIZE 1000   !\u0026lt;command_name\u0026gt;:p To search for a command in your history and run it, type !command_name. For example, if I want to run the last echo command and to do that:\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ history | tail -n 5 159 echo $HISTSIZE 160 history | tail -n5 161 echo $HISTSIZE 162 ls ~/ -a 163 history | tail -n 5 ubuntu@ip-172-31-87-26:~$ !echo echo $HISTSIZE 1000   Note that this can be also dangerous because you don\u0026rsquo;t know exactly what command was executed and there\u0026rsquo;s the risk of executing another command. What you can do is append :p behind the \u0026lt;command_name\u0026gt; and bash will print out the command to the terminal instead of running it.\n1 2  ubuntu@ip-172-31-87-26:~$ !echo:p echo $HISTSIZE   It\u0026rsquo;s just printing the command, it\u0026rsquo;s not running it. And if the command is correct, You can call it by just pressing up-arrow, because Linux has written the matched command into the history:\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:/$ cd home ubuntu@ip-172-31-87-26:/home$ cd ~ ubuntu@ip-172-31-87-26:~$ !echo:p echo $HISTSIZE ubuntu@ip-172-31-87-26:~$ history | tail -n 5 174 ls 175 cd home 176 cd ~ 177 echo $HISTSIZE 178 history | tail -n 5   reverse-i-search mode Bash also has a recall mode you can use to search for commands you\u0026rsquo;ve previously run; press control and r and then start typing a command to search your Bash history for it. If you do not want to run the matched command, you can press control and g to leave the history searching mode and clear the line.\nremove history If you want to delete a particular command you can add a -d option with the line number of that command:\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ history | tail -n 5 174 ls 175 cd home 176 cd ~ 177 echo $HISTSIZE 178 history | tail -n 5 ubuntu@ip-172-31-87-26:~$ history -d 176 ubuntu@ip-172-31-87-26:~$ history | tail -n 5 175 cd home 176 echo $HISTSIZE 177 history | tail -n 5 178 history -d 176 179 history | tail -n 5   To clear the entire content of the history file, execute history -c. The entire history will be removed:\n1 2 3  ubuntu@ip-172-31-87-26:~$ history -c ubuntu@ip-172-31-87-26:~$ history 1 history   $HISTCONTROL As we introduced last section, you can use option -d or -c to remove the commands histiory. But a deficiency of that is the removing trace will be stored by history at the same time.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ history | tail -n5 110 history 111 exit 112 history 113 ls 114 history | tail -n5 ubuntu@ip-172-31-87-26:~$ history -d 113 ubuntu@ip-172-31-87-26:~$ history | tail -n5 111 exit 112 history 113 history | tail -n5 114 history -d 113 115 history | tail -n5   The variable HISTCONTROL will help to solve this problem. This variable has three statuses : ignorespace, ignoredups and ignoreboth.\nignorespace If $HISTCONTROL=ignorespace, shell will ignore the command that begins with a space. The command will be executed normally, except being stored by history\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  ubuntu@ip-172-31-87-26:~$ HISTCONTROL=ignorespace # \u0026lt;-- Assign value to variable without symbol `$` ubuntu@ip-172-31-87-26:~$ echo $HISTCONTROL # \u0026lt;-- Print value of variable with symbol `$` ignorespace ubuntu@ip-172-31-87-26:~$ ls d1 d2 d3 d4 f1 f2 f3 f4 ubuntu@ip-172-31-87-26:~$ ls -a . .bash_logout .lesshst .sudo_as_admin_successful d3 f2 .. .bashrc .profile d1 d4 f3 .bash_history .cache .ssh d2 f1 f4 ubuntu@ip-172-31-87-26:~$ history | tail -n 5 117 $HISTCONTROL=ignorespace # \u0026lt;-- Yield error 118 HISTCONTROL=ignorespace 119 echo $HISTCONTROL 120 ls 121 history | tail -n 5   You can see that the command ls -a is not stored by history.\nignoredups If $HISTCONTROL=ignoredups, then shell will only save the repeated same commands as one command. For example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  ubuntu@ip-172-31-87-26:~$ HISTCONTROL=ignoredups ubuntu@ip-172-31-87-26:~$ echo $HISTCONTROL ignoredups ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ history | tail -n 5 122 HISTCONTROL=ignoredups 123 echo $HISTCONTROL 124 cd 125 cd 126 history | tail -n 5   Notice that if you set $HISTCONTROL=ignoredups, then shell will store the commands beginning with a space. Also Notice in shell\u0026rsquo;s perspective, cd is distinct with cd with a space in front.\nignoreboth If you want incorporate the features on ignorespace and ignoredups, you can use ignoreboth.\n.bashrc file Note that bash variables you set will only persist for the current session. After you log out or restart your system the settings will be lost. To save the settings after the system reboots, you have to add the settings to ~/.bashrc file in the user\u0026rsquo;s home directory.\nThe easiest way to do that is by running echo:\n1  ubuntu@ip-172-31-87-26:~$ echo \u0026#34;HISTCONTROL=ignoreboth\u0026#34; \u0026gt;\u0026gt; .bashrc   Then this string will be appended to the end of the file.\n Every user has his .bashrc file in his home directory, and everytime bash shell is opened, .bashrc file will be loaded.\n $HISTTIMEFORMAT history command will persist the command order index defaultly, but you can store the timestamp of the command when it was executed by setting the HISTTIMEFORMAT variable:\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ HISTTIMEFORMAT=\u0026#34;%y/%m/%d %T \u0026#34; ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ history | tail -n 5 132 20/04/18 07:09:00 cd 133 20/04/18 07:09:27 history | tail -n 5 134 20/04/18 07:10:18 HISTTIMEFORMAT=\u0026#34;%y/%m/%d %T \u0026#34; 135 20/04/18 07:10:21 cd 136 20/04/18 07:10:34 history | tail -n 5   Similarly, the variable setting will be eliminated after the log out. Thus you can append this setting into .bashrc file by echo:\n1  ubuntu@ip-172-31-87-26:~$ echo \u0026#39;HISTTIMEFORMAT=\u0026#34;%y/%m/%d %T \u0026#34;\u0026#39; \u0026gt;\u0026gt; .bashrc   Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  ########################## ## Bash History ########################## # showing the history history # removing a line (ex: 100) from the history history -d 100 # removing the entire history history -c # printing the no. of commands saved in the history file (~/.bash_history) echo $HISTFILESIZE # printing the no. of history commands saved in the memory echo $HISTSIZE # rerunning the last command from the history !! # running a specific command from the history (ex: the 20th command) !20 # running the last nth (10th) command from the history !-10 # running the last command starting with abc  !abc # printing the last command starting with abc  !abc:p # reverse searching into the history CTRL + R # recording the date and time of each command in the history HISTTIMEFORMAT=\u0026#34;%d/%m/%y %T\u0026#34; # making it persistent after reboot echo \u0026#34;HISTTIMEFORMAT=\\\u0026#34;%d/%m/%y %T\\\u0026#34;\u0026#34; \u0026gt;\u0026gt; ~/.bashrc # or echo \u0026#39;HISTTIMEFORMAT=\u0026#34;%d/%m/%y %T\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc   Root user There are two main categories of Linux users:\n non-privileged users; root. It is also referred to as the superuser or the administrator.  The root account has absolute power on Linux system. It can do anything on a Linux system, such as installing a new software or updating the existing ones, setting up the network, changing the ownership of files, managing other users accounts, or administering any service that\u0026rsquo;s running.\nOn the other hand, all other users, which are called non privileged or normal users, have no special rights on the system. They can only work in the user home directory (which is located in the /home/) and cannot perform any administrative tasks like installing new software, creating other users or running administrative commands. For example:\n1 2 3  ubuntu@ip-172-31-87-26:~$ apt install tree E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied) E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?   Because the root is so powerful, it\u0026rsquo;s not recommended to use it for ordinary tasks such as browsing the web, writing e-mails or documents. Even a simple mistake can cause problems to the entire system. So for ordinary tasks, it\u0026rsquo;s highly recommended to use a normal user account. And when root permissions are needed, you simply become root only to perform that particular administrative task. There are more approaches to to gain root access in the terminal.\nsudo su command The first way to temporarily become root in a terminal is to run: sudo su. By the way sudo stands for super user do and su stands for substitute user. It will ask for the user\u0026rsquo;s password (instead of the root password).\nIf the password is right and the user is part of a specific group which is called sudo on Ubuntu and wheel on CentOs, then a new shell with root privileges starts and the user can run there administrative commands as root. To see the user you are currently logged in run the id command. This command prints out the logged in user and the groups it belongs to. To log out, you type exit or press Ctrl + D.\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ sudo su password: root@ip-172-31-87-26:/home/ubuntu# id uid=0(root) gid=0(root) groups=0(root) root@ip-172-31-87-26:/home/ubuntu# exit exit ubuntu@ip-172-31-87-26:~$ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm),20(dialout),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),117(netdev),118(lxd)   Note that when becoming root, if you add a white space and a hyphen after su like sudo su -, then the working environment is activated. The current working directory was changed to the root home directory, i.e. /root/.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ sudo su root@ip-172-31-87-26:/home/ubuntu# pwd /home/ubuntu root@ip-172-31-87-26:/home/ubuntu# exit exit ubuntu@ip-172-31-87-26:~$ sudo su - root@ip-172-31-87-26:~# pwd /root root@ip-172-31-87-26:~# exit logout   This executed all of the root users shall initialization scripts and set all environmental variables as if the root user was logged into a fresh shell session.\n By convention on Linux, all users prompt ends in a dollar sign $, but root prompt ends in a hash #.\n sudo \u0026lt;command\u0026gt; A second way to run commands as root is without logging into the root account, but by using the sudo command and the command you want to run as root.\nIf you prefix a command with sudo, you are prompted for your password (not the root password), and if your user belongs to a specific group (sudo on Ubuntu and wheel on CentOS) the command will run with root privileges. For example:\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ mkdir /root/dir mkdir: cannot create directory /root/dir: Permission denied ubuntu@ip-172-31-87-26:~$ sudo mkdir /root/dir Password: ubuntu@ip-172-31-87-26:~$ ls /root ls: cannot open directory \u0026#39;/root\u0026#39;: Permission denied ubuntu@ip-172-31-87-26:~$ sudo ls /root dir snap t0 ubuntu@ip-172-31-87-26:~$ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm),20(dialout),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),117(netdev),118(lxd)   Notice that sudo policy caches for 5 minutes, the credentials I\u0026rsquo;ve entered in the first place, and any time I ran the sudo command the timestamp is updated to five minutes. In this example, after I ran the sudo mkdir command, I could run other commands (like sudo ls) as root without being asked for any password for exactly 5 minutes.\nBy running sudo with the -v option a user can update the cached credentials without running a command. And you can invalidate the user\u0026rsquo;s cached credentials running sudo-k, and the next time sudo is run, a passport will be required.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo -k ubuntu@ip-172-31-87-26:~$ sudo ls / Password:   su The last method of running commands as root is to temporarily become root in that terminal by running the su command and entering the root password.\nNote that on some distributions like Ubuntu based, this method is by default disabled because direct access to root is locked. Remember that you have installed Ubuntu in the first place without setting a password for the root account.\nTo unlock the account on such a distribution, you simply have to set its password. Note that not having a password doesn\u0026rsquo;t mean that without it you can login as root. The command used to change a password in Linux is passwd \u0026lt;user name\u0026gt;, you will be prompted for password so that the command will run as root.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ sudo passwd root New password: Retype new password: passwd: password updated successfully    A user can change its own password by running the passwd command and root can change the password of any user by running the passwd command and the username whose password will be changed.\n And then you can temporarily login as root by running su and entering the root password( you just setted ):\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ su Password: root@ip-172-31-87-26:/home/ubuntu# id uid=0(root) gid=0(root) groups=0(root)   At the end of this section, we will briefly discuss 3 \u0026ldquo;root\u0026rdquo; in Linux: root directory, root\u0026rsquo;s home directory and root user:\n root directory: / directory root\u0026rsquo;s home directory: /root/  1 2 3  ubuntu@ip-172-31-87-26:~$ sudo su root@ip-172-31-87-26:/home/ubuntu# echo ~ /root    root user: privilege user on Linux  Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  ########################## ## Running commands as root (sudo, su) ########################## # running a command as root (only users that belong to sudo group [Ubuntu] or wheel [CentOS]) sudo command # =\u0026gt; enter the user\u0026#39;s password # becoming root temporarily in the terminal sudo su # =\u0026gt; enter the user\u0026#39;s password # setting the root password sudo passwd root # changing a user\u0026#39;s password passwd username # becoming root temporarily in the terminal su # =\u0026gt; enter the root password       sudo su sudo \u0026lt;command\u0026gt; su     password user\u0026rsquo;s password user\u0026rsquo;s password root\u0026rsquo;s password   user only users that belong to sudo group [Ubuntu] or wheel [CentOS] only users that belong to sudo group [Ubuntu] or wheel [CentOS] -    ","date":"2021-04-13T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/linux-operation-system-i-terminal/","title":"Linux Operation System I (Terminal)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\nThis blog contains a lot of LaTeX code, please use Chrome for reading quality.\n Common functions in MATLAB about sparse matrix Run\n1  \u0026gt;\u0026gt; help sparfun   or click here.\nCreate sparse matrix sparse   S = sparse(A) converts a full matrix into sparse form by squeezing out any zero elements. If a matrix contains many zeros, converting the matrix to sparse storage saves memory.\n  S = sparse(m,n) generates an m-by-n all zero sparse matrix.\n  1 2 3  \u0026gt;\u0026gt; A = eye(10000); \u0026gt;\u0026gt; SA = sparse(A); \u0026gt;\u0026gt; whos A SA    Name Size Bytes Class Attributes A 10000x10000 800000000 double SA 10000x10000 240008 double sparse sprand, sprandsym   R = sprandn(S) creates a sparse matrix that has the same sparsity pattern as the matrix S, but with normally distributed random entries with mean 0 and variance 1.\n  R = sprandn(m,n,density) creates a random m-by-n sparse matrix with approximately density*m*n normally distributed nonzero entries for density in the interval [0,1].\n  1  \u0026gt;\u0026gt; S = sprandn(5,5,0.5);    (1,2) 1.7036 (3,2) -1.5975 (3,3) -0.8973 (4,3) -1.7799 (5,3) 0.5464 (1,4) 1.9679 (1,5) -0.9504 (2,5) -0.0473 (3,5) -0.0197 (5,5) -0.3547   R = sprandsym(S) returns a symmetric random matrix whose lower triangle and diagonal have the same structure as S. Its elements are normally distributed, with mean 0 and variance 1.\n  R = sprandsym(n,density) returns a symmetric random, n-by-n, sparse matrix with approximately density*n*n nonzeros; each entry is the sum of one or more normally distributed random samples, and (0 \u0026lt;= density \u0026lt;= 1).\n  1 2 3 4 5 6 7 8  %% S = sprandn(10,10,0.5); subplot(1,3,1); spy(S); title(\u0026#39;S\u0026#39;); SSl = sprandsym(S); subplot(1,3,2); spy(SSl); title(\u0026#39;SSl\u0026#39;); SSu = sprandsym(S\u0026#39;); subplot(1,3,3); spy(SSu); title(\u0026#39;SSu\u0026#39;);   speye   S = speye returns a sparse scalar 1.\n  S = speye(n) returns a sparse n-by-n identity matrix, with ones on the main diagonal and zeros elsewhere.\n  S = speye(n,m) returns a sparse n-by-m matrix, with ones on the main diagonal and zeros elsewhere.\n  1 2 3 4  %% speye S1 = speye(100); subplot(1,2,1); spy(S1); S2 = speye(100,150); subplot(1,2,2); spy(S2);   Display sparse matrix spy   spy(S) plots the sparsity pattern of matrix S. Nonzero values are colored while zero values are white. The plot displays the number of nonzeros in the matrix, nz = nnz(S).\n  spy(S,LineSpec) additionally specifies LineSpec to give the marker symbol and color to use in the plot.\n  1 2 3 4 5  %% spy S = bucky; subplot(1,2,1); spy(S); subplot(1,2,2); spy(S, \u0026#39;r*\u0026#39;)   full A = full(S) converts sparse matrix S to full storage organization, such that issparse(A) returns logical 0 (false).\n1 2 3 4 5 6  %% full S = speye(5); A = full(S); disp(issparse(A)) disp(A)    0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 Conjugate Gradients (CG) Algorithm Let $A (N\\times N)$ be symmetric positive definite (SPD), that is $A^T=A$ and all eigenvalues is positive, or say $x^TAx\u0026gt;0$ for any $x\\neq \\bf 0$. We can construct a general SPD matrix quickly in MATLAB by\n1 2 3  \u0026gt;\u0026gt; N = 500; \u0026gt;\u0026gt; A = randn(N); \u0026gt;\u0026gt; A = A * A\u0026#39;   Let $b$ be an $N\\times 1$ vector, we want to solve a $x_$ s.t. $Ax_=b$, since $A$ is SPD $A$ is non singular, and hence $x_$ is existed and unique. Conjugate Gradients (CG) find $x_$ iteratively starting from $x_0$, $x_0$ is could be any vector, and we will use $\\bf 0$. And $x_n$ belongs to the Krylov space:\n$$ K_n=\\lang b, Ab,\\cdots,A^{n-1}b\\rang $$\nwhich is spanned by vectors $b, Ab,\\cdots,A^{n-1}b$, and any $k\\in K_n$ is a linear combination of these vectors. It is direct to see that $K_n\\subseteq \\mathbb{R}^N$, so conjugate gradient is searching in bigger and bigger space for a good approximation to the solution.\nLet define a special way to measure the size of a vector: define \u0026ldquo;$A$ - norm\u0026rdquo; of a vector $x$ as\n$$ |x|_A=\\sqrt{x^TAx} $$\nSo $|x|_A$ is a positive real number if $x\\neq \\bf 0$, because by definition, $A$ is symmetric positive definite, and therefore $x^TAx$ is a positive number.\nAnd define the error of each steps: define $n$-th error as\n$$ e_n=x_*-x_n $$\nand define $n$-residual as\n$$ r_n=Ae_n=b-Ax_n $$\nIn principle, we like a small error, but in practice, the thing we can measure is the residual. Thus we minimize the residual as a mean of minimizing.\nTHM 1. Among all $x\\in K_n$, there exists one unique $x_n$ such that minimizes $|e_n|_A$.\nSo at every step of conjugate gradient, we can find the best approximation in a certain measure. And from this theorem, we obtain a corollary: $|e_0|_A\\geq |e_1|_A\\geq \\cdots \\geq 0$. That is the error keep going down monotonically.\nDefine $f(x)=\\frac{1}{2}x^{T}Ax-x^{T}b$, thus $\\nabla f(x)=Ax-b$, now we want $Ax=b$, which means we want $\\nabla f(x)=0$, i.e minimize of $f(x)$.\nNow we can introduce the conjugate gradient algorithm. First, we start form an initial guess $x_0=\\bf 0$, corresponding to this is a residual $r_0=b-Ax_0=b-A\\mathbf{0}=b$. And then we define search direction vector $p_0=r_0$ which indicate the search direction in Krylov space.\nFor $n=1, 2, 3,\\cdots$:\n $\\alpha_n = {r_{n-1}^{T}r_{n-1}}/{p_{n-1}^{T} Ap_{n-1}}$ $x_n = x_{n-1} + \\alpha_{n}p_{n-1}$ $r_n = r_{n-1} - \\alpha_n Ap_{n-1}$ $\\beta_n = {r^{T}nr_n}/{r{n-1}^{T}r_{n-1}}$ $p_n=r_n+\\beta_n p_{n-1}$  Here for each step $n$, $\\alpha_n$ which is a scalar, represents the step length, it is how far we are going along a search direction to improve our current guess. $x_n$ is the approximate solution at step $n$, and $r_n$ is the corresponding residual. And $\\beta_n$ measures the improvement in this step. And finally, $p_n$ is the search direction.\nAlthough MATLAB has CG function pcg(), we still achieve this algorithm by:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  % cg.m - Simple CG iteration for 10-digit accuracy. % Remember, A must be symmetric positive definite! function[x, errnorm, ii] =cg(A,b, maxit)N = length(A); x = zeros(N,1); errnorm = zeros(maxit,0); r = b; p = r; ii = 1; while norm(r) \u0026gt; 1e-10 Ap = A*p; rr = r\u0026#39;*r; a = rr/(p\u0026#39;*Ap); % step length x = x + a*p; % approximate solution r = r - a*Ap; % residual c = r\u0026#39;*r/rr; % improvement this step p = r + c*p; % search direction errnorm(ii) = norm(r); ii = ii + 1; if ii \u0026gt; maxit break end end   Convergence of CG Note that\n$$ e_0=x_-x_0=x_ $$\nand\n$$ e_n=x_*-x_n=e_0-x_n $$\nand since $b=Ax_*= Ae_0$ we have that\n$$ x_n\\in K_n=\\lang b,Ab,\\cdots,A^{n-1}b\\rang=\\lang Ae_0, A^2e_0,\\cdots, A^n e_0\\rang $$\nAnd hence $x_n$ is the linear combination of $Ae_0, A^2e_0,\\cdots, A^n e_0$ and can be interpreted as a polynomial of $A$. So in conjugate gradient, we implicitly dealing with n-degree polynomial of $A$ timing a constant $e_0$.\nThus $e_n=e_0-x_n$ where $x_n$ is a polynomial of $A$ can be rewritten as\n$$ e_n = e_0P_n(A) $$\nwhere $P_n$ is a polynomial of (at most) degree $n$ with the property $p_n(0)=1$. The THM 1. tells us that $|P_n(A)e_0|$ is the minimal among polynomials $P$ of degree $n$ with $P(0)=1$.\nSince $A$ is a symmetric positive defined matrix thus it has real valued, orthogonal eigenvectors and hence has an orthogonal basis. Let $A$ has positive eigenvalues $\\lambda_1,\\cdots, \\lambda_n$ and eigenvectors $v_1,\\cdots, v_n$, then $e_0=\\sum_{k=1}^{N}a_kv_k$ and\n$$ e_n = P_n(A)e_0=P_n(A)\\sum_{k=1}^{N}a_kv_k=\\sum_{k=1}^{N}a_kP_n(\\lambda_k)v_k $$\nHence if there exists polynomials that are small at eigenvalues, then CG converges quickly. Thus the whole logic of the argument is that hte existence of the polynomial. Note that the $P_n(0)\\equiv 0$, thus if some $\\lambda$ are too close to 0, then the polynomial would be non exist which means CG will not convergent, there is a theorem shows this fact:\nTHM 2. The ratio of the error at the end step to the initial error is bounded by the minimal overall polynomials with the restriction of maximal of the eigenvalues of the matrix:\n$$ \\frac{|e_n|A}{|e_0|A}\\leq \\min{P_n}\\max{\\lambda_k} | P_n(\\lambda_k)|. $$\nwhere $P_n(0)=1$.\nThe condition number of an SPD matrix is\n$$ \\kappa(A)=\\lambda_{\\max}(A)/\\lambda_{\\min}(A) $$\nand then we have a famous corollary:\n$$ \\frac{\\left|e_{n}\\right|{A}}{\\left|e{0}\\right|_{A}} \\leq 2\\left(\\frac{\\sqrt{\\kappa(A)}-1}{\\sqrt{\\kappa(A)}+1}\\right)^{n} $$\nThe right-hand side is approximately\n$$ 2\\exp{\\left(\\frac{-2n}{\\sqrt{\\kappa(A)}}\\right)} $$\nwhere $\\exp(-2)\\approx 0.14$ thus $|e_n|_A/|e_0|_A\\approx 2\\times 0.14^{\\frac{n}{\\sqrt{\\kappa(A)}}}$, which implies the errors rate will reduce to 0.14 times scale each $\\sqrt{\\kappa(A)}$ steps, thus if $A$ has a very small and a very large eigenvalues, that is it has a large condition number $\\kappa(A)$, then the CG on it converges very slowly. For example, we now create a SPD matrix which has a large condition number (b.t.w. we can compute condition number by cond() function):\n1 2 3 4  \u0026gt;\u0026gt; A = randn(500); A = A * A\u0026#39;; \u0026gt;\u0026gt; e = eig(A); \u0026gt;\u0026gt; lmin = min(e); lmax = max(e); \u0026gt;\u0026gt; disp([lmax/lmin; cond(A)])    1.0e+06 * 5.7872 5.7872 the condition number of $A$ is 6 million approximately, whose square root is:\n1  \u0026gt;\u0026gt; sqrt(cond(A))   ans = 2.4057e+03 this means we need probably 2500 steps to scale the error rate $|e_n|_A/|e_0|_A$ down to 0.14 times; we can modify this matrix so that it can have a smaller condition number:\n1 2  \u0026gt;\u0026gt; B = A + 100*eye(500); \u0026gt;\u0026gt; cond(B)   ans = 20.6279 which means we can reduce the error rate 10 times each 4~5 steps on average. We can check the difference of the convergence speed between $Ax=b$ and $Bx=b$:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  %% speed of convergence A = randn(500); A = A * A\u0026#39;; B = A + 100*eye(500); b = ones(500,1); [~,ra,sa] = cg(A,b,1e7); [~,rb,sb] = cg(B,b,1e7); subplot(1,2,1); semilogy(ra, \u0026#39;-ob\u0026#39;); xlabel(\u0026#39;steps\u0026#39;); ylabel(\u0026#39;$\\|e_n\\|_A/\\|e_0\\|_A$\u0026#39;, \u0026#39;Interpreter\u0026#39;,\u0026#39;latex\u0026#39;); title(\u0026#39;Ax=b\u0026#39;); subplot(1,2,2);semilogy(rb, \u0026#39;-*r\u0026#39;); xlabel(\u0026#39;steps\u0026#39;); ylabel(\u0026#39;$\\|e_n\\|_B/\\|e_0\\|_B$\u0026#39;, \u0026#39;Interpreter\u0026#39;,\u0026#39;latex\u0026#39;); title(\u0026#39;Bx=b\u0026#39;);   Preconditioned Conjugate Gradient (PCG) Preconditioning As we can see that we can speed up solutions by shifting by a multiple of $I$. This is useless, however, for theres no way to relate the solution of\n$$ (A + cI)x = b $$\nto that of $Ax = b$.\nPreconditioning is a related but multiplicative idea. For example, a left preconditioner is a nonsingular matrix $M$ such that\n$$ M^{-1}Ax=M^{-1}b, $$\nwhich is equivalent to the origin equation $Ax=b$, it is better behaved (e.g. better conditioned) than $Ax = b$, yet $M^{1}$ can be computed fast. We dont really mean that the inverse is computed, but that systems $My = c$ are solved fast.\nIf $A$ is SPD and $M = CC^T$ for some $C$, then the symmetry can be preserved by preconditioning, since\n$$ M^{-1}Ax=M^{-1}b \\Rightarrow C^{-T}C^{-1}AC^{-T}C^Tx=C^{-T}C^{-1}b $$\n$$ \\Rightarrow [C^{-1}AC^{-T}]C^Tx=C^{-1}b $$\nwhere $C^{-1}AC^{-T}$ is still SPD, so we can use CG to solve the $C^Tx$ and then the $x$. To use precondition CG in MATLAB use pcg() function:\n  x = pcg(A,b,tol,maxit,M) specifies a\n   tolerance tol, the default tolerance is 1e-6.     maximum number of iterations maxit.     preconditioner matrix M and computes x by effectively solving the system $H^{-1}AH^{-T}y=H^{-1}b$ for $y$, where $y=H^{T}x$ and $HH^T=M$. The algorithm does not form $H$ explicitly. Using a preconditioner matrix can improve the numerical properties of the problem and the efficiency of the calculation.    x = pcg(A,b,tol,maxit,M1,M2) specifies factors of the preconditioner matrix M such that M = M1*M2.\n  [x,flag,relres,iter,resvec] = pcg(___) also returns a\n   a flag that specifies whether the algorithm successfully converged. When flag = 0, convergence was successful.     the relative residual relres norm(b-A*x)/norm(b). If flag is 0, then relres \u0026lt;= tol.     iteration number iter at which x was computed.    vector of the residual norm at each iteration resvec, including the first residual norm(b-A*x0).\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  %% clear all; close all; A = delsq(numgrid(\u0026#39;S\u0026#39;,50)); b = ones(size(A,1),1); tol = 1e-8; maxit = 1e5; L = ichol(A); [~,~,~,~,rv0] = pcg(A,b,tol,maxit); [~,~,~,~,rv1] = pcg(A,b,tol,maxit,L,L\u0026#39;); semilogy(0:length(rv0)-1,rv0/norm(b),\u0026#39;r-o\u0026#39;) hold on semilogy(0:length(rv1)-1,rv1/norm(b),\u0026#39;b-o\u0026#39;) hold off yline(tol,\u0026#39;r--\u0026#39;); xlabel(\u0026#39;Iteration number\u0026#39;) ylabel(\u0026#39;Relative residual\u0026#39;) legend(\u0026#39;No Preconditioner\u0026#39;,\u0026#39;ICHOL\u0026#39;,\u0026#39;Tolerance\u0026#39;)   Survey of preconditioners See chapter 40 of Trefethen \u0026amp; Bau for general comments. for details.\n Diagonal scaling = Jacobi Incomplete factorisation (In MATLAB see LUINC and CHOLINC) Coarse-grid approximation ( -\u0026gt; multigrid) Local approximation (omitting long-range interactions) Block preconditioners and domain decomposition Low-order discretization (e.g. approximate spectral by FEM) Constant-coefficient or symmetric approximation Splitting of multi-term operator Dimensional splitting or ADI One step of a classical iteration (Gauss-Seidel, SOR, SSOR. . . ) Periodic approximation to nonperiodic problem (Toeplitz  circulant) Unstable direct method (iteration may circumvent instability) Polynomial preconditioners Sparse approximate inverses Low-rank approximations  ","date":"2021-02-21T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/matlab-in-engineering-ix-preconditioning-conjugate-gradient/","title":"MATLAB in Engineering IX (Preconditioning Conjugate Gradient)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Matrix Decomposition Matrix decomposition functions qr() - QR decomposition [Q,R] = qr(A) performs a QR decomposition on m-by-n matrix A such that A = Q*R. The factor R is an m-by-n upper-triangular matrix, and the factor Q is an m-by-m orthogonal matrix.\nldl() - Block LDL' factorization for Hermitian indefinite matrices [L,D] = ldl(A) stores a block diagonal matrix D and a permuted lower triangular matrix in L such that A = L*D*L'. The block diagonal matrix D has 1-by-1 and 2-by-2 blocks on its diagonal. Note, this syntax is not valid for sparse A.\n[L,D,P,S] = ldl(A) returns unit lower triangular matrix L, block diagonal D, permutation matrix P, and scaling matrix S such that P'*S*A*S*P = L*D*L'. This syntax is only available for real sparse matrices, and only the lower triangle of A is referenced.\nlu() - LU matrix factorization [L,U] = lu(A) factorizes the full or sparse matrix A into an upper triangular matrix U and a permuted lower triangular matrix L such that A = L*U.\nThus A*x=b -\u0026gt; L*U*x=b -\u0026gt; x = U^{-1}*L^{-1}*b:\n1 2 3 4 5 6 7 8 9  %% LU-F clc; clear; A = [1 2 1; 2 6 1; 1 1 4]; b = [2; 7; 3]; [L, U] = lu(A); % L*U=A x = inv(U)*inv(L)*b; disp(x)   -3.0000 2.0000 1.0000 chol() - Cholesky factorization R = chol(A) factorizes symmetric positive definite matrix A into an upper triangular R that satisfies A = R'*R.\nIf A is nonsymmetric , use R = chol(A,triangle) to specifies which triangular factor of A to use in computing the factorization. For example, if triangle is \u0026lsquo;lower\u0026rsquo;, then chol uses only the diagonal and lower triangular portion of A to produce a lower triangular matrix R that satisfies A = R*R'. The default value of triangle is \u0026lsquo;upper\u0026rsquo;.\nsvd() - Singular value decomposition [U,S,V] = svd(A) performs a singular value decomposition of an mxn matrix A, such that A = U*S*V'.\nHere U is an mxm matrix, S is an mxn matrix with all zeros except for the elements on the main diagonal, and each element on the main diagonal is a singular value, and V is an nxn matrix. Both U and V are unitary matrices, i.e., they satisfy U'U=I, V'V=I\n1 2 3  A = magic([3,5]); [U, S, V ] = svd(A); disp(U); disp(S); disp(V)    -0.5774 0.7071 0.4082 -0.5774 0.0000 -0.8165 -0.5774 -0.7071 0.4082 15.0000 0 0 0 6.9282 0 0 0 3.4641 -0.5774 0.4082 0.7071 -0.5774 -0.8165 -0.0000 -0.5774 0.4082 -0.7071 and A=U*S*V':\n1  \u0026gt;\u0026gt; disp(A); disp(U*S*V\u0026#39;)   8 1 6 3 5 7 4 9 2 8.0000 1.0000 6.0000 3.0000 5.0000 7.0000 4.0000 9.0000 2.0000 and U*U'=I, V*V'=I:\n1  \u0026gt;\u0026gt; disp(U*U\u0026#39;); disp(V*V\u0026#39;)    1.0000 0 -0.0000 0 1.0000 -0.0000 -0.0000 -0.0000 1.0000 1.0000 -0.0000 -0.0000 -0.0000 1.0000 0.0000 -0.0000 0.0000 1.0000 gsvd() - Generalized singular value decomposition [U,V,X,C,S] = gsvd(A,B) performs a generalized singular value decomposition of matrices A and B, and returns unitary matrices U and V, a matrix X, and nonnegative diagonal matrices C and S such that\n1 2 3  A = U*C*X\u0026#39; B = V*S*X\u0026#39; C\u0026#39;*C + S\u0026#39;*S = I   Linear Eqs and System rref() - Gaussian elimination method R = rref(A) returns the reduced row echelon form of A using Gauss-Jordan elimination. For example, solve the linear equation:\n$$ \\begin{array}{r} x+2 y+z=2 \\ 2 x+6 y+z=7 \\ x+y+4 z=3 \\end{array} $$\n1 2 3 4 5 6 7 8  %% G-E clc; clear; A = [1 2 1; 2 6 1; 1 1 4]; b = [2; 7; 3]; M = [A b]; R = rref(M); disp(R)   1 0 0 -3 0 1 0 2 0 0 1 1 lu() - LU matrix factorization $Ax=b \\Rightarrow LUx=b \\Rightarrow x = U^{-1}L^{-1}b$:\n1 2 3 4 5 6 7 8 9  %% LU-F clc; clear; A = [1 2 1; 2 6 1; 1 1 4]; b = [2; 7; 3]; [L, U] = lu(A); % L*U=A x = inv(U)*inv(L)*b; disp(x)   -3.0000 2.0000 1.0000 \\, mldivide() x = A\\B solves the system of linear equations A*x = B. The matrices A and B must have the same number of rows.\n  If A is a scalar, then A\\B is equivalent to A.\\B.\n  If A is a square nxn matrix and B is a matrix with n rows, then x = A\\B is a solution to the equation A*x = B, if it exists.\n  replace inv(A)*b with A\\b; replace b*inv(A) with b/A\n   MATLAB displays a warning message if A is badly scaled or nearly singular, but performs the calculation regardless.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  %% Left Div clc; clear; close; A = [1 2 1; 2 6 1; 1 1 4]; b = [2; 7; 3]; x = A\\b; disp(x) z1 = @(x,y) -x-2*y; z2 = @(x,y) -2*x-6*y; z3 = @(x,y) -(x+y)/4; itv = [-5,5]; hold on fsurf(z1,itv,\u0026#39;r\u0026#39;,\u0026#39;FaceAlpha\u0026#39;,0.5,\u0026#39;EdgeColor\u0026#39;,\u0026#39;none\u0026#39;); fsurf(z2,itv,\u0026#39;g\u0026#39;,\u0026#39;FaceAlpha\u0026#39;,0.5,\u0026#39;EdgeColor\u0026#39;,\u0026#39;none\u0026#39;); fsurf(z3,itv,\u0026#39;b\u0026#39;,\u0026#39;FaceAlpha\u0026#39;,0.5,\u0026#39;EdgeColor\u0026#39;,\u0026#39;none\u0026#39;); hold off   -3.0000 2.0000 1.0000 Matrix condition number Matrix norm In a geometric sense, a vector represents a specific point, while a matrix is used to transform the coordinates of a point. That is, vectors are entities and matrices are transformations of entities. The length of an entity is norm, and correspondingly, the ability of a matrix to transform an entity is also norm. This ability is reflected precisely in the degree of scaling of the length of a vector before and after its transformation; the larger the matrix norm, the more likely it is that a vector will be stretched longer. This concept is expressed in the following equation\n$$ |A|=\\max _{x} \\frac{|A x|}{|x|} $$\nThe maximum value of the ratio of the norm length of the vector $x$ before and after the transformation of the matrix $A$ is defined as the norm of $A$. Now, we calculate the norm of $A^{-1}$ based on the above definition:\n$$ \\left|A^{-1}\\right|=\\max _{y} \\frac{\\left|A^{-1} y\\right|}{|y|}=1 / \\min _{y} \\frac{|y|}{\\left|A^{-1} y\\right|}= 1/\\min _{x} \\frac{|Ax|}{|x|} $$\nwhere the third equation performs a variable substitution $Ax=y$ (or $x=A^{-1}y$) Thus, we find a correlation between the norm of the two matrices of the reciprocal inverse. The ability of $A$ to stretch the vector is equivalent to the ability of $A^{-1}$ to compress the vector.\nCondition number Define\n$$ \\kappa(A)=|A|\\left|A^{-1}\\right| $$\nis the condition number of the matrix $A$. In combination with the geometric meaning mentioned above, the condition number describes both the stretching and compression ability of the matrix $A$. The larger the condition number, the more the vector is likely to change after the transformation.\nStability of linear eq Considering a linear equation $Ax=b$, how do we measure its stability? In this equation, $A$ is the coefficient matrix, $x$ is the solutions to be solved, $b$ is generally the observed value.\nThe equation is said to be stable if small changes in the observed values do not cause dramatic changes in the solution of the equation. In other words, stability measures the resistance to noise of a linear equation. Adding noise $\\delta_b$ to the right-hand side of the equation , the solution of the equation then becomes $x+\\delta_x$, and the equation is expressed as follows\n$$ A(x+\\delta_x) = b+\\delta_b $$\nSubstituting $Ax = b$,\n$$ A\\delta_x=\\delta_b $$\nTake the norm of both sides.\n$$ \\left|A\\delta_x\\right|=\\left|\\delta_b\\right| $$\nCombine with the definition of the norm we have that $\\left|A\\right|\\geq \\left|A\\delta_x\\right|/\\left|\\delta_x\\right|$ , thus\n$$ |A||\\delta_x| \\geq|A \\delta_x|=|\\delta_b| $$\nOn the other hand, for any $x$, we have\n$$ \\left|A^{-1}\\right|\\geq \\frac{1}{\\left|Ax\\right|/\\left|x\\right|}=\\frac{\\left|x\\right|}{\\left|Ax\\right|}=\\frac{\\left|x\\right|}{\\left|b\\right|} $$\nThus\n$$ \\left|A^{-1}\\right|\\left|b\\right|\\geq \\left|x\\right| $$\nMultiply with $|A||\\delta_x| \\geq|\\delta_b|$ , we have\n$$ \\left|A\\right|\\left|A^{-1}\\right|\\left|b\\right|\\left|\\delta_x\\right|\\geq \\left|\\delta_{b}\\right|\\left|x\\right| $$\nThat is\n$$ \\frac{1}{\\kappa(A)} \\frac{|\\delta b|}{|b|} \\leq \\frac{|\\delta x|}{|x|} $$\nThis equation specifies a lower bound on the solution $x$ of the linear equation as affected by the observation $b$. Similarly, we have the upper bound :\n$$ \\frac{|\\delta x|}{|x|} \\leq \\kappa(A) \\frac{|\\delta b|}{|b|} $$\nThus collectively\n$$ \\frac{1}{\\kappa(A)} \\frac{|\\delta b|}{|b|} \\leq \\frac{|\\delta x|}{|x|} \\leq \\kappa(A) \\frac{|\\delta b|}{|b|} $$\nWe can draw some intuitive conclusions:\nFirst, the condition number of matrix $A$ only determines the degree to which the solution $x$ of the linear equation is affected by the noise of the observed value $b$. The larger the condition number is, the more severely $x$ is affected by the noise, which means that the rate of change of $x$ deviates from the rate of change of $b$ (note that it is not only the rate of change of $x$ that is much higher than the rate of change of $b$ that is the deviation. If the rate of change of $x$ is too small, it is also called deviation. For example, no matter how much $b$ changes, $x$ stays the same.\nSecondly, the stability of conditional number is not affected by scale, and any scaling of the whole will not affect the result of the inequality, because the degree of noise influence is measured by the rate of change, and has nothing to do with the absolute size.\ncond() C = cond(A,p) returns the p-norm condition number, where p can be 1, 2, Inf, or \u0026lsquo;fro\u0026rsquo;. (p=2 by default)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  %% cond N=100; M = 5; lb = zeros(1,N); ub = zeros(1,N); rx = zeros(1,N); dbs = zeros(1,N); A = magic(M); b = rand(M,1); x = A\\b; k = cond(A); nb = norm(b); nx = norm(x); j = 1; for i=linspace(0,5,N) db = i.*b; dx = A\\(b+db)-x; ndb = norm(db); ndx = norm(dx); rx(j) = ndx/nx; ub(j) = k*(ndb/nb); lb(j) = (ndb/nb)/k; dbs(j) = ndb/nb; j = j + 1; end plot(dbs, rx, \u0026#39;r\u0026#39;,\u0026#39;LineWidth\u0026#39;, 2); hold on plot(dbs, ub, \u0026#39;g\u0026#39;,\u0026#39;LineWidth\u0026#39;, 2); plot(dbs, lb, \u0026#39;b\u0026#39;,\u0026#39;LineWidth\u0026#39;, 2); hold off legend(\u0026#39;ratio of change of x\u0026#39;, \u0026#39;upper bound\u0026#39;, \u0026#39;lower bound\u0026#39;); xlabel(\u0026#39;$\\|\\delta_b\\|/\\|b\\|$\u0026#39;, \u0026#39;Interpreter\u0026#39;, \u0026#39;latex\u0026#39;); ylabel(\u0026#39;$\\|\\delta_x\\|/\\|x\\|$\u0026#39;, \u0026#39;Interpreter\u0026#39;, \u0026#39;latex\u0026#39;);   Linear system A typical linear time-invariant system is usually formulated as\n$$ y=\\frac{d x(t)}{d t}=\\dot{x}=A x $$\nY = expm(X) computes the matrix exponential of X:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  %% A = [0 -6 -1; 6 2 -16; -5 20 -10]; x0 = [1 1 1]\u0026#39;; X = []; for t = 0:.01:1 X = [X expm(t*A)*x0]; end plot3(X(1,:),X(2,:),X(3,:),\u0026#39;-o\u0026#39;); xlabel(\u0026#39;x_1\u0026#39;); ylabel(\u0026#39;x_2\u0026#39;); zlabel(\u0026#39;x_3\u0026#39;); grid on; axis tight square;   ","date":"2021-02-15T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/matlab-in-engineering-viii-matrix-decomposition/","title":"MATLAB in Engineering VIII (Matrix Decomposition)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Symbolic Calculation Create Symbolic Variable Symbolic number Use the sym() function to create symbolic numbers. Using symbolic numbers allows you to save real numbers exactly, without errors.\nFor example:\n $\\sin(\\pi)$  1 2 3 4 5 6  %%  clc; clear; a = sin(pi); b = sin(sym(pi)); disp(a); disp(b);   1.2246e-16 0 define $\\phi = (1+\\sqrt{5})/2$, compute $\\phi^2-\\phi-1$:  1 2 3 4 5 6 7 8 9 10  %% clc; clear; sphi = (1 + sqrt(sym(5)))/2;\tnphi = (1 + sqrt(5))/2;\tsf = sphi^2 - sphi - 1;\tnf = nphi^2 - nphi - 1;\tdisp(sf); disp(nf);   (5^(1/2)/2 + 1/2)^2 - 5^(1/2)/2 - 3/2 0 compute $50!$  1 2 3 4 5 6 7 8 9  %% clc; clear; n = factorial(50); s = factorial(sym(50)); format longE; disp(n); disp(s)   3.041409320171338e+64 30414093201713378043612608166064768844377641568960512000000000000 Symbolic scalar Symbolic variables can be created using sym and syms, but the difference is:\n sym can only create one symbolic variable at a time, while syms can create multiple symbolic variables at a time.  1 2 3 4 5 6  % sym: x = sym(\u0026#39;x\u0026#39;) % or sym x % The sym command can only create one symbolic variable % syms: syms x y z % The syms command can create more than one symbolic variable    If the specified symbolic variable already exists, sym will keep its value, and syms will clear its value.  1 2 3 4 5 6 7 8  syms x y f = x+y; % implicitly creates the symbolic variable f sym f; % does not clear the value of the variable f, i.e. f = x + y g = x + y; % implicitly create the symbolic variable g syms g; % Clear the original value of the variable g, i.e. g = g disp(f); disp(g)   x + y g  Use sym to create a symbolic matrix.  1 2 3 4 5 6  %% clc; clear; A = sym(\u0026#39;a\u0026#39;, [2 2]);\t% create a 2*2 symbolic matrix disp(A); disp(inv(A)); disp(A*A); whos   [a1_1, a1_2] [a2_1, a2_2] [ a2_2/(a1_1*a2_2 - a1_2*a2_1), -a1_2/(a1_1*a2_2 - a1_2*a2_1)] [-a2_1/(a1_1*a2_2 - a1_2*a2_1), a1_1/(a1_1*a2_2 - a1_2*a2_1)] [ a1_1^2 + a1_2*a2_1, a1_1*a1_2 + a1_2*a2_2] [a1_1*a2_1 + a2_1*a2_2, a2_2^2 + a1_2*a2_1] Name Size Bytes Class Attributes A 2x2 8 sym  Using sym and syms together you can quickly create a series of variables with subscripts  1 2 3 4 5  %%  clc; clear; syms(sym(\u0026#39;a\u0026#39;, [1 5])) whos    Name Size Bytes Class Attributes a1 1x1 8 sym a2 1x1 8 sym a3 1x1 8 sym a4 1x1 8 sym a5 1x1 8 sym Symbolic function We can define the symbolic function by syms y(x):\n1 2 3 4 5 6 7 8  %% symfun clc; clear; syms z(x,y); z(x,y) = x^y + y^x; Z.z1y = z(1,y); Z.zx1 = z(x,1); Z.z11 = z(1,1); disp(Z) whos    Name Size Bytes Class Attributes x 1x1 8 sym y 1x1 8 sym z 1x1 8 symfun z1y: y + 1 zx1: x + 1 z11: 2 The different between symbolic scalar (sym) and symbol function (symfun) is\n if z is a sym, then z(a,b) will be viewed as indexing; if z is a symfun, then z(a,b) will be viewed as passing parameters;  We can utilize symfun() function to transform the sym to symfun:\n1 2 3 4 5 6 7 8  %% clc; clear; syms x y; z = x^y + y^x; zf = symfun(z, [x y]); disp(zf) whos   x^y + y^x symbolic function inputs: x, y Name Size Bytes Class Attributes x 1x1 8 sym y 1x1 8 sym z 1x1 8 sym zf 1x1 8 symfun Symbolic functions accept array inputs. Calculate z for multiple values of x and y.\n1 2 3 4 5 6 7  %% symfun syms x y z(x,y); z(x,y) = x^y + y^x; xa = 1:5; ya = 1:5; disp(z(xa, ya))   [2, 8, 54, 512, 6250] Simplification and substitution Simplification Use the simplify() function to simplify symbolic expressions. For example, simplify $e^{c\\ln(\\sqrt{a+b})}$\n1 2 3 4 5 6 7  %% clc; clear; syms a b c x y = exp(c*log(sqrt(a+b))); sy = simplify(y); disp(sy)   (a + b)^(c/2) The criteria for simplifying expressions are uncertain, and the following three functions simplify the expressions according to different criteria:\n The expand() function expands the expression  1 2 3 4 5 6 7  %% clc; clear; syms x f = (x ^2- 1)*(x^4 + x^3 + x^2 + x + 1)*(x^4 - x^3 + x^2 - x + 1); sf = expand(f); disp(sf)   x^10 - 1  The factor() function can decompose the factorization  1 2 3 4 5 6 7 8 9  %% clc; clear; syms x g = x^3 + 6*x^2 + 11*x + 6; sg = factor(g);\tdisp(sg); disp(prod(sg)); disp(expand(prod(sg)))   [x + 3, x + 2, x + 1] (x + 1)*(x + 2)*(x + 3) x^3 + 6*x^2 + 11*x + 6  The horner() function can turn polynomials into nested forms  1 2 3 4 5 6 7 8  %% clc; clear; syms x h = x^5 + x^4 + x^3 + x^2 + x; sh = horner(h);\tdisp(sh)   x*(x*(x*(x*(x + 1) + 1) + 1) + 1) Substitution To substitute the variable old in the symbolic function/scalar s with new, use subs() function. snew = subs(s,old,new) returns a copy of s, replacing all occurrences of old with new, and then evaluates s.\n Note that subs() works for sym variable as well.\n 1 2 3 4 5 6 7 8 9 10 11  %% symfun clc, clear; syms x y z(x,y); z(x,y) = x^y + y^x; disp(z); z1 = subs(z, x, x^2); disp(z1); z2 = z(x^2, y); disp(z2); whos   x^y + y^x symbolic function inputs: x, y y^(x^2) + (x^2)^y symbolic function inputs: x, y y^(x^2) + (x^2)^y Name Size Bytes Class Attributes x 1x1 8 sym y 1x1 8 sym z 1x1 8 symfun z1 1x1 8 symfun z2 1x1 8 sym As we can see, you can achieve this by two methods:\n z1 = subs(z, x, x^2): return a symbolic function (symfun) z2 = z(x^2, y): return a symbolic scalar (sym)  which means when you call z2 with parentheses, MATLAB will view it as indexing (remember z2 is 1x1 variable) instead of assigning values for the function:\n1  \u0026gt;\u0026gt; disp([z1(1,1); z2(1,1)])    2 y^(x^2) + (x^2)^y of course, you can transform z2 to symbolic function by symfun() function:\n1 2 3 4 5  %%  z3 = symfun(z2, [x, y]); disp(z3) whos z3   y^(x^2) + (x^2)^y symbolic function inputs: x, y Name Size Bytes Class Attributes z3 1x1 8 symfun Root finding Equations Once we create the symbolic variable x and y (which is a symfun has x or a sym created by x) we can call the function solve(y,x) to find the root for the equation y(x)=0:\n1 2 3 4 5 6 7  %% root 1 clc; clear; syms x; y = x*sin(x)-x; r = solve(y,x); disp(r)    0 pi/2 System of equations To solve the system eqs, just pass the multi eqs and the independent variables (they are all sym variable) into solve() function. For example, solve the system of equations:\n$$ x-2 y=5 $$\n$$ x+y=6 $$\n1 2 3 4 5 6 7  %% eq system clc; syms x y; eq1 = x -2*y -5; eq2 = x + y -6; r = solve(eq1,eq2,x,y); disp(r);   x: 17/3 y: 1/3 here r is a struct variable:\n1  \u0026gt;\u0026gt; whos r; disp(r.x); disp(r.y);    Name Size Bytes Class Attributes r 1x1 352 struct 17/3 1/3 Equations with arguments To obtain the root formula of the general equation, we can define the arguments as sym variable as well, and specify the target variables need to be solved in solve() function. For example to get the root formula of $ax^2+bx+c=0$:\n1 2 3 4 5 6 7 8 9 10 11  %% arguments clc; clear; syms x a b c; eq = a*x^2 + b*x +c; rx = solve(eq, x); ra = solve(eq, a); rb = solve(eq, b); rc = solve(eq, c); R.x = rx; R.a = ra; R.b = rb; R.c = rc; disp(R); disp(R.x);   x: [21 sym] a: -(c + b*x)/x^2 b: -(a*x^2 + c)/x c: - a*x^2 - b*x -(b + (b^2 - 4*a*c)^(1/2))/(2*a) -(b - (b^2 - 4*a*c)^(1/2))/(2*a) Find the matrix inverse using symbolic approach:\n1 2 3 4 5 6 7  %% inv clc; clear; syms a b c d A=[a b; c d]; B=inv(A); disp(B)   [ d/(a*d - b*c), -b/(a*d - b*c)] [-c/(a*d - b*c), a/(a*d - b*c)] Limit Use the limit(expr, var, a) function to find the limit of the symbolic expression expr as the variable var converges to a. Add the parameter \u0026lsquo;left\u0026rsquo; or \u0026lsquo;right\u0026rsquo; to specify the left or right limit.\n1 2 3 4 5 6 7 8 9 10 11  %% clc, clear; syms x; expr = 1/x; a = limit(expr,x,0);\tb = limit(expr,x,0,\u0026#39;left\u0026#39;);\tc = limit(expr,x,0,\u0026#39;right\u0026#39;);\tdisp([a b c])   [NaN, -Inf, Inf] Differentiation Use the diff(expr, var, n) function to derive the n-th order differentiation of the symbolic expression expr with respect to the variable var.\nIf var and n is not specified, diff() will compute the first order differentiation w.r.t. the first defined symbolic variable, in this case it is x. (This feature is the same as solve() function.)\n1 2 3 4 5 6 7  %% diff clc; clear; syms x m n y = x^n + m^x; yp = diff(y); disp(yp)   n*x^(n - 1) + m^x*log(m) To solve the partial derivative:\n1 2 3 4 5 6 7 8 9 10 11  %% partial derivative clc; clear; syms x y g = x^2*y + y^2/x; gx = diff(g, x); gy = diff(g, y); gxx = diff(g, x, x); G.gx = gx; G.gy = gy; G.gxx = gxx; disp(G);   gx: 2*x*y - y^2/x^2 gy: (2*y)/x + x^2 gxx: 2*y + (2*y^2)/x^3 Integration Indefinite integral Use the int(expr, var) function to find the indefinite integral of the symbolic expression expr over the variable var.\nIf expr is a symbolic function, then int() function will return a symbolic function, if expr is a symbolic scalar then int() will return symbolic scalar.\n1 2 3 4 5 6 7 8 9 10 11  %% int clc; clear; syms x y z(x,y); z(x,y) = x^2 * y; szdx = int(z, x); szdy = int(z, y); szdxdy = int(szdx, y); szdydx = int(szdy, x); disp([szdx; szdy; szdxdy; szdydx])    (x^3*y)/3 (x^2*y^2)/2 (x^3*y^2)/6 (x^3*y^2)/6 symbolic function inputs: x, y Name Size Bytes Class Attributes szdx 1x1 8 symfun szdxdy 1x1 8 symfun szdy 1x1 8 symfun szdydx 1x1 8 symfun x 1x1 8 sym y 1x1 8 sym z 1x1 8 symfun As we can see that szdydx equals to szdxdxy since z(x,y) is continuous. To specify the initial value of z, for example, as z(0,0)=1, we can achieve that by 2 methods:\n1 2 3 4 5 6 7 8 9 10  %% int clc; clear; syms x y z(x,y); z(x,y) = x^2 * y; szdx = int(z, x); szdxdy = int(szdx, y); szdxdy = szdxdy - (szdxdy(0,0) - 1); disp(szdxdy)   (x^3*y^2)/6 + 1 symbolic function inputs: x, y or utilize subs() function:\n1 2 3 4 5 6 7 8 9 10 11  %% int clc; clear; syms x y z(x,y); z(x,y) = x^2 * y; szdx = int(z, x); szdxdy = int(szdx, y); szdxdy = szdxdy - (subs(szdxdy, x, 0) - 1); szdxdy = szdxdy - (subs(szdxdy, y, 0) - 1); disp(szdxdy)   (x^3*y^2)/6 + 1 symbolic function inputs: x, y Definite integral Using the int(expr, var, [a, b]) function you can specify upper and lower bounds for the definite integral, a and b can be symbolic expressions.\nFor example, calculate:\n$$ \\int_{0}^{10} \\frac{x^{2}-x+1}{x+3} d x $$\nWe can make it in 3 methods:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  %% int clc; clear; syms x; y1 = (x^2-x+1)/(x+3); z1 = int(y1,x); s1 = subs(z1,x,10)-subs(z1,x,0); y2 = symfun(y1, x); z2 = int(y2,x); s2 = z2(10) - z2(0); s3 = int(y1,x, [0, 10]); disp([s1; s2; s3]) whos    13*log(13) - 13*log(3) + 10 13*log(13) - 13*log(3) + 10 log(302875106592253/1594323) + 10 Name Size Bytes Class Attributes s1 1x1 8 sym s2 1x1 8 sym s3 1x1 8 sym x 1x1 8 sym y1 1x1 8 sym y2 1x1 8 symfun z1 1x1 8 sym z2 1x1 8 symfun Series Sum of series Use symsum(expr, k, [a b]) to compute the sum of the index k of the series expr from a to b.\n1 2 3 4 5 6 7 8 9 10  %% clc; clear; syms k x a = symsum(k^2, k);\tb = symsum(1/(2^k), k, [0 Inf]);\tc = symsum(x^k/factorial(k),k,1,Inf); % power series; disp([a;b;c])   k^3/3 - k^2/2 + k/6 2 exp(x) - 1 Taylor series Use taylor(expr,var,a) to compute the Taylor series of the expression expr at var=a. (a=0 by default)\n1 2 3 4 5 6 7 8 9 10  %% clc; clear; syms x a = taylor(exp(x));\tb = taylor(sin(x));\tc = taylor(cos(x), x ,1); disp([a;b;c])   x^5/120 + x^4/24 + x^3/6 + x^2/2 + x + 1 x^5/120 - x^3/6 + x cos(1) + (sin(1)*(x - 1)^3)/6 - (sin(1)*(x - 1)^5)/120 - sin(1)*(x - 1) - (cos(1)*(x - 1)^2)/2 + (cos(1)*(x - 1)^4)/24 The default truncation order is 6. Use Order to control the truncation order. For example, approximate the sin(x)/x up to the orders 7 and 9.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  %% syms x f = sin(x)/x; T6 = taylor(f,x); T8 = taylor(f,x,\u0026#39;Order\u0026#39;,8); T10 = taylor(f,x,\u0026#39;Order\u0026#39;,10); fplot([T6 T8 T10 f], \u0026#39;LineWidth\u0026#39;, 1) xlim([-4 4]) grid on legend(\u0026#39;approximation of sin(x)/x with error O(x^6)\u0026#39;, ... \u0026#39;approximation of sin(x)/x with error O(x^8)\u0026#39;, ... \u0026#39;approximation of sin(x)/x with error O(x^{10})\u0026#39;, ... \u0026#39;sin(x)/x\u0026#39;,\u0026#39;Location\u0026#39;,\u0026#39;Best\u0026#39;) title(\u0026#39;Taylor Series Expansion\u0026#39;)   Numeric root solvers fsolve() fsolve(f, a) is a numeric root solver, where f is a function handle, a is an initial guess (doc). For example, solve this equation: $f(x) = 1.2x+0.3+x\\cdot \\sin(x)=0$:\n1 2 3 4 5 6  %% fsolve clc; clear; f = @(x) 1.2*x + 0.3 + x.*sin(x); r = fsolve(f, 0); disp(r)   Equation solved. fsolve completed because the vector of function values is near zero as measured by the value of the function tolerance, and the problem appears regular as measured by the gradient. \u0026lt;stopping criteria details\u0026gt; -0.3500 To solve the system of equations:\n$$ e^{-e^{-\\left(x_{1}+x_{2}\\right)}}-x_{2}\\left(1+x_{1}^{2}\\right)=0 $$\n$$ x_{1} \\cos \\left(x_{2}\\right)+x_{2} \\sin \\left(x_{1}\\right)-\\frac{1}{2}=0 $$\nwith initial guess $x=0, y=0$, we need see the function as an array f of functions, and variable x as an array of variables:\n1 2 3 4 5 6 7 8 9  %% sys of eqs clc; clear; f = @(x) [exp(-exp(-(x(1)+x(2)))) - x(2)*(1+x(1)^2);... x(1)*cos(x(2)) + x(2)*sin(x(1)) - 0.5]; r = fsolve(f,[0 0]); disp(r);   Equation solved. fsolve completed because the vector of function values is near zero as measured by the value of the function tolerance, and the problem appears regular as measured by the gradient. \u0026lt;stopping criteria details\u0026gt; 0.3532 0.6061 or write it as a function .m file:\n1 2 3 4  functionF =root2d(x)F(1) = exp(-exp(-(x(1)+x(2)))) - x(2)*(1+x(1)^2); F(2) = x(1)*cos(x(2)) + x(2)*sin(x(1)) - 0.5;   and call fsolve() in the terminal:\n1  \u0026gt;\u0026gt; fsolve(@root2d, [0 0])   Equation solved. fsolve completed because the vector of function values is near zero as measured by the value of the function tolerance, and the problem appears regular as measured by the gradient. \u0026lt;stopping criteria details\u0026gt; ans = 0.3532 0.6061  fzero() is another numeric root solver, it find the zero if and only if the function crosses the x-axis.\n We can also specify options for fsolve() (or fzero()) like\n1  options=optimset(\u0026#39;MaxIter\u0026#39;,1e3,\u0026#39;TolFun\u0026#39;,1e-10);   here MaxIter and TolFun indicate the number of iterations and tolerance respectively.\n1 2 3 4 5 6 7 8  %% options clc; clear; f = @(x) x.^2-1; ops = optimset(\u0026#39;MaxIter\u0026#39;, 1e4, \u0026#39;TolFun\u0026#39;, 1e-10); r1 = fsolve(f, 0.1, ops); r2 = fzero(f, 0.1, ops); disp(r1); disp(r2);    Equation solved. fsolve completed because the vector of function values is near zero as measured by the value of the function tolerance, and the problem appears regular as measured by the gradient. \u0026lt;stopping criteria details\u0026gt; 1 1 roots() To find the roots of polynomial, we can use roots(p), where p is the array of coefficients of the polynomial.\n1 2 3 4 5 6 7 8 9 10 11 12  %% roots clc; clear; p = [3 -6 -12 81]; r = roots(p); disp(r) % display x = -5:.1:3; y = polyval(p,x); plot(x,y,\u0026#39;LineWidth\u0026#39;,2);    2.4092 + 1.9431i 2.4092 - 1.9431i -2.8184 + 0.0000i Bracketing methods Here we introduce two famous roots finding algorithms: Bracketing methods amd Newton-Raphson Method. Roots are found iteratively until some criteria are satisfied:\n Accuracy Number of iteration  Bracketing methods (e.g., bisection method) start with an interval [l,r] that contains the root, it requires f(x) is continuous. its pseudo code is like\n1 2 3 4 5 6 7 8 9  L = [l, r] for i=1:n % \u0026lt;-- Number of iteration m = (L(1)+L(2))/2 if f(l)f(m) \u0026lt; 0: L = [l, m] if f(m)f(r) \u0026lt; 0: L = [m, r] if len(L) \u0026lt; h: % \u0026lt;-- Accuracy break   Newton-Raphson Method Newton-Raphson Method requires f(x) is continuous and has derivative. Its pseudo code is like\n1 2 3 4  for n=1:N % \u0026lt;-- Number of iteration x_{n+1} = x_{n} - f(x_n)/f\u0026#39;(x_n) if |f(x_{n+1})| \u0026lt; h: % \u0026lt;-- Accuracy break   ","date":"2021-02-09T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/matlab-in-engineering-vii-symbolic-calculation-root-finding/","title":"MATLAB in Engineering VII (Symbolic Calculation \u0026 Root Finding)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Numerical operations on polynomials Polynomials and it\u0026rsquo;s values In MATLAB, a polynomial can be represented as a vector, where the elements of the vector are the coefficients of the polynomial (in descending power order): the first is the coefficient of the highest term of the polynomial, and the last is the constant term.\nFor example, the polynomial:\n$$ f ( x ) = x^3 - 2x - 5 $$\ncan be represented by the vector p = [1 0 -2 -5]. Furthermore, The coefficients of the polynomial product can be calculated by the conv() function. For example:\n$$ (x^2+1)(x^2-1)=(x^4-1) $$\nand\n1 2  \u0026gt;\u0026gt; p1 = [1 0 1]; p2 = [1 0 -1]; \u0026gt;\u0026gt; conv(p1, p2)   ans = 1 0 0 0 -1 After defining the coefficients p, we can use polyval(p, x) to compute the value of the polynomial at each point of x.\n1 2 3 4 5 6 7 8 9 10  %% representation a = [9, -5, 3, 7]; x = -2:.01:5; f = polyval(a,x); plot(x,f,\u0026#39;LineWidth\u0026#39;, 2); xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;) title(\u0026#39;$9x^3-5x^2+3x+7$\u0026#39;, \u0026#39;Interpreter\u0026#39;,\u0026#39;latex\u0026#39;);   Polynomials differentiation: polyder() The polyder(p) function can be used to calculate the derivative of a polynomial. For example, the derivative of polynomial $f(x)=9x^3-5x^2+3x+7$ is $f'(x)=27x^2-10x+3$:\n1 2  \u0026gt;\u0026gt; a = [9, -5, 3, 7]; \u0026gt;\u0026gt; polyder(a)   ans = 27 -10 3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  %% poly diff a = [9, -5, 3, 7]; b = polyder(a); x = -2:.01:5; f = polyval(a,x); g = polyval(b,x); plot(x,f, \u0026#39;--r\u0026#39;,... x,g, \u0026#39;b\u0026#39;, \u0026#39;LineWidth\u0026#39;, 2); xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); legend(\u0026#39;$f(x)=9x^3-5x^2+3x+7$\u0026#39;,... \u0026#34;$f\u0026#39;(x)=27x^2-10x+3$\u0026#34;,... \u0026#39;Interpreter\u0026#39;,\u0026#39;latex\u0026#39;);   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  %% poly product a1 = [20 -7 5 10]; a2 = [4 12 -3]; a = conv(a1, a2); b = polyder(a); x = -2:.01:1; plot(x,polyval(a,x), \u0026#39;--b\u0026#39;,... x,polyval(b,x), \u0026#39;r\u0026#39;, ... \u0026#39;LineWidth\u0026#39;,2); xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); legend(\u0026#39;f(x)\u0026#39;, \u0026#34;f\u0026#39;(x)\u0026#34;);   Polynomials integration: polyint() Use the polyint(p, k) function to calculate the original function of a polynomial p. The constant term of the integral result is set to k.\nFor example, to the original function, whose constant term is 3, of the polynomial $5x^42x^2+1$ is $x^5-2/3x^3 +x +3$ :\n1 2  \u0026gt;\u0026gt; p = [5 0 -2 0 1]; \u0026gt;\u0026gt; polyint(p, 3)   ans = 1.0000 0 -0.6667 0 1.0000 3 1 2 3 4 5 6 7 8 9 10 11 12 13 14  %% poly int a = [5 3 -2 2 1]; c = polyint(a, 3); % 3 specify the constant x= -2:.01:1; plot(x,polyval(a,x), \u0026#39;--b\u0026#39;,... x,polyval(c,x), \u0026#39;r\u0026#39;, ... \u0026#39;LineWidth\u0026#39;,2); xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); legend(\u0026#39;$f(x)$\u0026#39;, \u0026#34;$\\int f(x) dx$\u0026#34;,... \u0026#39;Interpreter\u0026#39;, \u0026#39;latex\u0026#39;);   Numerical operations on functions Numerical Differentiation Differentiation: diff() Use diff(X, n) to compute the n-th (n=1 default) order difference of the vector X. If X is 2 dimension array, the operation of diff() is row-wise.\n1 2  \u0026gt;\u0026gt; X = [1:5;1:2:10]; \u0026gt;\u0026gt; X   X = 1 2 3 4 5 1 3 5 7 9 1  \u0026gt;\u0026gt; diff(X)   ans = 0 1 2 3 4 Notice that the length of the corresponding dimension of X decreases by 1 each time you take a first order difference.\n1 2  \u0026gt;\u0026gt; y = @(x) x.^2; \u0026gt;\u0026gt; x = y(1:5)   x = 1 4 9 16 25 1  \u0026gt;\u0026gt; diff(x)   ans = 3 5 7 9 Derivative: diff(y)./diff(x) Use the definition of derivative\n$$ f^{\\prime}\\left(x_{0}\\right)=\\lim {h \\rightarrow 0} \\frac{f\\left(x{0}+h\\right)-f\\left(x_{0}\\right)}{h} $$\nYou can compute the approximate derivative of a function at some point.\n1 2 3 4 5 6 7 8 9 10  %% slop: sin\u0026#39;(pi/2) x0 = pi/2; h = 0.001; x = [x0 x0+h]; y = sin(x); m1 = diff(y)./diff(x); disp(m1);   -5.0000e-04 The smaller the value of h, the more accurate the approximate derivative.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  %% slop: h clear n=1000; h = linspace(1e-5, 1e-1, n); x0 = ones(1,n)*pi/2; x1 = h + x0; x = [x0\u0026#39;, x1\u0026#39;]; y = sin(x); m = diff(y\u0026#39;)\u0026#39;./diff(x\u0026#39;)\u0026#39;; semilogx(h, m, \u0026#39;r\u0026#39;, \u0026#39;LineWidth\u0026#39;, 2); xlabel(\u0026#39;h\u0026#39;); ylabel(\u0026#34;sin\u0026#39;(pi/2) given by h\u0026#34;); set ( gca, \u0026#39;xdir\u0026#39;, \u0026#39;reverse\u0026#39; ) grid on;   To calculate the values of the derivative over an interval:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  %% diff in an interval clear h = 0.1; x = 0:h:2*pi; y = sin(x); m = diff(y)./diff(x); plot(x,y,\u0026#39;b\u0026#39;,... x(1:length(x)-1),m,\u0026#39;--r\u0026#39;,... \u0026#39;LineWidth\u0026#39;, 2); set(gca, \u0026#39;XLim\u0026#39;, [0 2*pi]); set(gca, \u0026#39;XTick\u0026#39;, 0:pi/2:2*pi); set(gca, \u0026#39;XTickLabel\u0026#39;, {\u0026#39;0\u0026#39;, \u0026#39;\\pi/2\u0026#39;, \u0026#39;\\pi\u0026#39;, \u0026#39;3\\pi/2\u0026#39;, \u0026#39;2\\pi\u0026#39;});   Second order derivative:\n1 2 3 4 5 6 7 8 9 10 11 12 13  %% second diff x = -2:.005:2; y = x.^3; m = diff (y)./ diff(x); m2 = diff(m)./ diff(x(1:end-1)); plot(x, y, \u0026#39;r\u0026#39;,... x(1:end-1), m, \u0026#39;g\u0026#39;,... x(1:end-2), m2, \u0026#39;b\u0026#39;); xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); legend(\u0026#39;f\u0026#39;, \u0026#34;f\u0026#39;\u0026#34;, \u0026#34;f\u0026#39;\u0026#39;\u0026#34;);   Numerical Integration Midpoint Rule Midpoint Rule is to solve the Riemannian sum. For Riemannian integrable functions, the Riemannian sum will converge to the integral value.\n$$ \\int_{a}^{b} f(x) d x=2 h \\sum_{i=0}^{\\frac{n}{2}-1} f\\left(x_{2 i+1}\\right) $$\n1 2 3 4 5 6 7 8  %% int h = 0.05; x = 0:h:2; mp = (x(1:end-1)+x(2:end))./2; y = 4 * mp .^ 3; s = sum(h*y); disp(s)   15.9950 Trapezoid Rule $$ \\int_{a}^{b} f(x) d x=\\frac{h}{2}\\left[f(a)+2 \\sum_{i=1}^{n-1} f\\left(x_{i}\\right)+f(b)\\right] $$\n1 2 3 4 5 6 7 8 9  %% trap h=0.05; x=0:h:2; y=4*x.^3; tp = (y(1:end-1)+y(2:end))/2; s = h*sum(tp); disp(s)   16.0100 Alternatively,\n1 2 3 4 5 6 7  %% trap h = 0.05; x = 0:h:2; y = 4*x.^3; s = h*trapz(y); disp(s);   16.0100 Simpsons Rule $$ \\int_{a}^{b} f(x) d x=\\frac{h}{3}\\left[f(a)+2 \\sum_{i=1}^{(n / 2)-1} f\\left(x_{2 i}\\right)+4 \\sum_{i=1}^{n / 2} f\\left(x_{2 i-1}\\right)+f(b)\\right] $$\n1 2 3 4 5 6 7 8 9 10  %% Simpson h = 0.05; x = 0:h:2; y = 4*x.^3; s = h/3*(y(1)+... 2*sum(y(3:2:end-2))+... 4*sum(y(2:2:end))+... y(end)); disp(s)   16 Convergence of three algorithms As we can see, when h is small enough, all three algorithms converge to the integral value: 16.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  %% trap h n=10000; hs = linspace(1e-3,1e-1, n); ss1 = zeros(1,n); ss2 = zeros(1,n); ss3 = zeros(1,n); i = 1; for h = hs x = 0:h:2; y=4*x.^3; mp = (x(1:end-1)+x(2:end))./2; ym = 4 * mp .^ 3; s1 = sum(h*ym); ss1(i) = s1; tp = (y(1:end-1)+y(2:end))/2; s2 = h*sum(tp); ss2(i) = s2; s3 = h/3*(y(1)+... 2*sum(y(3:2:end-2))+... 4*sum(y(2:2:end))+... y(end)); ss3(i) = s3; i = i + 1; end subplot(1,3,1); semilogx(hs, ss1); xlabel(\u0026#39;h\u0026#39;); ylabel(\u0026#39;Riemann Sum\u0026#39;) set(gca, \u0026#39;XDir\u0026#39;,\u0026#39;reverse\u0026#39;,... \u0026#39;YLim\u0026#39;, [13, 16.5]); subplot(1,3,2); semilogx(hs, ss2, \u0026#39;g\u0026#39;); xlabel(\u0026#39;h\u0026#39;); ylabel(\u0026#39;Trape Sum\u0026#39;) set(gca, \u0026#39;XDir\u0026#39;,\u0026#39;reverse\u0026#39;,... \u0026#39;YLim\u0026#39;, [13, 16.5]); subplot(1,3,3); semilogx(hs, ss3, \u0026#39;r\u0026#39;); xlabel(\u0026#39;h\u0026#39;); ylabel(\u0026#39;Simpson Sum\u0026#39;) set(gca, \u0026#39;XDir\u0026#39;,\u0026#39;reverse\u0026#39;,... \u0026#39;YLim\u0026#39;, [13, 16.5]);   integral(),integral2(),integral3()  q = integral(fun,xmin,xmax) q = integral2(fun,xmin,xmax,ymin,ymax) q = integral2(fun,xmin,xmax,ymin,ymax,zmin,zmax)  Approximates the integral of the function over the planar region xmin  x  xmax (and ymin(x)  y  ymax(x) and zmin(x,y)  z  zmax(x,y)). Their first argument fun should be a function handle, instead of the name of function.\nFor example:\n$$ f(x, y)=\\frac{1}{(\\sqrt{x+y})(1+x+y)^{2}} $$\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  %% func f = @(x,y) 1./( sqrt(x + y) .* (1 + x + y).^2 ); close; x = 0:.001:1; y = 0:.001:1; [X, Y] = meshgrid(x,y); Z = f(X,Y); mesh(X,Y,Z); xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); zlabel(\u0026#39;f(x,y)\u0026#39;); set(gca,\u0026#39;zscale\u0026#39;,\u0026#39;log\u0026#39;) view(100,10); colorbar; colormap(jet);   Compute integrals for the triangle regions defined by 0x1 and 0y1x\n1 2 3 4 5 6  %% integral2 f = @(x,y) 1./( sqrt(x + y) .* (1 + x + y).^2 ); ymax = @(x) 1 - x; q = integral2(f,0,1,0,ymax); disp(q)   0.2854 ","date":"2021-02-04T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/matlab-in-engineering-vi-differentiation-integration/","title":"MATLAB in Engineering VI (Differentiation \u0026 Integration)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Functional Plots Logarithm plots  linspace(x1,x2,n) returns a row vector of n (=100 by default) evenly spaced points between x1 and x2. logspace(a,b,n) generates a row vector of n (=50 by default) logarithmically spaced points between decades 10^a and 10^b. That is  1 2 3  x = logspace(-1, 2, 4); % = 10 .^ linspace(-1, 2, 4)  % = [ 0.1, 1, 10, 100 ]    semilogx(): x-axis takes logarithmic coordinates, y-axis takes linear coordinates semilogy(): x-axis takes linear coordinates, y-axis takes logarithmic coordinates plotyy(): Linear coordinate system with two sets of y-axis  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  %% Log Plot x = logspace(-1, 1, 100); y = x .^ 2; subplot(2, 2, 1); plot(x, y); title(\u0026#39;Plot\u0026#39;); grid on; subplot(2, 2, 2); semilogx(x, y); title(\u0026#39;Semilogx\u0026#39;); grid on; subplot(2, 2, 3); semilogy(x, y); title(\u0026#39;Semilogy\u0026#39;); set(gca, \u0026#39;XGrid\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;YGrid\u0026#39;, \u0026#39;on\u0026#39;); % = grid on; subplot(2, 2, 4); loglog(x, y); title(\u0026#39;Loglog\u0026#39;); grid on;   plotyy() The return value of plotyy() is the array [ax,hlines1,hlines2], where:\n ax is a vector that holds the handles of two axis objects. hlines1 and hlines2 are the handles of two plot lines respectively.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  %% plotyy close all x = 0:0.01:20; y1 = 200 * exp(-0.05*x).*sin(x); y2 = 0.8 * exp(-0.5*x).*sin(10*x); [AX, H1, H2] = plotyy(x, y1, x, y2); set(get(AX(1), \u0026#39;YLabel\u0026#39;), \u0026#39;String\u0026#39;, \u0026#39;Left Y-axis for y1\u0026#39;); set(get(AX(2), \u0026#39;YLabel\u0026#39;), \u0026#39;String\u0026#39;, \u0026#39;Left Y-axis for y2\u0026#39;); title(\u0026#39;Labeling plotyy\u0026#39;); set(H1, \u0026#39;LineStyle\u0026#39;, \u0026#39;--\u0026#39;); set(H2, \u0026#39;LineStyle\u0026#39;, \u0026#39;:\u0026#39;); legend(\u0026#39;$y_1 = 200 \\cdot e^{-0.05x}\\cdot \\sin(x)$\u0026#39;,... \u0026#39;$y_2 = 0.8 \\cdot e^{-0.5x}\\cdot \\sin(10x)$\u0026#39;,... \u0026#39;Interpreter\u0026#39;, \u0026#39;latex\u0026#39;)   Note that YLabel is a \u0026lsquo;text\u0026rsquo; sub-object of axis, thus to get the handel of YLabel, use get(AX(i), 'YLabel').\n1  \u0026gt;\u0026gt; get(AX(1), \u0026#39;YLabel\u0026#39;)   ans = Text (Left Y-axis for y1) - : String: 'Left Y-axis for y1' FontSize: 12.1000 FontWeight: 'normal' FontName: 'Helvetica' Color: [0 0.4470 0.7410] HorizontalAlignment: 'center' Position: [-1.2837 1.9073e-04 -1.0000] Units: 'data'   polar()  polar(theta,rho,LineSpec) Creates a polar plot of angle theta with radius rho. LineSpec specifies the line type, drawing symbols, and the color of the lines drawn in the polar plot. polarplot(theta1,rho1,LineSpec1,\u0026hellip;,thetaN,rhoN,LineSpecN) Creates multi polar plot in one figure (it can not be done by plot() function)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  %% polar plot x = 1:100; theta = x/10; r = log10(x); subplot(1,4,1); polar(theta, r, \u0026#39;r\u0026#39;); theta = linspace(0, 2*pi); r = cos(4*theta); subplot(1,4,2); p = polar(theta, r); set(p, \u0026#39;Color\u0026#39;, \u0026#39;g\u0026#39;) theta = linspace(0, 2*pi, 7); r = ones(1, length(theta)); subplot(1,4,3); hold on polar(theta, r, \u0026#39;m\u0026#39;); polar(ax3, theta, 0.5*r, \u0026#39;m\u0026#39;); hold off theta = linspace(0, 2*pi); r1 = 1 - sin(theta); r2 = 1 - cos(theta); subplot(1,4,4); polarplot(theta, r1, \u0026#39;--r\u0026#39;,theta, r2, \u0026#39;b\u0026#39;);   Statistics Plots hist() Use hist(x,nbins) to plot the histogram, where:\n x is the original data nbins means the number of groups  1 2 3 4 5 6 7 8 9 10 11 12 13  %% hist close all clear all y = randn(1, 1000); subplot(2,1,1); hist(y, 10); title(\u0026#39;bins = 10\u0026#39;); subplot(2,1,2); hist(y, 50); title(\u0026#39;bins = 50\u0026#39;);   bar(), bar3(), barh() Use the bar(x) and bar3(x) functions to plot two-dimensional and three-dimensional bar plot, respectively. Each row of x is a group.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  %% bar x1 = [5:-1:1]; x2 = [x1; 1:5]; subplot(1,3,1); bar(x1); title(\u0026#39;A bar plot of x1\u0026#39;); subplot(1,3,2); bar(x2); title(\u0026#39;A bar plot of x2\u0026#39;); subplot(1,3,3); bar3(x2); title(\u0026#39;A 3D bar plot of x2\u0026#39;);   Use barh() function to draw a vertically aligned bar plot; specify 'stacked' argument in bar()/barh() function to plot stacked plot.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  %% bar-2 x1 = [5:-1:1]; x2 = [x1; 1:5]; subplot(1,3,1); bar(x1, \u0026#39;stacked\u0026#39;); title(\u0026#39;Stacked for x1\u0026#39;); subplot(1,3,2); barh(x2); title(\u0026#39;Horizontal bar for x2\u0026#39;); subplot(1,3,3); barh(x2, \u0026#39;stacked\u0026#39;); title(\u0026#39;Stacked horizontal bar for x2\u0026#39;);   assign bar() to a variable to get the handle to the plot, and we can modify the property of the plot, such as the color of the bar.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  %% bar-3 G = [46 38 29 24 13]; S = [29 27 17 26 8]; B = [29 23 19 32 7]; h = bar(1:5, [G\u0026#39; S\u0026#39; B\u0026#39;]); title(\u0026#39;Medal count for top 5 countries in 2012 Olympics\u0026#39;); ylabel(\u0026#39;Number of medals\u0026#39;); set(get(gca, \u0026#39;XLabel\u0026#39;), \u0026#39;String\u0026#39;, \u0026#39;Country\u0026#39;); set(gca, \u0026#39;XTickLabel\u0026#39;, {\u0026#39;USA\u0026#39;, \u0026#39;CHN\u0026#39;, \u0026#39;GBR\u0026#39;, \u0026#39;RUS\u0026#39;, \u0026#39;KOR\u0026#39;}); set(h(1), \u0026#39;FaceColor\u0026#39;, \u0026#39;r\u0026#39;); set(h(2), \u0026#39;FaceColor\u0026#39;, \u0026#39;g\u0026#39;); set(h(3), \u0026#39;FaceColor\u0026#39;, \u0026#39;b\u0026#39;); legend(\u0026#39;Gold\u0026#39;, \u0026#39;Silver\u0026#39;, \u0026#39;Bronze\u0026#39;);   pie(), pie3() Use pie() and pie3() to draw pie charts in two and three dimensions. Pass a boolean vector to indicate whether each part of the sector is offset.\n1 2 3 4 5 6 7 8 9 10 11 12  %% pie plot a = [10 5 20 30]; subplot(1,3,1); pie(a); subplot(1,3,2); pie(a, [0,0,0,1]); subplot(1,3,3); pie3(a, [1,1,1,1]);   stairs(), stem() The stairs() and stem() functions are used to draw stair and stem charts, respectively, to represent discrete numeric sequences.\n1 2 3 4 5 6 7 8 9 10 11 12 13  %% stair \u0026amp; stem close all x = linspace(0, 2*pi, 100); y = sin((pi .* (x.^2))./4); subplot(2,1,1); stairs(x,y); subplot(2,1,2); hold on plot(x,y); stem(x,y); hold off   boxplot(), errorbar() 1 2 3 4 5 6 7 8 9 10 11 12 13  %% box \u0026amp; error plot load carsmall subplot(1,2,1); boxplot(MPG, Origin); x = 0:pi/10:pi; y = sin(x); e = std(y) * ones(size(x)); subplot(1,2,2); errorbar(x,y,e);   fill() The fill() function fills the concave side of the drawn point.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  %% fill t = (1:2:15)\u0026#39; * pi/8; x = sin(t); y = cos(t); fill(x,y,\u0026#39;r\u0026#39;); text(0,0,\u0026#39;STOP\u0026#39;,... \u0026#39;Color\u0026#39;,\u0026#39;w\u0026#39;,... \u0026#39;FontSize\u0026#39;,80,... \u0026#39;FontWeight\u0026#39;,\u0026#39;bold\u0026#39;,... \u0026#39;HorizontalAlignment\u0026#39;,\u0026#39;center\u0026#39;); axis off   3D plot imagesc() The imagesc() function converts a three-dimensional view into a two-dimensional top view, indicating the height by the color of the points.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  %% image [x, y] = meshgrid(-3:.2:3, -3:.2:3); z = x.^2 + x.*y + y.^2; subplot(1,2,1); surf(x,y,z); box on xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); zlabel(\u0026#39;z\u0026#39;); xlim([-4 4]); ylim([-4 4]); subplot(1,2,2); imagesc(z); axis square xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); colorbar;    colorbar command adds a legend to the generated 2D diagram with the relationship between color and height, colormap() function change the color scheme. For details, refer to the documentation.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  %% color map [x, y] = meshgrid(-3:.2:3, -3:.2:3); z = x.^2 + x.*y + y.^2; ax(1) = subplot(2,2,1); imagesc(z); axis square xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); ax(2) = subplot(2,2,2); imagesc(z); axis square xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); ax(3) = subplot(2,2,3); imagesc(z); axis square xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); ax(4) = subplot(2,2,4); imagesc(z); axis square xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); colormap(ax(1),cool); colormap(ax(2),winter); colormap(ax(3),jet); colormap(ax(4),gray);   Note that the color scheme is a 256 * 3 matrix whose elements are from 0 to 1.\n The columns of the matrix represent \u0026lsquo;R\u0026rsquo;, \u0026lsquo;G\u0026rsquo;, \u0026lsquo;B\u0026rsquo; respectively. The larger the value of the color triple in the matrix, the darker the corresponding color. The color triple of the lower row corresponds to a larger value.  1  \u0026gt;\u0026gt; size(winter)   ans = 256 3 Thus we can customize ourselves color scheme:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  %% customized color map close all [x, y] = meshgrid(-3:.2:3, -3:.2:3); z = x.^2 + x.*y + y.^2; imagesc(z); axis square xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); rs = zeros(1,256); gs = zeros(1,256); bs = [1:256]./256; cm = [rs\u0026#39;, gs\u0026#39;, bs\u0026#39;]; colorbar; colormap(cm);   plot3() The plot3() function can be used to plot the 3D lines, the input should be three vectors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  %% plot3 close all x = 0:0.1:3*pi; z1 = sin(x); z2 = sin(2*x); z3 = sin(3*x); y1 = zeros(size(x)); y2 = ones(size(x)); y3 = 2 .* y2; plot3(x, y1, z1, \u0026#39;r\u0026#39;,... x, y2, z2, \u0026#39;b\u0026#39;,... x, y3, z3, \u0026#39;g\u0026#39;); grid on; xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); zlabel(\u0026#39;z\u0026#39;); legend(\u0026#39;y1\u0026#39;, \u0026#39;y2\u0026#39;, \u0026#39;y3\u0026#39;);   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  %% plot3 t = 0:pi/50:10*pi; subplot(1,2,1); plot3(sin(t), cos(t), t); grid on; axis square; turns = 40*pi; t = linspace(0, turns, 400); x = cos(t) .* (turns-t) ./ turns; y = sin(t) .* (turns-t) ./ turns; z = t./turns; subplot(1,2,2); plot3(x,y,z); grid on;   fplot3()   fplot3(funx,funy,funz) plots the parametric curve defined by x = funx(t), y = funy(t), and z = funz(t) over the default interval [-5,5] for t.\n  fplot3(funx,funy,funz,tinterval) plots over the specified interval. Specify the interval as a two-element vector of the form [tmin tmax].\n  1 2 3 4 5 6 7  %% fplot3(@(t)sin(t), @(t)cos(t), @(t)t, [0 2*pi], \u0026#39;LineWidth\u0026#39;, 2) hold on fplot3(@(t)sin(t), @(t)cos(t), @(t)t, [2*pi 4*pi], \u0026#39;--or\u0026#39;) fplot3(@(t)sin(t), @(t)cos(t), @(t)t, [4*pi 6*pi], \u0026#39;-.*c\u0026#39;) hold off   surf(), mesh() We pass two 2-dimension matrix X, Y to the function $z = f(x,y)$ to obtain a 2-dimension matrix Z which records the function value at each point of the domain. And pass the Z to surf()/mesh() function to plot the 3-dimension function plot.\nmeshgrid() function transforms two inputting vectors to the corresponding row expansion and column expansion to obtain two expansion 2-dimension matrixes, and the matrixes can be applied to the binary function.\n1 2 3 4 5 6 7 8 9 10 11  %% mesh \u0026amp; surf x = -3.5:0.2:3.5; y = x; [X, Y] = meshgrid(x,y); Z = X .* exp(-X.^2-Y.^2); subplot(1,2,1); mesh(X,Y,Z); colorbar; subplot(1,2,2); surf(Z); colorbar; colormap(jet);   Note that if X and Y is not specified, mesh(Z)/surf(Z) will creates a mesh/grid plot and uses the column and row indices of the elements in Z as the x and y coordinates.\nfsurf(), fmesh()   fsurf(f) creates a surface plot of the function z = f(x,y) over the default interval [-5 5] for x and y.\n  fsurf(f,xyinterval) plots over the specified interval. To use the same interval for both x and y, specify xy-interval as a two-element vector of the form [min max]. To use different intervals, specify a four-element vector of the form [xmin xmax ymin ymax].\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  %% function fs = fsurf(@(x,y) y.*sin(x)-x.*cos(y),... [-2*pi 2*pi]); fs.FaceAlpha = .5; fs.ShowContours = \u0026#39;on\u0026#39;; title(\u0026#39;ysin(x) - xcos(y) for x and y in [-2\\pi,2\\pi]\u0026#39;) xlabel(\u0026#39;x\u0026#39;); ylabel(\u0026#39;y\u0026#39;); zlabel(\u0026#39;z\u0026#39;); box on ax = gca; ax.XTick = -2*pi:pi/2:2*pi; ax.XTickLabel = {\u0026#39;-2\\pi\u0026#39;,\u0026#39;-3\\pi/2\u0026#39;,\u0026#39;-\\pi\u0026#39;,\u0026#39;-\\pi/2\u0026#39;,\u0026#39;0\u0026#39;,... \u0026#39;\\pi/2\u0026#39;,\u0026#39;\\pi\u0026#39;,\u0026#39;3\\pi/2\u0026#39;,\u0026#39;2\\pi\u0026#39;}; ax.YTick = -2*pi:pi/2:2*pi; ax.YTickLabel = {\u0026#39;-2\\pi\u0026#39;,\u0026#39;-3\\pi/2\u0026#39;,\u0026#39;-\\pi\u0026#39;,\u0026#39;-\\pi/2\u0026#39;,\u0026#39;0\u0026#39;,... \u0026#39;\\pi/2\u0026#39;,\u0026#39;\\pi\u0026#39;,\u0026#39;3\\pi/2\u0026#39;,\u0026#39;2\\pi\u0026#39;};     fsurf(funx,funy,funz) plots the parametric surface defined by x = funx(u,v), y = funy(u,v), z = funz(u,v) over the default interval [-5 5] for u and v.\n  fsurf(funx,funy,funz,uvinterval) plots over the specified interval. To use the same interval for both u and v, specify uvinterval as a two-element vector of the form [min max]. To use different intervals, specify a four-element vector of the form [umin umax vmin vmax].\n  1 2 3 4 5 6 7 8  %% parametric function: Mobiusband x=@(u,v) (2+u.*cos(v/2)).*cos(v); y=@(u,v) (2+u.*cos(v/2)).*sin(v); z=@(u,v) u.*sin(v/2); fsurf(x,y,z,[-0.5, 0.5, 0, 2*pi]) view(78,70)   or we can obtain this figure with surf() function as well:\n1 2 3 4 5 6 7 8 9  u=linspace(-0.5,0.5,50); v=linspace(0,2*pi,50); [U,V]=meshgrid(u,v); r=2+U.*cos(V/2); x=r.*cos(V); y=r.*sin(V); z=U.*sin(V/2); surf(x,y,z) view(20,30)   contour(), contourf() contour() and contourf() functions can be used to draw contour lines of 3D shapes, the former will not fill the grid while the latter will.\nPassing arguments to the contour() function or manipulating the graphics handle can change the details of the image:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  %% contour x = -3.5:0.2:3.5; y = x; [X, Y] = meshgrid(x,y); Z = X .* exp(-X.^2-Y.^2); subplot(2,2,1); contour(Z); axis square colorbar; subplot(2,2,2); contour(Z, [-.45:.05:.45]); axis square colorbar; subplot(2,2,3); [C, h] = contour(Z); clabel(C, h); axis square colorbar; subplot(2,2,4); contourf(Z); axis square colorbar;   1 2 3 4 5 6 7 8 9 10 11 12 13 14  %% contour x = -3.5:0.2:3.5; y = x; [X, Y] = meshgrid(x,y); Z = X .* exp(-X.^2-Y.^2); close all [C, h] = contourf(Z, [-.45:.05:.45]); clabel(C, h); axis square colorbar; colormap(jet);   surfc(), meshc() combine the surf()/mesh() and contour() function, i.e. plot the contour lines while plotting the 3D surf.\n1 2 3 4 5 6 7 8 9  %% meshc \u0026amp; surfc x = -3.5:0.2:3.5; y = x; [X, Y] = meshgrid(x,y); Z = X .* exp(-X.^2-Y.^2); subplot(1,2,1); meshc(X,Y,Z); subplot(1,2,2); surfc(X,Y,Z);   view() The view can be adjusted using the view() function, which takes two floating-point arguments representing the azimuth and elevation angles.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  %% view x = -3.5:0.2:3.5; y = x; [X, Y] = meshgrid(x,y); Z = X .* exp(-X.^2-Y.^2); subplot(1,2,1); surf(X,Y,Z); view(60,25); subplot(1,2,2); surf(X,Y,Z); view(39,55);   These two arguments are shown in the lower left corner of the window when the figure is manually rotated:\n","date":"2021-01-29T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/matlab-in-engineering-v-advanced-plotting/","title":"MATLAB in Engineering V (Advanced Plotting)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Basic Plot plot() Function The plot function has two uses:\n plot(x,y): plot vector pair (x,y) plot(y): plots vector pair (x,y), where x=1:n, n=length(y).  1 2 3 4 5  %% plot x = 0:pi/20:2*pi; y = cos(x); plot(x,y);   1 2 3 4  %% y = rand(100,1); plot(y);   fplot() Function   fplot(f) plots the curve defined by the function y = f(x) over the default interval [-5 5] for x.\n  fplot(f,xinterval) plots over the specified interval. Specify the interval as a two-element vector of the form [xmin xmax].\n  fplot(funx,funy) plots the curve defined by x = funx(t)and y = funy(t) over the default interval [-5 5] for t.\n  fplot(funx,funy,tinterval) plots over the specified interval. Specify the interval as a two-element vector of the form [tmin tmax].\n  Here f is the handle of the function.\n1 2 3 4 5  %% xt = @(t) cos(3*t); yt = @(t) sin(2*t); fplot(xt,yt,\u0026#39;LineWidth\u0026#39;,2)   hold on/off Keyword When calling plot, MATLAB will automatically close the previous figure window (you can also close it manually with the close keyword). So to plot multiple plot commands in a single figure you need to call the hold on/off keyword:\n1 2 3 4 5 6 7 8 9 10 11 12  %% hold on/off keyword close all x = 0:pi/20:2*pi; y1 = cos(x); y2 = sin(x); hold on plot(x,y1); plot(x,y2); hold off   Alternatively, you can run:\n1 2 3 4 5 6 7 8 9  %% hold on/off keyword close all x = 0:pi/20:2*pi; y1 = cos(x); y2 = sin(x); plot(x,y1, x,y2);   to obtain the same figure.\nhold on/off works on fplot() function as well:\n1 2 3 4 5 6 7 8  %% piecewise function close; clear; hold on fplot(@(x) exp(x),[-3 0],\u0026#39;b\u0026#39;,\u0026#39;LineWidth\u0026#39;,2) fplot(@(x) cos(x),[0 3],\u0026#39;b\u0026#39;,\u0026#39;LineWidth\u0026#39;,2) hold off grid on   Style The style of the graph line can be represented by three letters:\n Data markers Line types Colors  For details, refer to the documentation.\n1 2 3 4 5 6 7 8 9 10  %% style x = 0:pi/20:2*pi; y1 = cos(x); y2 = sin(x); hold on plot(x,y1,\u0026#39;*--c\u0026#39;); plot(x,y2,\u0026#39;+:m\u0026#39;); hold off   Legend Use the legend(label1, ... , labelN) function to add legends to images.\n1 2 3 4 5 6 7 8 9 10 11 12  %% legend  x = 0:0.5:4*pi; y1 = sin(x); y2 = cos(x); y3 = 1./(1+exp(-x)); y4 = (1/(2*pi*2)^0.5).*exp((-1.*(x-2*pi).^2)./(2*2^2)); plot(x, y1, \u0026#39;bd-\u0026#39;,... x, y2, \u0026#39;gp:\u0026#39;,... x, y3, \u0026#39;ro-\u0026#39;,... x, y4, \u0026#39;c^-\u0026#39;); legend(\u0026#39;sin\u0026#39;, \u0026#39;cos\u0026#39;, \u0026#39;sigmoid\u0026#39;, \u0026#39;gauss\u0026#39;);   To use $\\LaTeX$, you need to specify the interpreter as \u0026lsquo;latex\u0026rsquo; in the legend function:\n1 2 3 4 5 6 7 8 9 10 11 12  %% legend  x = 0:0.5:4*pi; y1 = sin(x); y2 = cos(x); y3 = 1./(1+exp(-x)); y4 = (1/(2*pi*2)^0.5).*exp((-1.*(x-2*pi).^2)./(2*2^2)); plot(x, y1, \u0026#39;bd-\u0026#39;,... x, y2, \u0026#39;gp:\u0026#39;,... x, y3, \u0026#39;ro-\u0026#39;,... x, y4, \u0026#39;c^-\u0026#39;); legend(\u0026#39;$\\sin(x)$\u0026#39;, \u0026#39;$\\cos(x)$\u0026#39;, \u0026#39;sigmoid\u0026#39;, \u0026#39;gauss\u0026#39;, \u0026#39;Interpreter\u0026#39;, \u0026#39;latex\u0026#39;);   Label and title Use title() and xlabel(), ylabel() and zlabel() to add titles and labels to images:\n1 2 3 4 5 6 7 8 9 10 11  %% label \u0026amp; title x = 0:pi/20:2*pi; y1 = cos(x); y2 = exp(-x); plot(x, y1, \u0026#39;c^-\u0026#39;, x, y2, \u0026#39;ro-\u0026#39;); legend(\u0026#39;$\\cos(x)$\u0026#39;, \u0026#39;$e^{-x}$\u0026#39;, \u0026#39;Interpreter\u0026#39;, \u0026#39;latex\u0026#39;); xlabel(\u0026#39;$x = 0$ to $2\\pi$\u0026#39;, \u0026#39;Interpreter\u0026#39;, \u0026#39;latex\u0026#39;); ylabel(\u0026#39;$\\cos(x)$ and $e^{-x}$\u0026#39;, \u0026#39;Interpreter\u0026#39;, \u0026#39;latex\u0026#39;); title(\u0026#39;function plots\u0026#39;);   Text and annotation   Call text function to add a text description to a data point.\n  Call annotation to draw arrow, box, circle etc. shapes on the figure.\nX,Y is the scale of the arrow\u0026rsquo;s position relative to the whole box. [0.32,0.5] means: the x coordinate goes from 32% of the whole figure to 50% (left to right) [0.6,0.4] means: y-coordinate from 60% of the place to 40% of the place (from top to bottom)\n  Call line function to create line. [2,2] means: the x coordinate goes from 2 to 2 [0,2^2*sin(2)] means: the x coordinate goes from 0 to 2^2*sin(2)\n  1 2 3 4 5 6 7 8 9  %% text \u0026amp; annotation x = linspace(0, 3); y = x.^2.*sin(x); plot(x, y); line([2,2], [0,2^2*sin(2)]); % behind the plot func. str = \u0026#39;$$ \\int_{0}^{2} x^2\\sin(x)dx $$\u0026#39;; text(0.25, 2.5, str, \u0026#39;Interpreter\u0026#39;, \u0026#39;latex\u0026#39;); annotation(\u0026#39;arrow\u0026#39;, \u0026#39;X\u0026#39;, [0.32, 0.5], \u0026#39;Y\u0026#39;, [0.6, 0.4]);    line, annotation both create \u0026lsquo;line\u0026rsquo; object; text creates \u0026lsquo;text\u0026rsquo; object, we will discuss them later.\n Axis, box and grid Use the following commands to control current axes, borders and grids.\n grid on/off to set grid visibility box on/off sets the border visibility.  1 2 3 4 5 6 7 8  %% grid x = 0:pi/20:2*pi; y1 = cos(x); y2 = sin(x); plot(x,y1,\u0026#39;*--c\u0026#39;, x,y2,\u0026#39;+:m\u0026#39;); grid on    axis on/off sets the axis visibility axis normal restores the default behavior and sets the properties of the box aspect ratio mode and data aspect ratio mode to automatic. axis square uses the same length of axes and adjusts the - increments between data units accordingly axis equal uses the same length of data units along each axis axis tight sets the axis range to be equal to the data range, so that the axes are tightly around the data. axis equal tight   axis ij place the origin of the coordinate system in the upper left corner axis xy Place the origin in the lower left corner  For more details, refer to the documentation.\nMulti plot Use subplot function to plot multi axis in one figure, its syntax is\n1  subplot(m,n,p)   This command divides the current figure into m rows n columns blocks and draws the image in the pth grid. Note that p is counted by rows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  %% multi plot subplot(2,2,1); x = linspace(-3.8,3.8); y_cos = cos(x); plot(x,y_cos); title(\u0026#39;Subplot 1: Cosine\u0026#39;) subplot(2,2,2); y_poly = 1 - x.^2./2 + x.^4./24; plot(x,y_poly,\u0026#39;g\u0026#39;); title(\u0026#39;Subplot 2: Polynomial\u0026#39;) subplot(2,2,[3,4]); plot(x,y_cos,\u0026#39;b\u0026#39;,x,y_poly,\u0026#39;g\u0026#39;); title(\u0026#39;Subplot 3 and 4: Both\u0026#39;)   Object In MATLAB, graphs are stored in memory as objects, and their properties can be manipulated by getting the handles of the objects. For example, a graph has an axis object with the handle gca, through which the properties of the axis, such as XLim, can be modified.\nObtain handles A graphics handle is essentially a floating point number that uniquely identifies a graphics object. The following functions are used to get the graph handle.\n gca gets the handle of the current axis gcf gets the handle of the current image allchild(handle_list) gets the handles of all the children of the object ancestor(h,type) get the ancestor node of the nearest type of the object delete(h) delete an object findall(handle_list) get the descendants of the object  All drawing functions, such as plot(), legend(), *label(), title(), text(), annotation(), also return a handle to the drawing object.\nManipulating graphics properties via handles Use the get() and set() functions to access and modify the properties of the graphical object.\naxis The handle of current axis is gca, we can access the properties of current axis by\n1  get(gca)   Once we find the properties we need, we can modify them.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  %% axis: gca close all x = 0:pi/20:2*pi; y1 = cos(x); y2 = exp(-x); subplot(1,2,1) hold on p1 = plot(x, y1, \u0026#39;c^-\u0026#39;); p2 = plot(x, y2, \u0026#39;ro-\u0026#39;); hold off subplot(1,2,2) hold on p1 = plot(x, y1, \u0026#39;c^-\u0026#39;); p2 = plot(x, y2, \u0026#39;ro-\u0026#39;); hold off set(gca, \u0026#39;FontSize\u0026#39;, 15); set(gca, \u0026#39;XLim\u0026#39;, [0, 2*pi]); set(gca, \u0026#39;XTick\u0026#39;, 0:pi/2:2*pi); set(gca, \u0026#39;XTickLabel\u0026#39;, {\u0026#39;0\u0026#39;, \u0026#39;\\pi/2\u0026#39;, \u0026#39;\\pi\u0026#39;, \u0026#39;3\\pi/2\u0026#39;, \u0026#39;2\\pi\u0026#39;}); set(gca, \u0026#39;FontName\u0026#39;, \u0026#39;symbol\u0026#39;);   If we want to align the subplot axis, we can call get() function inside the set() function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  %% axis align close all x = 0:pi/20:2*pi; y1 = cos(x); y2 = exp(-x); ax1 = subplot(1,2,1); p1 = plot(x, y1, \u0026#39;c^-\u0026#39;); ax2 = subplot(1,2,2); p2 = plot(x, y2, \u0026#39;ro-\u0026#39;); set(ax2, \u0026#39;YLim\u0026#39;, get(ax1, \u0026#39;YLim\u0026#39;));   axis.XLabel, axis.Title Note that the label (XLabel, YLabel, Zlabel) and title (Title) of the the axis is a sub-object of the axis (or gca), instead of a property. Thus when we want to modify the label, we need to obtain its handel first by\n1  get(gca, \u0026#39;XLabel\u0026#39;)   and then change its properties like String, Color, FontSize, etc.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  %% close all x = 0:pi/20:2*pi; y1 = cos(x); y2 = exp(-x); ax1 = subplot(1,2,1); p1 = plot(x, y1, \u0026#39;c^-\u0026#39;); ax2 = subplot(1,2,2); p2 = plot(x, y2, \u0026#39;ro-\u0026#39;); set(get(ax1, \u0026#39;XLabel\u0026#39;), \u0026#39;String\u0026#39;, \u0026#39;x\u0026#39;); set(get(ax1, \u0026#39;YLabel\u0026#39;), \u0026#39;String\u0026#39;, \u0026#39;cos(x)\u0026#39;); set(get(ax1, \u0026#39;Title\u0026#39;), \u0026#39;String\u0026#39;, \u0026#39;Plot1\u0026#39;,... \u0026#39;Color\u0026#39;, \u0026#39;c\u0026#39;); set(get(ax2, \u0026#39;XLabel\u0026#39;), \u0026#39;String\u0026#39;, \u0026#39;x\u0026#39;); set(get(gca, \u0026#39;YLabel\u0026#39;), \u0026#39;String\u0026#39;, \u0026#39;exp(-x)\u0026#39;); % gca is ax2 here set(get(gca, \u0026#39;Title\u0026#39;), \u0026#39;String\u0026#39;, \u0026#39;Plot2\u0026#39;,... \u0026#39;Color\u0026#39;, \u0026#39;r\u0026#39;);   plot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  %% line close all x = 0:pi/20:2*pi; y1 = cos(x); y2 = exp(-x); hold on p1 = plot(x, y1, \u0026#39;c^-\u0026#39;, \u0026#39;LineWidth\u0026#39;, 2.0); p2 = plot(x, y2); hold off set(p2, \u0026#39;LineStyle\u0026#39;, \u0026#39;-.\u0026#39;,... \u0026#39;LineWidth\u0026#39;, 2.0,... \u0026#39;Color\u0026#39;, \u0026#39;g\u0026#39;,... \u0026#39;Marker\u0026#39;, \u0026#39;+\u0026#39;);   These properties can also be specified inside the plot function:\n1 2 3 4 5 6 7 8 9 10 11  %% line-marker close all y = rand(20,1); plot(y, \u0026#39;-bo\u0026#39;,... \u0026#39;LineWidth\u0026#39;, 2,... \u0026#39;MarkerEdgeColor\u0026#39;, \u0026#39;k\u0026#39;,... \u0026#39;MarkerFaceColor\u0026#39;, \u0026#39;c\u0026#39;,... \u0026#39;MarkerSize\u0026#39;, 10); set(gca,\u0026#39;FontSize\u0026#39;, 15); xlim([1, 20]); % = set(gca, \u0026#39;XLim\u0026#39;, [1, 20]);   text, annotation and line 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  %% line-text close all x = linspace(0, 3); y = x.^2.*sin(x); p = plot(x, y); l = line([2,2], [0,2^2*sin(2)]); % behind the plot func. str = \u0026#39;$$ \\int_{0}^{2} x^2\\sin(x)dx $$\u0026#39;; t = text(0.25, 2.5, str, \u0026#39;Interpreter\u0026#39;, \u0026#39;latex\u0026#39;); a = annotation(\u0026#39;arrow\u0026#39;, \u0026#39;X\u0026#39;, [0.32, 0.5], \u0026#39;Y\u0026#39;, [0.6, 0.4]); % set arrowshape: a set(a, \u0026#39;LineStyle\u0026#39;, \u0026#39;--\u0026#39;,... \u0026#39;LineWidth\u0026#39;, 2.0,... \u0026#39;Color\u0026#39;, \u0026#39;b\u0026#39;); % set text: t set(t, \u0026#39;FontSize\u0026#39;, 18, \u0026#39;Color\u0026#39;, \u0026#39;r\u0026#39;); % set line: l set(l, \u0026#39;LineStyle\u0026#39;, \u0026#39;:\u0026#39;,... \u0026#39;LineWidth\u0026#39;, 2);   Save To save the figure into files, run:\n1  saveas(gcf,\u0026#39;\u0026lt;filename\u0026gt;\u0026#39;,\u0026#39;\u0026lt;formattype\u0026gt;\u0026#39;);   some available format type:\nDistortion can occur when saving images as bitmaps using the saveas() function. To precisely control the quality of the generated image, you can use the print function.\n","date":"2021-01-25T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/matlab-in-engineering-iv-basic-plotting/","title":"MATLAB in Engineering IV (Basic Plotting)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Data Type The main data types in MATLAB are as follows:\nThe following is an introduction to the main data types in order, the MATLAB documentation describes all data types.\nnumeric The numeric types supported by MATLAB are as follows:\n double(default): double-precision floating-point number single: single-precision floating-point number int8: 8-bit signed integer int16: 16-bit signed integer int32: 32-bit signed integer int64: 64-bit signed integer uint8: 8-bit unsigned integer uint16: 16-bit unsigned integer uint32: 32-bit unsigned integer uint64: 64-bit unsigned integer  In MATLAB, variables of numeric type are defaulted to double type and can be converted to other numeric types using type conversion.\n1 2 3 4  \u0026gt;\u0026gt; a = -3; \u0026gt;\u0026gt; b = uint8(a); \u0026gt;\u0026gt; c = int32(a); \u0026gt;\u0026gt; whos    Name Size Bytes Class Attributes a 1x1 8 double b 1x1 1 uint8 c 1x1 4 int32 char In MATLAB, a string type is defined by a pair of single quotes ' wrapped around a piece of text. Standard ASCII characters can be converted to the corresponding ASCII code:\n1 2  \u0026gt;\u0026gt; s1 = \u0026#39;h\u0026#39;; \u0026gt;\u0026gt; disp(int8(s1))   104 The string is stored in memory as a matrix of characters, which can be compared:\n1 2 3  \u0026gt;\u0026gt; s2 = \u0026#39;string\u0026#39;; \u0026gt;\u0026gt; s3 = \u0026#39;spring\u0026#39;; \u0026gt;\u0026gt; disp(s2 == s3)   1 0 1 1 1 1 indexed and assigned:\n1 2  \u0026gt;\u0026gt; s2(s2 ~= s3) = \u0026#39;T\u0026#39;; \u0026gt;\u0026gt; disp(s2)   sTring and concatenate (we will elaborate it later):\n1 2 3  \u0026gt;\u0026gt; s4 = [s2 s3]; \u0026gt;\u0026gt; s5 = [s2; s3]; \u0026gt;\u0026gt; whos s4 s5    Name Size Bytes Class Attributes s4 1x12 24 char s5 2x6 24 char Note that the spring in MATLAB has dimension!\nstructure In MATLAB, a struct is a data structure that stores {key: value}, similar to a dictionary in the Python language.\nBasic use of struct MATLAB uses . to create and access the fields in a structure:\n1 2 3  \u0026gt;\u0026gt; S.name = \u0026#39;haoming\u0026#39;; \u0026gt;\u0026gt; S.major = \u0026#39;finance\u0026#39;; \u0026gt;\u0026gt; disp(S.name)   haoming You can also create a struct by struct function like struct('name1', data1, 'name2', data2, ...)\n1 2  \u0026gt;\u0026gt; D = struct(\u0026#39;name\u0026#39;, \u0026#39;haoming\u0026#39;, \u0026#39;id\u0026#39;, 13453532, \u0026#39;score\u0026#39;, [100 100; 99 99]); \u0026gt;\u0026gt; disp(D.name)   haoming As we see, struct can hold multi data types:\n1 2 3 4 5 6  student.name = \u0026#39;haoming\u0026#39;; student.id = \u0026#39;41704xxx\u0026#39;; student.number = 31415926; student.grade = [100, 75, 73; ... 95, 91, 85.5; ... 100, 98, 72];   more general, struct can be nested:\n1 2 3 4 5 6 7  \u0026gt;\u0026gt; clear \u0026gt;\u0026gt; S.s1 = \u0026#39;S.s1\u0026#39;; \u0026gt;\u0026gt; S.s2 = \u0026#39;S.s2\u0026#39;; \u0026gt;\u0026gt; SS = struct(\u0026#39;ss1\u0026#39;, \u0026#39;ss1\u0026#39;, \u0026#39;S\u0026#39;, S); \u0026gt;\u0026gt; SSS.sss1 = \u0026#39;sss1\u0026#39;; \u0026gt;\u0026gt; SSS.SS = SS; \u0026gt;\u0026gt; disp(SSS.SS.S.s1)   S.s1 struct can be expanded by parentheses index, you can image it as a list of dict in Python or a table whose rows are different items and columns are different fields:\n1 2 3 4 5  \u0026gt;\u0026gt; student(2).name = \u0026#39;yachong\u0026#39;; \u0026gt;\u0026gt; student(2).id = \u0026#39;41705xxx\u0026#39;; \u0026gt;\u0026gt; student(3).name = \u0026#39;shaonan\u0026#39;; \u0026gt;\u0026gt; student(3).grade = [90: 92; 91: 93; 92: 94]; \u0026gt;\u0026gt; student(2)   ans =  struct: name: 'yachong' id: '41705xxx' number: [] grade: [] 1  \u0026gt;\u0026gt; student.name   ans = 'haoming' ans = 'yachong' ans = 'shaonan' or delete some item:\n1  \u0026gt;\u0026gt; whos student    Name Size Bytes Class Attributes student 1x3 1346 struct 1 2  \u0026gt;\u0026gt; student(1) = [] \u0026gt;\u0026gt; whos student    Name Size Bytes Class Attributes student 1x2 820 struct Common functions of Struct  struct: Create a structure struct2cell: Converts a structure to a cell array cell2struct: converts a cell array to a structure isstruct: determines whether a variable is a structure or not structfun: applies a function to each field of a structure fieldnames: Get all the field names of a structure isfield: determines if a structure contains a field getfield: gets the value of a field in a structure setfield: assigns a value to a field in a structure rmfield: Delete a field from a structure orderfields: Sort the fields of a structure  cell In MATLAB, a cell is a data structure that can hold elements of different types, similar to a list in the Python language.\nBasic use of cell We can create a cell like a matrix using {} in 3 methods:\n1  \u0026gt;\u0026gt; C1 = {\u0026#39;abvr\u0026#39; [1:100]; 2-5*j 12435};   1 2 3 4  \u0026gt;\u0026gt; C2(1,1) = {\u0026#39;srggrdfg\u0026#39;}; \u0026gt;\u0026gt; C2(1,2) = {2364234}; \u0026gt;\u0026gt; C2(2,1) = {[12 43 645]}; \u0026gt;\u0026gt; C2(2,2) = {C1};   1 2 3 4 5 6  \u0026gt;\u0026gt; C3{1,1} = C1; \u0026gt;\u0026gt; C3{1,2} = C2; \u0026gt;\u0026gt; S.s = \u0026#39;struct\u0026#39;; \u0026gt;\u0026gt; C3{2,1} = S; \u0026gt;\u0026gt; C3{2,2} = magic(5); \u0026gt;\u0026gt; whos C1 C2 C3    Name Size Bytes Class Attributes C1 2x2 1248 cell C2 2x2 1712 cell C3 2x2 3756 cell The above three methods are equivalent, where the second method uses the cell index to assign values, and the third method uses the content index to assign values. For more information about the difference between cell indexing and content indexing, refer to the documentation.\nThere are two ways to access the data in a cell, namely: cell index () and content index {}. Since the sub-cell of a cell is still a cell, in the use of the indexer content, it is necessary to specify whether we want to access a sub-cell or the content of the corresponding region of the cell.\nUsing the cell index (), we get a sub-cell array.\n1 2  \u0026gt;\u0026gt; c21 = C2(2,1); \u0026gt;\u0026gt; whos c21    Name Size Bytes Class Attributes c21 1x1 128 cell Using the content index {}, we get the contents of the corresponding region of the cell.\n1 2  \u0026gt;\u0026gt; c11 = C2{1,1}; \u0026gt;\u0026gt; whos c11    Name Size Bytes Class Attributes c11 1x8 16 char Note that to access the content of cell, we need to use content index:\n1  \u0026gt;\u0026gt; C3{1,2}{2,2}{2,1}   ans = 2.0000 - 5.0000i Common functions of cell  cell Create a cell array iscell determines if a variable is a cell array. cell2mat convert a tuple to a matrix cell2struct convert a cell array to a structure mat2cell converts an array to a tuple of a specified size num2cell converts an array to a tuple of the same size struct2cell converts a structure to a tuple celldisp recursively displays the contents of a tuple array cellplot plots the structure of a tuple as an image cellfun applies a function to each cell of a tuple  Note that the mat2cell function can be used to specify the size of each cell of the cell array during the conversion.\n1 2 3 4  \u0026gt;\u0026gt; a = magic(3); \u0026gt;\u0026gt; b = num2cell(a); \u0026gt;\u0026gt; c = mat2cell(a, [1 2], 3); \u0026gt;\u0026gt; whos a b c    Name Size Bytes Class Attributes a 3x3 72 double b 3x3 1008 cell c 2x1 280 cell and\n1  \u0026gt;\u0026gt; a   a = 8 1 6 3 5 7 4 9 2 1  \u0026gt;\u0026gt; b   b = 33 cell  {[8]} {[1]} {[6]} {[3]} {[5]} {[7]} {[4]} {[9]} {[2]} 1  \u0026gt;\u0026gt; c   c = 21 cell  {[ 8 1 6]} {23 double} 1  \u0026gt;\u0026gt; c{2, 1}   ans = 3 5 7 4 9 2 High-dimensional cell A three-dimensional tuple can have three dimensions: row, column, and layer. When indexing cell arrays, the order of priority from highest to lowest is: row  column  layer.\nUse the cat function to concatenate the cell in the specified dimension.\nCheck the type of a variable The following functions can check the type of a variable:\n isinteger determines if the input parameter is an array of integers islogical determines if the input parameter is a logical array isnumeric determines if the input parameter is a numeric array isreal determines if the input parameter is a real array. ischar determines if the input parameter is an array of characters iscell determines if the input parameter is a tuple array isfloat determines if the input array is a floating point array. ishandle determines if the input array is a valid graphics handle. isempty determines if the input array is empty isprime determines which array elements are prime isnan determines which array elements are NaN isinf determines which array elements are Inf isequal determines if the arrays are equal  I/O The file types supported by MATLAB are as follows:\n   File content Extension Function to read the file Function to write the file     .mat load save   .csv, .txt, .xsl, etc. readtable writetable    I/O for .mat The data in the MATLAB workspace can be saved in a file in .mat format. Use the save function to save the data to the file and the load function to read the data from the file.\nThe syntax of the save function is as follows:\n save(filename,variables) stores specified variables in binary form in a file. save(filename,variables,'-ascii') stores specified variables in text form in the file.  The syntax of the load function is as follows:\n load(filename) reads data from a file in binary form. load(filename,'-ascii') reads data from a text file.  The arguments filename and variables are in string format, if variables is not specified, all variables in the current work area will be stored in the file.\n Complex data formats, such as struct and cell, are not supported to be stored in binary format.\n I/O for .csv, .txt,.xls Refer to the documentation:\n readtable writetable  ","date":"2021-01-21T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/matlab-in-engineering-iii-data-structure/","title":"MATLAB in Engineering III (Data Structure)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Script Editor Script is a .m file containing MATLAB commands, to create a MATLAB script, press command+N.\nThe following features are commonly used in the script editor: find function, breakpoint debugging, sectioning, commenting, smart indentation etc.\nStructure Control Logical Operators  \u0026lt; less than \u0026gt; greater than \u0026lt;= less than or equal to \u0026gt;- greater than or equal to == equal to ~= not equal to \u0026amp;\u0026amp; and || or  1  \u0026gt;\u0026gt; (2 \u0026gt;= 1) \u0026amp;\u0026amp; (2 ~= 4)   ans = logical 1 Boolean variables can participate in operations:\n1  \u0026gt;\u0026gt; ((2 \u0026gt;= 1) \u0026amp;\u0026amp; (2 ~= 4)) + (3 == 3)   ans = 2 Flow Control The following are the main flow control keywords in MATLAB:\n if-elseif-else: Execute statements if condition is true switch-case-otherwise: Execute one of several groups of statements for: Execute statements specified number of times while: Repeat execution of statements while condition is true try-catch: Execute statements and catch resulting errors  and the corresponding\n break: Terminate execution of for or while loop continue: Pass control to next iteration of for or while loop end: Terminate block of any flow control codes, or indicate last array index. pause: Halt execution temporarily return: Return control to invoking function  if-elseif-else The syntax of if-elseif-else flow control is\n1 2 3 4 5 6 7  if condition1 statement1 elseif condition2 statement2 else statement3 end   Note that there is no : behind the if, elseif and else. For example, run the following script section:\n1 2 3 4 5 6 7 8 9 10  %% a = 53456453; if rem(a, 3) == 0 disp([num2str(a) ,\u0026#39;  3 (mod 0)\u0026#39;]) elseif rem(a, 3) == 1 disp([num2str(a) ,\u0026#39;  3 (mod 1)\u0026#39;]) else disp([num2str(a) ,\u0026#39;  3 (mod 2)\u0026#39;]) end   53456453  3 (mod 2) Here, disp(X) displays the value of variable X without printing the variable name. disp only accept one argument, thus we need to assemble num2str(a) and '  3 (mod 0)' into an array.\nswitch-case-otherwise The syntax of switch-case-otherwise flow control is\n1 2 3 4 5 6 7 8 9 10  switch expression case value1 statement1 case value2 statement2 case value3 statement3 otherwise statement4 end   For example, run the following script section:\n1 2 3 4 5 6 7 8 9 10 11  %% a = 9329874214285; switch rem(a, 3) case 0 disp([num2str(a) ,\u0026#39;  3 (mod 0)\u0026#39;]) case 1 disp([num2str(a) ,\u0026#39;  3 (mod 1)\u0026#39;]) case 2 disp([num2str(a) ,\u0026#39;  3 (mod 2)\u0026#39;]) end   9329874214285  3 (mod 1) while The syntax of while flow control is\n1 2 3  while expression statement end   For example\n1 2 3 4 5 6 7  %% n = 1; while prod(1:n) \u0026lt; 1e100 % n! \u0026lt; 1e100 n = n + 1; end disp(n)   70 This means 69! \u0026lt; 1e100 whereas 70! \u0026gt; 1e100.\nfor and Pre-allocating The syntax of for flow control is\n1 2 3  for variable = strat [: increment] : end commands end   To iterate from back to front, we can run\n1 2 3 4 5 6 7  %% A = [12 26 33]; for i = length(A):-1:1 B(length(A)-i+1) = A(i); end disp(B)   33 26 12 or simply\n1 2  \u0026gt;\u0026gt; B = A(end:-1:1); \u0026gt;\u0026gt; disp(B)   33 26 12 The later method is strikingly faster than the former one:\n1 2 3 4 5 6 7 8 9 10 11 12 13  %% A = [1:1e7]; tic for i = length(A):-1:1 B(length(A)-i+1) = A(i); end toc tic C = A(end:-1:1); toc    2.370994   0.038670  If a variable requires a predictable amount of memory space, we should try to allocate memory space for it in advance, which is called pre-allocating. This can significantly increase the speed of operations. For example,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  %% tic for ii = 1:2000 for jj = 1: 2000 A(ii, jj) = ii + jj; end end toc tic A = zeros(2000, 2000); for ii = 1: size(A,1) for jj = 1: size(A, 2) A(ii, jj) = ii + jj; end end toc    1.826418   0.028722  tic works with the toc function to measure elapsed time. The tic function records the current time, and the toc function uses the recorded value to calculate the elapsed time.\nbreak Sometimes, the convergence of iteration is not guaranteed, to terminate the execution of for or while loops, use break keyword:\n1 2 3 4 5 6 7 8 9 10 11 12 13  %% x = 2; k = 0; error = inf; error_t = 1e-32; while error \u0026gt; error_t if k \u0026gt; 100 break end x = x - sin(x)/cos(x); error = abs(x - pi); k = k + 1; end disp([k, x, error])   6.0000 3.1416 0 Tips for script write  At the beginning of the script, use command clear all to remove previous variables; close all to close all figures. Semicolons ; should be added to all operations and assignments to suppress the output. If a variable needs to be output to the terminal, it should call the disp method. The ellipsis ... can stitch multiple lines into one line, and the flexible use of this statement can improve the readability of the code.  1 2 3 4 5  annPoints_sampled = annPoints( annPoints(:,1) \u0026gt; x1 \u0026amp;\u0026amp; ... annPoints(:,1) \u0026lt; x2 \u0026amp;\u0026amp; ... annPoints(:,2) \u0026gt; y1 \u0026amp;\u0026amp; ... annPoints(:,2) \u0026lt; y2);    Press Control+C to terminate the script before conclusion.  Functions Scripts and functions are both .m files that contain MATLAB commands. Functions are written when we need to perform routines. The difference between script and function are as follows.\n   Scripts Functions     No input arguments May have input arguments   No output arguments May have output arguments   Operate on data in the global workspace Operate on data in the local workspace    We can use the which command to view the location of the source file of the built-in function, combined with the edit command to view the source code of the built-in function. For example, run the following statement to open the source file of MATLAB\u0026rsquo;s built-in mean function:\n1  \u0026gt;\u0026gt; edit(which(\u0026#39;mean.m\u0026#39;))   Note that the function\u0026rsquo;s .m file name need to match the function name.\nStandard structure The standard structure of function .m file is\n1 2 3  functiony =funcname(x1, x2)% comment code   MATLAB will return the outcome of y to the variable you specified, just like the return keyword in python. For example, calculate the displacement of free falling for given initial displacement x0 , initial velocity v0 , and duration of falling t:\n$$ x = x_0 + v_0t + \\frac{1}{2} g t^2 $$\n1 2 3 4 5 6 7  functionx =freebody(x0,v0,t)% calculation of free falling  % x0: initial displacement in m  % v0: initial velocity in m/sec  % t: the elapsed time in sec  % x: the depth of falling in m x = x0 + v0.*t + 1/2*9.8*t.*t;   Call this function in the terminal:\n1  \u0026gt;\u0026gt; freebody(0, 0, 10)   ans = 490 Note that we write function body as t.*t, which represents the element-wise operation, instead of matrix operation. Thus we can achieve freebody([x1, x2, x3], [v1, v2, v3], [t1, t2, t3]) -\u0026gt; [freebody(x1, v1, t1), freebody(x2, v2, t2), freebody(x3, v3, t3)]\n1 2 3  \u0026gt;\u0026gt; X = [4, 43, 64, -353]; V = [0, 0, 3524, -432]; T = [0, 0, 2, 1]; \u0026gt;\u0026gt; X1 = freebody(X, V, T); \u0026gt;\u0026gt; X1   X1 = 1.0e+03 * 0.0040 0.0430 7.1316 -0.7801 If you want to return multi outcomes, you can assemble them into a list. For example. The acceleration of a particle and the force acting on it are as follows:\n$$ a = \\frac{v_2-v_1}{t_2-t_1}, $$\n$$ F = ma $$\n1 2 3  function[a F] =acc(v2,v1,t2,t1,m)a = (v2-v1)./(t2-t1); F = m.*a;   1 2  \u0026gt;\u0026gt; [Acc Force] = acc(20,10,5,4,1); \u0026gt;\u0026gt; disp([Acc Force])    10 10 No input or output The input and output are optional:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  functionconverttemp()disp(\u0026#39;/ input space to escape /\u0026#39;) str_qus = \u0026#39;\u0026lt;== F\u0026#39;; str_ans = \u0026#39;C ==\u0026gt;\u0026#39;; while 1 f = input(str_qus); if isempty(f) break; else c = (f-32)*5/9; disp([str_ans,num2str(c,\u0026#39;%.2f\u0026#39;)]); end end   1 2 3 4 5 6 7 8 9  \u0026gt;\u0026gt; converttemp() / input space to escape / \u0026lt;== F1234 C ==\u0026gt;667.78 \u0026lt;== F432 C ==\u0026gt;222.22 \u0026lt;== F432 C ==\u0026gt;222.22 \u0026lt;== F   Built-in parameters In MATLAB, there are some built-in function parameters as follows:\n imputname:\tVariable name of function input mfilename:\tFile name of currently running function nargin:\tNumber of function input arguments nargout:\tNumber of function output arguments varargin:\tVariable length input argument list varargout:\tVariable length output argument list  MATLAB does not provide the syntax for specifying default parameter values and function overloading of other high-level languages, but the flexible use of the above built-in function parameters can, to a certain extent, specify default parameter values and method overloading:\n1 2 3 4 5  function[volume]=pillar(Do,Di,height)if nargin==2, height=1; end volume=abs(Do.^2-Di.^2).*height*pi/4;   Recursive functions Functions that call themselves, for example, the factorial of an integer n:\n$$ n! = n\\times\\cdots \\times 1 $$\ncan be defined in terms of another factorial: $n!=n\\times (n-1)!$\nThe recursive function includes a recursive case and a base case. The function stops when it reaches the base case:\n1 2 3 4 5 6 7 8  functionop =fact(n)% fact recursively if n == 1 op = 1; % \u0026lt;-- Base case else op = n * fact(n-1); % \u0026lt;-- Recursive case end end   1  \u0026gt;\u0026gt; fact(10)   ans = 3628800 Syntax of @ f = @(parameter) expression Sometimes you need to define a function to calculate the value of a certain expression, you can directly use the syntax:\n1  f = @(parameter) expression   to create an anonymous function that returns a handle of the function to variable f. Then you can use f to call the handle:\n1 2  \u0026gt;\u0026gt; f = @(x,y) x.^2 + y.^2; \u0026gt;\u0026gt; f(3,4)   ans = 25 1 2  \u0026gt;\u0026gt; x = 1:5; y = 1:5; \u0026gt;\u0026gt; f(x,y)   ans = 2 8 18 32 50 Here f is a variable that represents the handle of the function, this means you can pass f to the other function as a parameter.\n1  \u0026gt;\u0026gt; f   f =  function_handle: @(x,y)x.^2+y.^2 A corresponding concept is the name of function, note that the name of function does not represent the function, thus it cannot be assigned or passed to the other function as a parameter.\nf = @\u0026lt;function-name\u0026gt; To get the function handle from the function name, add @ before the function name, and MATLAB will return the function handle:\n1  \u0026gt;\u0026gt; sin    sin  1  \u0026gt;\u0026gt; @sin;   ans =  function_handle: @sin Once you obtain the function handle, you can assign it to another variable\n1 2  \u0026gt;\u0026gt; y = @sin; \u0026gt;\u0026gt; y(pi)   ans = 1.2246e-16 or pass it to another function as a parameter, for example let\u0026rsquo;s define a function func_plot.m:\n1 2 3 4 5 6 7 8 9 10  functionfunc_plot(x, func, style, LineWidth)% plot the input function % arguments: % func: the handle of the function % x: array of x values y = func(x); plot(x,y,style,... \u0026#39;LineWidth\u0026#39;, LineWidth); end   Call it and pass function handle to plot\n1 2 3 4 5 6 7 8 9 10 11  %% plot funcs x = 0:0.01:2*pi; f1 = @sin; f2 = @(x) sin(x) + exp(-10*x); subplot(1,2,1); func_plot(x, f1, \u0026#39;r\u0026#39;, 2); subplot(1,2,2); func_plot(x, f2, \u0026#39;b\u0026#39;, 2);   ","date":"2021-01-17T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/matlab-in-engineering-ii-script-and-function/","title":"MATLAB in Engineering II (Script and Function)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Introduction Operation interface The operator interface of MATLAB consists of the following parts:\n Current folder Command line window Workspace: storing variables Command history  Variable Variable name Variables in MATLAB do not need to be declared, they are assigned directly using =.\n1 2 3 4  \u0026gt;\u0026gt; a = 10; \u0026gt;\u0026gt; b = a; \u0026gt;\u0026gt; a = 5; \u0026gt;\u0026gt; b, a   b = 10 a = 5 Like most programming languages, variable names in MATLAB are case-sensitive. Variable names can only be composed of 0~9, a~z, A~z, _, and variable names cannot start with a number. Some variables in MATLAB have specific meaning and are not suitable to be used as variable names.\n ans: outcome of the last command i, j: complex number Inf:  eps: 2.2204e-16 NaN: not a number pi:   In addition, the iskeyword command allows you to view all keywords in the MATLAB language, which are also not allowed to be used as variable names. In MATLAB, the calling priority of variables is higher than that of functions, so variable names should not override built-in functions.\n1 2  \u0026gt;\u0026gt; cos=\u0026#39;This string.\u0026#39;; \u0026gt;\u0026gt; cos(8) % Index the string and get \u0026#39;r\u0026#39;   ans = 'r' If a function is overridden by a variable name, call clear \u0026lt;variable name\u0026gt; to unbind the variable name to that function name.\n1 2  \u0026gt;\u0026gt; clear cos % clear the variables bound to cos \u0026gt;\u0026gt; cos(8) % call the built-in cosine function to get -0.1455   ans = -0.1455  clear is a dangerous command, because it clears all variables in the current workspace if it is not followed by an argument.\n type and format The types of variables in MATLAB are: logical, char, numeric, cell, struct and arrays or matrices composed of them.\nThe numeric variables we define directly are stored as double by default. We can check out the type of variable by whos command.\n1 2 3 4 5  \u0026gt;\u0026gt; 1 + 1; \u0026gt;\u0026gt; a = 10; \u0026gt;\u0026gt; b = 1.1; \u0026gt;\u0026gt; c = pi; \u0026gt;\u0026gt; whos   Name Size Bytes Class Attributes a 1x1 8 double ans 1x1 8 double b 1x1 8 double c 1x1 8 double We can change the display format of numeric variables with format \u0026lt;display format\u0026gt;.\n   Display Format Description Example     short Short fixed-point format. Displaying 4 decimal places 3.1416   long long fixed-point format. Display 15 decimal places for double variables and 7 decimal places for float variables. 3.141592653589793   shortE Short scientific notation, displaying 4 decimal places. with scientific notation. 3.1416e+00   longE Long scientific notation. Displays 15 decimal places for double variables and 7 decimal places for float variables. with scientific notation. 3.141592653589793e+00   bank Bank format. Displays 2 decimal places. 3.14   hex Hexadecimal format. 400921fb54442d18   rat Proportional format 355/113    1 2  \u0026gt;\u0026gt; format hex \u0026gt;\u0026gt; pi   ans = 400921fb54442d18 1 2  \u0026gt;\u0026gt; format rat \u0026gt;\u0026gt; 3/13 + 4/14 + 5/15   ans = 232/273 Command line Use ; at the end of the command line to mute the outcome, otherwise the outcome of the command line will display in the command line window. To recall previous commands, press uparrow key, just like shell. Some useful commands line:\n clc: Clear the output of the terminal clear: clears all variables in the current workspace who: displays all variables in the workspace in an abbreviated format whos: displays all variables in the workspace in a long format format: change the format displayed  Operators Common MATLAB operators are: +, -, *, /, ^. After a mathematical expression is computed, its value is stored in the variable ans. The order of operations is left-to-right with equal priority. The order of precedence (from highest to lowest) is:\n Association () Power ^ Multiplication and division *, / Addition and subtraction +, -  Ex 1: Compute $\\cos\\left( \\sqrt{\\frac{(1+2+3+4)^3}{6}}\\right)$:\n1  \u0026gt;\u0026gt; cos(sqrt((1+2+3+4)^3/5))   ans = -0.0050 To make the code neat, we can store the result of the operation in a variable:\n1 2  \u0026gt;\u0026gt; x = (1+2+3+4)^3/5; \u0026gt;\u0026gt; cos(sqrt(x))   ans = -0.0050 Built-in function Arithmetic Operations Documentation\nAdd, Subtraction:\n +: Add numbers, append strings sum: Sum of array elements cumsum: Cumulative sum movsum: Moving sum -: Subtraction diff: Differences and approximate derivatives  Multiplication:\n .*: Element-wise Multiplication *: Matrix multiplication prod: Product of array elements cumprod: Cumulative product pagemtimes: Page-wise matrix multiplication tensorprod: Tensor products between two tensors  Division:\n ./: Right array division .\\ : Left array division /: Solve systems of linear equations xA = B for x \\: Solve systems of linear equations Ax = B for x pagemldivide: Page-wise left matrix divide pagemrdivide: Page-wise right matrix divide  Power:\n .^: Element-wise power ^: Matrix power  Mod:\n mod: Remainder after division (modulo operation) rem: Remainder after division idivide: Integer division with rounding option ceil: Round toward positive infinity fix: Round toward zero floor: Round toward negative infinity round: Round to nearest decimal or integer  Trigonometry Documentation\nSine\n sin:\tSine of argument in radians sind:\tSine of argument in degrees sinpi:\tCompute sin(X*pi) accurately asin:\tInverse sine in radians asind:\tInverse sine in degrees  Cosine\n cos:\tCosine of argument in radians cosd:\tCosine of argument in degrees cospi:\tCompute cos(X*pi) accurately acos:\tInverse cosine in radians acosd:\tInverse cosine in degrees  Tangent\n tan:\tTangent of argument in radians tand:\tTangent of argument in degrees atan:\tInverse tangent in radians atand:\tInverse tangent in degrees  Conversions\n deg2rad:\tConvert angle from degrees to radians rad2deg:\tConvert angle from radians to degrees cart2pol:\tTransform Cartesian coordinates to polar or cylindrical cart2sph:\tTransform Cartesian coordinates to spherical pol2cart:\tTransform polar or cylindrical coordinates to Cartesian sph2cart:\tTransform spherical coordinates to Cartesian  Exponents and Logarithms Documentation\n exp: Exponential expm1: Compute exp(x)-1 accurately for small values of x log: Natural logarithm log10: Common logarithm (base 10) log1p: Compute log(1+x) accurately for small values of x log2: Base 2 logarithm and floating-point number dissection nthroot: Real nth root of real numbers sqrt: Square root  Complex Numbers Documentation\n abs\tAbsolute value and complex magnitude angle\tPhase angle complex\tCreate complex array conj\tComplex conjugate i\tImaginary unit j\tImaginary unit real\tReal part of complex number imag\tImaginary part of complex number isreal\tDetermine whether array uses complex storage sign\tSign function (signum function)  Array Operations Create matrix Direct definition In MATLAB, use [] to enclose the contents of the matrix to be entered, use spaces   or commas , to separate variables within rows, and use semicolon ; to separate each row. For example:\n1 2 3 4  \u0026gt;\u0026gt; a = [1 2 3]; \u0026gt;\u0026gt; b = [1; 2; 3]; \u0026gt;\u0026gt; c = [1 2 3; 4 5 6]; \u0026gt;\u0026gt; a, b, c   a = 1 2 3 b = 1 2 3 c = 1 2 3 4 5 6 We can also define array in a iteration:\n1 2 3 4 5 6 7  %% X = []; for i = 1:5 X = [X i]; end disp(X)   1 2 3 4 5 Using the colon operator to create a vector Using the colon : operator: you can create a long vector with the following syntax:\n i:j: [i, i+1, ..., j]. Note that it contains both i and j; i:k:j: [i, i+k, i+2k, ..., i+mk], where i+mk =\u0026lt; j and i+(m+1)k \u0026gt; j.  1 2 3 4 5  \u0026gt;\u0026gt; a = 1:5; \u0026gt;\u0026gt; b = 1:2:6; \u0026gt;\u0026gt; c = [1:5; 2:3:15; -2:0.5:0]; \u0026gt;\u0026gt; d = \u0026#39;a\u0026#39;:2:\u0026#39;z\u0026#39;; \u0026gt;\u0026gt; a, b, c, d   a = 1 2 3 4 5 b = 1 3 5 c = 1 2 3 4 5 2 5 8 11 14 -2 -3/2 -1 -1/2 0 d = 'acegikmoqsuwy' reshape and repeat   B = reshape(A,sz) reshapes A using the size vector, sz, to define size(B). For example, reshape(A,[2,3]) reshapes A into a 2-by-3 matrix. sz must contain at least 2 elements, and prod(sz) must be the same as numel(A).\n  B = reshape(A,sz1,...,szN) reshapes A into a sz1-by-\u0026hellip;-by-szN array where sz1,...,szN indicates the size of each dimension. You can specify a symbol [] to have the dimension size automatically calculated. For example, if A is a 10-by-10 matrix, then reshape(A,2,2,[]) reshapes A into a 2-by-2-by-25 array.\n  1  \u0026gt;\u0026gt; reshape(1:25,5,[])   ans = 1 6 11 16 21 2 7 12 17 22 3 8 13 18 23 4 9 14 19 24 5 10 15 20 25  B = repmat(A,n) returns an array containing n copies of A in the row and column dimensions. The size of B is size(A)*n when A is a matrix.  1  \u0026gt;\u0026gt; repmat([1 2; 3 4],2)   ans = 1 2 1 2 3 4 3 4 1 2 1 2 3 4 3 4 1  \u0026gt;\u0026gt; repmat(2,5)   ans = 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2   B = repmat(A,r1,...,rN) specifies a list of scalars, r1,..,rN, that describes how copies of A are arranged in each dimension. When A has N dimensions, the size of B is size(A).*[r1...rN]. For example, repmat([1 2; 3 4],2,3) returns a 4-by-6 matrix.\n  B = repmat(A,r) specifies the repetition scheme with row vector r. For example, repmat(A,[2 3]) returns the same result as repmat(A,2,3).\n  1  \u0026gt;\u0026gt; repmat([1 2; 3 4],2,3)   ans = 1 2 1 2 1 2 3 4 3 4 3 4 1 2 1 2 1 2 3 4 3 4 3 4 Special matrix  eye(n): get an n*n unit matrix zeros(n1, n2): get an n1*n2 all-0 matrix ones(n1, n2): get an n1*n2 all-1 matrix diag(vector): get a diagonal matrix with the contents of the vector vector as the diagonal  1  \u0026gt;\u0026gt; diag(1:2:9)   ans = 1 0 0 0 0 0 3 0 0 0 0 0 5 0 0 0 0 0 7 0 0 0 0 0 9  magic(n): returns an n-by-n matrix constructed from the integers 1 through n^2 with equal row and column sums. The order n must be a scalar greater than or equal to 3 in order to create a valid magic square.  1 2  \u0026gt;\u0026gt; A = magic(5); \u0026gt;\u0026gt; A, sum(A, 1), sum(A, 2)   A = 17 24 1 8 15 23 5 7 14 16 4 6 13 20 22 10 12 19 21 3 11 18 25 2 9 ans = 65 65 65 65 65 ans = 65 65 65 65 65 Indexing matrices normal Matrices in MATLAB are stored in columnar preorder. The index subscript starts from 1 (instead of 0).\nThere are two ways to index a matrix:\n by one-dimensional indexing: A(n) by two-dimensional indexing: A(r,c).  For a general matrix, the indexing order is as follows:\nThus A(6)=A(3,2)=a32. For example:\n1 2  \u0026gt;\u0026gt; A = [1:3; 4:6; 7:9]; \u0026gt;\u0026gt; disp([A(6), A(3,2)])    8 8 colon Recall that you can create an array using a colon like start : increment : end.\n1 2  \u0026gt;\u0026gt; A = [1:100; 101:200]; \u0026gt;\u0026gt; disp(A(1, 50:55))    50 51 52 53 54 55 In particular, you can index all row or column without specifying the start and end.\n1  \u0026gt;\u0026gt; disp(A(:, 50:2:60))    50 52 54 56 58 60 150 152 154 156 158 160 To index from the tail to the head of the array, you can use end keyword:\n1  \u0026gt;\u0026gt; disp(A(:, end:-1:95))    100 99 98 97 96 95 200 199 198 197 196 195 another example:\n1 2  \u0026gt;\u0026gt; str1 = \u0026#39;abcdef\u0026#39;; \u0026gt;\u0026gt; disp(str1(end:-1:1))   fedcba or\n1 2  \u0026gt;\u0026gt; a = [1:10]; \u0026gt;\u0026gt; a(end-4:end)   ans = 6 7 8 9 10 And to eliminate some rows or columns, assign [] to the index outcomes:\n1  \u0026gt;\u0026gt; a = reshape([1:9],3,3)   a = 1 4 7 2 5 8 3 6 9 1 2  \u0026gt;\u0026gt; a(:,2) = []; \u0026gt;\u0026gt; a   a = 1 7 2 8 3 9 vectors The index of a matrix can be one or two vectors, indicating that all rows or columns in the vector are selected. In particular:\nMethod 1:\n A([1 3 5]): [A(1) A(3) A(5)] A([1 2; 3 4]): [A(1) A(2); A(3) A(4)]  Method 2:\n A([1,2], :) A([1 3], [1 2])  1 2 3 4 5  \u0026gt;\u0026gt; a1 = A([1 3 5]); \u0026gt;\u0026gt; a2 = A([1 2; 3 4]); \u0026gt;\u0026gt; a3 = A([1,2], :); \u0026gt;\u0026gt; a4 = A([1 3], [1 2]); \u0026gt;\u0026gt; A, a1, a2, a3, a4   A = 1 2 3 4 5 6 7 8 9 a1 = 1 7 5 a2 = 1 4 7 2 a3 = 1 2 3 4 5 6 a4 = 1 2 7 8 conditions Given an array A, command A==0 will return a boolean matrix report the location of the matched entries, and we can index these entries by A(A==0):\n1 2  \u0026gt;\u0026gt; A = randn(5); \u0026gt;\u0026gt; A(5:4:21) = (1:5).^2   A = -0.4126 0.0618 -0.6114 -1.3612 25.0000 -0.9718 0.3739 -0.1093 16.0000 -1.8784 0.3624 1.9314 9.0000 -0.1601 0.9327 2.6409 4.0000 -0.5230 0.5490 1.2820 1.0000 1.0013 -0.3426 0.4724 0.0896 1  \u0026gt;\u0026gt; A ~= round(A)   ans = 55 logical  1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 and finally\n1  \u0026gt;\u0026gt; A(A~=round(A)) = 999   A = 999 999 999 999 25 999 999 999 16 999 999 999 9 999 999 999 4 999 999 999 1 999 999 999 999 find()   k = find(X) returns a vector containing the linear indices of each nonzero element in array X.\nIf X is a vector, then find returns a vector with the same orientation as X.\nIf X is a multidimensional array, then find returns a column vector of the linear indices of the result.\n  k = find(X,n) returns the first n indices corresponding to the nonzero elements in X.\n  k = find(X,n,direction), where direction is 'last', finds the last n indices corresponding to nonzero elements in X. The default for direction is 'first', which finds the first n indices corresponding to nonzero elements.\n  1 2  \u0026gt;\u0026gt; A = randn(5); \u0026gt;\u0026gt; A(A \u0026gt; 0) = 0   A = 0 -0.2378 -1.0304 0 0 -0.0398 0 0 -1.6064 -0.7434 -0.1206 -0.3935 -0.5933 0 -0.0771 0 -0.7744 -0.3850 0 -0.0366 -0.9600 -0.1752 -0.6025 -0.0856 -0.8984 1  \u0026gt;\u0026gt; find(A==0)   ans = 1 4 7 12 16 18 19 21   [row,col] = find(___) returns the row and column subscripts of each nonzero element in array X using any of the input arguments in previous syntaxes.\n  [row,col,v] = find(___) also returns vector v, which contains the nonzero elements of X.\n  1  \u0026gt;\u0026gt; [row,col] = find(A==0); disp([row col])    1 1 4 1 2 2 2 3 1 4 3 4 4 4 1 5 Operations on matrices Operators for manipulating matrices Let A, B are matrixes, a is a scalar:\nEach matrix element performs operations with scalar a:\n A + a A - a A .* a (= A * a) A ./ a (= A / a) A .^ a (= A ^ a)  Matrix and matrix corresponding position elements perform operations:\n A + B A - B A .* B A ./ B  Matrix operations:\n A * B A / B: A * inv(B) A \\ B: inv(A) * B A ^ a: for example, A^2 = A * A A': transfer  Functions for manipulating matrices Aggregation: dim represents the aggregation dimension, dim=1 indicates aggregate by columns, dim=2 means by rows. The default value of dim is 1.\n max(A, [], dim): returns the maximal element along dimension dim min(A, [], dim): returns the minimal element along dimension dim sum(A, dim): return the sum along the dimensions dim mean(A, dim): return the mean along the dimensions dim   1 2 3 4 5  \u0026gt;\u0026gt; A = [1 9 3; 4 6 1; 6 7 4]; \u0026gt;\u0026gt; maxc = max(A, [], 1); \u0026gt;\u0026gt; maxr = max(A, [], 2); \u0026gt;\u0026gt; maxv = max(maxc); \u0026gt;\u0026gt; A, maxc, maxr, maxv   A = 1 9 3 4 6 1 6 7 4 maxc = 6 9 4 maxr = 9 6 7 maxv = 9  sort(A, dim, direction): sorts the elements of A in direction (\u0026lsquo;ascend\u0026rsquo; (default) or \u0026lsquo;descend\u0026rsquo;) order by dimension dim (=1 by default). sortrows(A, column, direction): sorts the rows of a matrix in direction order based on the elements in the column (=1 by default) column.  1 2 3 4  \u0026gt;\u0026gt; s1 = sort(A); \u0026gt;\u0026gt; s2 = sort(A, 2, \u0026#39;descend\u0026#39;); \u0026gt;\u0026gt; s3 = sortrows(A, 2, \u0026#39;descend\u0026#39;); \u0026gt;\u0026gt; A, s1, s2, s3   A = 1 9 3 4 6 1 6 7 4 s1 = 1 6 1 4 7 3 6 9 4 s2 = 9 3 1 6 4 1 7 6 4 s3 = 1 9 3 6 7 4 4 6 1 \n size(A, dim): returns the shape of matrix A in dim dimension. If dim is not specified, the shape of the entire matrix is returned. find(A, n): returns the first n indices corresponding to the nonzero elements in X.  ","date":"2021-01-12T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/matlab-in-engineering-i-array-operations/","title":"MATLAB in Engineering I (Array Operations)"},{"content":"Overview 123\n1 2 3 4 5 6 7  df.head(x) df.info() df.describe() df.shape df.values df.columns df.index   1 2  df.sort_values(by=\u0026#34;col\u0026#34;).tail(n) # n largest, e.t. df.nlargest(n, \u0026#34;col\u0026#34;)   1 2 3  df.sample(n) # Return n random samples df.sample(frac=0.3) # Return a random sample of 30 percent df.sample(frac=1) # Shuffle   1 2  df.corr() df.diff()   Sort 1 2 3 4 5 6 7 8 9 10 11  df.sort_values( by=\u0026#34;col_name\u0026#34;, axis=0, ascending=True, inplace=False ) df.sort_values( by=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;, \u0026#34;col3\u0026#34;], ascending=[True, False, True] # sort col1 by ascending; sort col2 by descending; sort col3 by ascending )   1 2 3 4  df.sort_index( level=[\u0026#34;outter_index\u0026#34;, \u0026#34;inner_index\u0026#34;], ascending=False )   Set index 1 2 3 4 5 6 7  df.set_index(\u0026#34;col\u0026#34;) df.set_index(\u0026#34;outter_index\u0026#34;, \u0026#34;inner_index\u0026#34;) df.reset_index() df.reset_index(drop=False) df = pd.read_csv(\u0026#34;xxxx.csv\u0026#34;, index_col=[\u0026#34;col\u0026#34;])   Subset and Slice  By row 1 2 3 4 5 6 7 8 9 10 11  # Subset by row df[df[\u0026#34;col\u0026#34;] \u0026gt; 0] df[ (df.col1 \u0026gt; 0) \u0026amp; (df.col2 \u0026gt;0) ] df[ df.col.isin([\u0026#34;xxx\u0026#34;, \u0026#34;yyy\u0026#34;]) ]   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # With loc method ## The first input to .loc[] accessor allows you to select the rows it returns. df.loc[ [\u0026#34;outer_index1\u0026#34;, \u0026#34;outer_index_2\u0026#34;] ] df.loc[ [ (\u0026#34;outer_index_1\u0026#34;, \u0026#34;inner_index_1\u0026#34;), (\u0026#34;outer_index_2\u0026#34;, \u0026#34;inner_index_2\u0026#34;), (\u0026#34;outer_index_3\u0026#34;, \u0026#34;inner_index_3\u0026#34;), ] ] df.loc[ \u0026#34;outer_index_i\u0026#34; : \u0026#34;outer_index_j\u0026#34; ] df.loc[ (\u0026#34;outer_index_i\u0026#34;, \u0026#34;inner_index_i\u0026#34;): (\u0026#34;outer_index_j\u0026#34;, \u0026#34;inner_index_j\u0026#34;) ]   1 2  # With iloc method df[df[\u0026#34;y\u0026#34;]==+1][np.dot(df[df[\u0026#34;y\u0026#34;]==+1].iloc[:,0:df.shape[1]-1], w) \u0026lt;= 0] # subset by row and then by row   1 2 3 4 5 6 7  # With query method  # df.query(\u0026#34;SQL Statements string\u0026#34;) df.query( \u0026#34;col1 == \u0026#39;xxx\u0026#39; or (col1 == \u0026#39;yyy\u0026#39; and col2 \u0026lt; 90)\u0026#34; ) df.query(\u0026#39;date \u0026gt;= \u0026#34;1991-01-01\u0026#34;\u0026#39;) # date could be DateFrame index name.   By col 1 2 3 4 5 6 7  # Subset \u0026amp; Slice by col df[\u0026#34;col\u0026#34;] df.col df[df.col == df.col.max()].col # equal to df.col.max() df[ [\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;] ]   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # With loc method df_sort_id.loc[ [ (\u0026#34;outer_index_1\u0026#34;, \u0026#34;inner_index_1\u0026#34;), (\u0026#34;outer_index_2\u0026#34;, \u0026#34;inner_index_2\u0026#34;), (\u0026#34;outer_index_3\u0026#34;, \u0026#34;inner_index_3\u0026#34;), ] , [\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;] ] df_sort_id.loc[ (\u0026#34;Julia\u0026#34;, \u0026#34;1\u0026#34;): (\u0026#34;Julia\u0026#34;, \u0026#34;3\u0026#34;) , \u0026#34;col_i\u0026#34;: \u0026#34;col_j\u0026#34; ] df.loc[ df.col1 == \u0026#34;xxx\u0026#34;, # the first input of .loc select the rows \u0026#34;col2\u0026#34; # the second input of .loc select the columns ]   1 2 3  # Add a col df[\u0026#34;newcol\u0026#34;] = scalar or list df.insert(0, \u0026#34;newcol\u0026#34;, scalar or list) # 0 is the location of the new column   Aggregate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  df.col.mean() df[[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;]].mean() def pct30(col): return col.quantile(0.3) df.col.agg(min) df[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;].agg([min, max, np.mean, np.median, pct30]) df[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;].agg([cumsum, cummin, cumprod]) df.agg( { \u0026#34;col1\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;col2\u0026#34;: \u0026#34;mean\u0026#34;, # There is no need to call the numpy module at this time. \u0026#34;col3\u0026#34;: \u0026#34;median\u0026#34;, \u0026#34;col4\u0026#34;: \u0026#34;count\u0026#34; } )   1 2  df.mean(axis=\u0026#34;columns\u0026#34;) # calculate for every rows df.mean(axis=\u0026#34;index\u0026#34;) # calculate for every columns   Duplicate Value 1 2 3 4 5 6 7 8 9  df.drop_duplicates( subset=\u0026#34;col\u0026#34; ) df.drop_duplicates( subset=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;] ) df.col.unique()   Counting 1 2 3 4 5 6 7 8  df.col.value_counts( sort=True, normalize=False ) df.col.value_counts() # e.t. df.groupby(\u0026#34;col\u0026#34;).count()   1 2 3 4 5  # count missing value of each columns df.isna() df.isna().any() df.isna().sum() df.isna().sum().plot(kind=\u0026#34;bar\u0026#34;)   Group Groupby 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Groupby method df.groupby( \u0026#34;col\u0026#34; ) df.groupby( [\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;] ) # e.t. df.groupby( [\u0026#34;col1\u0026#34;, df[\u0026#34;col2\u0026#34;]] ) df.groupby(\u0026#34;col1\u0026#34;).agg(\u0026#34;sum\u0026#34;)[\u0026#34;col2\u0026#34;].plot(kind=\u0026#34;bar\u0026#34;) # e.t. df.groupby(\u0026#34;col1\u0026#34;).agg(\u0026#34;sum\u0026#34;).plot(kind=\u0026#34;bar\u0026#34;, y=\u0026#34;col2\u0026#34;)   Argument level = 0 separates rows with the same (outest) index.\n1 2 3 4  ind = [1, 2, 3, 1, 2, 3, 4] s = pd.Series([1, 2, 3, 10, 20, 30, 40], index=ind) sg = s.groupby(level=0) print(sg.first(), sg.last())   1 1 2 2 3 3 4 40 dtype: int64 1 10 2 20 3 30 4 40 dtype: int64 1 2 3 4 5  df1_2_3 = pd.concat( [df1, df2, df3], keys=[\u0026#39;7Jul\u0026#39;, \u0026#39;8Aug\u0026#39;, \u0026#39;9Sep\u0026#39;] ) print(df1_2_3.sample(5))    iid cid invoice_date total bill_ctry 8Aug 18 220 6 2011-08-22 5.94 Czech Republic 7Jul 28 371 8 2013-07-02 1.98 Belgium 8Aug 28 378 46 2013-08-02 1.98 Ireland 9Sep 27 386 27 2013-09-02 1.98 USA 23 309 22 2012-09-26 3.98 USA # \u0026lt;-- there is a Multi indexs 1 2 3 4  sumdf = df_1_2_3.groupby(level=0).agg(sum) print(sumdf) sumdf.plot(kind=\u0026#34;bar\u0026#34;) plt.show()    iid cid total 7Jul 7385 961 190.1 8Aug 7630 1170 198.1 9Sep 7417 961 196.2 Pivot 1 2 3 4 5 6 7 8 9 10 11 12 13  # Pivot table df.pivot_table( values=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;], index=[\u0026#34;col3\u0026#34;, \u0026#34;col4\u0026#34;], columns=[\u0026#34;col5\u0026#34;, \u0026#34;col6\u0026#34;], aggfunc=\u0026#39;mean\u0026#39;, fill_value=None, margins=False, dropna=True, margins_name=\u0026#39;All\u0026#39; ) df.pivot_table(\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;, \u0026#34;col3\u0026#34;) # values = col1, index = col2, columns = col3   Melt melt method will unpivot a table from wide to long format. This is often a much more computer-friendly format. Imagine a situation where you have merged many columns, making your table very wide. The merge() method can then be used to reshape that table into a more computer-friendly format.\n Recall that we call each row of the table an observation and each column a variable. The melt method integrates certain columns (name) into a variable column and the values of those columns into the value column.\n The first input argument to the method is id_vars. These are columns to be used as identifier variables. We can also think of them as columns in our original dataset that we do not want to change.\n1 2 3 4 5 6 7 8 9  df = pd.DataFrame( { \u0026#34;a\u0026#34; : np.random.randint(0,5, 20), \u0026#34;b\u0026#34; : np.random.randint(0,5, 20), \u0026#34;c\u0026#34; : np.random.randint(0,5, 20), \u0026#34;d\u0026#34; : np.random.randint(0,5, 20), } ) print(df.melt(id_vars=\u0026#34;a\u0026#34;).sample(5))    a variable value 56 4 d 2 17 3 b 3 27 4 c 2 47 4 d 2 10 0 b 1 1  print(df.melt(id_vars=[\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;]).sample(5))    a b variable value 12 3 1 c 1 19 2 1 c 0 4 3 1 c 1 32 3 1 d 1 5 3 1 c 3 The argument value_vars with the melt() will allow us to control which columns are unpivoted. If you set this argument, the others columns (except for id_vars and value_vars) will be ignored.\n1  print(df.melt(id_vars=\u0026#34;a\u0026#34;, value_vars=[\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;]).sample(10))    a variable value 2 3 b 2 32 3 c 1 18 2 b 3 16 4 b 2 3 2 b 2 37 3 c 1 21 2 c 1 19 2 b 1 34 4 c 0 28 3 c 1 The var_name argument will allow us to set the name of the variable column in the output. Similarly, the value_name argument will allow us to set the name of the value column in the output.\n1  print(df.melt(id_vars=\u0026#34;a\u0026#34;, value_vars=[\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;], var_name=\u0026#34;bc\u0026#34;, value_name=\u0026#34;va\u0026#34;).sample(5))    a bc va 13 2 b 3 7 4 b 4 36 4 c 1 38 2 c 0 9 3 b 3 Plot Histograms We can create a histogram of the variable by selecting the column and calling .hist().\nWe can adjust the number of bars, or bins, using the bins argument. Plots can also be layered on top of one another. We can use plt.legend(), passing in a list of labels, and then call show. We can use hist\u0026rsquo;s alpha argument, which takes a number. 0 means completely transparent that is, invisible, and 1 means completely opaque.\n1 2 3 4 5 6 7  # Histogram of conventional avg_price  avo[avo[\u0026#34;type\u0026#34;] == \u0026#34;conventional\u0026#34;][\u0026#34;avg_price\u0026#34;].hist(bins= 30, alpha=0.5) # Histogram of organic avg_price avo[avo[\u0026#34;type\u0026#34;] == \u0026#34;organic\u0026#34;][\u0026#34;avg_price\u0026#34;].hist(bins=30, alpha=0.5) # Add a legend plt.legend([\u0026#34;conventional\u0026#34;, \u0026#34;organic\u0026#34;])   You can also plot histograms for multiple variables at a time as follows: df[[\u0026quot;col1\u0026quot;, \u0026quot;col2\u0026quot;]].hist()\n1  avo[[\u0026#34;avg_price\u0026#34;, \u0026#34;nb_sold\u0026#34;]].hist()   Bar plots Bar plots can reveal relationships between a categorical variable and a numeric variable, like lang and rand1. we group by lang, select the rand1 column, and take the mean, giving us the average rand1 values of each lang.\nNow we can create a bar plot from the mean using the .plot() method, setting kind equal to bar. To add a title to our plot, we can use the title argument of the plot method. We may want to rotate the x-axis labels to make the text easier to read. This can be done by passing an angle in degrees with the rot argument\n1 2 3 4 5  # Get the total number of avocados sold of each size nb_sold_by_size = avo.groupby(by=\u0026#34;size\u0026#34;)[\u0026#34;nb_sold\u0026#34;].sum() # Create a bar plot of the number of avocados sold by size nb_sold_by_size.plot(kind=\u0026#34;bar\u0026#34;, rot=45)   You can set the color of the graph by setting the color argument. Color accepts scalar or lists.\n1 2 3  is_recession = [\u0026#39;r\u0026#39; if s==\u0026#39;recession\u0026#39; else \u0026#39;g\u0026#39; for s in gdp_recession[\u0026#39;econ_status\u0026#39;]] gdp_recession.plot(kind=\u0026#34;bar\u0026#34;, y=\u0026#34;gdp\u0026#34;, x=\u0026#34;date\u0026#34;, color=is_recession, rot=90) plt.show()   Line plots Line plots are great for visualizing changes in numeric variables over time. We can use the .plot() method again, but this time, we pass in three arguments: date as x, rand2 as y, and kind equals line\n1 2 3 4 5  # Get the total number of avocados sold on each date nb_sold_by_date = avo.groupby(by=\u0026#34;date\u0026#34;)[\u0026#34;nb_sold\u0026#34;].sum() # \u0026lt;-- note groupby date # Create a line plot of the number of avocados sold by date nb_sold_by_date.plot(kind=\u0026#34;line\u0026#34;, rot=45)   You can also draw multiple lines by setting a list of columns for the y argument:\n1 2  df.plot(y=[\u0026#34;close_jpm\u0026#34;, \u0026#34;close_wells\u0026#34;, \u0026#34;close_bac\u0026#34;]) plt.show()   Scatter plots Scatter plots are great for visualizing relationships between two numeric variables. we call the .plot() method with x equal to rand2, y equal to rand3, and kind equal to scatter.\n1 2 3 4 5 6 7  # Scatter plot of nb_sold vs avg_price with title avo.plot( x=\u0026#34;nb_sold\u0026#34;, y=\u0026#34;avg_price\u0026#34;, kind=\u0026#34;scatter\u0026#34;, title=\u0026#34;Number of avocados sold vs. average price\u0026#34; )   10. Merge  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # Inner Join df1_df2 = df1.merge(df2, on=\u0026#34;col\u0026#34;, suffixes=(\u0026#34;_l\u0026#34;, \u0026#34;_r\u0026#34;)) df1_df2 = df1.merge(df2, on=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;]) df1_df2 = df1.merge(df2, left_on=\u0026#34;col1\u0026#34;, right_on=\u0026#34;col2\u0026#34;) # The columns to be merged in the two tables have different names # Left Join df1_2 = df1.merge(df2, on=\u0026#34;col\u0026#34;, how=\u0026#34;left\u0026#34;) # default argument how is \u0026#34;inner\u0026#34; # Right Join df1_2 = df1.merge(df2, on=\u0026#34;col\u0026#34;, how=\u0026#34;right\u0026#34;) # Outer Join df1_2 = df1.merge(df2, on=\u0026#34;col\u0026#34;, how=\u0026#34;outer\u0026#34;) # Symmetric Difference df1_2 = df1.merge(df2, on=\u0026#34;col\u0026#34;, how=\u0026#34;outer\u0026#34;) m = (df1_2[\u0026#34;col1_x\u0026#34;].isna()) | (df1_2[\u0026#34;col1_y\u0026#34;].isna()) df1_2[m] # Self Join df_1s = df1.merge(df1, left_on=\u0026#34;c1\u0026#34;, right_on=\u0026#34;c3\u0026#34;, suffixes=(\u0026#34;_x\u0026#34;, \u0026#34;_y\u0026#34;))   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # Merge on Index df1.merge(df2, left_index=True, right_index=True) df1.merge(df2, on=\u0026#34;index_name\u0026#34;) df1.merge( df2, left_on=\u0026#34;left_index_name\u0026#34;, left_index=True, right_on=\u0026#34;right_index_name\u0026#34;, right_index=True, ) df1.merge( df2, left_on=\u0026#34;col_name\u0026#34;, right_on=\u0026#34;right_index_name\u0026#34;, right_index=True, )   1 2 3 4 5  # Merge multiple DataFrame df1_df2_df3 = df1.merge(df2, on=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;]).merge(df3, on=\u0026#34;col3\u0026#34;, suffixes=(\u0026#34;_2\u0026#34;, \u0026#34;_3\u0026#34;)) # e.t. (equal to) df1_df2_df3 = df1.merge(df2, on=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;])\\ # \u0026lt;-- note this backslash .merge(df3, on=\u0026#34;col3\u0026#34;, suffixes=(\u0026#34;_2\u0026#34;, \u0026#34;_3\u0026#34;))   1 2 3 4 5 6 7 8  # Merge with other operation df_1_2_3 = df_1.merge(df_2, on=\u0026#34;col1\u0026#34;).merge(df_3, on=\u0026#34;col2\u0026#34;) df_1_2_3.groupby(\u0026#34;col3\u0026#34;).agg({\u0026#39;col4\u0026#39;: \u0026#39;sum\u0026#39;}).plot(kind=\u0026#34;bar\u0026#34;) # e.t. df_1_2_3.groupby(\u0026#34;col3\u0026#34;).col4.agg(sum).plot(kind=\u0026#34;bar\u0026#34;) # e.t. df_1_2_3.groupby(\u0026#34;col3\u0026#34;).agg(sum).plot(kind=\u0026#34;bar\u0026#34;, y=\u0026#34;col4\u0026#34;)   11. Advanced Merge Semi Join A semi join filters the left table down to those observations that have a match in the right table. It is similar to an inner join where only the intersection between the tables is returned, but unlike an inner join, only the columns from the left table are shown. Finally, no duplicate rows from the left table are returned, even if there is a one-to-many relationship.\nSemi Join Three Steps:\n Merge the left and right tables on key column using an inner join; Search if the key column in the left table is in the merged tables using the .isin() method creating a Boolean Series; Subset the rows of left table.  1 2 3 4 5 6 7 8  # 1. df_merged = df1.merge(df2, on=\u0026#34;col1\u0026#34;) # 2. bs = df1[\u0026#34;col2\u0026#34;].isin(df_merged[\u0026#34;col2\u0026#34;]) # bs means boolean series # 3. df3 = df1[bs]   Anti Join An anti join returns the observations in the left table that do not have a matching observation in the right table. It also only returns the columns from the left table.\nWith indicator set to True, the merge method adds a column called \u0026ldquo;_merge\u0026rdquo; to the output. This column tells the source of each row. For example, if rows found a match in both tables, the _merge column shows both; if rows can only be found in the left table, the _merge column shows left_only. Obviously, right_only will not appear in a left join.\n1 2 3 4 5 6 7 8 9 10 11  # 1. df_merge = df1.merge(df2, on=\u0026#34;col1\u0026#34;, how=\u0026#34;left\u0026#34;, indicater=True) # 2. ls = df_merge.loc[ df_merge[\u0026#34;_merge\u0026#34;]==\u0026#34;left_only\u0026#34;, \u0026#34;col1\u0026#34; ] # 3. df3 = df1[df1[\u0026#34;col1\u0026#34;].isin(ls)]   Concatenate So far, we have only discussed how to merge two tables, which mainly grows them horizontally. Now we concern how to grow them vertically.\n1 2 3  pd.concat( [df1, df2, df3] )   Notice the column headers are the same. The result is a vertically combined table. Notice each table\u0026rsquo;s index value was retained.\n1 2 3 4  pd.concat( [df1, df2, df3], ignore_index=True )   If the index contains no valuable information, then we can ignore it in the concat method by setting ignore_index to True. The result is that the index will go from 0 to n-1.\nNow, suppose we wanted to associate specific keys with each of the pieces of our three original tables. We can provide a list of labels to the keys argument. Make sure that ignore_index argument is False, since you can\u0026rsquo;t add a key and ignore the index at the same time. This results in a table with a multi-index, with the label on the first level.\n1 2 3 4 5  pd.concat( [df1, df2, df3], ignore_index=False, keys=[\u0026#34;df1\u0026#34;, \u0026#34;df2\u0026#34;, \u0026#34;df3\u0026#34;] )   When we need to combine tables that have different column names, the concat method by default will include all of the columns in the different tables it\u0026rsquo;s combining. If we only want the matching columns between tables, we set the join argument to \u0026ldquo;inner\u0026rdquo;. Its default value is equal to \u0026ldquo;outer\u0026rdquo;\n1 2 3 4  pd.concat( [df1, df2, df3], join=\u0026#34;inner\u0026#34; )   Append is a simplified concat method. It supports the ignore_index argument. However, it does not support keys or join. Join is always set to outer.\n1 2 3 4  df1.append( [df2, df3], ignore_index=True )   Verifying Integrity Both the merge and concat methods have special features that allow us to verify the structure of our data. The validate and verify_integrity arguments of the merge and concat methods respectively will allow us to verify the data.\nWhen merging two tables, we might expect the tables to have a one-to-one relationship. However, one of the columns we are merging on may have a duplicated value, which will turn the relationship into a one-to-many.\nIf we provide the validate argument one of these key strings:\n one_to_one one_to_many many_to_one many_to_many  it will validate the relationship between the two tables. For example, if we specify we want a one-to-one relationship, but it turns out the relationship is not one-to-one, then an error is raised.\n1 2 3 4 5 6 7 8 9 10 11 12 13  df1 = pd.DataFrame( { \u0026#34;a\u0026#34;: np.arange(5), \u0026#34;b\u0026#34;: list(\u0026#34;adfgh\u0026#34;) } ) df2 = pd.DataFrame( { \u0026#34;a\u0026#34;: [0, 1, 2, 3, 3], \u0026#34;b\u0026#34;: list(\u0026#34;tvdsv\u0026#34;) } ) df1.merge(df2, on=\u0026#34;a\u0026#34;, validate=\u0026#34;one_to_one\u0026#34;)   MergeError: Merge keys are not unique in right dataset; not a one-to-one merge When concatenating tables vertically, we might unintentionally create duplicate records if a record exists in both tables.\n Notice, the recode represent index, not columns values.\n concat method has the argument verify_integrity, which by default is False. However, if set to True, it will check if there are duplicate values in the index and raise an error if there are. It will only check the index values and not the columns.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  df1 = pd.DataFrame( { \u0026#34;a\u0026#34;: np.random.rand(5), \u0026#34;b\u0026#34;: np.random.rand(5), \u0026#34;c\u0026#34;: np.random.rand(5), }, index=np.arange(5) ) df2 = pd.DataFrame( { \u0026#34;a\u0026#34;: np.random.rand(5), \u0026#34;b\u0026#34;: np.random.rand(5), \u0026#34;c\u0026#34;: np.random.rand(5), }, index=np.arange(4,9) ) print(df1, \u0026#39;\\n\u0026#39;, df2)    a b c 0 0.131960 0.559110 0.803359 1 0.862007 0.349336 0.594149 2 0.805661 0.313103 0.937367 3 0.893554 0.633904 0.475403 4 0.381487 0.096156 0.219184 a b c 4 0.469545 0.142471 0.906317 5 0.035282 0.481996 0.207780 6 0.241841 0.355997 0.599351 7 0.684347 0.592812 0.833834 8 0.036764 0.347563 0.895287 1 2 3 4  pd.concat( [df1, df2], verify_integrity=True )   ValueError: Indexes have overlapping values: Int64Index([4], dtype='int64') .merge_ordered() The merge_ordered method are similar to the standard merge method with an outer join, but the results are sorted. The sorted results make this a useful method for ordered or time-series data.\nIt has many of the same arguments we have already covered with the merge method:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  pd.merge_ordered( df1, df2, # \u0026lt;-- no bracket  on=\u0026#34;col\u0026#34;, suffixes(\u0026#34;_l\u0026#34;, \u0026#34;_r\u0026#34;) ) pd.merge_ordered( df1, df2, left_on=\u0026#34;col1\u0026#34;, right_on=\u0026#34;col2\u0026#34;, how=\u0026#34;left\u0026#34;) pd.merge_ordered( df1, df2, on=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;] # Note the difference with `on=[\u0026#34;col2\u0026#34;, \u0026#34;col1\u0026#34;]` )   # on=[\u0026quot;date\u0026quot;, \u0026quot;country\u0026quot;] date country gdp series_code_x pop series_code_y 0 1990-01-01 Australia 158051.132 NYGDPMKTPSAKD 17065100 SP.POP.TOTL 1 1990-01-01 Sweden 79837.846 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 2 1990-04-01 Australia 158263.582 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 3 1990-04-01 Sweden 80582.286 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 4 1990-07-01 Australia 157329.279 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 5 1990-07-01 Sweden 79974.360 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 6 1990-09-01 Australia 158240.678 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 7 1990-09-01 Sweden 80106.497 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 8 1991-01-01 Australia 156195.954 NYGDPMKTPSAKD 17284000 SP.POP.TOTL 9 1991-01-01 Sweden 79524.242 NYGDPMKTPSAKD 8617375 SP.POP.TOTL # on=[\u0026quot;country\u0026quot;, \u0026quot;date\u0026quot;] date country gdp series_code_x pop series_code_y 0 1990-01-01 Australia 158051.132 NYGDPMKTPSAKD 17065100 SP.POP.TOTL 1 1990-04-01 Australia 158263.582 NYGDPMKTPSAKD 17065100 SP.POP.TOTL 2 1990-07-01 Australia 157329.279 NYGDPMKTPSAKD 17065100 SP.POP.TOTL 3 1990-09-01 Australia 158240.678 NYGDPMKTPSAKD 17065100 SP.POP.TOTL 4 1991-01-01 Australia 156195.954 NYGDPMKTPSAKD 17284000 SP.POP.TOTL 5 1991-04-01 Australia 155989.033 NYGDPMKTPSAKD 17284000 SP.POP.TOTL 6 1991-07-01 Australia 156635.858 NYGDPMKTPSAKD 17284000 SP.POP.TOTL 7 1991-09-01 Australia 156744.057 NYGDPMKTPSAKD 17284000 SP.POP.TOTL 8 1992-01-01 Australia 157916.081 NYGDPMKTPSAKD 17495000 SP.POP.TOTL 9 1992-04-01 Australia 159047.827 NYGDPMKTPSAKD 17495000 SP.POP.TOTL We can fill in the missing data by setting the fill_method argument to \u0026ldquo;ffill\u0026rdquo; for forward fill. It will interpolate missing data by filling the missing values with the previous value (the upper row).\n1 2 3 4 5  pd.merge_ordered( df1, df2, on=\u0026#34;col\u0026#34;, fill_method=\u0026#34;ffill\u0026#34; )   .merge_asof() The merge_asof() method is similar to an ordered left join. It has similar features as merge_ordered(). However, unlike an ordered left join, merge_asof() will match on the nearest value columns rather than equal values.\nFor each row in the left DataFrame, merge_asof() select the last row in the right DataFrame where the value for the key is less than or equal the value for the left key. This brings up an important point - whatever columns you merge on must be sorted. (left and right key columns must be sorted)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  df1 = pd.DataFrame( { \u0026#34;a\u0026#34;: [1, 3, 4, 7, 5], \u0026#34;b\u0026#34;:np.arange(5) } ) df2 = pd.DataFrame( { \u0026#34;a\u0026#34;: [1, 2, 3, 8, 5], \u0026#34;b\u0026#34;:np.arange(5) } ) df_1_2 = pd.merge_asof( df1.sort_values(by=\u0026#34;a\u0026#34;), df2.sort_values(by=\u0026#34;a\u0026#34;), on=\u0026#34;a\u0026#34; ) print(df1.sort_values(by=\u0026#34;a\u0026#34;),\u0026#39;\\n\u0026#39;, df2.sort_values(by=\u0026#34;a\u0026#34;), \u0026#39;\\n\u0026#39;,df_1_2)    a b 0 1 0 1 3 1 2 4 2 4 5 4 3 7 3 a b 0 1 0 1 2 1 2 3 2 4 5 4 3 8 3 a b_x b_y 0 1 0 0 1 3 1 2 2 4 2 2 3 5 4 4 4 7 3 4 Setting the direction argument as \u0026ldquo;forward\u0026rdquo; will change the behavior of the method to select the row in the right table. The default value for the direction argument is \u0026ldquo;backward\u0026rdquo;. Setting the direction argument as nearest to match rows with the nearest times in either direction.\n1 2 3 4 5  pd.merge_asof( df1, df2, on=\u0026#34;col1\u0026#34;, direction=\u0026#34;forward\u0026#34; )   ","date":"2020-08-14T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/feature-engineering-cleaning/","title":"Feature Engineering (Cleaning)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Graphical Exploratory Data Analysis Explore the data means organizing and plotting the data, and maybe computing a few numerical summaries about them. This idea is known as exploratory data analysis, or EDA, and was developed by John Tukey. He wrote a book entitled Exploratory Data Analysis in 1977 where he laid out the principles. In that book, he said, \u0026ldquo;Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone.\u0026rdquo;\nScatter We have talked about the relevant topics in the previous blog, so I won\u0026rsquo;t repeat it.\n1 2 3 4 5 6 7 8 9 10 11  plt.plot( versicolor_petal_length, # x-axis versicolor_petal_width, # y-axis marker=\u0026#39;.\u0026#39;, linestyle=\u0026#39;none\u0026#39; ) plt.xlabel(\u0026#34;versicolor_petal_length\u0026#34;) plt.ylabel(\u0026#34;versicolor_petal_width\u0026#34;) plt.show()   Histogram 1 2 3 4 5 6 7 8 9 10 11 12  # Import plotting modules import matplotlib.pyplot as plt import seaborn as sns # Set default Seaborn style sns.set() # Plot histogram of versicolor petal lengths plt.hist(versicolor_petal_length) # Show histogram plt.show()   1 2 3 4 5 6 7 8 9  # Plot histogram of versicolor petal lengths plt.hist(versicolor_petal_length) # Label axes plt.xlabel(\u0026#34;petal length (cm)\u0026#34;) plt.ylabel(\u0026#34;count\u0026#34;) # Show histogram plt.show()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # Import numpy import numpy as np # Compute number of data points: n_data n_data = len(versicolor_petal_length) # Number of bins is the square root of number of data points: n_bins n_bins = np.sqrt(n_data) # Convert number of bins to integer: n_bins n_bins = int(n_bins) # Plot the histogram _ = plt.hist(versicolor_petal_length, bins=n_bins) # Label axes _ = plt.xlabel(\u0026#39;petal length (cm)\u0026#39;) _ = plt.ylabel(\u0026#39;count\u0026#39;) # Show histogram print(n_bins) plt.show()   7 Bee swarm plots 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  print(df.head()) # Create bee swarm plot with Seaborn\u0026#39;s default settings sns.swarmplot( x=\u0026#34;species\u0026#34;, y=\u0026#34;petal length (cm)\u0026#34;, data=df ) # Label the axes plt.xlabel(\u0026#34;species\u0026#34;) plt.ylabel(\u0026#34;petal lengths\u0026#34;) # Show the plot plt.show()    sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa ECDFs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  def ecdf(data): \u0026#34;\u0026#34;\u0026#34;Compute ECDF for a one-dimensional array of measurements.\u0026#34;\u0026#34;\u0026#34; # Number of data points: n n = len(data) # x-data for the ECDF: x x = np.sort(data) # y-data for the ECDF: y y = np.arange(1, n+1) / n return x, y # Compute ECDF for versicolor data: x_vers, y_vers x_vers, y_vers = ecdf(versicolor_petal_length) # Generate plot plt.plot( x_vers, y_vers, marker=\u0026#34;.\u0026#34;, linestyle = \u0026#39;none\u0026#39; ) # Label the axes plt.xlabel(\u0026#34;versicolor petal length\u0026#34;) plt.ylabel(\u0026#34;ECDF\u0026#34;) # Display the plot plt.show()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # Compute ECDFs x_set, y_set = ecdf(setosa_petal_length) x_vers, y_vers = ecdf(versicolor_petal_length) x_virg, y_virg = ecdf(virginica_petal_length) # Plot all ECDFs on the same plot plt.plot(x_set, y_set, marker=\u0026#39;.\u0026#39;, linestyle=\u0026#39;none\u0026#39;) plt.plot(x_vers, y_vers, marker=\u0026#39;.\u0026#39;, linestyle=\u0026#39;none\u0026#39;) plt.plot(x_virg, y_virg, marker=\u0026#39;.\u0026#39;, linestyle=\u0026#39;none\u0026#39;) # Annotate the plot plt.legend((\u0026#39;setosa\u0026#39;, \u0026#39;versicolor\u0026#39;, \u0026#39;virginica\u0026#39;), loc=\u0026#39;lower right\u0026#39;) plt.xlabel(\u0026#39;petal length (cm)\u0026#39;) plt.ylabel(\u0026#39;ECDF\u0026#39;) # Display the plot plt.show()   Quantitative Exploratory Data Analysis The sample mean and median 1 2 3 4 5 6 7  # Compute the mean: mean_length_vers mean_length_vers = np.mean(versicolor_petal_length) median_length_vers = np.median(versicolor_petal_length) # Print the result with some nice formatting print(\u0026#39;mean of I. versicolor:\u0026#39;, mean_length_vers, \u0026#39;cm\u0026#39;) print(\u0026#39;median of I. versicolor:\u0026#39;, median_length_vers, \u0026#39;cm\u0026#39;)   mean of I. versicolor: 4.26 cm median of I. versicolor: 4.35 cm Percentiles, outliers, and box plots The median is a special name for the 50th percentile;\nthat is 50% of the data are less than the median. Similarly, the 25th percentile is the value of the data point that is greater than 25% of the sorted data, and so on for any other percentile we want. Percentiles are useful summary statistics, and can be computed by\n1 2 3  np.percentile(data, 25) # or np.percentile(data, [25, 50, 75])   and it returns the data that match those percentiles.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # Specify array of percentiles: percentiles percentiles = np.array([2.5, 25, 50, 75, 97.5]) # Compute percentiles: ptiles_vers ptiles_vers = np.percentile(versicolor_petal_length, percentiles) # Print the result print(ptiles_vers) # Plot the ECDF plt.plot(x_vers, y_vers, \u0026#39;.\u0026#39;) plt.xlabel(\u0026#39;petal length (cm)\u0026#39;) plt.ylabel(\u0026#39;ECDF\u0026#39;) # Overlay percentiles as red diamonds. plt.plot( ptiles_vers, percentiles/100, marker=\u0026#39;D\u0026#39;, color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;none\u0026#39; ) # Show the plot plt.show()   [3.3 4. 4.35 4.6 4.9775] Box plots were invented by John Tukey himself to display some of the salient features of a data set based on percentiles. The center of box is the median, which is the 50th percentile of the data. The edges of the boxes are the 25th and 75th percentile. The total height of the box contains the middle 50% of the data, and is called the interquartile range, or IQR. The whiskers extend a distance of 1-point-5 times the IQR, or to the extent of the data, whichever is more extreme. Finally, any points outside of the whiskers are plotted as individual points, which we often demarcate as outliers. While there is no single definition for an outlier, being more than 2 IQRs away from the median is a common criterion.\n It is important to remember that an outlier is not necessarily an erroneous data point. We should not assume an outlier is erroneous unless you have some known reason to.\n 1 2 3 4 5 6 7 8 9 10 11 12 13  # Create box plot with Seaborn\u0026#39;s default settings sns.boxplot( x=\u0026#34;species\u0026#34;, y=\u0026#34;petal length (cm)\u0026#34;, data=df ) # Label the axes plt.xlabel(\u0026#34;species\u0026#34;) plt.ylabel(\u0026#34;petal length (cm)\u0026#34;) # Show the plot plt.show()   Variance and standard deviation To calculate the variance and standard deviation, call np.var() and np.std() respectively.\n1 2 3 4 5 6 7 8 9  # Calculate var by hand differences = versicolor_petal_length - np.mean(versicolor_petal_length) diff_sq = differences ** 2 variance_explicit = np.mean(diff_sq) # Calculate var by np.var variance_np = np.var(versicolor_petal_length) print(variance_explicit, variance_np)   0.21640000000000004 0.21640000000000004 1 2 3 4  variance = np.var(versicolor_petal_length) print(np.sqrt(variance)) print(np.std(versicolor_petal_length))   0.4651881339845203 0.4651881339845203 Covariance and the Correlation coefficient Tor calculate the Cov and Corr, call np.cav() and np.corrcoef() respectively.\n1 2 3 4 5 6 7 8 9 10 11  # Compute the covariance matrix: covariance_matrix covariance_matrix = np.cov( versicolor_petal_length, versicolor_petal_width ) # Extract covariance of length and width petal_cov = covariance_matrix[0,1] print(covariance_matrix) print(petal_cov)   [[0.22081633 0.07310204] [0.07310204 0.03910612]] 0.07310204081632653 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def pearson_r(x, y): \u0026#34;\u0026#34;\u0026#34;Compute Pearson correlation coefficient between two arrays.\u0026#34;\u0026#34;\u0026#34; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x,y) # equal to: corr_mat = np.cov(x,y)[0,1]/(np.std(x) * np.std(y)) # Return entry [0,1] return corr_mat[0,1] r = pearson_r( versicolor_petal_length, versicolor_petal_width ) print(r)   0.7866680885228169 Random Variables Seed The pseudorandom number generator works by starting with an integer, called a seed, and then generates random numbers in succession. The same seed gives the same sequence of random numbers, hence the name, \u0026ldquo;psuedorandom number generation\u0026rdquo;. So, if you want to have reproducible code, it is a good idea to seed the random number generator using the np dot random dot seed function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def test(): print(np.random.random(4)) # 0-1 uniform distr., U(0,1) print(np.random.random(4)) print(np.random.randint(2,10)) # integer uniform distr print(np.random.rand(5)) # 0-1 uniform distr., U(0,1) print(np.random.randn(np.random.randint(2,10))) # standard normal dist., N(0,1) print(np.random.uniform(np.random.randint(2,4), np.random.randint(5,8))) # a-b uniform distr., U(a,b) print(\u0026#34;----------------\u0026#34;) np.random.seed(42) test() np.random.seed(42) test() test()   [0.37454012 0.95071431 0.73199394 0.59865848] [0.15601864 0.15599452 0.05808361 0.86617615] 5 [0.14286682 0.65088847 0.05641158 0.72199877 0.93855271] [-0.46341769 -0.46572975 0.24196227] 3.582458280396084 ---------------- [0.37454012 0.95071431 0.73199394 0.59865848] [0.15601864 0.15599452 0.05808361 0.86617615] 5 [0.14286682 0.65088847 0.05641158 0.72199877 0.93855271] [-0.46341769 -0.46572975 0.24196227] 3.582458280396084 ---------------- [0.61185289 0.13949386 0.29214465 0.36636184] [0.45606998 0.78517596 0.19967378 0.51423444] 9 [0.46676289 0.85994041 0.68030754 0.45049925 0.01326496] [-1.91328024 -0.60063869] 5.733054075301833  np.random.random((a,b)) and np.random.rand(3,5) generate both 3*5 U(0,1) random matrix. More see here.\n 1 2 3 4 5 6 7 8 9 10 11 12 13  # Seed the random number generator np.random.seed(42) # Initialize random numbers: random_numbers random_numbers = np.empty(100000) # Generate random numbers by looping over range(100000) for i in range(100000): random_numbers[i] = np.random.random() # Plot a histogram plt.hist(random_numbers) plt.show()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  def perform_bernoulli_trials(n, p): \u0026#34;\u0026#34;\u0026#34;Perform n Bernoulli trials with success probability p and return number of successes.\u0026#34;\u0026#34;\u0026#34; # Initialize number of successes: n_success n_success = 0 # Perform trials for i in range(n): # Choose random number between zero and one: random_number random_number = np.random.random() # If less than p, it\u0026#39;s a success so add one to n_success if random_number \u0026lt; p: n_success += 1 return n_success # Seed random number generator np.random.seed(42) # Initialize the number of defaults: n_defaults n_defaults = np.empty(1000) # Compute the number of defaults for i in range(1000): n_defaults[i] = perform_bernoulli_trials(100, 0.05) # Plot the histogram with default number of bins;  # `normed=True` keyword argument make the height of the bars of the histogram indicate the probability. _ = plt.hist(n_defaults, normed=True) _ = plt.xlabel(\u0026#39;number of defaults out of 100 loans\u0026#39;) _ = plt.ylabel(\u0026#39;probability\u0026#39;) # Show the plot plt.show()   1 2 3 4 5 6 7 8 9 10  # Compute ECDF: x, y x, y = ecdf(n_defaults) plt.plot(x, y, marker = \u0026#39;.\u0026#39;, linestyle = \u0026#39;none\u0026#39;) plt.show() # Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money n_lose_money = np.sum(n_defaults \u0026gt;= 10) # Compute and print probability of losing money print(\u0026#39;Probability of losing money =\u0026#39;, n_lose_money / len(n_defaults))   Binomial distribution Draw samples out of the Binomial distribution using np.random.binomial(n, p, size).\n n: times of random trials for each binomial random variable p: probability of positive size: size of the sample (number of random variable)  1 2 3 4 5 6 7 8 9 10 11  # Take 10,000 samples out of the binomial distribution: n_defaults n_defaults = np.random.binomial(n=100, p=.05, size=10000) # Compute CDF: x, y x, y = ecdf(n_defaults) # Plot the CDF plt.plot(x,y) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.show()   To plot the PMF of the Binomial distribution as a histogram, set up the edges of the bins to pass to plt.hist() via the bins keyword argument. We want the bins centered on the integers. So, the edges of the bins should be -0.5, 0.5, 1.5, 2.5, \u0026hellip; up to max(n_defaults) + 1.5. we can generate an array like this using np.arange() and then subtracting 0.5 from the array.\n1 2 3 4 5 6 7 8  # Compute bin edges: bins bins = np.arange(0, max(n_defaults) + 1.5) - 0.5 # Generate histogram _ = plt.hist(n_defaults, normed=True, bins=bins) _ = plt.xlabel(\u0026#39;number of defaults out of 100 loans\u0026#39;) _ = plt.ylabel(\u0026#39;PMF\u0026#39;) plt.show()   Poisson distribution Draw samples out of the Poisson distribution using np.random.poisson(lam, size).\n lam: expect of r.v. size: size of the sample  1 2 3 4 5 6 7 8 9 10 11 12  # Draw 10,000 samples out of Poisson distribution: samples_poisson samples_poisson = np.random.poisson(10, 10000) print(\u0026#39;Poisson: \u0026#39;, np.mean(samples_poisson), np.std(samples_poisson)) # Draw 10,000 samples for each n,p pair: samples_binomial n = [20, 100, 1000] p = [0.5, 0.1, 0.01] for i in range(3): samples_binomial = np.random.binomial(n[i], p[i], 10000) print(\u0026#39;n =\u0026#39;, n[i], \u0026#39;Binom:\u0026#39;, np.mean(samples_binomial), np.std(samples_binomial))   Poisson: 10.0186 3.144813832327758 n = 20 Binom: 9.9637 2.2163443572694206 n = 100 Binom: 9.9947 3.0135812433050484 n = 1000 Binom: 9.9985 3.139378561116833 1 2 3 4 5 6 7 8 9 10 11  # Draw 10,000 samples out of Poisson distribution: n_nohitters n_nohitters = np.random.poisson(251/115, 10000) # Compute number of samples that are seven or greater: n_large n_large = np.sum(n_nohitters \u0026gt;= 7) # Compute probability of getting seven or more: p_large p_large = n_large / len(n_nohitters) # Print the result print(\u0026#39;Probability of seven or more no-hitters:\u0026#39;, p_large)   Probability of seven or more no-hitters: 0.0067 Continuous Variables Normal distribution Draw samples out of the normal distribution using np.random.normal(loc, scale, size).\n loc: expect of r.v. scale: std of r.v. size: size of the sample  1 2 3 4 5 6 7 8 9 10 11 12 13  # Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10 samples_std1 = np.random.normal(20, 1, size=100000) samples_std3 = np.random.normal(20, 3, size=100000) samples_std10 = np.random.normal(20, 10, size=100000) # Plot PDFs plt.hist(samples_std1, normed=True, histtype=\u0026#39;step\u0026#39;, bins=100) plt.hist(samples_std3, normed=True, histtype=\u0026#39;step\u0026#39;, bins=100) plt.hist(samples_std10, normed=True, histtype=\u0026#39;step\u0026#39;, bins=100) plt.legend((\u0026#39;std = 1\u0026#39;, \u0026#39;std = 3\u0026#39;, \u0026#39;std = 10\u0026#39;)) plt.ylim(-0.01, 0.42) plt.show()   1 2 3 4 5 6 7 8 9 10 11  # Generate CDFs x_std1, y_std1 = ecdf(samples_std1) x_std3, y_std3 = ecdf(samples_std3) x_std10, y_std10 = ecdf(samples_std10) # Plot CDFs plt.plot(x_std1, y_std1, marker=\u0026#39;.\u0026#39;, linestyle=\u0026#39;none\u0026#39;) plt.plot(x_std3, y_std3, marker=\u0026#39;.\u0026#39;, linestyle=\u0026#39;none\u0026#39;) plt.plot(x_std10, y_std10, marker=\u0026#39;.\u0026#39;, linestyle=\u0026#39;none\u0026#39;) plt.legend((\u0026#39;std = 1\u0026#39;, \u0026#39;std = 3\u0026#39;, \u0026#39;std = 10\u0026#39;), loc=\u0026#39;lower right\u0026#39;) plt.show()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # Compute mean and standard deviation: mu, sigma mu = np.mean(belmont_no_outliers) sigma = np.std(belmont_no_outliers) # Sample out of a normal distribution with this mu and sigma: samples samples = np.random.normal(mu, sigma, size=10000) # Get the CDF of the samples and of the data x_theor, y_theor = ecdf(samples) x, y = ecdf(belmont_no_outliers) # Plot the CDFs and show the plot _ = plt.plot(x_theor, y_theor) _ = plt.plot(x, y, marker=\u0026#39;.\u0026#39;, linestyle=\u0026#39;none\u0026#39;) _ = plt.xlabel(\u0026#39;Belmont winning time (sec.)\u0026#39;) _ = plt.ylabel(\u0026#39;CDF\u0026#39;) plt.show()   Exponential distribution Draw samples out of the Exponential distribution using np.random.exponential(scale, size).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def successive_poisson(tau1, tau2, size=1): \u0026#34;\u0026#34;\u0026#34;Create the sum of two exp dist. r.v.\u0026#34;\u0026#34;\u0026#34; t1 = np.random.exponential(tau1, size) t2 = np.random.exponential(tau2, size) return t1 + t2 # Draw samples  waiting_times = successive_poisson(764, 715, 100000) # Plot the PDFs plt.hist(waiting_times, bins=100, normed=True, histtype=\u0026#39;step\u0026#39;) # Label axes plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.show()   ","date":"2020-07-01T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/statistics-by-python-i/","title":"Statistics by Python I"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Importing data from the Internet Importing flat files from the web ","date":"2020-05-01T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/importing-data-in-python-ii/","title":"Importing Data in Python II"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n 1. Importing data from flat files We\u0026rsquo;ll discuss how to import data from 3 kind of data sources\n flat files such as dot txts and dot csvs; files native to other software such as Excel spreadsheets, Stata, SAS and MATLAB files; relational databases such as SQLite \u0026amp; PostgreSQL.  1.1 Importing flat files using open() To check out any plain text file, you can use Pythons basic open() function to open a connection to the file. To do so, you pass the filename to the function open() and also pass it the argument mode=\u0026quot;r\u0026quot;, which makes sure that we can only read it (we wouldn\u0026rsquo;t want to accidentally write to it!), assign the text from the file to a variable by applying the method read() to the connection to the file. After you do this, make sure that you close the connection to the file using the close() method.\n1 2 3 4 5  file = open(\u0026#34;moby_dick.txt\u0026#34;, mode=\u0026#34;r\u0026#34;) print(file.read()[0:200]) print(file.closed) # Check whether the file is closed file.close() print(file.closed)   CHAPTER 1. Loomings. Call me Ishmael. Some years ago--never mind how long precisely--having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail abou False True  If you wanted to open a file in order to write to it, you would pass it the argument mode=\u0026quot;w\u0026quot;.\n using a with statement. This allows you to create a context in which you can execute commands with the file open. Once out of this context, the file will be closed, for this reason, it is called a Context Manager.\n1 2 3 4 5 6  with open(\u0026#39;moby_dick.txt\u0026#39;) as file: print(file.readline()) print(file.readline()) print(file.readline()) print(file.readline()) print(file.readline())   CHAPTER 1. Loomings. Call me Ishmael. Some years ago--never mind how long precisely--having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of 1.2 Importing flat files using NumPy If all the data are numerical, you can use the package numpy to import the data as a numpy array with the NumPy functions loadtxt( ). We call loadtxt() and pass it the filename as the first argument, along with the delimiter as the 2nd argument. Note that the default delimiter is any white space so well usually need to specify it explicitly.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  import numpy as np file = \u0026#39;digits.csv\u0026#39; digits = np.loadtxt(file, delimiter=\u0026#34;,\u0026#34;) print(type(digits), digits.shape) fig, ax = plt.subplots(5,5) for i in range(5): for j in range(5): im = digits[np.random.randint(0,100), 1:] im_sq = np.reshape(im, (28, 28)) ax[i][j].imshow( im_sq, cmap=\u0026#39;Greys\u0026#39;, interpolation=\u0026#39;nearest\u0026#39; ) plt.show()   \u0026lt;class 'numpy.ndarray'\u0026gt; (100, 785) You can skip the first row by calling loadtxt with the argument skiprows=1; You can select the 1st and 3rd columns of the data, by setting usecols=[0,2]. You can also import different datatypes into NumPy arrays: for example, setting the argument dtype=\u0026quot;str\u0026quot; will ensure that all entries are imported as strings.\n1 2 3 4 5  import numpy as np file = \u0026#39;digits_header.txt\u0026#39; data = np.loadtxt(file, delimiter=\u0026#34;\\t\u0026#34;, skiprows=1, usecols=[0,2]) print(data[:5])   [[1. 0.] [0. 0.] [1. 0.] [4. 0.] [0. 0.]] Notice that loadtxt tends to break down when we have mixed datatypes, for example, columns consisting of floats and columns consisting of strings. And you should import in different datatypes respectively.\n1 2 3 4 5 6 7 8 9 10 11  file = \u0026#39;seaslug.txt\u0026#39; data = np.loadtxt(file, delimiter=\u0026#39;\\t\u0026#39;, dtype=str) print(data[0]) data_float = np.loadtxt(file, delimiter=\u0026#34;\\t\u0026#34;, dtype=float, skiprows=1) print(data_float[9]) plt.scatter(data_float[:, 0], data_float[:, 1]) plt.xlabel(\u0026#39;time (min.)\u0026#39;) plt.ylabel(\u0026#39;percentage of larvae\u0026#39;) plt.show()   ['Time' 'Percent'] [0. 0.357] Much of the time you will need to import datasets which have different datatypes in different columns; one column may contain strings and another floats, for example. The function np.loadtxt() will freak at this. There is another function, genfromtxt( ), which can handle such structures. If we pass dtype=None to it, it will figure out what types each column should be.\n1 2 3 4 5 6 7  data = np.genfromtxt( \u0026#39;titanic.csv\u0026#39;, delimiter=\u0026#39;,\u0026#39;, names=True, dtype=None ) data[0:10]   array([( 1, 0, 3, b'male', 22., 1, 0, b'A/5 21171', 7.25 , b'', b'S'), ( 2, 1, 1, b'female', 38., 1, 0, b'PC 17599', 71.2833, b'C85', b'C'), ( 3, 1, 3, b'female', 26., 0, 0, b'STON/O2. 3101282', 7.925 , b'', b'S'), ( 4, 1, 1, b'female', 35., 1, 0, b'113803', 53.1 , b'C123', b'S'), ( 5, 0, 3, b'male', 35., 0, 0, b'373450', 8.05 , b'', b'S'), ( 6, 0, 3, b'male', nan, 0, 0, b'330877', 8.4583, b'', b'Q'), ( 7, 0, 1, b'male', 54., 0, 0, b'17463', 51.8625, b'E46', b'S'), ( 8, 0, 3, b'male', 2., 3, 1, b'349909', 21.075 , b'', b'S'), ( 9, 1, 3, b'female', 27., 0, 2, b'347742', 11.1333, b'', b'S'), (10, 1, 2, b'female', 14., 1, 0, b'237736', 30.0708, b'', b'C')], dtype=[('PassengerId', '\u0026lt;i8'), ('Survived', '\u0026lt;i8'), ('Pclass', '\u0026lt;i8'), ('Sex', 'S6'), ('Age', '\u0026lt;f8'), ('SibSp', '\u0026lt;i8'), ('Parch', '\u0026lt;i8'), ('Ticket', 'S18'), ('Fare', '\u0026lt;f8'), ('Cabin', 'S15'), ('Embarked', 'S1')]) In addition to genfromtxt, the numpy module provides several convenience functions derived from genfromtxt. These functions work the same way as the original, but they have different default values.\n  recfromtxt Returns a standard numpy.recarray (if usemask=False) or a MaskedRecords array (if usemaske=True). The default dtype is dtype=None, meaning that the types of each column will be automatically determined.\n  recfromcsv Like recfromtxt, but with a default delimiter=\u0026quot;,\u0026quot;.\n  1 2 3  file = \u0026#39;titanic.csv\u0026#39; d= np.recfromcsv(file) print(d[:3])   [(1, 0, 3, b'male', 22., 1, 0, b'A/5 21171', 7.25 , b'', b'S') (2, 1, 1, b'female', 38., 1, 0, b'PC 17599', 71.2833, b'C85', b'C') (3, 1, 3, b'female', 26., 0, 0, b'STON/O2. 3101282', 7.925 , b'', b'S')] 1.3 Importing flat files using Pandas If we wish to import a CSV in the most basic case all we need to do is to call the function read_csv( ) and supply it with a single argument, the name of the file, it will return a DataFrame object of the file.\n As Hadley Wickham tweeted, \u0026ldquo;A matrix has rows and columns. A data frame has observations and variables.\u0026rdquo;\n 1 2 3 4 5  file = \u0026#39;digits.csv\u0026#39; # Read the first 5 rows of the file except header into a DataFrame: data data=pd.read_csv(file, nrows=5, header=None) data_array=np.array(data) print(type(data_array))   \u0026lt;class 'numpy.ndarray'\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  import matplotlib.pyplot as plt file = \u0026#39;titanic_corrupt.txt\u0026#39; data = pd.read_csv( file, sep=\u0026#34;\\t\u0026#34;, # \u0026lt;-- the pandas version of delim comment=\u0026#34;#\u0026#34;, # \u0026lt;-- indicates remainder of line should not be parsed. na_values=\u0026#34;Nothing\u0026#34; # \u0026lt;-- takes str (in this case, \u0026#34;Nothing\u0026#34;) to recognize as NA/NaN ) data_origin = pd.read_csv(file) print(data.head()) print(data_origin.head()) pd.DataFrame.hist(data[[\u0026#39;Age\u0026#39;]]) plt.xlabel(\u0026#39;Age (years)\u0026#39;) plt.ylabel(\u0026#39;count\u0026#39;) plt.show()   PassengerId Survived Pclass Sex Age ... Parch Ticket Fare Cabin Embarked 0 1 0 3 male 22.0 ... 0 A/5 21171 7.250 NaN S 1 2 1 1 female 38.0 ... 0 PC 17599 NaN NaN NaN # \u0026lt;-- Notice here 2 3 1 3 female 26.0 ... 0 STON/O2. 3101282 7.925 NaN S 3 4 1 1 female 35.0 ... 0 113803 53.100 C123 S 4 5 0 3 male 35.0 ... 0 373450 8.050 NaN S PassengerId\\tSurvived\\tPclass\\tSex\\tAge\\tSibSp\\tParch\\tTicket\\tFare\\tCabin\\tEmbarked 0 1\\t0\\t3\\tmale\\t22.0\\t1\\t0\\tA/5 21171\\t7.25\\tNo... 1 2\\t1\\t1\\tfemale\\t38.0\\t1\\t0\\tPC 17599#to\\t71.2... # \u0026lt;-- Notice here \u0026quot;#\u0026quot; 2 3\\t1\\t3\\tfemale\\t26.0\\t0\\t0\\tSTON/O2. 3101282\\... 3 4\\t1\\t1\\tfemale\\t35.0\\t1\\t0\\t113803\\t53.1\\tC12... 4 5\\t0\\t3\\tmale\\t35.0\\t0\\t0\\t373450\\t8.05\\t\\tS 2. Importing data from other file types 2.1 Importing Pickle and Excel file Pickle Files is a file type native to Python, which can store data-types in python such as dictionaries, tuples, lists etc. It can serialize objects so that they can be saved into a file and loaded again later. when opening such a file, you can specify that it is read only and is a binary file, by passing the string \u0026lsquo;rb\u0026rsquo; as the second argument of open.\n1 2 3 4 5 6 7 8 9 10 11  d = dict(name=\u0026#39;Bob\u0026#39;, age=20, score=88) print(d) f = open(\u0026#39;dump.pkl\u0026#39;, \u0026#39;wb\u0026#39;) # wirte pickle.dump(d,f) f.close() # read g = open(\u0026#39;./dump.pkl\u0026#39;, \u0026#39;rb\u0026#39;) e=pickle.load(g) g.close() print(e)   {'name': 'Bob', 'age': 20, 'score': 88} {'name': 'Bob', 'age': 20, 'score': 88} 1  ! hexdump -x ./dump.pkl   0000000 0480 2495 0000 0000 0000 7d00 2894 048c 0000010 616e 656d 8c94 4203 626f 8c94 6103 6567 0000020 4b94 8c14 7305 6f63 6572 4b94 7558 002e 000002f An Excel working book generally consists of a number of working sheets, and each working sheets has a mass of working cells. There are many ways (like openpyxl and pyexcel module) to import Excel files and we\u0026rsquo;ll use pandas to do so because it produces dataframes natively.\nWe can use the function Excelfile to assign an Excel file to a variable data. As an Excel file consists of sheets, the first thing to do is figure out what the sheets are. This is straightforward with the data.sheet_names attribute.\n1 2  data = pd.ExcelFile(\u0026#34;/Users/wanghaoming/Documents/gpa.xlsx\u0026#34;) data.sheet_names   ['Sheet1', 'Sheet2', 'Sheet3', 'Sheet4', 'Sheet5', 'Sheet6'] To then load a particular sheet as a dataframe, we need only apply the method parse() to the object data with a single argument, which is either the name as a string or the index as a float of the sheet.\n1 2 3 4 5 6  df6 = data.parse( sheet_name=5, usecols=\u0026#34;A:F\u0026#34;, nrows=12 ) print(df6)       .1 .1 .1 0  3 97  4 93 1  4 96  4 93 2  4 96  5 93 3  4 96  4 93 4  3 95  4 92 5  4 95  4 91 6  4 94  5 91 7  4 94  4 91 8  4 94  3 90 9  3 94  4 90 10  4 94  4 90 11  4 93  4 80  pd.ExcelFile.parse() is equivalent to pd.read_excel method.\n 2.2 Importing SAS/Stata files The most common SAS files have the extension .sas7bdat and .sas7bcat, which are dataset files and catalog files respectively. We can import the former as DataFrames using the function SAS7BDAT (upper case) from the package sas7bdat (lower case). We can bind the variable file to a connection to the file \u0026lsquo;xxx.sas7bdat\u0026rsquo; in a context manager. Within this context, We can assign to a variable df_sas the result of applying method to_data_frame to file.\n1 2 3 4 5 6 7 8 9  from sas7bdat import SAS7BDAT with SAS7BDAT(\u0026#39;sales.sas7bdat\u0026#39;) as file: df_sas = file.to_data_frame() print(df_sas.head()) pd.DataFrame.hist(df_sas[[\u0026#39;P\u0026#39;]]) plt.ylabel(\u0026#39;count\u0026#39;) plt.show()    The other way to import sas (xport and sas7bdat) file is pd.read_sas.\n Stata files have extension .dta and we can import them using pandas. We don\u0026rsquo;t even need to initialize a context manager in this case. We merely pass the filename to the function pd.read_stata and assign it to a variable.\n1 2 3 4 5 6 7 8 9  import pandas as pd df = pd.read_stata(\u0026#39;disarea.dta\u0026#39;) print(df.head()) pd.DataFrame.hist(df[[\u0026#39;disa10\u0026#39;]]) plt.xlabel(\u0026#39;Extent of disease\u0026#39;) plt.ylabel(\u0026#39;Number of countries\u0026#39;) plt.show()   2.3 Importing HDF5 files  \u0026ldquo;In the Python world, consensus is rapidly converging on Hierarchical Data Format version 5, or \u0026lsquo;HDF5,\u0026rsquo; as the standard mechanism for storing large quantities of numerical data.\u0026rdquo; \u0026mdash;- 2013 O\u0026rsquo;Reilly book Python and HDF5 by Andrew Collette,\n Intro to HDF5 i. Group \u0026amp; Dataset The Hierarchical Data Format version 5 (HDF5) format can be thought of as a file system contained and described within one single file. Think about the files and folders stored on your computer. You might have a data directory with some temperature data for multiple field sites. These temperature data are collected every minute and summarized on an hourly, daily and weekly basis. Within one HDF5 file, you can store a similar set of data organized in the same way that you might organize files and folders on your computer. However in a HDF5 file, what we call \u0026ldquo;directories\u0026rdquo; or \u0026ldquo;folders\u0026rdquo; on our computers, are called groups and what we call files on our computer are called datasets.\n Group: A folder like element within an HDF5 file that might contain other groups or datasets within it. Dataset: The actual data contained within the HDF5 file. Datasets are often (but don\u0026rsquo;t have to be) stored within groups in the file.  ii. Metadata HDF5 format is self describing. This means that each file, group and dataset can have associated metadata that describes exactly what the data are. One key benefit of having metadata that are attached to each file, group and dataset, is that this facilitates automation without the need for a separate (and additional) metadata document. Using a programming language, like R or Python, we can grab information from the metadata that are already associated with the dataset, and which we might need to process the dataset.\niii. Compressed \u0026amp; Subsetting The HDF5 format is a compressed format. The size of all data contained within HDF5 is optimized which makes the overall file size smaller. Even when compressed, however, HDF5 files often contain big data and can thus still be quite large. A powerful attribute of HDF5 is data slicing, by which a particular subsets of a dataset can be extracted for processing. This means that the entire dataset doesn\u0026rsquo;t have to be read into memory (RAM); very helpful in allowing us to more efficiently work with very large (gigabytes or more) datasets!\niv. Heterogeneous HDF5 files can store many different types of data within in the same file. For example, one group may contain a set of datasets to contain integer (numeric) and text (string) data. Or, one dataset can contain heterogeneous data types (e.g., both text and numeric data even images in one dataset).\nv. Summary Points  Self-Describing: The datasets with an HDF5 file are self describing. This allows us to efficiently extract metadata without needing an additional metadata document. Supporta Heterogeneous Data: Different types of datasets can be contained within one HDF5 file. Supports Large, Complex Data: HDF5 is a compressed format that is designed to support large, heterogeneous, and complex datasets. Supports Data Slicing: \u0026ldquo;Data slicing\u0026rdquo;, or extracting portions of the dataset as needed for analysis, means large files don\u0026rsquo;t need to be completely read into the computers memory or RAM.  h5py package We import the package h5py and then import the file using h5py.File function, remembering to use \u0026lsquo;r\u0026rsquo; in order to specify read only.\nYou can explore HDF5\u0026rsquo;s hierarchical structure as you would that of a Python dictionary using the method keys. Each of keys is an HDF group. You can think of these groups as directories too.\n1 2 3 4 5 6 7 8  import numpy as np import h5py file = \u0026#34;LIGO_data.hdf5\u0026#34; data = h5py.File(file, \u0026#34;r\u0026#34;) print(type(data)) for key in data.keys(): print(key)   \u0026lt;class 'h5py._hl.files.File'\u0026gt; meta quality strain 1 2 3 4 5 6 7 8 9 10 11 12 13  group = data[\u0026#39;strain\u0026#39;] for key in group.keys(): print(key) strain = data[\u0026#39;strain\u0026#39;][\u0026#39;Strain\u0026#39;].value num_samples = 10000 time = np.arange(0, 1, 1/num_samples) plt.plot(time, strain[:num_samples]) plt.xlabel(\u0026#39;GPS Time (s)\u0026#39;) plt.ylabel(\u0026#39;strain\u0026#39;) plt.show()   Strain 2.4 Importing MATLAB files The standard library scipy.io has functions loadmat and savemat, which allow us to read and write .mat files, respectively.\nA .mat file is simply a collection of many objects like strings, floats, vectors and arrays. This means when importing a .mat file in Python, we should expect to see a number of different variables and objects.\nWe first import scipy.io and then load the .mat file. Then a dictionary object results returns. How this dictionary relates to a MATLAB workspace is straightforward: the keys of the Python dictionary are the MATLAB variable names and the values of the Python dictionary are the objects that are assigned to the variables.\n1 2 3 4  import scipy.io mat = scipy.io.loadmat(\u0026#34;albeck_gene_expression.mat\u0026#34;) print(type(mat))   \u0026lt;class 'dict'\u0026gt; 1 2 3 4 5 6 7 8 9 10  print(mat.keys()) print(type(mat[\u0026#34;CYratioCyt\u0026#34;])) print(mat[\u0026#34;CYratioCyt\u0026#34;].shape) data = mat[\u0026#39;CYratioCyt\u0026#39;][25, 5:] fig = plt.figure() plt.plot(data) plt.xlabel(\u0026#39;time (min.)\u0026#39;) plt.ylabel(\u0026#39;normalized fluorescence (measure of expression)\u0026#39;) plt.show()   dict_keys(['__header__', '__version__', '__globals__', 'rfpCyt', 'rfpNuc', 'cfpNuc', 'cfpCyt', 'yfpNuc', 'yfpCyt', 'CYratioCyt']) \u0026lt;class 'numpy.ndarray'\u0026gt; (200, 137) 3. Relational databases in Python 3.1 Creating a database engine in Python Intro to Relational database Relational database is a type of database that is based upon the Relational model of data, first described by Ted Codd in the late 1960s.\nA database consists of tables. A table generally represents one entity type. Notice that the table looks a great deal like a dataframe. In a relational database table, each row or record represents an instance of the entity type. Each column represents an attribute of each instance. In this sense, a table is entirely analogous to a dataframe. It is essential that each row contain a unique identifier, known as a primary key, that we can use to explicitly access the row in question.\nThe really cool thing about relational databases is not merely that you have a bunch of tables, but that the tables are linked. Some column in a table corresponds precisely to the primary keys in other tables. This is cool because it means that you don\u0026rsquo;t need to store all the details in one table, it saves an incredible amount of space.\nBuild engine We\u0026rsquo;ll use an SQLite database as an example because SQLite is fast and simple while still containing enough functionality to introduce the necessary concepts of querying a database. There are many packages we could use to access an SQLite database such as sqlite3 and SQLAlchemy. We\u0026rsquo;ll use SQLAlchemy as it works with many other Relational Database Management Systems, such as Postgres and MySQL.\nTo connect to database, we need to import the relevant funtion create_engine from the package sqlalchemy. We then use the function create_engine to fire up an SQL engine that will communicate our queries to the database. The only required argument of create_engine is a string that indicates the type of database you\u0026rsquo;re connecting to and the name of the database.\n1 2 3  from sqlalchemy import create_engine engine = create_engine(\u0026#34;sqlite:///Chinook.sqlite\u0026#34;)   We would like to know the names of the tables it contains. To do this, apply the method table_names to the object engine. This will return a list of the table names.\n1 2 3 4 5  from sqlalchemy import create_engine engine = create_engine(\u0026#34;sqlite:///Chinook.sqlite\u0026#34;) table_names = engine.table_names() print(table_names)   ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track'] 3.2 Querying relational databases in Python The workflow of SQL querying will be as follows.\n import the required packages and functions; create the engine; Create the engine object using the function create_engine(). connect to it; To connect to the database after creating the engine, you create a connection object by applying the method connect( ) to the engine object. query the database ; To query the DB, apply the method execute() to the connection object and pass it the relevant SQL query. The query SELECT * FROM Table_Name, where Table_name is the name of any of the tables in the database, returns all columns of all rows of the Table of interest. save the results to a dataframe; query manipulation creates a sqlalchemy results object. To turn the results object into a dataframe, we apply the method fetchmany(size=None) or fetchall( ) to result object and save it as a dataframe using the pandas function DataFrame(). Don\u0026rsquo;t forget setting dataframe columns name by df.columns = rs.keys() close the connection. To close the connection, execute close() method on connection object.  1 2 3 4 5 6 7 8 9 10 11  from sqlalchemy import create_engine import pandas as pd # step 1. import package engine = create_engine(\u0026#39;sqlite:///Chinook.sqlite\u0026#39;) # step2. create engine con = engine.connect() # step3. create connection rs = con.execute(\u0026#34;select * from Album\u0026#34;) # step4. query df = pd.DataFrame(rs.fetchall()) # step5. save result  df.columns = rs.keys() con.close() # step6. close connection print(df.head())    AlbumId Title ArtistId 0 1 For Those About To Rock We Salute You 1 1 2 Balls to the Wall 2 2 3 Restless and Wild 2 3 4 Let There Be Rock 1 4 5 Big Ones 3 You can also leverage the context manager construct to open a connection, which will save you the trouble of closing the connection.\n1 2 3 4 5 6 7 8  engine = create_engine(\u0026#39;sqlite:///Chinook.sqlite\u0026#39;) with engine.connect() as con: rs = con.execute(\u0026#34;select LastName, Title from Employee\u0026#34;) df = pd.DataFrame(rs.fetchmany(size=3)) df.columns = rs.keys() print(len(df)) print(df.head())   3 LastName Title 0 Adams General Manager 1 Edwards Sales Manager 2 Peacock Sales Support Agent 1 2 3 4 5 6 7 8 9 10 11 12 13  def query_sql(dbtype, dbname, query): engine = create_engine(dbtype+\u0026#34;:///\u0026#34;+dbname) with engine.connect() as con: rs = engine.execute(query) df = pd.DataFrame(rs.fetchall()) df.columns = rs.keys() return df df = query_sql( dbtype=\u0026#34;sqlite\u0026#34;, dbname=\u0026#34;Chinook.sqlite\u0026#34;, query=\u0026#34;select * from Employee where ReportsTo=2.0 order by BirthDate\u0026#34; ) print(df.head())   FirstName Title ReportsTo ... Country PostalCode Phone Fax Email 0 4 Park Margaret Sales Support Agent 2 ... Canada T2P 5G3 +1 (403) 263-4423 +1 (403) 263-4289 margaret@chinookcorp.com 1 5 Johnson Steve Sales Support Agent 2 ... Canada T3B 1Y7 1 (780) 836-9987 1 (780) 836-9543 steve@chinookcorp.com 2 3 Peacock Jane Sales Support Agent 2 ... Canada T2P 5M5 +1 (403) 262-3443 +1 (403) 262-6712 jane@chinookcorp.com 3.3 Querying relational databases directly with pandas You can actually query datebase in 2 line, utilizing the pandas function read_sql_query and passing it 2 arguments. The first argument will be the query you wish to make, the 2nd argument the engine you want to connect to.\n1 2 3 4 5 6  from sqlalchemy import create_engine import pandas as pd engine = create_engine(\u0026#34;sqlite:///Chinook.sqlite\u0026#34;) df = pd.read_sql_query(\u0026#34;select * from Album\u0026#34;, engine) print(df.head())    AlbumId Title ArtistId 0 1 For Those About To Rock We Salute You 1 1 2 Balls to the Wall 2 2 3 Restless and Wild 2 3 4 Let There Be Rock 1 4 5 Big Ones 3 1 2 3 4 5 6 7 8  from sqlalchemy import create_engine import pandas as pd engine = create_engine(\u0026#34;sqlite:///Chinook.sqlite\u0026#34;) query = \u0026#34;select * from Employee where EmployeeId \u0026gt;= 6 order by BirthDate\u0026#34; df = pd.read_sql_query(query, engine) print(df.head())   EmployeeId LastName FirstName Title ReportsTo ... Country PostalCode Phone Fax Email 0 8 Callahan Laura IT Staff 6 ... Canada T1H 1Y8 +1 (403) 467-3351 +1 (403) 467-8772 laura@chinookcorp.com 1 7 King Robert IT Staff 6 ... Canada T1K 5N8 +1 (403) 456-9986 +1 (403) 456-8485 robert@chinookcorp.com 2 6 Mitchell Michael IT Manager 1 ... Canada T3B 0C5 +1 (403) 246-9887 +1 (403) 246-9899 michael@chinookcorp.com Recall SQL inner join syntax:\n1  select*fromtable1innerjointable2ontable1.col1=table2.col2  1 2 3 4 5 6  with engine.connect() as con: rs = con.execute(\u0026#34;select Title, Name from Album inner join Artist on Album.ArtistID=Artist.ArtistID\u0026#34;) df = pd.DataFrame(rs.fetchall()) df.columns = rs.keys() print(df.head())    Title Name 0 For Those About To Rock We Salute You AC/DC 1 Balls to the Wall Accept 2 Restless and Wild Accept 3 Let There Be Rock AC/DC 4 Big Ones Aerosmith 1 2 3 4 5 6  df = pd.read_sql_query( \u0026#34;select * from PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId where Milliseconds \u0026lt; 250000\u0026#34;, engine ) print(df.head())    PlaylistId TrackId TrackId Name AlbumId ... GenreId Composer Milliseconds Bytes UnitPrice 0 1 3390 3390 One and the Same 271 ... 23 None 217732 3559040 0.99 1 1 3392 3392 Until We Fall 271 ... 23 None 230758 3766605 0.99 2 1 3393 3393 Original Fire 271 ... 23 None 218916 3577821 0.99 3 1 3394 3394 Broken City 271 ... 23 None 228366 3728955 0.99 4 1 3395 3395 Somedays 271 ... 23 None 213831 3497176 0.99 ","date":"2020-04-26T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/importing-data-in-python-i/","title":"Importing Data in Python I"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n [toc]\nSeaborn Introduction Distribution plot The pandas library supports simple plotting of data, which is very convenient when data is already likely to be in a pandas DataFrame.\n1 2 3 4  df[\u0026#39;Award_Amount\u0026#39;].plot.hist() plt.show() plt.clf() # \u0026lt;-- Clear out the pandas histogram   Seaborn generally does more statistical analysis on data and can provide more sophisticated insight into the data. (v.s. the seaborn distplot.)\n1 2 3 4  sns.distplot(df[\u0026#39;Award_Amount\u0026#39;]) plt.show() plt.clf()   Now we will customize distribution plots in Seaborn. The Seaborn API supports customizing the type of plot by using different arguments. We can use the same distplot() function to create a standard histogram and customize the number of bins. Trying different combinations of the kde and rug lot can yield important insights.\n1 2 3 4 5  sns.distplot(df[\u0026#39;Award_Amount\u0026#39;], kde=False, # \u0026lt;-- Kernel Density Estimate curve, default True bins=20) plt.show()   There are many functions in Seaborn that build upon each other. The distplot() function we have been discussing relies on using additional Seaborn functions such as the kdeplot() and rugplot(). By understanding this relationship, you can further customize Seaborn plots by passing additional arguments to the underlying functions. For example, we can tell the underlying kde() function to shade the plot by passing the kde_kws dictionary.\n1 2 3 4 5 6  sns.distplot(df[\u0026#39;Award_Amount\u0026#39;], hist=False, rug=True, kde_kws={\u0026#39;shade\u0026#39;:True}) plt.show()   Regression Plots Now we will transition to another basic visualization process by plotting linear regression lines.\nThe regplot() function is the basis function for building regression plots in Seaborn. We explicitly define the x and y variables as well as the source of the data.\n1 2 3 4 5 6  sns.regplot( x=\u0026#34;insurance_losses\u0026#34;, y=\u0026#34;premiums\u0026#34;, data=df ) plt.show()    As mentioned earlier, you can turn off the confidence interval by setting argument ci =False.\n One of the confusing points about Seaborn is that it may seem like there is more than one way to do the same plot. We have looked at distplot() and briefly discussed kde() plots as a building block for the more robust distplot(). In a similar manner, the lower level regplot() and higher level lmplot() are related. You can specify hue or col (row) in lmplot() to visualize category regression plot.\n   lower level func higher level func     kde(), rug() distplot()   countplot(), \u0026hellip; catplot()   scatterplot, \u0026hellip; relplot()   regplot() lmplot()    1 2 3 4 5 6 7 8  sns.lmplot( x=\u0026#34;insurance_losses\u0026#34;, y=\u0026#34;premiums\u0026#34;, data=df, ci=False ) plt.show()   1 2 3 4 5 6  sns.lmplot(data=df, x=\u0026#34;insurance_losses\u0026#34;, y=\u0026#34;premiums\u0026#34;, hue=\u0026#34;Region\u0026#34;) plt.show()   1 2 3 4 5 6  sns.lmplot(data=df, x=\u0026#34;insurance_losses\u0026#34;, y=\u0026#34;premiums\u0026#34;, row=\u0026#34;Region\u0026#34;) plt.show()   Customizing Seaborn Plots Seaborn styles Seaborn has several default built in themes that are more appealing than the default matplotlib styles. In order to set a default theme, use the sns.set() function.\n1 2 3 4  sns.set() df[\u0026#39;fmr_2\u0026#39;].plot.hist() plt.show() plt.clf()   1 2 3 4 5  sns.set_style(\u0026#34;whitegrid\u0026#34;) sns.distplot(df[\u0026#34;fmr_2\u0026#34;]) plt.show() plt.clf()   A common use case is to remove the lines around the axes called spines. Seaborn\u0026rsquo;s despine() function removes one or more of the spines on a chart. The default is to remove the top and right lines but more can be removed by passing arguments to the despine() function.\n1 2 3 4 5 6 7 8  sns.set_style(\u0026#39;white\u0026#39;)= sns.lmplot(data=df, x=\u0026#39;pop2010\u0026#39;, y=\u0026#39;fmr_2\u0026#39;) sns.despine() plt.show() plt.clf()   Colors in Seaborn Since Seaborn is built on top of matplotlib, it is able to interpret and apply matplotlib color codes. By using the familiar sns.set() function with color_codes=True, any matplotlib color codes, like color='g', will be appropriately mapped to the Seaborn palette.\n1 2 3 4  sns.set(color_codes=True) sns.distplot(df[\u0026#39;fmr_3\u0026#39;], color=\u0026#39;m\u0026#39;) plt.show()   You can set a palette of colors that can be cycled through in a plot, which can be helpful when there are many items that must be encoded with color. Seaborn has six default palettes in sns.palettes.SEABORN_PALETTES including deep, muted, pastel, bright, dark, and colorblind. Seaborn offers several convenience functions for working with palettes:\n sns.set_palette() method set current palette. sns.color_palette(p) function return palette p\u0026rsquo;s (R,G,B) tuples (default return the current palette\u0026rsquo;s (R,G,B) tuples). The second parameter specifies how many colors the palette contains. sns.palplot() function display palette with (R,G,B) tuples (in a Jupyter notebook).  1 2 3 4 5 6 7 8  t=0 for p in sns.palettes.SEABORN_PALETTES: t += 1 sns.set_palette(p) print(p) print(sns.color_palette()) if t == 4: break   deep [(0.2980392156862745, 0.4470588235294118, 0.6901960784313725), (0.8666666666666667, 0.5176470588235295, 0.3215686274509804), (0.3333333333333333, 0.6588235294117647, 0.40784313725490196), (0.7686274509803922, 0.3058823529411765, 0.3215686274509804), (0.5058823529411764, 0.4470588235294118, 0.7019607843137254), (0.5764705882352941, 0.47058823529411764, 0.3764705882352941), (0.8549019607843137, 0.5450980392156862, 0.7647058823529411), (0.5490196078431373, 0.5490196078431373, 0.5490196078431373), (0.8, 0.7254901960784313, 0.4549019607843137), (0.39215686274509803, 0.7098039215686275, 0.803921568627451)] deep6 [(0.2980392156862745, 0.4470588235294118, 0.6901960784313725), (0.3333333333333333, 0.6588235294117647, 0.40784313725490196), (0.7686274509803922, 0.3058823529411765, 0.3215686274509804), (0.5058823529411764, 0.4470588235294118, 0.7019607843137254), (0.8, 0.7254901960784313, 0.4549019607843137), (0.39215686274509803, 0.7098039215686275, 0.803921568627451)] muted [(0.2823529411764706, 0.47058823529411764, 0.8156862745098039), (0.9333333333333333, 0.5215686274509804, 0.2901960784313726), (0.41568627450980394, 0.8, 0.39215686274509803), (0.8392156862745098, 0.37254901960784315, 0.37254901960784315), (0.5843137254901961, 0.4235294117647059, 0.7058823529411765), (0.5490196078431373, 0.3803921568627451, 0.23529411764705882), (0.8627450980392157, 0.49411764705882355, 0.7529411764705882), (0.4745098039215686, 0.4745098039215686, 0.4745098039215686), (0.8352941176470589, 0.7333333333333333, 0.403921568627451), (0.5098039215686274, 0.7764705882352941, 0.8862745098039215)] muted6 [(0.2823529411764706, 0.47058823529411764, 0.8156862745098039), (0.41568627450980394, 0.8, 0.39215686274509803), (0.8392156862745098, 0.37254901960784315, 0.37254901960784315), (0.5843137254901961, 0.4235294117647059, 0.7058823529411765), (0.8352941176470589, 0.7333333333333333, 0.403921568627451), (0.5098039215686274, 0.7764705882352941, 0.8862745098039215)] 1 2 3 4 5 6 7 8 9  t=0 for p in sns.palettes.SEABORN_PALETTES: t += 1 sns.set_palette(p) print(p) sns.palplot(sns.color_palette()) plt.show() if t == 4: break   1 2 3 4  sns.palplot( sns.color_palette(\u0026#34;Purples\u0026#34;, 8) ) plt.show()   There are three main types of color palettes.\n Circular color palettes are used for categorical data that is not ordered. Sequential palettes are useful when the data has a consistent range from high to low values. Diverging color palette is best used when both the high and the low values are interesting.  1 2 3 4  cp = [\u0026#34;Paired\u0026#34;, \u0026#34;Blues\u0026#34;, \u0026#34;BrBG\u0026#34;] for i in cp: print(i) print(sns.color_palette(i))   Paired [(0.6509803921568628, 0.807843137254902, 0.8901960784313725), (0.12156862745098039, 0.47058823529411764, 0.7058823529411765), (0.6980392156862745, 0.8745098039215686, 0.5411764705882353), (0.2, 0.6274509803921569, 0.17254901960784313), (0.984313725490196, 0.6039215686274509, 0.6), (0.8901960784313725, 0.10196078431372549, 0.10980392156862745), (0.9921568627450981, 0.7490196078431373, 0.43529411764705883), (1.0, 0.4980392156862745, 0.0), (0.792156862745098, 0.6980392156862745, 0.8392156862745098), (0.41568627450980394, 0.23921568627450981, 0.6039215686274509), (1.0, 1.0, 0.6), (0.6941176470588235, 0.34901960784313724, 0.1568627450980392)] Blues [(0.8584083044982699, 0.9134486735870818, 0.9645674740484429), (0.7309496347558632, 0.8394771241830065, 0.9213225682429834), (0.5356862745098039, 0.746082276047674, 0.8642522106881968), (0.32628988850442137, 0.6186236063052672, 0.802798923490965), (0.16696655132641292, 0.48069204152249134, 0.7291503267973857), (0.044059976931949255, 0.3338869665513264, 0.6244521337946944)] BrBG [(0.6313725490196078, 0.3951557093425605, 0.09573241061130335), (0.8572856593617839, 0.7257977700884274, 0.4471357170319107), (0.9636293733179546, 0.9237985390234525, 0.8185313341022683), (0.8299115724721262, 0.9294886582083814, 0.9152633602460593), (0.4615916955017304, 0.7748558246828146, 0.7299500192233758), (0.0878892733564014, 0.479123414071511, 0.44775086505190315)] 1 2 3 4 5  cp = [\u0026#34;Paired\u0026#34;, \u0026#34;Blues\u0026#34;, \u0026#34;BrBG\u0026#34;] for i in cp: print(i) sns.palplot(sns.color_palette(i)) plt.show()   Customizing with matplotlib Since Seaborn is based on matplotlib, there is a wide variety of options for further modifying your Seaborn plots. By using matplotlib\u0026rsquo;s axes objects, you can customize almost any element of your plot.\nThe most important concept is to add additional code to create the subplots using matplotlib\u0026rsquo;s plt.subplots() functions and pass the resulting axes object to the Seaborn function. Seaborn will then plot the data on the given axes.\n1 2 3 4 5 6 7 8 9 10 11  fig, ax = plt.subplots() sns.set_palette(\u0026#34;Reds\u0026#34;) sns.distplot(df[\u0026#39;fmr_3\u0026#39;], ax=ax) ax.set( xlabel=\u0026#34;3 Bedroom Fair Market Rent\u0026#34;, ylabel=\u0026#34;rate\u0026#34;, xlim=(0,4000), title=\u0026#34;Title\u0026#34; ) plt.show()   You can use ax.axvline() or ax.axhline() to create a line in the plot, and then call ax.legned() to show it.\n1 2 3 4 5 6 7 8 9 10  fig, ax = plt.subplots() sns.distplot(df[\u0026#39;fmr_1\u0026#39;], ax=ax) ax.set(xlabel=\u0026#34;1 Bedroom Fair Market Rent\u0026#34;, xlim=(100,1500), title=\u0026#34;US Rent\u0026#34;) ax.axvline(x=df[\u0026#39;fmr_1\u0026#39;].median(), color=\u0026#39;g\u0026#39;, label=\u0026#39;Median\u0026#39;, linestyle=\u0026#39;--\u0026#39;, linewidth=2) ax.axvline(x=df[\u0026#39;fmr_1\u0026#39;].mean(), color=\u0026#39;r\u0026#39;, label=\u0026#39;Mean\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=2) ax.axhline(y=0.002, color=\u0026#39;b\u0026#39;, label=\u0026#39;y*\u0026#39;, linestyle=\u0026#39;:\u0026#39;, linewidth=2) ax.legend() plt.show()   1 2 3 4 5 6 7 8  fig, ax = plt.subplots() sns.set(color_codes=True) sns.distplot(df[\u0026#39;fmr_1\u0026#39;], ax=ax, color=\u0026#34;r\u0026#34;) sns.distplot(df[\u0026#39;fmr_2\u0026#39;], ax=ax, color=\u0026#34;c\u0026#34;) ax.set(xlabel=\u0026#34;1 Bedroom Fair Market Rent\u0026#34;, xlim=(100,1500)) plt.show()   1 2 3 4 5 6 7 8 9  fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, sharey=True) # \u0026lt;-- Make the Y-axis of the two subgraphs consistent sns.distplot(df[\u0026#39;fmr_1\u0026#39;], ax=ax0) ax0.set(xlabel=\u0026#34;1 Bedroom Fair Market Rent\u0026#34;, xlim=(100,1500)) sns.distplot(df[\u0026#39;fmr_2\u0026#39;], ax=ax1) ax1.set(xlabel=\u0026#34;2 Bedroom Fair Market Rent\u0026#34;, xlim=(100,1500)) plt.show()   You can also make this by\n1 2 3 4 5 6 7 8 9  fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True) sns.distplot(df[\u0026#39;fmr_1\u0026#39;], ax=ax[0]) ax[0].set(xlabel=\u0026#34;1 Bedroom Fair Market Rent\u0026#34;, xlim=(100,1500)) sns.distplot(df[\u0026#39;fmr_2\u0026#39;], ax=ax[1]) ax[1].set(xlabel=\u0026#34;2 Bedroom Fair Market Rent\u0026#34;, xlim=(100,1500)) plt.show()   Additional Plot Types Categorical Plot Types Seaborn supports many plot types with categorical data. Seaborn breaks categorical data plots into three groups.\n  The first group includes the stripplot() and swarmplot(), which show all the individual observations on the plot.\n  The second category contains the boxplot(), as well as the violinplot() and lvplot(). These plots show an abstract representation of the categorical data.\n  The final group of plots show statistical estimates of the categorical variables. The barplot() and pointplot() contain useful summaries of data. The countplot() shows the number of instances of each observation.\n  stripplot( ) shows every observation in the dataset. Sometimes, the data\u0026rsquo;s distribution is too busy, and we can set parameters jitter = True to scatter data rather than gather it in a single line.\n1 2 3 4 5  sns.stripplot(data=df, x=\u0026#39;Award_Amount\u0026#39;, y=\u0026#39;Model Selected\u0026#39;,) plt.show()   1 2 3 4 5 6  sns.stripplot(data=df, x=\u0026#39;Award_Amount\u0026#39;, y=\u0026#39;Model Selected\u0026#39;, jitter=True) plt.show()   We can plot a more sophisticated visualization of all the data using a swarmplot( ). This plot uses a complex algorithm to place the observations in a manner where they do not overlap. swarmplot() does not scale well to large datasets.\n1 2 3 4 5 6  sns.swarmplot(data=df, x=\u0026#39;Award_Amount\u0026#39;, y=\u0026#39;Model Selected\u0026#39;, hue=\u0026#39;Region\u0026#39;) plt.show()   boxplot( ) is the most common plots show abstract representations of the data. This plot is used to show several measures related to the distribution of data, including the median, upper and lower quartiles, as well as outliers.\n1 2 3 4 5 6  sns.boxplot(data=df, x=\u0026#39;Award_Amount\u0026#39;, y=\u0026#39;Model Selected\u0026#39;) plt.show() plt.clf()   The violinplot( ) is a combination of a kernel density plot and a box plot and can be suitable for providing an alternative view of the distribution of data.\n1 2 3 4 5 6 7  sns.violinplot(data=df, x=\u0026#39;Award_Amount\u0026#39;, y=\u0026#39;Model Selected\u0026#39;, palette=\u0026#39;husl\u0026#39;) plt.show() plt.clf()   lvplot() stands for Letter Value plot. The API is the same as the boxplot() and violinplot() but can scale more effectively to large datasets.\n1 2 3 4 5 6 7 8  sns.lvplot(data=df, x=\u0026#39;Award_Amount\u0026#39;, y=\u0026#39;Model Selected\u0026#39;, palette=\u0026#39;Paired\u0026#39;, hue=\u0026#39;Region\u0026#39;) plt.show() plt.clf()   The barplot( ) shows an estimate of the value as well as a confidence interval. The pointplot( ) is similar to the barplot() in that it shows a summary measure and confidence interval. the countplot( ) displays the number of instances of each variable.\nRegression Plots regplot( ), like most of the Seaborn functions we have reviewed, requires the definition of the data and the x and y variables and you include a marker for the observations by setting marker=\u0026quot;+\u0026quot;. If a value greater than 1 is passed to the order parameter of regplot(), then Seaborn will attempt a polynomial fit using underlying NumPy functions.\nThe residplot( ) plot is a very useful plot for understanding the appropriateness of a regression model. Ideally, the residual values in the plot should be plotted randomly across the horizontal line.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  fig, ax = plt.subplots(nrows=1, ncols=2) sns.regplot(data=df, y=\u0026#39;Tuition\u0026#39;, x=\u0026#34;SAT_AVG_ALL\u0026#34;, marker=\u0026#39;^\u0026#39;, color=\u0026#39;g\u0026#39;, ax=ax[0]) ax[0].set(title=\u0026#34;Regression plot\u0026#34;) sns.residplot(data=df, y=\u0026#39;Tuition\u0026#39;, x=\u0026#34;SAT_AVG_ALL\u0026#34;, color=\u0026#39;g\u0026#39;, ax=ax[1]) ax[1].set(title=\u0026#34;Residual plot\u0026#34;) plt.show()   Seaborn also supports regression plots with categorical variables. you can use the jitter parameter makes it easier to see the individual distribution of the categorical variable. In some cases, even with the jitter, it may be difficult to see if there are any trends based on the value of the variable. Using an x_estimator=func for the x value can apply this function to each unique value of x and plot the resulting estimate.\nWhen there are continuous variables, it can be helpful to break them into different bins by setting parameter x_bins. When this parameter is used, it implies that the default of x_estimator is numpy.mean.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  fig, ax = plt.subplots(nrows=2, ncols=2) sns.regplot( data=df, y=\u0026#39;Tuition\u0026#39;, x=\u0026#34;PCTPELL\u0026#34;, ax=ax[0][0] ) sns.regplot( data=df, y=\u0026#39;Tuition\u0026#39;, x=\u0026#34;PCTPELL\u0026#34;, x_bins=5, ax=ax[0][1] ) sns.regplot( data=df, y=\u0026#39;Tuition\u0026#39;, x=\u0026#34;PCTPELL\u0026#34;, x_bins=5, order=2, ax=ax[1][0] ) sns.residplot( data=df, y=\u0026#39;Tuition\u0026#39;, x=\u0026#34;PCTPELL\u0026#34;, order=2, ax=ax[1][1] ) ax[0][0].set(title=\u0026#34;Reg plot\u0026#34;) ax[0][1].set(title=\u0026#34;Reg bins plot\u0026#34;) ax[1][0].set(title=\u0026#34;2 orders Reg bins plot\u0026#34;) ax[1][1].set(title=\u0026#34;2 orders Resid plot\u0026#34;) plt.show()   Matrix plots Seaborn\u0026rsquo;s heatmap( ) function expects the data to be in a matrix. You can use pd.crosstab() or pd.pivot_table() to create this matirx.\n pd.crosstab(s1, s2) is a special kind of pd.pivot_table, it is a shortcut to count the number of (s1_i, s2_j).\n The display of the heatmap can be customized in multiple ways to present the most information as clearly as possible. First, we use annot equals True to turn on annotations in the individual cells. The fmt=d ensures that the results are displayed as integers. Next, we use a custom cmap to change the shading we use. By setting cbar equals False, the color bar is not displayed. we can use center to center the colormap at the value we want. Finally, passing a variable to linewidths puts some small spacing between the cells so that the values are simpler to view.\n1 2 3 4 5 6 7 8 9  pd_crosstab = pd.crosstab(df[\u0026#34;Group\u0026#34;], df[\u0026#34;YEAR\u0026#34;]) sns.heatmap(pd_crosstab, cbar=True, cmap=\u0026#34;BuGn\u0026#34;, linewidths=0.3, annot=True) plt.yticks(rotation=0) plt.xticks(rotation=90) plt.show() plt.clf()   One common usage for a heatmap is to visually represent the correlation between variables. pandas DataFrames have a corr() function that calculates the correlation between the values in the columns. The output of this function is ideally structured to be displayed as a heatmap.\nCreating Plots on Data Aware Grids FacetGrid, factorplot and lmplot One of Seaborn\u0026rsquo;s most powerful features is its ability to combine multiple smaller plots into a larger visualization that can help identify trends in data with many variables.\n One very important requirement for Seaborn to create these plots is that the data must be in tidy format. This means that each row of the data is a single observation and the columns contain the variables.\n There are two step processes to use FacetGrid( ) make multi subplots.\n Call sns.FacetGrid() to initialize the grid axes.  1 2 3 4 5 6 7 8  g = sns.FacetGrid( data, row=None, col=None, hue=None, palette=None, row_order=None, col_order=None, hue_order=None, hue_kws=None, col_wrap=None, sharex=True, sharey=True, height=3, aspect=1, dropna=True, legend_out=True, despine=True, margin_titles=False, xlim=None, ylim=None, subplot_kws=None, gridspec_kws=None, size=None )    Call g.map() to specify plot type, data, color and style.  1 2 3 4 5  # single variable plot g.map(plt.hist, \u0026#34;total_bill\u0026#34;, bins=bins, color=\u0026#34;r\u0026#34;) # double variables plot g.map(plt.scatter, \u0026#34;total_bill\u0026#34;, \u0026#34;tip\u0026#34;, edgecolor=\u0026#34;w\u0026#34;, marker=\u0026#34;.\u0026#34;)    1 2 3 4 5 6 7 8 9 10 11  g2 = sns.FacetGrid( df, row=\u0026#34;Degree_Type\u0026#34;, row_order=[ \u0026#39;Graduate\u0026#39;, \u0026#39;Bachelors\u0026#39;, \u0026#39;Associates\u0026#39;, \u0026#39;Certificate\u0026#39; ] ) g2.map(sns.pointplot, \u0026#39;SAT_AVG_ALL\u0026#39;) plt.show() plt.clf()   Seaborn\u0026rsquo;s FacetGrid() is very powerful and flexible but involves multiple steps to create. The factorplot() function returned value is also a FacetGrid but the process for creating one is much simpler.\n1 2 3 4 5 6 7 8  sns.factorplot(data=df, x=\u0026#39;SAT_AVG_ALL\u0026#39;, kind=\u0026#39;point\u0026#39;, row=\u0026#39;Degree_Type\u0026#39;, row_order=[\u0026#39;Graduate\u0026#39;, \u0026#39;Bachelors\u0026#39;, \u0026#39;Associates\u0026#39;, \u0026#39;Certificate\u0026#39;]) plt.show() plt.clf()   1 2 3 4 5 6 7  sns.factorplot(data=df, x=\u0026#39;Tuition\u0026#39;, kind=\u0026#39;box\u0026#39;, row=\u0026#39;Degree_Type\u0026#39;) plt.show() plt.clf()   1 2 3 4 5 6 7 8  seaborn.lmplot( x, y, data, hue=None, col=None, row=None, palette=None, hue_order=None, col_order=None, row_order=None, col_wrap=None, height=5, aspect=1, markers=\u0026#39;o\u0026#39;, sharex=True, sharey=True, legend=True, legend_out=True, x_estimator=None, x_bins=None, x_ci=\u0026#39;ci\u0026#39;, scatter=True, fit_reg=True, ci=95, n_boot=1000, units=None, order=1, logistic=False, lowess=False, robust=False, logx=False, x_partial=None, y_partial=None, truncate=False, x_jitter=None, y_jitter=None, scatter_kws=None, line_kws=None, size=None )   The lmplot( ) function combines regplot() and FacetGrid. It is intended as a convenient interface to fit regression models across conditional subsets of a dataset. When thinking about how to assign variables to different facets, a general rule is that it makes sense to use hue for the most important comparison, followed by col and row.\n1 2 3 4 5 6 7 8 9 10 11  g = sns.FacetGrid( df, col=\u0026#34;Ownership\u0026#34;, row=\u0026#39;Degree_Type\u0026#39;, row_order=[\u0026#39;Graduate\u0026#39;, \u0026#39;Bachelors\u0026#39;], hue=\u0026#39;WOMENONLY\u0026#39;, col_order=inst_ord ) g.map(plt.scatter, \u0026#39;SAT_AVG_ALL\u0026#39;, \u0026#34;Tuition\u0026#34;) plt.show() plt.clf()   1 2 3 4 5 6 7 8 9 10 11  sns.lmplot(data=df, x=\u0026#39;SAT_AVG_ALL\u0026#39;, y=\u0026#39;Tuition\u0026#39;, col=\u0026#34;Ownership\u0026#34;, row=\u0026#39;Degree_Type\u0026#39;, row_order=[\u0026#39;Graduate\u0026#39;, \u0026#39;Bachelors\u0026#39;], hue=\u0026#39;WOMENONLY\u0026#39;, col_order=inst_ord) plt.show() plt.clf()    Notice, factorplot(), lmplot(), relplot(), catplot().\n PairGrid and pairplot 1 2 3 4 5 6  PairGrid( data, hue=None, hue_order=None, palette=None, hue_kws=None, vars=None, x_vars=None, y_vars=None, diag_sharey=True, height=2.5, aspect=1, despine=True, dropna=True, size=None )   PairGrid and pairplot are similar to the FacetGrid, factorplot, and lmplots, exercise because they allow us to see interactions across different columns of data.\nThe PairGrid plot allows us to build plots that show the relationships between two data elements. The process for creating a PairGrid is similar to a FacetGrid in that we must create the grid.Notice that we define the variables instead of the row and column parameters. Three cases:\n seaborn will create row and column respectively for each of variable you specify in vars arguments. seaborn will create row for variable you specify in x_vars arguments, and column for variable in y_vars arguments. seaborn will create row and column respectively for each of volumns in the dateframe if you don\u0026rsquo;t specify vars argument.  Then map a plot type to the grid. You can use the map_diag() function to define the plotting function for the main diagonal. The map_offdiag() function defines the other diagonal.\n1 2 3 4 5 6 7 8  iris = sns.load_dataset(\u0026#34;iris\u0026#34;) g = sns.PairGrid(iris, hue=\u0026#34;species\u0026#34;) g.map_diag(plt.hist, histtype=\u0026#34;step\u0026#34;) g.map_offdiag(plt.scatter) g.add_legend() plt.show() plt.clf()   you can also use map_upper(), map_lower() to specify different plot type on the both sides of the diagonal.\n1 2 3 4 5 6 7 8  iris = sns.load_dataset(\u0026#34;iris\u0026#34;) g = sns.PairGrid(iris) g.map_upper(plt.scatter) g.map_lower(sns.kdeplot, cmap=\u0026#34;Blues_d\u0026#34;) g.map_diag(sns.kdeplot) plt.show() plt.clf()   1 2 3 4 5 6 7  seaborn.pairplot( data, hue=None, hue_order=None, palette=None, vars=None, x_vars=None, y_vars=None, kind=\u0026#39;scatter\u0026#39;, diag_kind=\u0026#39;auto\u0026#39;, markers=None, height=2.5, aspect=1, dropna=True, plot_kws=None, diag_kws=None, grid_kws=None, size=None )   Pairplot is a high-level interface for PairGrid that is intended to make it easy to draw a few common styles. You should use PairGrid directly if you need more flexibility. Two arguments:\n kind{scatter, reg}, optional Kind of plot for the non-identity relationships. diag_kind{auto, hist, kde}, optional Kind of plot for the diagonal subplots.  1 2 3 4 5 6 7 8 9 10 11 12 13  iris = sns.load_dataset(\u0026#34;iris\u0026#34;) sns.pairplot( iris, hue=\u0026#34;species\u0026#34;, palette=\u0026#34;husl\u0026#34;, markers=[\u0026#34;o\u0026#34;, \u0026#34;+\u0026#34;, \u0026#34;v\u0026#34;], diag_kind=\u0026#34;kde\u0026#34;, diag_kws={\u0026#39;alpha\u0026#39;:.5, \u0026#34;shade\u0026#34;:True}, kind=\u0026#34;reg\u0026#34; ) plt.show() plt.clf()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  sns.pairplot( data=df, vars=[\u0026#34;insurance_losses\u0026#34;, \u0026#34;premiums\u0026#34;], kind=\u0026#39;reg\u0026#39;, palette=\u0026#39;BrBG\u0026#39;, diag_kind = \u0026#39;kde\u0026#39;, diag_kws={ \u0026#34;shade\u0026#34;:True, \u0026#34;alpha\u0026#34;:.5 }, hue=\u0026#39;Region\u0026#39; ) plt.show() plt.clf()   JointGrid and jointplot 1 2 3 4 5  seaborn.JointGrid( x, y, data=None, height=6, ratio=5, space=0.2, dropna=True, xlim=None, ylim=None, size=None )   JoinGrid creates grid for drawing a bivariate plot with marginal univariate plots. Initialize the figure but dont draw any plots onto it:\n1 2 3 4 5  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) g = sns.JointGrid(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips) plt.show() plt.clf()   Add plots using default parameters:\n1 2 3 4 5 6  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) g = sns.JointGrid(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips) g.plot(sns.regplot, sns.distplot) plt.show() plt.clf()   Draw the join and marginal plots separately:\n1 2 3 4 5 6 7 8  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) g = sns.JointGrid(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips) g.plot_joint(plt.scatter, color=\u0026#34;.5\u0026#34;, edgecolor=\u0026#34;white\u0026#34;) g.plot_marginals(sns.distplot, kde=False, color=\u0026#34;.5\u0026#34;) plt.show() plt.clf()   or\n1 2 3 4 5 6 7 8 9 10 11  import numpy as np tips = sns.load_dataset(\u0026#34;tips\u0026#34;) g = sns.JointGrid(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips) g.plot_joint(plt.scatter, color=\u0026#34;m\u0026#34;, edgecolor=\u0026#34;white\u0026#34;) g.ax_marg_x.hist(tips[\u0026#34;total_bill\u0026#34;], color=\u0026#34;b\u0026#34;, alpha=.6, bins=np.arange(0, 60, 4)) g.ax_marg_y.hist(tips[\u0026#34;tip\u0026#34;], color=\u0026#34;r\u0026#34;, alpha=.6, orientation=\u0026#34;horizontal\u0026#34;, bins=np.arange(0, 12, .5)) plt.show() plt.clf()   Add an annotation with a statistic summarizing the bivariate relationship:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  from scipy import stats tips = sns.load_dataset(\u0026#34;tips\u0026#34;) g = sns.JointGrid(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips) g.plot_joint(plt.scatter) g.plot_marginals(sns.kdeplot, shade=True) rsquare = lambda a, b: stats.pearsonr(a, b)[0] ** 2 g.annotate( rsquare, template=\u0026#34;{stat}: {val:.2f}\u0026#34;, stat=\u0026#34;$R^2$\u0026#34;, loc=\u0026#34;upper left\u0026#34;, fontsize=12 ) plt.show() plt.clf()    1 2 3 4 5 6 7  seaborn.jointplot( x, y, data=None, kind=\u0026#39;scatter\u0026#39;, stat_func=None, color=None, height=6, ratio=5, space=0.2, dropna=True, xlim=None, ylim=None, joint_kws=None, marginal_kws=None, annot_kws=None, **kwargs )   jointplot( ) function provides a convenient interface to the JointGrid class, with several canned plot kinds. This is intended to be a fairly lightweight wrapper; if you need more flexibility, you should use JointGrid directly.\nDraw a default jointplot, i.e. scatterplot with marginal histograms:\n1 2  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) sns.jointplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips)   Add (3 order) regression and kernel density fits\n1 2 3 4 5 6 7 8 9 10 11 12  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) sns.jointplot( \u0026#34;total_bill\u0026#34;, \u0026#34;tip\u0026#34;, data=tips, kind=\u0026#34;reg\u0026#34;, order=3, xlim=(0,50) ) plt.show() plt.clf()   the corresponding residual plot:\n1 2 3 4 5 6 7 8 9 10 11 12  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) sns.jointplot( \u0026#34;total_bill\u0026#34;, \u0026#34;tip\u0026#34;, data=tips, kind=\u0026#34;resid\u0026#34;, order=3, xlim=(0,50) ) plt.show() plt.clf()   Replace the scatterplot with a joint histogram using hexagonal bins:\n1 2 3 4 5  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) sns.jointplot(\u0026#34;total_bill\u0026#34;, \u0026#34;tip\u0026#34;, data=tips, kind=\u0026#34;hex\u0026#34;) plt.show() plt.clf()   Replace the scatterplots and histograms with density estimates and align the marginal Axes tightly with the joint Axes:\n1 2 3 4 5  iris = sns.load_dataset(\u0026#34;iris\u0026#34;) sns.jointplot(\u0026#34;sepal_width\u0026#34;, \u0026#34;petal_length\u0026#34;, data=iris, kind=\u0026#34;kde\u0026#34;, space=0, color=\u0026#34;g\u0026#34;) plt.show() plt.clf()   Draw a scatterplot, then add a joint density estimate:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  iris = sns.load_dataset(\u0026#34;iris\u0026#34;) ( sns.jointplot( \u0026#34;sepal_length\u0026#34;, \u0026#34;sepal_width\u0026#34;, data=iris, color=\u0026#34;b\u0026#34;, kind=\u0026#34;scatter\u0026#34; ).plot_joint( sns.kdeplot, n_levels=10, cmap=\u0026#34;RdBu\u0026#34; ) ) plt.show() plt.clf()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  iris = sns.load_dataset(\u0026#34;iris\u0026#34;) (sns.jointplot( \u0026#34;sepal_length\u0026#34;, \u0026#34;sepal_width\u0026#34;, data=iris, color=\u0026#34;b\u0026#34;, kind=\u0026#34;kde\u0026#34; ).plot_joint( sns.kdeplot, n_levels=10, cmap=\u0026#34;RdBu\u0026#34; ) ) plt.show() plt.clf()      Grid class high-level function     FacetGrid factorplot(), lmplot(), relplot(), catplot()   PairGrid pairplot   JointGrid jointplot, jointplot.plot_joint    Union Frame The final section of this blog will bring all of the concepts together and give a framework for deciding when to use each Seaborn plot.\nUnivariate Distribution Analysis One of the first steps in analyzing numerical data is looking at its distribution. Seaborn\u0026rsquo;s distplot() combines many of the features of the rugplot(), kdeplot(), and plt.histplot() into a single function. The distplot() function is the best place to start when trying to do distribution analysis with Seaborn.\nRegression Analysis A regression plot is an example of a plot that shows the relationship between two variables. matplotlib\u0026rsquo;s scatter() plot is a very simple method to compare the interaction of two variables on the x and y-axis. The lmplot() combines many of these features of the underlying regplot() and residplot() in addition to the ability to plot the data on a FacetGrid(). In many instances, lmplot() is the best function to use for determining linear relationships between data.\nCategorical Plots Seaborn has many types of categorical plots as well. In most scenarios, it makes sense to use one of the categorical plots such as the boxplot() or violinplot() to examine the distribution of the variables. Then, follow up with the statistical estimation plots such as the point, bar, or countplot. If you need to facet the data across rows or columns, use a factorplot().\nRelation The pairplot() and jointplot() visualizations are going to be most useful after you have done some preliminary analysis of regressions or distributions of the data. Once you are familiar with the data, the pairplot() and jointplot() can be very useful in understanding how two or more variables interact with each other.\n","date":"2020-04-23T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/data-visualization-iii-seaborn/","title":"Data Visualization III (Seaborn)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n [toc]\nIntroduction to Seaborn Using pandas with Seaborn 1 2 3  # Import Matplotlib and Seaborn import matplotlib.pyplot as plt import seaborn as sns   To create a count plot with a pandas DataFrame column instead of a list of data, set x (or y) equal to the name of the column in the DataFrame. Then, we\u0026rsquo;ll set the data parameter equal to DataFrame. Finally calling \u0026ldquo;plt.show()\u0026rdquo;.\n1 2 3 4 5 6  df = pd.read_csv(\u0026#34;xxxx.csv\u0026#34;) sns.countplot( x=\u0026#34;Spiders\u0026#34;, # column name data=df ) plt.show()    Notice that because we\u0026rsquo;re using a named column in the DataFrame, Seaborn automatically adds the name of the column as the x-axis label at the bottom.\n Adding a third variable with hue we\u0026rsquo;ll be using Seaborn\u0026rsquo;s built-in tips dataset. You can access it by using the \u0026ldquo;load_dataset\u0026rdquo; function in Seaborn and passing in the name of the dataset. These are the first five rows of the tips dataset.\n1 2  df=sns.load_dataset(\u0026#34;tips\u0026#34;) df.head()    total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 Let\u0026rsquo;s explore the relationship between the \u0026ldquo;total_bill\u0026rdquo; and \u0026ldquo;tip\u0026rdquo; columns using a scatter plot.\n1 2 3 4 5 6  sns.scatterplot( x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=df, ) plt.show()   If you want to add a third variable to plots by adding color, you can set the \u0026ldquo;hue\u0026rdquo; parameter equal to the DataFrame column name or a list, and then Seaborn will automatically color each point by groups.\n1 2 3 4 5 6 7  sns.scatterplot( x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;smoker\u0026#34;, data=df, ) plt.show()   The \u0026ldquo;hue_order\u0026rdquo; parameter takes in a list of values and will set the order of the values in the plot accordingly. You can control the colors assigned to each value using the \u0026ldquo;palette\u0026rdquo; parameter. This parameter takes in a dictionary. This dictionary should map the variable values to the colors you want to represent the value.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  hc = { \u0026#34;Yes\u0026#34;: \u0026#34;c\u0026#34;, \u0026#34;No\u0026#34;: \u0026#34;g\u0026#34; } sns.scatterplot( x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;smoker\u0026#34;, data=df, hue_order=[\u0026#34;No\u0026#34;, \u0026#34;Yes\u0026#34;], palette=hc ) plt.show()   Here is the list of Matplotlib colors and their names. Note that you can use a single-letter Matplotlib abbreviation instead of the full name.\nhue is available in most of Seaborn\u0026rsquo;s plot types. For example, countplot\n1 2 3 4 5 6 7 8 9 10  palette_colors = {\u0026#34;Rural\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;Urban\u0026#34;: \u0026#34;blue\u0026#34;} sns.countplot( x=\u0026#34;school\u0026#34;, data=student_data, hue=\u0026#34;location\u0026#34;, palette=palette_colors ) plt.show()    The other way to add the third variable into a plot is calling arguments col or row in sns.relplot() or sns.catplot() method.\n Relational Plot relational plots and subplots Sometimes we suspect that the relationship may be different within certain subgroups. In the last chapter, we started to look at subgroups by using the \u0026ldquo;hue\u0026rdquo; parameter to visualize each subgroup using a different color on the same plot.\nNow we\u0026rsquo;ll try out a different method relplot() (relationship plot) to creat a separate plot per subgroup. We call the \u0026ldquo;relplot()\u0026rdquo; method and use the \u0026ldquo;kind\u0026rdquo; parameter to specify what kind of relational plot to use - scatter plot or line plot.\nBy setting \u0026ldquo;col\u0026rdquo; equal to \u0026ldquo;smoker\u0026rdquo;, we get a separate scatter plot groups by \u0026ldquo;smoker\u0026rdquo;, arranged horizontally in columns. You can use the \u0026ldquo;col_wrap\u0026rdquo; parameter to specify how many subplots you want per row. If you want to arrange these vertically in rows instead, you can use the \u0026ldquo;row\u0026rdquo; parameter instead of \u0026ldquo;col\u0026rdquo;. We can also change the order of the subplots by using the \u0026ldquo;col_order\u0026rdquo; and \u0026ldquo;row_order\u0026rdquo; parameters and giving it a list of ordered values.\n1 2 3 4 5 6 7 8 9 10  sns.relplot( x=\u0026#34;G1\u0026#34;, y=\u0026#34;G3\u0026#34;, data=student_data, kind=\u0026#34;scatter\u0026#34;, col=\u0026#34;schoolsup\u0026#34;, col_order=[\u0026#34;yes\u0026#34;, \u0026#34;no\u0026#34;] ) plt.show()   1 2 3 4 5 6 7 8 9  sns.relplot( x=\u0026#34;absences\u0026#34;, y=\u0026#34;G3\u0026#34;, data=student_data, kind=\u0026#34;scatter\u0026#34;, row=\u0026#34;study_time\u0026#34; ) plt.show()   It is possible to use both \u0026ldquo;col\u0026rdquo; and \u0026ldquo;row\u0026rdquo; at the same time. Here, we set \u0026ldquo;col\u0026rdquo; equal to smoking status and \u0026ldquo;row\u0026rdquo; equal to the time of day (lunch or dinner). Now we have a subplot for each combination of these two categorical variables.\n1 2 3 4 5 6 7 8 9 10 11 12  sns.relplot( x=\u0026#34;G1\u0026#34;, y=\u0026#34;G3\u0026#34;, data=student_data, kind=\u0026#34;scatter\u0026#34;, col=\u0026#34;schoolsup\u0026#34;, col_order=[\u0026#34;yes\u0026#34;, \u0026#34;no\u0026#34;], row=\u0026#34;famsup\u0026#34;, row_order=[\u0026#39;yes\u0026#39;, \u0026#39;no\u0026#39;] ) plt.show()   Customizing scatter plots Seaborn allows you to add more information to scatter plots by varying the size, the style, and the transparency of the points.\n All of these options can be used in both the \u0026ldquo;scatterplot()\u0026rdquo; and \u0026ldquo;relplot()\u0026rdquo; functions, but we\u0026rsquo;ll continue to use \u0026ldquo;relplot()\u0026rdquo; since it\u0026rsquo;s more flexible and allows us to create subplots.\n We want each point on the scatter plot to be sized based on the number of variable with larger number having bigger points on the plot. To do this, we\u0026rsquo;ll set the \u0026ldquo;size\u0026rdquo; parameter equal to the variable name from the dataset.\nWe can make it easier by using the \u0026ldquo;size\u0026rdquo; parameter in combination with the \u0026ldquo;hue\u0026rdquo; parameter. To do this, set \u0026ldquo;hue\u0026rdquo; equal to the variable name.\n1 2 3 4 5 6 7 8 9 10  sns.relplot( x=\u0026#34;horsepower\u0026#34;, y=\u0026#34;mpg\u0026#34;, data=mpg, kind=\u0026#34;scatter\u0026#34;, size=\u0026#34;cylinders\u0026#34;, hue=\u0026#34;cylinders\u0026#34; ) plt.show()    Notice that because \u0026ldquo;size\u0026rdquo; is a quantitative variable, Seaborn will automatically color the points the same color with different depth, instead of different colors per category value like we saw in previous plots.\n We use \u0026ldquo;hue\u0026rdquo; to create different colored points based on groups. Setting \u0026ldquo;style\u0026rdquo; equal to allows us to better distinguish these subgroups by plotting points with a different point style in addition to a different color.\n1 2 3 4 5 6 7 8 9 10  sns.relplot( x=\u0026#34;acceleration\u0026#34;, y=\u0026#34;mpg\u0026#34;, data=mpg, hue=\u0026#34;origin\u0026#34;, style=\u0026#34;origin\u0026#34;, kind=\u0026#34;scatter\u0026#34; ) plt.show()   Setting the \u0026ldquo;alpha\u0026rdquo; parameter to a value between 0 and 1 will vary the transparency of the points in the plot, with 0 being completely transparent and 1 being completely non-transparent.\nLine plots In Seaborn, we have two types of relational plots: scatter plots and line plots. Line plots are the visualization of choice when we need to track the same thing over time.\nBy specifying \u0026ldquo;kind\u0026rdquo; equals \u0026ldquo;line\u0026rdquo;, we can create a line plot. We can also track subgroups over time with line plots. Setting the \u0026ldquo;style\u0026rdquo; and \u0026ldquo;hue\u0026rdquo; parameters equal to the variable name creates different lines for each group that vary in both line style and color.\n1 2 3 4 5 6 7 8 9 10 11  sns.relplot( x=\u0026#34;model_year\u0026#34;, y=\u0026#34;horsepower\u0026#34;, data=mpg, kind=\u0026#34;line\u0026#34;, ci=None, style=\u0026#34;origin\u0026#34;, hue=\u0026#34;origin\u0026#34; ) plt.show()   1 2 3 4 5 6 7 8 9 10 11 12  sns.relplot( x=\u0026#34;model_year\u0026#34;, y=\u0026#34;horsepower\u0026#34;, data=mpg, kind=\u0026#34;line\u0026#34;, ci=None, col=\u0026#34;origin\u0026#34;, hue=\u0026#34;origin\u0026#34;, style=\u0026#34;origin\u0026#34; ) plt.show()   Setting the \u0026ldquo;markers\u0026rdquo; parameter equal to \u0026ldquo;True\u0026rdquo; will display a marker for each data point. The marker will vary based on the subgroup you\u0026rsquo;ve set using the \u0026ldquo;style\u0026rdquo; parameter. If you don\u0026rsquo;t want the line styles to vary by subgroup, set the \u0026ldquo;dashes\u0026rdquo; parameter equal to \u0026ldquo;False\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  sns.relplot( x=\u0026#34;model_year\u0026#34;, y=\u0026#34;horsepower\u0026#34;, data=mpg, kind=\u0026#34;line\u0026#34;, ci=None, col=\u0026#34;origin\u0026#34;, hue=\u0026#34;origin\u0026#34;, style=\u0026#34;origin\u0026#34;, markers=True, dashes=False ) plt.show()   Line plots can also be used when you have more than one observation per x-value. If a line plot is given multiple observations per x-value, it will aggregate them into a single summary measure. By default, it will display the mean. This is the scatter plot\n1 2 3 4 5 6 7  sns.relplot( x=\u0026#34;model_year\u0026#34;, y=\u0026#34;mpg\u0026#34;, data=mpg, ) plt.show()   and the line plot\n1 2 3 4 5 6 7 8  sns.relplot( x=\u0026#34;model_year\u0026#34;, y=\u0026#34;mpg\u0026#34;, data=mpg, kind=\u0026#34;line\u0026#34; ) plt.show()    Notice that Seaborn will automatically calculate a confidence interval for the mean, displayed by the shaded region.\n Set the \u0026ldquo;ci\u0026rdquo; parameter equal to the string \u0026ldquo;sd\u0026rdquo; to make the shaded area represent the standard deviation instead of the confidence interval for the mean which shows the spread of the distribution of observations at each x value. We can also turn off the confidence interval by setting the \u0026ldquo;ci\u0026rdquo; parameter equal to \u0026ldquo;None\u0026rdquo;.\n1 2 3 4 5 6 7 8 9  sns.relplot( x=\u0026#34;model_year\u0026#34;, y=\u0026#34;mpg\u0026#34;, data=mpg, kind=\u0026#34;line\u0026#34;, ci=\u0026#34;sd\u0026#34; ) plt.show()   Categorical Plot Count plots and bar plots Count plots and bar plots are two types of visualizations that Seaborn calls \u0026ldquo;categorical plots\u0026rdquo;.\nWe call catplot() method and use the \u0026ldquo;kind\u0026rdquo; parameter to specify what kind of categorical plot to use. \u0026ldquo;catplot()\u0026rdquo; offers the same flexibility that \u0026ldquo;relplot()\u0026rdquo; does, which means it will be easy to create subplots if we need to using the same \u0026ldquo;col\u0026rdquo; and \u0026ldquo;row\u0026rdquo; parameters.\n1 2 3 4  sns.catplot(x=\u0026#34;Internet usage\u0026#34;, data=survey_data, kind=\u0026#34;count\u0026#34;, row=\u0026#34;Age Category\u0026#34;) plt.show()   Use the \u0026ldquo;order\u0026rdquo; parameter which accepts a list can specific ordering of categories. This works for all types of categorical plots.\nBar plots look similar to count plots, but instead of the count of observations in each category, they show the mean of a quantitative variable among observations in each category. To create this bar plot, we use \u0026ldquo;catplot\u0026rdquo;. Specify the categorical variable on the x-axis, the quantitative variable on the y-axis, and set the \u0026ldquo;kind\u0026rdquo; parameter equal to \u0026ldquo;bar\u0026rdquo;.\n Of course, you can also change the orientation of the bars in bar plots and count plots by switching the x and y parameters.\n 1 2 3 4 5 6 7 8 9 10 11  category_order = [\u0026#34;\u0026lt;2 hours\u0026#34;, \u0026#34;2 to 5 hours\u0026#34;, \u0026#34;5 to 10 hours\u0026#34;, \u0026#34;\u0026gt;10 hours\u0026#34;] sns.catplot(x=\u0026#34;study_time\u0026#34;, y=\u0026#34;G3\u0026#34;, data=student_data, kind=\u0026#34;bar\u0026#34;, order=category_order) plt.show()   Notice also that Seaborn automatically shows 95% confidence intervals for these means. we can turn off these confidence intervals by setting the \u0026ldquo;ci\u0026rdquo; parameter equal to \u0026ldquo;None\u0026rdquo;.\nBox plot A box plot shows the distribution of quantitative data. The colored box represents the 25th to 75th percentile, and the line in the middle of the box represents the median. The whiskers give a sense of the spread of the distribution, and the floating points represent outliers.\nWe can use the \u0026ldquo;catplot()\u0026rdquo; function to crate a box plot. We\u0026rsquo;ll put the categorical variable on the x-axis and the quantitative variable on the y-axis, and specify kind=\u0026quot;box\u0026quot;.\n As a reminder, \u0026ldquo;catplot\u0026rdquo; allows you to change the order of the categories with a list by \u0026ldquo;order\u0026rdquo; parameter.\n 1 2 3 4 5 6 7 8 9 10 11 12  study_time_order = [\u0026#34;\u0026lt;2 hours\u0026#34;, \u0026#34;2 to 5 hours\u0026#34;, \u0026#34;5 to 10 hours\u0026#34;, \u0026#34;\u0026gt;10 hours\u0026#34;] sns.catplot( x=\u0026#34;study_time\u0026#34;, y=\u0026#34;G3\u0026#34;, data=student_data, kind=\u0026#34;box\u0026#34;, order=study_time_order ) plt.show()   You can omit the outliers using the specify sym=\u0026quot;\u0026quot;. \u0026ldquo;sym\u0026rdquo; can also be used to change the appearance of the outliers instead of omitting them.\nYou can change the way the whiskers using the \u0026ldquo;whis\u0026rdquo; parameter. You can have the whiskers define specific lower and upper percentiles by passing in a list of the lower and upper values such as whis=[5, 95] or whis=[0, 100].\n1 2 3 4 5 6 7 8 9 10 11  sns.catplot( x=\u0026#34;internet\u0026#34;, y=\u0026#34;G3\u0026#34;, data=student_data, kind=\u0026#34;box\u0026#34;, sym=\u0026#34;\u0026#34;, whis=[0, 100], # \u0026lt;-- Min \u0026amp; Max hue=\u0026#34;location\u0026#34; ) plt.show()   Point plots Point plots show the mean of a quantitative variable for the observations in each category, plotted as a single point.\nBoth line plots and point plots show the mean of a quantitative variable and 95% confidence intervals for the mean. However, there is a key difference. Line plots are relational plots, so both the x- and y-axis are quantitative variables. In a point plot, one axis - usually the x-axis - is a categorical variable, making it a categorical plot.\nSince Point plot is a categorical plot, we use \u0026ldquo;catplot\u0026rdquo; and set \u0026ldquo;kind\u0026rdquo; equal to \u0026ldquo;point\u0026rdquo;. To remove the lines connecting each point, set the \u0026ldquo;join\u0026rdquo; parameter equal to False. You can also add caps to the end of the confidence intervals, set the capsize parameter equal to the desired width of the caps. Finally, you can turn the confidence intervals off by setting the \u0026ldquo;ci\u0026rdquo; parameter equal to None.\n1 2 3 4 5 6  sns.catplot(x=\u0026#34;famrel\u0026#34;, y=\u0026#34;absences\u0026#34;, data=student_data, kind=\u0026#34;point\u0026#34;, capsize=0.2) plt.show()   To have the points and confidence intervals be calculated for the median instead of the mean, set \u0026ldquo;estimator=np.median.\n1 2 3 4 5 6 7 8 9 10  from numpy import median sns.catplot(x=\u0026#34;romantic\u0026#34;, y=\u0026#34;absences\u0026#34;, data=student_data, kind=\u0026#34;point\u0026#34;, hue=\u0026#34;school\u0026#34;, ci=None, estimator=median) plt.show()   Customizing Seaborn Plots Changing plot style and color Seaborn has five preset figure styles which change the background and axes of the plot. You can refer to them by name: \u0026ldquo;white\u0026rdquo; (default), \u0026ldquo;dark\u0026rdquo;, \u0026ldquo;whitegrid\u0026rdquo;, \u0026ldquo;darkgrid\u0026rdquo;, and \u0026ldquo;ticks\u0026rdquo;. To set one of these as the global style for all of your plots, use the \u0026ldquo;sns.set_style()\u0026rdquo; method.\nYou can change the color of the main elements of the plot with Seaborn\u0026rsquo;s \u0026ldquo;sns.set_palette()\u0026rdquo; method. Seaborn has many preset color palettes that you can refer to by name, or you can create your own custom palette with list.\nSeaborn has a group of preset palettes called diverging palettes that are great to use if your visualization deals with a scale where the two ends of the scale are opposites and there is a neutral midpoint.\nAnother group of palettes are called sequential palettes. These are a single color (or two colors blended) moving from light to dark values. Sequential palettes are great for emphasizing a variable on a continuous scale.\nYou can also create your own custom palettes by passing in a list of color names.\n1 2 3 4 5 6 7 8 9 10 11 12  sns.set_style(\u0026#34;whitegrid\u0026#34;) sns.set_palette(\u0026#34;RdBu\u0026#34;) category_order = [\u0026#34;Never\u0026#34;, \u0026#34;Rarely\u0026#34;, \u0026#34;Sometimes\u0026#34;, \u0026#34;Often\u0026#34;, \u0026#34;Always\u0026#34;] sns.catplot(x=\u0026#34;Parents Advice\u0026#34;, data=survey_data, kind=\u0026#34;count\u0026#34;, order=category_order) plt.show()   1 2 3 4 5 6 7 8  sns.set_style(\u0026#34;darkgrid\u0026#34;) color = [\u0026#34;#39A7D0\u0026#34;, \u0026#34;#36ADA4\u0026#34;] sns.set_palette(color) sns.catplot(x=\u0026#34;Gender\u0026#34;, y=\u0026#34;Age\u0026#34;, data=survey_data, kind=\u0026#34;box\u0026#34;) plt.show()   Finally, you can change the scale of your plot by using the \u0026ldquo;sns.set_context()\u0026rdquo; method. The scale options from smallest to largest are \u0026ldquo;paper\u0026rdquo; (default), \u0026ldquo;notebook\u0026rdquo;, \u0026ldquo;talk\u0026rdquo;, and \u0026ldquo;poster\u0026rdquo;.\nAdding titles and labels In this section, we will focus on the title, label and x axis tick label in the plot.\nSeaborn\u0026rsquo;s plot functions create two different types of objects: FacetGrids and AxesSubplots. To figure out which type of object you\u0026rsquo;re working with, first assign the plot output to a variable such as g, and call the function type(g) .\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  g1 = sns.relplot(x=\u0026#34;weight\u0026#34;, y=\u0026#34;horsepower\u0026#34;, data=mpg, kind=\u0026#34;scatter\u0026#34;) g2 = sns.catplot(x=\u0026#34;weight\u0026#34;, data=mpg, kind=\u0026#34;count\u0026#34;) g3 = sns.scatterplot(x=\u0026#34;weight\u0026#34;, y=\u0026#34;horsepower\u0026#34;, data=mpg) g4 = sns.countplot(x=\u0026#34;weight\u0026#34;, data=mpg) print(type(g1), type(g2), type(g3), type(g4)   \u0026lt;class 'seaborn.axisgrid.FacetGrid'\u0026gt; \u0026lt;class 'seaborn.axisgrid.FacetGrid'\u0026gt; \u0026lt;class 'matplotlib.axes._subplots.AxesSubplot'\u0026gt; \u0026lt;class 'matplotlib.axes._subplots.AxesSubplot'\u0026gt; Recall that \u0026ldquo;relplot()\u0026rdquo; and \u0026ldquo;catplot()\u0026rdquo; both support making subplots. This means that they are creating FacetGrid objects. In contrast, single-type plot functions like \u0026ldquo;scatterplot()\u0026rdquo; and \u0026ldquo;countplot()\u0026rdquo; return a single AxesSubplot object.\n A FacetGrid consists of one or more AxesSubplots, which is how it supports subplots.\n To add a title to a FacetGrid object, first assign the plot to the variable \u0026ldquo;g\u0026rdquo;. And then, you can set the title using \u0026ldquo;g.fig.suptitle()\u0026rdquo;. To adjust the height of the title, you can use the \u0026ldquo;y\u0026rdquo; parameter whose default value is 1.\n1 2 3 4 5 6 7  g = sns.relplot(x=\u0026#34;weight\u0026#34;, y=\u0026#34;horsepower\u0026#34;, data=mpg, kind=\u0026#34;scatter\u0026#34;) g.fig.suptitle(\u0026#34;Car Weight vs. Horsepower\u0026#34;, y=1) plt.show()   1 2 3 4 5 6 7 8  sns.set_palette(\u0026#34;Blues\u0026#34;) g = sns.catplot(x=\u0026#34;Gender\u0026#34;, y=\u0026#34;Age\u0026#34;, data=survey_data, kind=\u0026#34;box\u0026#34;, hue=\u0026#34;Interested in Pets\u0026#34;) g.fig.suptitle(\u0026#34;Age of Those Interested in Pets vs. Not\u0026#34;) plt.show()   To add a title to an AxesSubplot object, call the method g.set_titles(). You can also use the y parameter here to adjust the height of the title. Notice that a FacetGrid consists of one or more AxesSubplots, thus you can set title on each of subplot in a plot.\nSince g is a FacetGrid object, using \u0026ldquo;g.fig.suptitle()\u0026rdquo; will add a title to the figure as a whole. To alter the subplot titles, call \u0026ldquo;g.set_titles()\u0026rdquo; repeatly to set the titles for each AxesSubplot. If you want to use the variable name in the title, you can use {col_name} to reference the column value.\n1 2 3 4 5 6 7 8 9 10  g = sns.relplot(x=\u0026#34;model_year\u0026#34;, y=\u0026#34;mpg_mean\u0026#34;, data=mpg_mean, col=\u0026#34;origin\u0026#34;, kind=\u0026#34;line\u0026#34;) g.fig.suptitle(\u0026#34;Average MPG Over Time\u0026#34;) g.set_titles(\u0026#34;EU\u0026#34;) g.set_titles(\u0026#34;JP\u0026#34;) g.set_titles(\u0026#34;US\u0026#34;) plt.show()   or\n1 2 3 4 5 6 7 8  g = sns.relplot(x=\u0026#34;model_year\u0026#34;, y=\u0026#34;mpg_mean\u0026#34;, data=mpg_mean, col=\u0026#34;origin\u0026#34;, kind=\u0026#34;line\u0026#34;) g.fig.suptitle(\u0026#34;Average MPG Over Time\u0026#34;) g.set_titles(\u0026#34;{col_name}\u0026#34;) plt.show()   To add axis labels, assign the plot to a variable and then call the \u0026ldquo;set\u0026rdquo; function. Set the parameters \u0026ldquo;xlabel\u0026rdquo; and \u0026ldquo;ylabel\u0026rdquo; to set the desired x-axis and y-axis labels, respectively. This works with both FacetGrid and AxesSubplot objects.\n1 2 3 4 5 6 7 8 9 10 11  g = sns.lineplot(x=\u0026#34;model_year\u0026#34;, y=\u0026#34;mpg_mean\u0026#34;, data=mpg_mean, hue=\u0026#34;origin\u0026#34;) g.set_title(\u0026#34;Average MPG Over Time\u0026#34;) g.set( xlabel=\u0026#34;Car Model Year\u0026#34;, ylabel=\u0026#34;Average MPG\u0026#34; ) plt.show()   1 2 3 4 5 6 7 8 9 10 11 12  sns.set_style(\u0026#34;darkgrid\u0026#34;) sns.set_palette(\u0026#34;RdBu_r\u0026#34;) g = sns.catplot(x=\u0026#34;Village - town\u0026#34;, y=\u0026#34;Likes Techno\u0026#34;, data=survey_data, kind=\u0026#34;bar\u0026#34;, col=\u0026#34;Gender\u0026#34;) g.fig.suptitle(\u0026#34;Percentage of Young People Who Like Techno\u0026#34;, y=1.02) g.set_titles(\u0026#34;{col_name}\u0026#34;) g.set(xlabel=\u0026#34;Location of Residence\u0026#34;, ylabel=\u0026#34;% Who Like Techno\u0026#34;) plt.show()   Sometimes your tick labels may overlap, one way to address this is by rotating the tick labels. To do this, after we create the plot, we call the matplotlib function \u0026ldquo;plt.xticks()\u0026rdquo; and set \u0026ldquo;rotation=90\u0026rdquo;. This works with both FacetGrid and AxesSubplot objects.\n1 2 3 4 5 6 7 8 9 10  sns.catplot(x=\u0026#34;origin\u0026#34;, y=\u0026#34;acceleration\u0026#34;, data=mpg, kind=\u0026#34;point\u0026#34;, join=False, capsize=0.1) plt.xticks(rotation=90) plt.show()   ","date":"2020-04-20T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/data-visualization-ii-seaborn/","title":"Data Visualization II (Seaborn)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Introduction to Matplotlib Intro to data visualization with matplotlib we will use the main object-oriented interface. This interface is provided through the pyplot submodule. The plt.subplots() command, when called without any inputs, creates two different objects: a Figure object and an Axes object.\n1 2  import matplotlib.pyplot as plt fig, ax = plt.subplots()   The Figure object is a container that holds everything that you see on the page. Meanwhile, the Axes is the part of the page that holds the data. It is the canvas on which we will draw with our data.\nTo add the data to the Axes, we call a ax.plot() command. If you want, you can add more data to the plot by this way. Finally, we call the plt.show() function to show the effect of the plotting command.\n1 2 3 4 5 6 7 8  import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.plot(x=df.col1, y=df.col2) ax.plot(x=df.col1, y=df.col3) ax.plot(df2[\u0026#34;col1\u0026#34;], df2[\u0026#34;col2\u0026#34;]) # you can also add data to a figure from different tables. plt.show()   Here is what all of the code to create this figure would then look like.\n First, we create the Figure and the Axes objects. We call the Axes method plot to add first the df.col2, and then the df.col3 to the Axes. Finally, we ask Matplotlib to show us the figure.  Customizing Plots The plot method takes an optional keyword argument, marker, which lets you indicate what kind of markers you\u0026rsquo;d like. To see all the possible marker styles, you can visit this page in the Matplotlib online documentation.\nYou can change the appearance of these connecting lines by adding the linestyle keyword argument. Like marker shapes, there are a few linestyles you can choose from, listed in this page.\nYou can even go so far as to eliminate the lines altogether, by passing the string \u0026ldquo;None\u0026rdquo; as input to this keyword argument.\nyou can also add the color argument. For example, here we\u0026rsquo;ve chosen to show this data in red, indicated by the letter \u0026ldquo;r\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # Plot Seattle data, setting data appearance ax.plot( seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;], color=\u0026#34;b\u0026#34;, marker=\u0026#34;o\u0026#34;, linestyle=\u0026#34;--\u0026#34; ) # Plot Austin data, setting data appearance ax.plot( austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;], color=\u0026#34;r\u0026#34;, marker=\u0026#34;v\u0026#34;, linestyle=\u0026#34;--\u0026#34; ) plt.show()   The Axes object has several methods that start with the word set. These are methods that you can use to change certain properties of the object, before calling show to display it.\nFor example, there is a set_xlabel method that you can use to set the value of the label of the x-axis.\nSimilarly, a set_ylabel method customizes the label that is associated with the y-axis.\nFinally, you can also add a title to your Axes using the set_title method.\n1 2 3 4 5 6 7 8  ax.plot(seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;]) ax.plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;]) ax.set_xlabel(\u0026#34;Time (months)\u0026#34;) ax.set_ylabel(\u0026#34;Precipitation (inches)\u0026#34;) ax.set_title(\u0026#34;Weather patterns in Austin and Seattle\u0026#34;) plt.show()   Small Multiples In some cases, adding more data to a plot can make the plot too busy, obscuring patterns rather than revealing them. One way to overcome this kind of mess is to use what are called small multiples. These are multiple small plots that show similar data across different conditions.\nIn Matplotlib, small multiples are called sub-plots. That is also the reason that the function that creates these is called subplots.\nPreviously, we called this function with no inputs. This creates one subplot. Now, we\u0026rsquo;ll give it some inputs. Small multiples are typically arranged on the page as a grid with rows and columns. Here, we are creating a Figure object with 2 rows of subplots, and 2 columns. This is what this would look like before we add any data to it.\n1 2  fig, ax = plt.sublpots(2, 2) plt.show()   In this case, the variable ax is no longer only one Axes object. Instead, it is an numpy.ndarray of Axes objects with a shape of 2 by 2.\n1 2  print(ax.shape) print(type(ax))   (2, 2) \u0026lt;class 'numpy.ndarray'\u0026gt; To add data, we would now have to index into this object and call the plot method on an element of the array.\n1 2 3 4 5 6 7 8  fig, ax = plt.subplots(2, 2) ax[0, 0].plot(seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;]) ax[0, 1].plot(seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-TAVG-NORMAL\u0026#34;]) ax[1, 0].plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;]) ax[1, 1].plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-TAVG-NORMAL\u0026#34;]) plt.show()   There is a special case for situations where you have only one row or only one column of plots. In this case, the resulting array will be one-dimensional and you will only have to provide one index to access the elements of this array.\n1 2  fig, ax = plt.subplots(3, 1) ax.shape   (3,) To make sure that all the subplots have the same range of y-axis values, we initialize the figure and its subplots with the key-word argument sharey set to True. This means that both subplots will have the same range of y-axis values.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  fig, ax = plt.subplots(2, 1, sharey=True) ax[0].plot(seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;], color = \u0026#34;b\u0026#34;) ax[0].plot(seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-PRCP-25PCTL\u0026#34;], color = \u0026#34;b\u0026#34;, linestyle = \u0026#34;--\u0026#34;) ax[0].plot(seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-PRCP-75PCTL\u0026#34;], color = \u0026#34;b\u0026#34;, linestyle = \u0026#34;--\u0026#34;) ax[1].plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;], color = \u0026#34;r\u0026#34;) ax[1].plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-PRCP-25PCTL\u0026#34;], color = \u0026#34;r\u0026#34;, linestyle = \u0026#34;--\u0026#34;) ax[1].plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-PRCP-75PCTL\u0026#34;], color = \u0026#34;r\u0026#34;, linestyle = \u0026#34;--\u0026#34;) ax[0].set_ylabel(\u0026#34;Seattle\u0026#34;) ax[1].set_ylabel(\u0026#34;Austin\u0026#34;) ax[1].set_xlabel(\u0026#34;MONTH\u0026#34;) plt.show()   Plotting time-series Plotting time-series data If we want pandas to recognize column with YYYY-MM-DD format as time-series, we\u0026rsquo;ll need to tell it to parse the this column as a date by parse_dates argument in read_csv method. To use the full power of pandas indexing facilities, we\u0026rsquo;ll also designate the date column as our index by using the index_col argument in read_csv method.\n1 2 3 4 5 6 7  (base) wanghaoming@localhost ~ % head -6 ~/Downloads/climate_change.csv date,co2,relative_temp 1958-03-06,315.71,0.1 1958-04-06,317.45,0.01 1958-05-06,317.5,0.08 1958-06-06,,-0.05 1958-07-06,315.86,0.06   1 2 3 4 5  climate_change = pd.read_csv( \u0026#34;~/Downloads/climate_change.csv\u0026#34;, parse_dates=[\u0026#34;date\u0026#34;], index_col=\u0026#34;date\u0026#34; )   Matplotlib automatically chooses to show the time on the x-axis with intervals of 10 years or years or month depending on the length of data.\n1 2 3 4 5 6 7 8 9 10 11  fig, ax = plt.subplots() ax.plot( climate_change.index, climate_change[\u0026#34;relative_temp\u0026#34;] ) ax.set_xlabel(\u0026#34;Time\u0026#34;) ax.set_ylabel(\u0026#34;Relative temperature (Celsius)\u0026#34;) plt.show()   1 2 3 4 5 6  fig, ax = plt.subplots() seventies = climate_change[\u0026#34;1970-01-01\u0026#34;: \u0026#34;1979-12-31\u0026#34;] ax.plot(seventies.index, seventies[\u0026#34;co2\u0026#34;]) plt.show()   Plotting time-series with different variables You could plot two time-series in separate sub-plots. Here, we\u0026rsquo;re going to plot them in the same sub-plot, using two different y-axis scales.\nWe start by adding the first variable to the Axes. Then, we use the twinx method to create a twin of this Axes. This means that the two Axes share the same x-axis, but the y-axes are separate.\nWe add the other variable to this second Axes object and show the figure. There is one y-axis scale on the left, and another y-axis scale to the right.\n1 2 3 4 5 6 7 8  fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[\u0026#34;co2\u0026#34;], color=\u0026#34;b\u0026#34;) ax2 = ax.twinx() ax2.plot(climate_change.index, climate_change[\u0026#34;relative_temp\u0026#34;], color=\u0026#34;r\u0026#34;) plt.show()   We set color argument in our calls to plot and set_ylabel method from the origin Axes twin Axes object. In the resulting figure, each variable has its own color and the y-axis labels clearly tell us which scale belongs to which variable.\nWe can also make encoding by color more distinct by setting y-axis ticks and the y-axis tick labels. This is done by adding a call to the tick_params method. This method takes either y or x as its first argument, pointing to the fact that we are modifying the parameters of the y-axis or x-axis ticks and tick labels. To change their color, we use the colors key-word argument.\n1 2 3 4 5 6 7 8 9 10 11 12 13  fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[\u0026#34;co2\u0026#34;], color=\u0026#34;b\u0026#34;) ax.set_ylabel(\u0026#34;CO2\u0026#34;, color=\u0026#34;b\u0026#34;) ax.tick_params(\u0026#34;y\u0026#34;, colors=\u0026#34;b\u0026#34;) # \u0026lt;-- Notice \u0026#34;s\u0026#34; ax.set_xlabel(\u0026#34;Year\u0026#34;) ax2 = ax.twinx() ax2.plot(climate_change.index, climate_change[\u0026#34;relative_temp\u0026#34;], color=\u0026#34;r\u0026#34;) ax2.set_ylabel(\u0026#34;Relative Temp\u0026#34;, color=\u0026#34;r\u0026#34;) ax2.tick_params(\u0026#34;y\u0026#34;, colors=\u0026#34;r\u0026#34;) plt.show()   We can implement this as a function that we can reuse. Using our function, we don\u0026rsquo;t have to repeat these calls, and the code is simpler.\n1 2 3 4 5  def plot_timeseries(axes, x, y, color, xlabel, ylabel): axes.plot(x, y, color=color) axes.set_xlabel(xlabel) axes.set_ylabel(ylabel, color=color) axes.tick_params(\u0026#39;y\u0026#39;, colors=color)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  fig, ax = plt.subplots() plot_timeseries( ax, climate_change.index, climate_change[\u0026#34;co2\u0026#34;], \u0026#34;blue\u0026#34;, \u0026#34;Time (years)\u0026#34;, \u0026#34;CO2 levels\u0026#34; ) ax2 = ax.twinx() plot_timeseries( ax2, climate_change.index, climate_change[\u0026#34;relative_temp\u0026#34;], \u0026#34;red\u0026#34;, \u0026#34;Time (years)\u0026#34;, \u0026#34;Relative temperature (Celsius)\u0026#34; ) plt.show()   Annotating time-series data Annotations are usually small pieces of text that refer to a particular part of the visualization, focusing our attention on some feature of the data and explaining this feature.\nWe call a method of the Axes object called annotate. This function takes the annotation text as input, and the xy coordinate that we would like to annotate. The annotate method takes an optional xy text argument that selects the xy position of the text.\n1 2 3 4 5 6 7 8 9 10  fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[\u0026#34;relative_temp\u0026#34;]) ax.annotate( \u0026#34;\u0026gt;1 degree\u0026#34;, xy=(pd.Timestamp(\u0026#39;2015-10-06\u0026#39;), 1), xytext=(pd.Timestamp(\u0026#39;2010-10-06\u0026#39;), 0) ) plt.show()   To connect between the annotation text and the annotated data, we can add an arrow. The key-word argument of annotate method to do this is called arrowprops. This argument takes as input a dictionary that defines the properties of the arrow that we would like to use.\nIf we pass an empty dictionary into the key-word argument, the arrow will have the default properties. We can also customize the appearance of the arrow, by dictionary key \u0026ldquo;arrowstyle\u0026rdquo; and \u0026ldquo;color\u0026rdquo;. Look more customizing method here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  fig, ax = plt.subplots() plot_timeseries( ax, climate_change.index, climate_change[\u0026#34;co2\u0026#34;], \u0026#39;blue\u0026#39;, \u0026#34;Time (years)\u0026#34;, \u0026#34;CO2 levels\u0026#34; ) ax2 = ax.twinx() plot_timeseries( ax2, climate_change.index, climate_change[\u0026#34;relative_temp\u0026#34;], \u0026#39;red\u0026#39;, \u0026#34;Time (years)\u0026#34;, \u0026#34;Relative temp (Celsius)\u0026#34; ) ax2.annotate( \u0026#34;\u0026gt;1 degree\u0026#34;, xy=(pd.Timestamp(\u0026#39;2015-10-06\u0026#39;), 1), xytext=(pd.Timestamp(\u0026#39;2008-10-06\u0026#39;), -0.2), arrowprops = { \u0026#34;arrowstyle\u0026#34;: \u0026#34;-\u0026gt;\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;gray\u0026#34; } ) plt.show()   Quantitative comparisons and statistical visualizations Quantitative comparisons: bar-charts We will use a dataset that contains information about the number of medals won by a few countries in the 2016 Olympic Games.\n1 2 3 4 5 6 7  (base) wanghaoming@localhost ~ % head -6 ~/Downloads/medals_by_country_2016.csv ,Bronze,Gold,Silver United States,67,137,52 Germany,67,47,43 Great Britain,26,64,55 Russia,35,50,28 China,35,44,30   We tell pandas to create a DataFrame from a file that contains the data and to use the first column as the index for the DataFrame.\n1  medals = pd.read_csv(\u0026#34;~/Downloads/medals_by_country_2016.csv\u0026#34;, index_col=0)   We create a Figure and an Axes object and call the Axes bar method to create a bar chart. The labels of the x-axis ticks can be rotated by using the set_xticklabels method of the Axes.\n1 2 3 4 5 6 7  fig, ax = plt.subplots() ax.bar(medals.index, medals[\u0026#34;Gold\u0026#34;]) ax.set_xticklabels(medals.index, rotation=90) # \u0026lt;-- Notice here ax.set_ylabel(\u0026#34;Number of medals\u0026#34;) plt.show()   To add others information into the same plot, we\u0026rsquo;ll create a stacked bar chart. We add another call to the bar method to add the other data. We add the bottom key-word argument to tell Matplotlib that the bottom of this data should be at the height of the previous data.\nWe also need to add the label key-word argument to each call of the bar method with the label for the bars plotted in this call. Then add a call to the Axes legend method before calling show.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  fig, ax = plt.subplots() ax.bar(medals.index, medals[\u0026#34;Gold\u0026#34;], label=\u0026#34;Gold\u0026#34;) ax.bar( medals.index, medals[\u0026#34;Silver\u0026#34;], bottom=medals[\u0026#34;Gold\u0026#34;] , label=\u0026#34;Silver\u0026#34; ) ax.bar( medals.index, medals[\u0026#34;Bronze\u0026#34;], bottom=medals[\u0026#34;Gold\u0026#34;] + medals[\u0026#34;Silver\u0026#34;], # \u0026lt;-- Notice here label=\u0026#34;Bronze\u0026#34; ) ax.legend() plt.show()   Quantitative comparisons: histograms A histogram would show the full distribution of values within each variable. We call the Axes hist method with the column of the DataFrame.\nAs before, we can label a variable by calling the hist method with the label key-word argument and then calling the legend method before we call plt.show.\n1 2 3 4 5 6 7 8 9 10  fig, ax = plt.subplots() ax.hist(mens_rowing[\u0026#34;Weight\u0026#34;], label=\u0026#34;Rowing\u0026#34;) ax.hist(mens_gymnastics[\u0026#34;Weight\u0026#34;], label=\u0026#34;Gymnastics\u0026#34;) ax.set_xlabel(\u0026#34;Weight (kg)\u0026#34;) ax.set_ylabel(\u0026#34;# of observations\u0026#34;) ax.legend() plt.show()   The number of default bars or bins in a histogram is 10, but we can customize that. If we provide an scalar to the bins key-word argument, the histogram will have that number of bins.\nIf we instead provide a list, these numbers will be set to be the boundaries between the bins.\nThe occlusion of two histogram plots can be eliminated by changing the type of histogram that is used. you can specify a histtype of \u0026ldquo;step\u0026rdquo;, which displays the histogram as thin lines, instead of solid bars.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  fig, ax = plt.subplots() ax.hist( mens_rowing[\u0026#34;Weight\u0026#34;], label=\u0026#34;Rowing\u0026#34;, bins=5, histtype=\u0026#34;step\u0026#34; ) ax.hist( mens_gymnastics[\u0026#34;Weight\u0026#34;], label=\u0026#34;Gymnastics\u0026#34;, bins=5, histtype=\u0026#34;step\u0026#34; ) ax.set_xlabel(\u0026#34;Weight (kg)\u0026#34;) ax.set_ylabel(\u0026#34;# of observations\u0026#34;) ax.legend() plt.show()   Statistical plotting The first is the use of error bars in plots. These are additional markers on a plot or bar chart that tell us something about the distribution of the data.\n Histograms show the entire distribution. Error bars instead summarize the distribution of the data in one number, such as the standard deviation of the values.\n There are at least two different ways to display error bars. Here, we add the error bar as an argument to a bar chart. Each call to the ax.bar method takes an x argument and a y argument. The yerr key-word argument takes an additional number.\n1 2 3 4 5 6 7  fig, ax = plt.subplots() ax.bar(\u0026#34;Rowing\u0026#34;, mens_rowing[\u0026#34;Height\u0026#34;].mean(), yerr=mens_rowing[\u0026#34;Height\u0026#34;].std()) ax.bar(\u0026#34;Gymnastics\u0026#34;, mens_gymnastics[\u0026#34;Height\u0026#34;].mean(), yerr=mens_gymnastics[\u0026#34;Height\u0026#34;].std()) ax.set_ylabel(\u0026#34;Height (cm)\u0026#34;) plt.show()   It summarizes the full distribution that you saw in the histograms in two numbers: the mean value, and the spread of values, quantified as the standard deviation.\nWe can also add error bars to a line plot. To plot this data with error bars, we will use the ax.errorbar method. This method takes a sequence of x values, y values and yerr values.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  fig, ax = plt.subplots() ax.errorbar( seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-TAVG-NORMAL\u0026#34;], seattle_weather[\u0026#34;MLY-TAVG-STDDEV\u0026#34;] ) ax.errorbar( austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-TAVG-NORMAL\u0026#34;], austin_weather[\u0026#34;MLY-TAVG-STDDEV\u0026#34;] ) ax.set_ylabel(\u0026#34;Temperature (Fahrenheit)\u0026#34;) plt.show()   The second statistical visualization technique is the boxplot. It is implemented by ax.boxplot method. We can call it with a sequence of sequences. We add labels on each of the variables separately by calling ax.set_xticklabels method, labeling the y-axis as well.\n1 2 3 4 5 6 7 8 9 10 11 12 13  fig, ax = plt.subplots() ax.boxplot( [ mens_rowing[\u0026#34;Height\u0026#34;], mens_gymnastics[\u0026#34;Height\u0026#34;], ] ) ax.set_xticklabels([\u0026#34;Rowing\u0026#34;, \u0026#34;Gymnastics\u0026#34;]) ax.set_ylabel(\u0026#34;Height (cm)\u0026#34;) plt.show()   Quantitative comparisons: scatter plots A standard visualization for bi-variate comparisons is a scatter plot. To create this plot, we initialize a Figure and Axes objects and call the Axes scatter method.\nWe can customize scatter plots in a manner that is similar to the customization that we introduced in other plots. For example, we set the color argument to set color on each group of points.\n1 2 3 4 5 6 7 8 9 10 11 12 13  fig, ax = plt.subplots() eighty = climate_change[\u0026#34;1980-01-01\u0026#34;: \u0026#34;1989-12-31\u0026#34;] ninety = climate_change[\u0026#34;1990-01-01\u0026#34;: \u0026#34;1999-12-31\u0026#34;] ax.scatter(eighty[\u0026#34;co2\u0026#34;], eighty[\u0026#34;relative_temp\u0026#34;], color=\u0026#34;b\u0026#34;, label=\u0026#34;1980s\u0026#34;) ax.scatter(ninety[\u0026#34;co2\u0026#34;], ninety[\u0026#34;relative_temp\u0026#34;], color=\u0026#34;r\u0026#34;, label=\u0026#34;1990s\u0026#34;) ax.set_xlabel(\u0026#34;CO2 (ppm)\u0026#34;) ax.set_ylabel(\u0026#34;Relative temperature (C)\u0026#34;) ax.legend() plt.show()   But we can also enter a Serise as input to the c key-word argument, this variable will get encoded as color.\n Note that this is not the color key-word argument that we used before, but is instead just the letter c.\n Now, time of the measurements is encoded in the brightness of the color applied to the points, with dark blue points early on and later points in bright yellow.\n1 2 3 4 5 6 7 8 9 10 11 12  fig, ax = plt.subplots() ax.scatter( climate_change[\u0026#34;co2\u0026#34;], climate_change[\u0026#34;relative_temp\u0026#34;], c=climate_change.index ) ax.set_xlabel(\u0026#34;CO2 (ppm)\u0026#34;) ax.set_ylabel(\u0026#34;Relative temperature (C)\u0026#34;) plt.show()   Sharing visualizations with others Plot style we\u0026rsquo;ll change the overall style of the figure. we call plt.style.use() method before the plotting code, the figure style will look completely different.\nwe can choose \u0026ldquo;ggplot\u0026rdquo; style, or \u0026ldquo;seaborn-colorblind\u0026rdquo;, click here to find more style.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  plt.style.use(\u0026#34;seaborn\u0026#34;) fig, ax = plt.subplots() ax2 = ax.twinx() ax.plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-TAVG-NORMAL\u0026#34;], color=\u0026#34;b\u0026#34;) ax2.plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-TAVG-STDDEV\u0026#34;], color=\u0026#34;r\u0026#34;) ax.set_xlabel(\u0026#34;MONTH\u0026#34;) ax.set_ylabel(\u0026#34;MLY-TAVG-NORMAL\u0026#34;, color=\u0026#34;b\u0026#34;) ax.tick_params(\u0026#34;y\u0026#34;, colors=\u0026#34;b\u0026#34;) ax2.set_ylabel(\u0026#34;MLY-TAVG-NORMAL\u0026#34;, color=\u0026#34;r\u0026#34;) ax2.tick_params(\u0026#34;y\u0026#34;, colors=\u0026#34;r\u0026#34;) ax.set_title(\u0026#34;Title\u0026#34;) plt.show()   Saving visualizations Now, we replace the call to plt.show() with a call to the Figure object\u0026rsquo;s fig.savefig method. We provide a file-path as input to the function. If we do this, the figure will no longer appear on our screen, but instead appear as a file on our file-system.\nUsing savefig method, we can saved the figure as a png, jpg, svg \u0026hellip; file. You can control how small the resulting file will be, and the degree of loss of quality, by setting the quality key-word argument. This will be a number between 1 and 100. you can also control the quality through setting the dpi key-word argument.\n1  fig.savefig(\u0026#34;~/Documents/figure.png\u0026#34;, dpi=300)   Finally, you can call set_size_inches method to control is the size of the figure. This function takes a list. The first number sets the width of the figure and the second number sets the height of the figure.\n1 2  fig.set_size_inches([3,5]) fig.savefig(\u0026#34;~/Documents/figure_3_5.png\u0026#34;)   Automating figures from data you can write functions and programs that automatically adjust what they are doing based on the input data.\n1 2 3 4 5 6 7 8 9 10 11 12 13  sports_column = summer_2016_medals[\u0026#34;Sport\u0026#34;] sports = sports_column.unique() fig, ax = plt.subplots() for sport in sports: sport_df = summer_2016_medals[summer_2016_medals[\u0026#34;Sport\u0026#34;]==sport] ax.bar(sport, sport_df[\u0026#34;Weight\u0026#34;].mean(), yerr=sport_df[\u0026#34;Weight\u0026#34;].std()) ax.set_ylabel(\u0026#34;Weight\u0026#34;) ax.set_xticklabels(sports, rotation=90) plt.show()   You can also do this to achieve a similar effect:\n1  summer_2016_medals.groupby(\u0026#34;Sport\u0026#34;)[\u0026#34;Weight\u0026#34;].mean().plot(kind=\u0026#34;bar\u0026#34;)   ","date":"2020-04-16T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/data-visualization-i-matplotlib/","title":"Data Visualization I (Matplotlib)"},{"content":"","date":"2020-03-04T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/high-performance-python-v-big-data/","title":"High-Performance Python V (Big Data)"},{"content":"1 2 3 4  import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline   Linear separable Generate the data: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  randlist1 = np.array([np.random.uniform(-5,5) for i in range(500)]) randlist2 = np.array([np.random.uniform(0,3) for i in range(500)]) data_p1_0 = pd.DataFrame( { \u0026#34;x0\u0026#34;: 1, \u0026#34;x1\u0026#34;: randlist1, \u0026#34;x2\u0026#34;: randlist1 + randlist2, \u0026#34;y\u0026#34; : -1 } ) data_m1_0 = pd.DataFrame( { \u0026#34;x0\u0026#34;: 1, \u0026#34;x1\u0026#34;: randlist1, \u0026#34;x2\u0026#34;: randlist1 - randlist2, \u0026#34;y\u0026#34; : +1 } ) df = pd.concat([data_m1_0, data_p1_0], axis=0)   PLA model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  plt.figure(figsize=(8,6)) plt.scatter(data_m1_0[\u0026#34;x1\u0026#34;], data_m1_0[\u0026#34;x2\u0026#34;], c=\u0026#34;r\u0026#34;, alpha=0.5, linewidths=0) plt.scatter(data_p1_0[\u0026#34;x1\u0026#34;], data_p1_0[\u0026#34;x2\u0026#34;], c=\u0026#34;b\u0026#34;, alpha=0.5, linewidths=0) w = pd.Series([0, 0, 0], index=[\u0026#34;x0\u0026#34;, \u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;]); times = 1; wl = []; x=np.linspace(-5,5,100) wl.append(w) mis_p = df[df[\u0026#34;y\u0026#34;]==+1][np.dot(df[df[\u0026#34;y\u0026#34;]==+1].iloc[:,0:df.shape[1]-1], w) \u0026lt;= 0] mis_n = df[df[\u0026#34;y\u0026#34;]==-1][np.dot(df[df[\u0026#34;y\u0026#34;]==-1].iloc[:,0:df.shape[1]-1], w) \u0026gt; 0] mis = pd.concat([mis_p, mis_n], axis=0) length = mis.shape[0] while length \u0026gt; 0: mis_point = mis.iloc[np.random.randint(length),:] w = w + mis_point[\u0026#34;y\u0026#34;] * mis_point.iloc[0: mis_point.shape[0]-1] mis_p = df[df[\u0026#34;y\u0026#34;]==+1][np.dot(df[df[\u0026#34;y\u0026#34;]==+1].iloc[:,0:df.shape[1]-1], w) \u0026lt;= 0] mis_n = df[df[\u0026#34;y\u0026#34;]==-1][np.dot(df[df[\u0026#34;y\u0026#34;]==-1].iloc[:,0:df.shape[1]-1], w) \u0026gt; 0] mis = pd.concat([mis_p, mis_n], axis=0) length = mis.shape[0] times += 1; wl.append(w) if times in [1, 2, 3, 4, 5, 10, 15, 50, 100]: plt.plot(x, -(w[1]/w[2])*x - (w[0]/w[2]), \u0026#34;--\u0026#34;, label=f\u0026#34;{times}hypothesis\u0026#34;) if times \u0026gt; 50000 : print(\u0026#34;over 50000 times loops\u0026#34;) break plt.plot(x, -(w[1]/w[2])*x - (w[0]/w[2]), \u0026#34;c\u0026#34;, label=f\u0026#34;end hypothesis (times={times})\u0026#34;) plt.legend() plt.savefig(\u0026#34;/Users/wanghaoming/Documents/LaTeX_doc/Machine_Learning/pla.png\u0026#34;, bbox_inches=\u0026#39;tight\u0026#39;, dpi=500)   Convergence process Convergence process of weight vector: $\\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_t}{||\\mathbf{w}_f||\\cdot||\\mathbf{w}_t||}$\n1 2 3 4 5 6 7 8 9 10 11 12  wl1 = pd.DataFrame(wl) wl2 = wl1[[\u0026#34;x0\u0026#34;, \u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;]].reset_index(drop=True) wf = np.array([0,1,-1]) wt = wl2.dot(wf) / (np.array([wl2.loc[v].dot(wl2.loc[v]) for v in wl2.index]) * wf.dot(wf)) ** 0.5 plt.figure(figsize=(8,6)) plt.plot( range(wt.shape[0]), wt, label= r\u0026#34;$\\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_t}{||\\mathbf{w}_f||\\cdot||\\mathbf{w}_t||}$\u0026#34; ) plt.legend() plt.savefig(\u0026#34;/Users/wanghaoming/Documents/LaTeX_doc/Machine_Learning/converg.png\u0026#34;, bbox_inches=\u0026#39;tight\u0026#39;, dpi=500)   Calculate the maximum number of iterations: we have\n$$ \\mathbb{w}_{f}^{T} = \\mathbb{w}_{t} $$\nand\n$$ \\begin{aligned} ||\\mathbf{w}_{t+1}||^2 \u0026amp;= || \\mathbf{w}_{t}+y_{n(t)}\\mathbf{x}_{n(t)} ||^2\\\\ \u0026amp;= ||\\mathbf{w}_{t}||^2+2y_{n(t)}\\mathbf{w}_{t}^{T}\\mathbf{x}_{n(t)} + y_{n(t)}^{2}||\\mathbf{x}_{n}||^2\\\\ \u0026amp;\\leq ||\\mathbf{w}_{t}||^2+||\\mathbf{x}_{n}||^2\\\\ \u0026amp;\\leq ||\\mathbf{w}_{t}||^2 + \\max_{n}||\\mathbf{x}_{n}||^2. \\end{aligned} $$\ndefine $R^2 = \\max_{n}||\\mathbf{x}||^2$ and $\\rho = \\min_{n}\\frac{y_n\\mathbf{w}_{f}^{T}\\mathbf{x}_n}{||\\mathbf{w}_f||}$\nwe have\n$$ \\begin{aligned} ||\\mathbf{w}t||^2 \u0026amp;\\leq ||\\mathbf{w}{t-1}||^2 + R^2\\ \u0026amp;\\leq ||\\mathbf{w}{t-2}||^2 + 2R^2\\ \u0026amp;\\cdots\\ \u0026amp;\\leq ||\\mathbf{w}{0}||^2 + tR^2\\ \u0026amp;=tR^2. \\end{aligned} $$\ni.e. $||\\mathbf{w}_t||\\leq R\\sqrt{t}$\nsince\n1 2 3 4 5 6  $$\\begin{aligned} x +y \u0026amp;=z \\\\z +s \u0026amp;=t \\end{aligned} $$   $$ \\begin{aligned} x + y \u0026amp;= z \\\\ z + s \u0026amp;= t \\end{aligned} $$\n1 2 3 4 5 6 7 8 9 10 11 12  $$\\begin{aligned} \\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_t}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} \u0026amp; =\\frac{\\mathbf{w}_{f}^{T}(\\mathbf{w}_{t-1}+y_{n(t-1)}\\mathbf{x}_{n(t-1)})}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||}\\\\\u0026amp;=\\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_{t-1}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} +\\frac{y_{n(t-1)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(t-1)}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||}\\\\\u0026amp;=\\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_{t-2}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} +\\frac{y_{n(t-2)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(t-2)}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} +\\frac{y_{n(t-1)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(t-1)}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||}\\\\\u0026amp;\\cdots\\\\\u0026amp;=\\frac{1}{||\\mathbf{w}_t||}\\cdot\\sum^{t}_{i=1}\\frac{y_{n(i)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(i)}}{||\\mathbf{w}_{f}^{T}||} \\\\\u0026amp;\\geq\\frac{1}{||\\mathbf{w}_t||}\\cdott\\cdot\\rho\\geq\\frac{t\\rho}{R\\sqrt{t}}\\\\\u0026amp;=\\frac{\\rho}{R}\\sqrt{t}. \\end{aligned} $$   $$ \\begin{aligned} \\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_t}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} \u0026amp; = \\frac{\\mathbf{w}_{f}^{T}(\\mathbf{w}_{t-1}+y_{n(t-1)}\\mathbf{x}_{n(t-1)})}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||}\\\\ \u0026amp;=\\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_{t-1}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} + \\frac{y_{n(t-1)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(t-1)}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||}\\\\ \u0026amp;= \\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_{t-2}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} + \\frac{y_{n(t-2)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(t-2)}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} + \\frac{y_{n(t-1)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(t-1)}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||}\\\\ \u0026amp;\\cdots\\\\ \u0026amp;=\\frac{1}{||\\mathbf{w}_t||}\\cdot\\sum^{t}_{i=1}\\frac{y_{n(i)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(i)}}{||\\mathbf{w}_{f}^{T}||} \\\\ \u0026amp;\\geq \\frac{1}{||\\mathbf{w}_t||}\\cdot t\\cdot \\rho\\geq \\frac{t\\rho}{R\\sqrt{t}}\\\\ \u0026amp;=\\frac{\\rho}{R}\\sqrt{t}. \\end{aligned} $$\nwe have that $\\frac{\\sqrt{t}\\rho}{R}\\leq 1$ and $t\\leq \\frac{R^2}{\\rho^2}$\n1 2 3 4 5 6  x_vec = df[[\u0026#34;x0\u0026#34;, \u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;]].reset_index(drop=True) y = df[\u0026#34;y\u0026#34;] R = (np.array([x_vec.loc[r].dot(x_vec.loc[r]) for r in x_vec.index]).max()) ** 0.5 rhot = ((np.array([wf.dot(x_vec.loc[v]) for v in x_vec.index]) * y) / (wf.dot(wf))**0.5) rho = rhot.min() print(R, rho)   9.391078270239522 0.0014954984184524147  therefore the maximum iterations of this model is\n1 2  maxt = (R/rho)**2 maxt   39432926.04282378  Non-Linear separable Generate the data: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  randlist1 = np.array([np.random.uniform(-5,5) for i in range(500)]) randlist2 = np.array([np.random.uniform(-0.1,3) for i in range(500)]) data_p1_0 = pd.DataFrame( { \u0026#34;x0\u0026#34;: 1, \u0026#34;x1\u0026#34;: randlist1, \u0026#34;x2\u0026#34;: randlist1 + randlist2, \u0026#34;y\u0026#34; : -1 } ) data_m1_0 = pd.DataFrame( { \u0026#34;x0\u0026#34;: 1, \u0026#34;x1\u0026#34;: randlist1, \u0026#34;x2\u0026#34;: randlist1 - randlist2, \u0026#34;y\u0026#34; : +1 } ) df = pd.concat([data_m1_0, data_p1_0], axis=0) plt.figure(figsize=(8,6)) plt.scatter(data_m1_0[\u0026#34;x1\u0026#34;], data_m1_0[\u0026#34;x2\u0026#34;], c=\u0026#34;r\u0026#34;, alpha=0.5, linewidths=0) plt.scatter(data_p1_0[\u0026#34;x1\u0026#34;], data_p1_0[\u0026#34;x2\u0026#34;], c=\u0026#34;b\u0026#34;, alpha=0.5, linewidths=0) plt.savefig(\u0026#34;/Users/wanghaoming/Documents/LaTeX_doc/Machine_Learning/non_sep.png\u0026#34;, bbox_inches=\u0026#39;tight\u0026#39;, dpi=500)   Building Pocket Model: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  plt.figure(figsize=(8,6)) plt.scatter(data_m1_0[\u0026#34;x1\u0026#34;], data_m1_0[\u0026#34;x2\u0026#34;], c=\u0026#34;r\u0026#34;, alpha=0.5, linewidths=0) plt.scatter(data_p1_0[\u0026#34;x1\u0026#34;], data_p1_0[\u0026#34;x2\u0026#34;], c=\u0026#34;b\u0026#34;, alpha=0.5, linewidths=0) w = pd.Series([0, 0, 0], index=[\u0026#34;x0\u0026#34;, \u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;]); times = 1; x=np.linspace(-5,5,100) mis_p = df[df[\u0026#34;y\u0026#34;]==+1][np.dot(df[df[\u0026#34;y\u0026#34;]==+1].iloc[:,0:df.shape[1]-1], w) \u0026lt;= 0] mis_n = df[df[\u0026#34;y\u0026#34;]==-1][np.dot(df[df[\u0026#34;y\u0026#34;]==-1].iloc[:,0:df.shape[1]-1], w) \u0026gt; 0] mis = pd.concat([mis_p, mis_n], axis=0) length = mis.shape[0] lengthl = [] T = 50000 for i in range(T): lengthl.append(length) mis_point = mis.iloc[np.random.randint(mis.shape[0]),:] w1 = w + mis_point[\u0026#34;y\u0026#34;] * mis_point.iloc[0: mis_point.shape[0]-1] mis_p = df[df[\u0026#34;y\u0026#34;]==+1][np.dot(df[df[\u0026#34;y\u0026#34;]==+1].iloc[:,0:df.shape[1]-1], w1) \u0026lt;= 0] mis_n = df[df[\u0026#34;y\u0026#34;]==-1][np.dot(df[df[\u0026#34;y\u0026#34;]==-1].iloc[:,0:df.shape[1]-1], w1) \u0026gt; 0] mis1 = pd.concat([mis_p, mis_n], axis=0) length1 = mis1.shape[0] if length1 \u0026lt; length: w = w1 mis = mis1 length=length1 lengthl.append(length) plt.plot(x, -(w[1]/w[2])*x - (w[0]/w[2]), \u0026#34;c\u0026#34;, label=f\u0026#34;end hypothesis (times={T})\u0026#34;) plt.legend() plt.savefig(\u0026#34;/Users/wanghaoming/Documents/LaTeX_doc/Machine_Learning/pocket.png\u0026#34;, bbox_inches=\u0026#39;tight\u0026#39;, dpi=500)   Error prediction number 1 2 3 4 5 6 7 8  ll = pd.Series(lengthl) plt.figure(figsize=(8,6)) ll.plot() plt.xscale(\u0026#34;log\u0026#34;) for i in range(1, len(ll)): if ll[i] \u0026lt; ll[i-1]: plt.text(i ,ll[i], ll[i]) plt.savefig(\u0026#34;/Users/wanghaoming/Documents/LaTeX_doc/Machine_Learning/minlen.png\u0026#34;, bbox_inches=\u0026#39;tight\u0026#39;, dpi=500)   $\\mathbb{R}\\times\\mathbb{R}$ $$ \\mathbf{R}\\mathbf{R} $$\n\\href{https://katex.org/}{\\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}t}{||\\mathbf{w}{f}^{T}||\\cdot}}\n$$ \\mathbf{w}_{t}^T\\mathbf{w}_{t} $$\n$$ \\mathbf{R}^T\\mathbf{R} $$\n","date":"2020-03-04T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/perceptron-learning-algorithm/","title":"Perceptron Learning Algorithm"},{"content":"","date":"2020-02-16T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/high-performance-python-iv-distribute-computing-pyspark/","title":"High-Performance Python IV (Distribute Computing \u0026 PySpark)"},{"content":"","date":"2020-02-01T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/high-performance-python-iii-multi-process/","title":"High-Performance Python III (Multi Process)"},{"content":"","date":"2020-01-13T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/high-performance-python-ii-multi-threading/","title":"High-Performance Python II (Multi Threading)"},{"content":"","date":"2020-01-01T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/high-performance-python-i-asyncio-coroutine/","title":"High-Performance Python I (Asyncio \u0026 Coroutine)"},{"content":"We have discussed many special objects in Python, like __doc__, __closure__, __name__, __defaults__ and __init()__, __eq__(), __str__(), __repr__, the former names (without parentheses) are called Special Attributes; the later names are called Magic Method. We will elaborate others spacial attribute and magic method detailedly in this blog.\nMagic Method __init__() Constructor constructor will be called automatically when class is called:\n1 2 3 4 5 6  class Cat: def __init__(self, name): self.name = name print(\u0026#34;this is a cat, named {}.\u0026#34;.format(self.name)) cat = Cat(\u0026#34;yaochong\u0026#34;)   this is a cat, named yaochong. __del__() Destructor Destructor will be called automatically when the object is collected (call del keyword).\n1 2 3 4 5 6 7 8 9 10 11 12  class Cat: def __init__(self, name): self.name = name print(\u0026#34;this is a cat, named {}.\u0026#34;.format(self.name)) def __del__(self): print(\u0026#34;cat {}is collected by system.\u0026#34;.format(self.name)) cat = Cat(\u0026#34;yaochong\u0026#34;) print(cat) del cat print(cat)   this is a cat, named yaochong. \u0026lt;__main__.Cat object at 0x104500dc0\u0026gt; cat yaochong is collected by system. --------------------------------------------------------------------------- NameError Traceback (most recent call last) /Users/wanghaoming/Documents/doc1/oop/test2.ipynb Cell 2' in \u0026lt;module\u0026gt; 9 cat = Cat(\u0026quot;yaochong\u0026quot;) 10 del cat ---\u0026gt; 11 print(cat) NameError: name 'cat' is not defined __call__() As we discussed before, there are two kinds of objects in Python, the callable objects and uncallable ones. For example, str, list, dirt can not be called, while function/method, class can. You can check weather or not an object could be called by callable() function.\n1 2  print(callable([1,2,3])) print(callable(list)) # \u0026lt;-- No parenthese   False True __call__() method is a callable object.\n1  callable(object.__call__)   True It will be called when you use a pair of parentheses behind the instance of the classes defined __call__(), and return the result. Thus __call__() method can transform a uncallabel object (instance) to a callable object. As a method, __call__() has nothing special, you can also call it by calling instance.__call__() directly.\nFor example, if we want to map a function to every element of a list, we would have three ways to attain that: The first is define a method; the second and three are transmuting the list into a method and call __call__() (inside the class) in different ways.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class mapls(list): def __init__(self, ls): self.ls = ls def maps(self,func): return [func(i) for i in self.ls] def __call__(self, func): return [func(i) for i in self.ls] ls = mapls([1,2,3]) print(ls.maps(lambda x: x+1)) print(ls(lambda x: x+1)) print(ls.__call__(lambda x: x+1))   [2, 3, 4] [2, 3, 4] [2, 3, 4] __len__() len is a function to get the length of a collection. It works by calling an object\u0026rsquo;s __len__ method, in the other word, __len__() (inside the class) is called when len()/__len__() (outside the class) is called, just like __call__(). Thus when you call len(instance) outside the class, it is equivalent with instance.__len__().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class Cat: def __init__(self, attr1, attr2): self.attr1 = attr1 self.attr2 = attr2 def func1(self): print(\u0026#34;func1\u0026#34;) def __len__(self): return self.attr2 ** 2 cat = Cat(\u0026#34;haoming\u0026#34;, 15) print(len(cat)) print(cat.__len__())   225 225 __iter__() Just like __call__() method, the object in Python can also be classified as iterable and non-iterable. Using __iter__() method can transmute a non-iterable object into iterabel one.\nLike __call__() and __len__(), __iter__()(inside the class) will be called when you call iter(instance)/instance.__iter__() outside the class, and return a interator.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  class Cat: def __init__(self, attr1, attr2): self.attr1 = attr1 self.attr2 = attr2 def func1(self): print(\u0026#34;func1\u0026#34;) def __iter__(self): print(\u0026#34;__iter__() method has been called\u0026#34;) return iter(self.attr2) cat = Cat(\u0026#34;haoming\u0026#34;, \u0026#34;abcdefg\u0026#34;) itcat1 = iter(cat) itcat2 = cat.__iter__() print(next(itcat1)) print(next(itcat1)) print(next(itcat1)) print(\u0026#34;------------\u0026#34;) print(next(itcat2)) print(next(itcat2)) print(next(itcat2))   __iter__() method has been called __iter__() method has been called a b c ------------ a b c But you can also replace iter(iterable) with iterable.__iter__() inside the class:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  class Dog: def __init__(self, attr1, attr2): self.attr1 = attr1 self.attr2 = attr2 def func1(self): print(\u0026#34;func1\u0026#34;) def __iter__(self): print(\u0026#34;__iter__() method has been called\u0026#34;) return self.attr2.__iter__() dog = Dog(\u0026#34;yaochong\u0026#34;, \u0026#34;abcdefg\u0026#34;) itcat1 = iter(dog) itcat2 = dog.__iter__() print(next(itcat1)) print(next(itcat1)) print(next(itcat1)) print(\u0026#34;------------\u0026#34;) print(next(itcat2)) print(next(itcat2)) print(next(itcat2))   __iter__() method has been called __iter__() method has been called a b c ------------ a b c __getitem__(), __setitem__() and __delitem__() Let\u0026rsquo;s recall the operations on dict class, we can refer, modify and delete the element of dictionary using brackets:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  dct = { k:v for k,v in zip( list(\u0026#34;abcd\u0026#34;), range(4) ) } # refer print(dct[\u0026#34;a\u0026#34;]) # modify dct[\u0026#34;a\u0026#34;] = 4 print(dct[\u0026#34;a\u0026#34;]) # delete del dct[\u0026#34;b\u0026#34;] print(dct)   0 4 {'a': 4, 'c': 2, 'd': 3} But how to add this property to our general class? The solution is __getitem__(), __setitem__() and __delitem__(). These methods will be called when you add bracket with the key behind the instance:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  class Dog: def __init__(self, attr1, attr2, attr3=10): self.attr1 = attr1 self.attr2 = attr2 self.attr3 = attr3 def __getitem__(self, key): print(\u0026#34;__getitem__ is called\u0026#34;) if key in self.__dict__: return self.__dict__[key] else: print(\u0026#34;No such attr\u0026#34;) def __setitem__(self, key, value): print(\u0026#34;__setitem__ is called\u0026#34;) self.__dict__[key] = value def __delitem__(self, key): print(\u0026#34;__delitem__ is called\u0026#34;) del self.__dict__[key] dog = Dog(1,2,3) print(dog[\u0026#34;attr1\u0026#34;]) dog[\u0026#34;attr1\u0026#34;] = 4 print(dog[\u0026#34;attr1\u0026#34;]) del dog[\u0026#34;attr1\u0026#34;] print(dog.__dict__)   __getitem__ is called 1 __setitem__ is called __getitem__ is called 4 __delitem__ is called {'attr2': 2, 'attr3': 3} In the same way, you can also call them by instance.__xxxitem__(\u0026quot;key\u0026quot;) directly.\n1 2 3 4 5 6 7 8 9  dog = Dog(1,2,3) print(dog.__getitem__(\u0026#34;attr1\u0026#34;)) dog.__setitem__(\u0026#34;attr1\u0026#34;,4) print(dog[\u0026#34;attr1\u0026#34;]) dog.__delitem__(\u0026#34;attr1\u0026#34;) print(dog.__dict__)   __getitem__ is called 1 __setitem__ is called __getitem__ is called 4 __delitem__ is called {'attr2': 2, 'attr3': 3} Mathematics operations The reason why we name the method mentioned in this blog as Magic Method is that it can magically change the operation and relationship of the objects. For example, we can transform uncallable/unlenable/uniterable/unindexedable object to callable/lenable/iterable/indexedable ones. Something even more amazing is you can even define mathematics operations on general class.\nFor example, there are two list class instance here\n1 2  ls1 = [i for i in range(10)] ls2 = [i for i in range(10)]   the following operation rules are allowed:\n list class instance + list class instance list class instance * int calss instance  but the others are invalid. The former operation concatenates two list; the later concatenates same list multi-times. We now are going to modify this operation rules as called vector operation.\n1 2 3 4 5 6  ls1 = [i for i in range(4)] ls2 = [i for i in range(4)] print(ls1 + ls2) print(ls1 * 3) print(ls1 - ls2)   --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /Users/wanghaoming/Documents/doc1/oop/test2.ipynb Cell 63' in \u0026lt;module\u0026gt; 4 print(ls1 + ls2) 5 print(ls1 * 3) ----\u0026gt; 6 print(ls1 - ls2) TypeError: unsupported operand type(s) for -: 'list' and 'list' These are magic methods of mathematics operation:\n +: __add__() -: __sub__() *: __mul__() /: __div__() %: __mod__() ^: __pow__()  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  class vlist(list): def __add__(self,other): if isinstance(other, vlist): lses = [self[i]+other[i] for i in range(len(self))] return lses else: raise ValueError(\u0026#34;Must be vlist obejct\u0026#34;) def __sub__(self,other): if isinstance(other, vlist): lses = [self[i]-other[i] for i in range(len(self))] return lses else: raise ValueError(\u0026#34;Must be vlist obejct\u0026#34;) ls1 = vlist([i for i in range(10)]) ls2 = vlist([i for i in range(10)]) print(ls1 + ls2) print(ls1 - ls2)   [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] Special Attribute __name__ When a Python module or package is imported, __name__ is set to the module\u0026rsquo;s name. Usually, this is the name of the Python file itself without the .py extension. If the file is part of a package, __name__ will also include the parent package\u0026rsquo;s path.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import os filename = \u0026#34;./test_pkg/test_pkg_module.py\u0026#34; os.makedirs(os.path.dirname(filename), exist_ok=True) tm = \u0026#34;\u0026#34;\u0026#34; class Cat: def __init__(self, name): self.name = name def cat_func(self): print(\u0026#34;cat_func is called\u0026#34;) \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;test_module.py\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(tm) with open(filename, \u0026#34;w\u0026#34;) as f: f.write(tm) import test_module as tm import test_pkg.test_pkg_module as tpm print(tm.__name__) print(tpm.__name__)   test_module test_pkg.test_pkg_module However, if the module is executed in the top-level code environment, its __name__ is set to the string \u0026lsquo;__main__'.\n__main__ So what is the \u0026ldquo;top-level code environment\u0026rdquo;? __main__ is the name of the environment where top-level code is run. \u0026ldquo;Top-level code\u0026rdquo; is the first user-specified Python module that starts running. It\u0026rsquo;s \u0026ldquo;top-level\u0026rdquo; because it imports all other modules that the program needs. Sometimes \u0026ldquo;top-level code\u0026rdquo; is called an entry point to the application. The most common top-level code environment are the scope of an interactive prompt \u0026gt;\u0026gt;\u0026gt;, script, IPython console or Jupyter notebook cells:\n interactive prompt:  1 2 3 4 5 6  (base) wanghaoming@localhost oop % python3 Python 3.9.1 (v3.9.1:1e5d33e9b9, Dec 7 2020, 12:44:01) [Clang 12.0.0 (clang-1200.0.32.27)] on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; __name__ \u0026#39;__main__\u0026#39;    python script:  1  print(__name__)   (base) wanghaoming@localhost doc1 % /usr/local/bin/python3 /Users/wanghaoming/Documents/doc1/oop/test3.py __main__  IPython console:  1 2  In[1]: __name__ Out[1]: \u0026#39;__main__\u0026#39;    Jupyter notebook cells:  1  __name__   '__main__'  You can click here for more top-level environment occasions.\n As a result, a module can discover whether or not it is running in the top-level environment by checking its own __name__, which allows a common idiom, if __name__ == '__main__':, for conditionally executing code when the module is not initialized from an import statement. We add lines of codes to test_module.py module:\n1 2 3 4 5 6 7 8 9 10  codes = \u0026#34;\u0026#34;\u0026#34; print(__name__) if __name__ == \u0026#39;__main__\u0026#39;: print(\u0026#34;__name__ = \u0026#39;__main__\u0026#39;\u0026#34;) \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;test_module.py\u0026#34;, \u0026#34;a\u0026#34;) as f: f.write(codes) !cat test_module.py   class Cat: def __init__(self, name): self.name = name def cat_func(self): print(\u0026quot;cat_func is called\u0026quot;) print(__name__) if __name__ == '__main__': print(\u0026quot;__name__ = '__main__'\u0026quot;) and execute it, the outcome is desired:\n1  !python test_module.py   __main__ __name__ = '__main__' But if we import the module, we have\n1 2  import test_module as tm print(tm.__name__)   test_module test_module So you will find that a module can contain executable statements as well as function definitions. These statements are intended to initialize the module. They are executed in __main__ the first time the module name is encountered in an import statement. (They are also run if the file is executed as a script.)\nSome modules contain code that is intended for script use only, like parsing command-line arguments or fetching data from standard input. If a module like this was imported from a different module, the script code would unintentionally execute as well. This is where using the code block comes in handy. Code within if __name__ == '__main__': block won\u0026rsquo;t run unless the module is executed in the top-level environment.\nThere is a confusing concept here: __main__ with __main__.py. You can create a directory or zipfile full of code, and include a __main__.py.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  import os from pathlib import Path dir_name = \u0026#34;./pkg/\u0026#34; os.makedirs(dir_name) Path(\u0026#34;./pkg/__init__.py\u0026#34;).touch() tm = \u0026#34;\u0026#34;\u0026#34; class Cat: \u0026#39;\u0026#39;\u0026#39; this is a Cat class in pkg.module. Args: name(str): name of the cat. \u0026#39;\u0026#39;\u0026#39; def __init__(self, name): \u0026#39;\u0026#39;\u0026#39; this is a constructor in pkg.module. \u0026#39;\u0026#39;\u0026#39; self.name = name print(\u0026#39;Cat in pkg.module has been called.\u0026#39;) def cat_func(self): print(\u0026#34;cat_func in pkg.module is called\u0026#34;) print(\u0026#39;pkg.module has been initialized\u0026#39;) \u0026#34;\u0026#34;\u0026#34; tm1 = \u0026#34;\u0026#34;\u0026#34; import test_pkg_module as tpm catt = tpm.Cat(\u0026#34;yachong\u0026#34;) catt.cat_func() \u0026#34;\u0026#34;\u0026#34; with open(dir_name+\u0026#34;test_pkg_module.py\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(tm) with open(dir_name+\u0026#34;__main__.py\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(tm1)   Then you can simply name the directory or zipfile on the command line, and it executes the __main__.py automatically.\n1  (base) wanghaoming@localhost oop % python pkg   pkg.module has been initialized Cat in module has been called. cat_func in pkg.module is called Note that a __main__ module usually doesn\u0026rsquo;t come from a __main__.py file. It can, but it usually doesn\u0026rsquo;t. When you run a script like python my_program.py, the script will run as the __main__ module instead of the my_program module. This also happens for modules run as $ python -m my_module, or in several other ways. If you saw the name __main__ in an error message, that doesn\u0026rsquo;t necessarily mean you should be looking for a __main__.py file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54  class Cat: \u0026#34;\u0026#34;\u0026#34; this is a Cat class in main. Args: name(str): name of the cat. \u0026#34;\u0026#34;\u0026#34; def __init__(self, name): \u0026#34;\u0026#34;\u0026#34; this is a constructor in main. \u0026#34;\u0026#34;\u0026#34; self.name = name print(\u0026#39;Cat in main has been called.\u0026#39;) def cat_func(self): print(\u0026#34;cat_func in main is called\u0026#34;) tm = \u0026#34;\u0026#34;\u0026#34; class Cat: \u0026#39;\u0026#39;\u0026#39; this is a Cat class in module. Args: name(str): name of the cat. \u0026#39;\u0026#39;\u0026#39; def __init__(self, name): \u0026#39;\u0026#39;\u0026#39; this is a constructor in module. \u0026#39;\u0026#39;\u0026#39; self.name = name print(\u0026#39;Cat in module has been called.\u0026#39;) def cat_func(self): print(\u0026#34;cat_func in module is called\u0026#34;) print(\u0026#39;module has been initialized\u0026#39;) \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;test_module.py\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(tm) import __main__ as mm import test_module as tm import pkg.test_pkg_module as tpm cat0 = Cat(\u0026#34;000\u0026#34;) cat1 = mm.Cat(\u0026#34;111\u0026#34;) cat2 = tm.Cat(\u0026#34;222\u0026#34;) cat3 = tpm.Cat(\u0026#34;333\u0026#34;) cats = [cat0, cat1, cat2, cat3] print(\u0026#34;------------------\u0026#34;) cls = [cat.__class__ for cat in cats] [ [ True if clsa==clsb else False for clsa in cls ] for clsb in cls ]   module has been initialized pkg.module has been initialized ------------------ Cat in main has been called. Cat in main has been called. Cat in module has been called. Cat in pkg.module has been called. ------------------ [[True, True, False, False], [True, True, False, False], [False, False, True, False], [False, False, False, True]] __doc__, __module__ \u0026amp; __class__ __doc__/__module__/__class__ represents the docstring/module/class of the object.\nDefine two class, one is defined in __main__ and another is defined in test_module.py module. Import the outside module, and created two instances with __main__.Cat and test_module.Cat respectively.\n1 2  cat0 = Cat(\u0026#34;yaochong\u0026#34;) cat1 = tm.Cat(\u0026#34;shaonan\u0026#34;)   Cat in main has been called. Cat in module has been called. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  print(\u0026#34;1.--------docstring of class and method---------\u0026#34;) print(cat0.__doc__) print(cat0.__init__.__doc__) print(cat1.__doc__) print(cat1.__init__.__doc__,\u0026#39;\\n\u0026#39;) print(\u0026#34;2.-------class \u0026amp; module of class--------\\n\u0026#34;) print(cat0.__module__) print(cat1.__module__) print(cat0.__class__) print(cat1.__class__) print(\u0026#34;\\n3.-------class of attribute--------\\n\u0026#34;) # print(cat0.name.__module__) # print(cat1.name.__module__) print(cat0.name.__class__) print(cat1.name.__class__) print(\u0026#34;\\n4.-------class \u0026amp; module of method---------\\n\u0026#34;) print(cat0.cat_func.__module__) print(cat1.cat_func.__module__) print(cat0.cat_func.__class__) print(cat1.cat_func.__class__)   1.--------docstring of class and method--------- this is a Cat class in main. Args: name(str): name of the cat. this is a constructor in main. this is a Cat class in module. Args: name(str): name of the cat. this is a constructor in module. 2.-------class \u0026amp; module of class-------- __main__ test_module \u0026lt;class '__main__.Cat'\u0026gt; \u0026lt;class 'test_module.Cat'\u0026gt; 3.-------class of attribute-------- \u0026lt;class 'str'\u0026gt; \u0026lt;class 'str'\u0026gt; 4.-------class \u0026amp; module of method--------- __main__ test_module \u0026lt;class 'method'\u0026gt; \u0026lt;class 'method'\u0026gt; __dict__ __dict__ represents a dictatory that contains the attributes with values of an object excluding special attribute. Notice that __dict__ does not incorporate the method. dic() is similar with __dict__ in a way, it return a list that contains all attribute and method of the object, but is does not provide those values.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  class Cat: def __init__(self, attr1): self.attr1 = attr1 self._attr1 = attr1 self.__attr1 = attr1 def func1(self): print(\u0026#34;func1\u0026#34;) def _func1(self): print(\u0026#34;func1\u0026#34;) def __func1(self): print(\u0026#34;func1\u0026#34;) cat = Cat(\u0026#34;haoming\u0026#34;) print(cat.__dict__) print(dir(cat))   {'attr1': 'haoming', '_attr1': 'haoming', '_Cat__attr1': 'haoming'} ['_Cat__attr1', '_Cat__func1', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_attr1', '_func1', 'attr1', 'func1'] ","date":"2019-09-15T00:00:00Z","image":"https://wanghaoming177.netlify.app/head_img/oop_hi/oo4.png","permalink":"https://wanghaoming177.netlify.app/p/object-oriented-programming-v-special-attributes-magic-methods/","title":"Object Oriented Programming V (Special Attributes \u0026 Magic Methods)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n OOP Fundamental The fundamental concepts of OOP are objects and classes. An object is a data structure incorporating information about state and behavior. For example, an object representing a customer can have a certain phone number and email associated with them, and behaviors like placeOrder or cancelOrder. The distinctive feature of OOP is that state and behavior are bundled together: instead of thinking of customer data separately from customer actions, we think of them as one unit representing a customer. This is called encapsulation, and it\u0026rsquo;s one of the core tenets of object-oriented programming.\nClasses are like blueprints for objects. They describe the possible states and behaviors that every object of a certain type could have. For example, if you say \u0026ldquo;every customer will have a phone number and an email, and will be able to place and cancel orders\u0026rdquo;, you just defined a class. This way, you can talk about customers in a unified way. In Python, everything is an object. In particular, everything you deal with in Python has a class, a blueprint associated with it under the hood. The existence of these unified interfaces, is why you can use one kind of object in the same way.\nClasses incorporate information about state and behavior. State information in Python is contained in attributes, and behavior information \u0026ndash; in methods. For example, every numpy array has an attribute \u0026ldquo;shape\u0026rdquo; and a method \u0026ldquo;reshape()\u0026rdquo;. Attributes (or states) in Python objects are represented by variables \u0026ndash; like numbers, or strings, or tuples. Methods, or behaviors, are represented by functions. Both are accessible from an object using the dot . syntax.\nYou can call type() on any Python object to find out its class. And You can list all the attributes and methods that an object has by calling dir() on it.\nAttributes and Method To start a new class definition, all you need is a class statement, followed by the name of the class, followed by a colon. Everything in the indented block after will be considered a part of the class. You can create an \u0026ldquo;empty\u0026rdquo; class by including the pass statement after the class declaration.\nWe can create objects of the class by specifying the name of the class, followed by parentheses. Here, c1 and c2 are two different objects of the empty class Customer.\n1 2 3 4 5 6 7 8  class Customer: pass c1 = Customer() c2 = Customer() print(c1) print(c2)   \u0026lt;__main__.Customer object at 0x7fbc4944ad00\u0026gt; \u0026lt;__main__.Customer object at 0x7fbc4944adc0\u0026gt; Defining a method is simple. Methods are functions, so the definition of a method looks just like a regular Python function, with one exception: the self argument that every method will have as the first argument, possibly followed by other arguments.\nClasses are templates. Objects of a class don\u0026rsquo;t yet exist when a class is being defined, but we often need a way to refer to the data of a particular object within class definition. That is the purpose of self - it\u0026rsquo;s a stand-in for the future object. That\u0026rsquo;s why every method should have the self argument \u0026ndash; so we could use it to access attributes and call other methods from within the class definition even when no objects were created yet. Python will handle self when the method is called from an object using the object.method() syntax. In fact, using object.method() is equivalent to passing that object as an argument. That\u0026rsquo;s why we don\u0026rsquo;t specify it explicitly when calling the method from an existing object.\n1 2 3 4 5 6  class Customer: def identify(self, name): print(\u0026#34;This is \u0026#34;+name) c1 = Customer() c1.identify(\u0026#34;haoming\u0026#34;)   This is haoming By the principles of OOP, the data describing the state of the object should be bundled into the object. For example, customer name should be an attribute of a customer object, instead of a parameter passed to a method. In Python attributes \u0026ndash; like variables \u0026ndash; are created by assignment, meaning an attribute manifests into existence only when a value is assigned to it. (thus you can modify attr by assignment either inside the class, i.e. method; or outside the class, assignment.)\nHere is a method set_name with arguments self and new_name. To create an attribute of the Customer class called \u0026ldquo;name\u0026rdquo;, all we need to do is to assign something to self.name. Here, we set the name attribute to the new_name parameter of the function. When we create a customer, it does not yet have a name attribute. But after the set_name method was called, the name attribute is created, and we can access it through .name.\n Remember, self is a stand-in for object, so self.attribute should remind you of the object.attribute syntax.\n 1 2 3 4 5 6 7 8  class Customer: def set_name(self, name): self.name = name def identify(self): print(\u0026#34;This is \u0026#34;+self.name) c1 = Customer() print(c1.name)   --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_74663/4057745890.py in \u0026lt;module\u0026gt; 6 7 c1 = Customer() ----\u0026gt; 8 print(c1.name) AttributeError: 'Customer' object has no attribute 'name' 1 2  c1.set_name(\u0026#34;haoming\u0026#34;) c1.identify()   This is haoming 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  # -------- Example -------- # class Employee: def set_name(self, new_name): self.name = new_name def set_salary(self, new_salary): self.salary = new_salary def give_raise(self, amount): self.salary = self.salary + amount def monthly_salary(self): return self.salary/12 emp = Employee() emp.set_name(\u0026#39;Korel Rossi\u0026#39;) emp.set_salary(50000) emp.give_raise(15000) mon_sal = emp.monthly_salary() print(\u0026#34;annual package: \u0026#34;, emp.salary) print(\u0026#34;month salary: \u0026#34;, mon_sal)   annual package: 65000 month salary: 5416.666666666667 __init__ constructor We have discussed that methods are functions within class with a special first argument self, and attributes are created by assignment and referred to using the self variable within methods.\nA better strategy would be to add data to the object when creating it, like you do when creating a numpy array or a DataFrame. Python allows you to add a special method __init__ called the constructor that is automatically called every time an object is created.\nSo now, we can pass the parameter in the parentheses when creating the customer object, and the __init__ method will be automatically called, and the attribute created. The init constructor is also a good place to set the default values for attributes.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # -------- Example -------- # 1. from datetime import datetime class Employee: def __init__(self, name, salary=0): self.name = name if salary \u0026gt; 0: self.salary = salary else: self.salary = 0 print(\u0026#34;Invalid salary!\u0026#34;) self.hire_date = datetime.today() self.hire_date = datetime.today() def give_raise(self, amount): self.salary += amount def monthly_salary(self): return self.salary/12 emp = Employee(\u0026#34;Korel Rossi\u0026#34;, -1000)   Invalid salary! 1 2 3 4 5 6  print(emp.name) print(emp.salary) emp.give_raise(12324) print(emp.monthly_salary) print(emp.monthly_salary()) print(emp.hire_date)   Korel Rossi 0 \u0026lt;bound method Employee.monthly_salary of \u0026lt;__main__.Employee object at 0x7fbc68aa6b80\u0026gt;\u0026gt; 1027.0 2019-08-21 11:56:58.019767 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # -------- Example -------- # 2. class Point: def __init__(self,x=0,y=0): \u0026#34;\u0026#34;\u0026#34;Accepts two arguments that initialize the corresponding attributes. Args: x(float,optional) y(float,optional) \u0026#34;\u0026#34;\u0026#34; self.x = x self.y = y def distance_to_origin(self): \u0026#34;\u0026#34;\u0026#34;Returns the distance from the point to the origin.\u0026#34;\u0026#34;\u0026#34; dis = (self.x ** 2 + self.y ** 2) ** .5 return dis def reflect(self, axis): \u0026#34;\u0026#34;\u0026#34;Reflects the point with respect to the x- or y-axis\u0026#34;\u0026#34;\u0026#34; if axis == \u0026#34;x\u0026#34;: self.y = -self.y elif axis == \u0026#34;y\u0026#34;: self.x = -self.x else: print(\u0026#34;error message\u0026#34;) pt = Point(x=3.0) pt.reflect(\u0026#34;y\u0026#34;) print((pt.x, pt.y)) pt.y = 4.0 print(pt.distance_to_origin())   (-3.0, 0) 5.0 There are some conventions that will make your code more reader-friendly.\n  There are two ways to define attributes: we can define an attribute in any method in a class; and then calling the method will add the attribute to the object. Alternatively, we can define them all together in the constructor. If possible, use latter, which makes your code more organized, readable, and maintainable code.\n  For classes, words should be camel case, which means that if your class name contains several words, they should be written without delimiters, and each word should start with a capital letter. For methods and attributes, words should be separated by underscores and start with lowercase letters.\n  the name \u0026ldquo;self\u0026rdquo; is a convention. You could actually use any name for the first variable of a method, it will always be treated as the object reference regardless.\n  classes, like functions, allow for docstrings which are displayed when help() is called on the object.\n  Inheritance and Polymorphism Instance and class data Class-level attribute Remember the class we defined above. It had attributes like name and salary, and we were able to assign specific values to them for each new instance of the class. These were Instance-level attributes. We used self to bind them to a particular instance.\nBut if you needed to store some data that is shared among all the instances of a class, you can define an attribute directly in the class body. This will create a Class-level attribute, that will serve as a \u0026ldquo;global variable\u0026rdquo; within a class.\nWe can define Class-level attribute MIN_SALARY, and set it to 30000. We refer to the attribute this attribute inside the class like we would use any global variable, only follow by the class name, instead of self. This MIN_SALARY variable will be shared among all the instances of the employee class. We can access it like any other attribute from an object instance, and the value will be the same across instances. Here we print the MIN_SALARY class attribute from two employee objects.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  class Employee: MIN_SALARY = 30000 # \u0026lt;-- assign Class-level attr, directly without `self` def __init__(self, name, salary=0): self.name = name # \u0026lt;-- assign Instance-level attr, bounded with instance by `self` if salary \u0026gt; Employee.MIN_SALARY: # \u0026lt;-- refer Class-level attr inside the class self.salary = salary else: self.salary = Employee.MIN_SALARY emp1 = Employee(\u0026#34;Haoming\u0026#34;, 37000) emp2 = Employee(\u0026#34;Yachong\u0026#34;, 36999) print(emp1.name) print(emp1.salary) print(emp1.MIN_SALARY) # \u0026lt;-- refer Class-level attr outside the class print(emp2.MIN_SALARY) print(Employee.MIN_SALARY)   Haoming 37000 30000 30000 30000 Notice that you can modify Class-level attr by assigning value to obj.ATTR directly. But if you do that, the only thing you changed is the specific instance\u0026rsquo;s attr, rather than the all instance.\n1 2 3 4 5  emp1.MIN_SALARY = 34000 print(emp1.MIN_SALARY) print(emp2.MIN_SALARY) print(Employee.MIN_SALARY)   34000 30000 30000 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # -------- Example -------- # 1 class Player: MAX_POSITION = 10 def __init__(self): self.position = 0 def move(self, steps): if self.position + steps \u0026lt; Player.MAX_POSITION: self.position += steps else: self.position = Player.MAX_POSITION def draw(self): drawing = \u0026#34;-\u0026#34; * self.position + \u0026#34;|\u0026#34; +\u0026#34;-\u0026#34;*(Player.MAX_POSITION - self.position) print(drawing) p = Player(); p.draw() p.move(4); p.draw() p.move(5); p.draw() p.move(3); p.draw()   |---------- ----|------ ---------|- ----------| Class-level method Instance-level methods are already shared between instances: the same code gets executed for every instance. The only difference is the data (different instance\u0026rsquo;s attr) that is fed into it.\nIt is possible to define methods bound to class rather than an instance, but they have a narrow application scope, because these methods will not be able to use any instance-level data. To define a class method, you start with a classmethod decorator, followed by a method definition. The only difference is that now the first argument is not self, but cls, referring to the class, just like the self argument was a reference to a particular instance. To call a class method, we use class.method syntax, rather than object.method syntax.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  class Employee: MIN_SALARY = 30000 def __init__(self, name, salary=0): self.name = name if salary \u0026gt; Employee.MIN_SALARY: self.salary = salary else: self.salary = Employee.MIN_SALARY @classmethod def minsalary(cls, min_salary): Employee.MIN_SALARY = min_salary @classmethod def delta_minsalary(cls, delta): Employee.MIN_SALARY += delta emp1 = Employee(\u0026#34;Haoming\u0026#34;, 37000) emp2 = Employee(\u0026#34;Yachong\u0026#34;, 36999) emp1.MIN_SALARY = 27000 print(\u0026#34;1: \u0026#34;, emp1.MIN_SALARY) print(\u0026#34;2: \u0026#34;, emp2.MIN_SALARY) Employee.minsalary(25000) print(\u0026#34;3: \u0026#34;, emp1.MIN_SALARY) print(\u0026#34;4: \u0026#34;, emp2.MIN_SALARY) Employee.delta_minsalary(500) print(\u0026#34;5: \u0026#34;, emp1.MIN_SALARY) print(\u0026#34;6: \u0026#34;, emp2.MIN_SALARY)   1: 27000 2: 30000 3: 27000 4: 25000 5: 27000 6: 25500 The main use of class methods is alternative constructors. A class can only have one __init__ method, but there might be multiple ways to initialize an object. For example, we might want to create an Employee object from data stored in a file. We can\u0026rsquo;t use a instance method, because it would require an instance, and there isn\u0026rsquo;t one yet.\nHere we introduce a class method from_file that accepts a file name, reads the first line from the file that presumably contains the name of the employee, and returns an object instance. In the return statement, we use the cls variable, which refers to the class that class method follows, so this will call (notice the parentheses) the class and activities the __init__ constructor, just like using Employee with parentheses, and finally return object instance.\nThen we can call the method from_file by using class.method syntax, which will create an employee object without explicitly calling the constructor.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  !echo haoming \u0026gt; ~/Documents/names.txt !echo yachong \u0026gt;\u0026gt; ~/Documents/names.txt !echo shaonan \u0026gt;\u0026gt; ~/Documents/names.txt class Employee: MIN_SALARY = 30000 def __init__(self, name, salary=0): self.name = name if salary \u0026gt; Employee.MIN_SALARY: self.salary = salary else: self.salary = Employee.MIN_SALARY @classmethod def from_file(cls, filename, index): with open(filename, \u0026#34;r\u0026#34;) as f: names = f.readlines() name = names[index] return cls(name) filename = \u0026#34;/Users/wanghaoming/Documents/names.txt\u0026#34; objs = [ Employee.from_file(filename, i) for i in range(3) ] info = [(obj.name, obj.salary) for obj in objs] print(info)   [('haoming\\n', 30000), ('yachong\\n', 30000), ('shaonan\\n', 30000)]  Notice that class is also an object, just like function.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # -------- Example -------- # from datetime import datetime class BetterDate: def __init__(self, year, month, day): self.year, self.month, self.day = year, month, day @classmethod def from_str(cls, datestr): year, month, day = map(int, datestr.split(\u0026#34;-\u0026#34;)) return cls(year, month, day) @classmethod def from_datetime(cls, datetime): year, month, day = datetime.year, datetime.month, datetime.day return cls(year, month, day) today = datetime.today() bd = BetterDate.from_datetime(today) print(bd.year) print(bd.month) print(bd.day)   2019 9 4 Class inheritance Object-oriented programming is fundamentally about code reuse. There are millions of people out there writing code, so there\u0026rsquo;s a good chance that someone has already written code that solves a part of your problem! But what if that code doesn\u0026rsquo;t match your needs exactly? For example, you might want to modify the to_csv method of a pandas DataFrame to adjust the output format. You could do that by importing pandas and writing a new function, but it will not be integrated into the DataFrame interface. OOP will allow you to keep interface consistent while customizing functionality.\nWe can accomplish this with inheritance. Class inheritance is mechanism by which we can define a new class that gets all the the functionality of another class plus maybe something extra without re-implementing the code.\nDeclaring a class that inherits from another class is very straightforward: you simply add parentheses after the class name, and then specify the class to inherit from. Here, we define a rudimentary BankAccount class and a seemingly empty SavingsAccount class inherited from it.\u0026ldquo;Seemingly\u0026rdquo; because SavingsAccount actually has exactly as much in it as the BankAccount class.\n1 2 3 4 5 6 7 8 9 10 11 12 13  class BankAccount: def __init__(self, balance): self.balance = balance def withdraw(self, amount): self.balance -= amount class SavingsAccount(BankAccount): pass sa1 = SavingsAccount(10000) sa1.withdraw(1024) print(sa1.balance)   8976 Inheritance represents \u0026ldquo;is-a\u0026rdquo; relationship: a savings account is a bank account (The opposite is not true), just with some extra features. Calling isinstance function on a SavingsAccount object shows that Python treats it like an instance of both SavingsAccount and BankAccount classes, which is not the case for a generic BankAccount object.\n1 2 3 4 5 6  print(isinstance(sa1, BankAccount)) print(isinstance(sa1, SavingsAccount)) ba1 = BankAccount(1000) print(isinstance(ba1, BankAccount)) print(isinstance(ba1, SavingsAccount))   True True True False Class-level attributes CAN be inherited, and the value of class attributes CAN be overwritten in the child class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  class Player: MAX_POSITION = 10 MAX_SPEED = 3 def __init__(self): self.position = 0 def move(self, steps): if self.position + steps \u0026lt; Player.MAX_POSITION: self.position += steps else: self.position = Player.MAX_POSITION class Racer(Player): MAX_SPEED = 5 p = Player() r = Racer() print(\u0026#34;p.MAX_SPEED = \u0026#34;, p.MAX_SPEED) print(\u0026#34;r.MAX_SPEED = \u0026#34;, r.MAX_SPEED) print(\u0026#34;p.MAX_POSITION = \u0026#34;, p.MAX_POSITION) print(\u0026#34;r.MAX_POSITION = \u0026#34;, r.MAX_POSITION)   p.MAX_SPEED = 3 r.MAX_SPEED = 5 p.MAX_POSITION = 10 r.MAX_POSITION = 10 Customizing functionality via inheritance Let\u0026rsquo;s customize SavingsAccount class by adding a constructor specifically for SavingsAccount. It will take a balance parameter, just like BankAccount, and an additional interest_rate parameter. In that constructor, we first run the code for creating a generic BankAccount by explicitly calling the __init__ method follows the bankAccount class name.\nNotice that we use BankAccount.__init__() to tell Python to call (note the parentheses) the constructor from the parent class, and we also pass self to that constructor. self in this case refers a SavingsAccount object, but also a BankAccount object. so we can pass it to the __init__ method of BankAccount. Then we can add more functionality, in this case just initializing an attribute.\nNow when we create an instance of the SavingsAccount class, the new constructor will be called, and the interest_rate attribute will be initialized.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  class BankAccount: def __init__(self, balance): self.balance = balance def withdraw(self, amount): self.balance -= amount class SavingsAccount(BankAccount): def __init__(self, balance, interest_rate): BankAccount.__init__(self, balance) self.interest_rate = interest_rate sa = SavingsAccount(1000, 0.05) print(sa.balance, sa.interest_rate)   1000 0.05  Remember that in Python, instances of a subclass are also instances of the parent class.\n SavingsAccount inherits the withdraw method from the parent BankAccount class. Calling withdraw on a savings account instance will execute exactly the same code as calling it on a generic bank account instance.\nYou can add new methods to a subclass. In these methods you can use data from both the child and the parent class. For example here, we add a compute_interest method by an expression involving the balance attribute that inherits from the parent clsss ,and interest_rate attribute that exists only in the child SavingsAccount class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  class BankAccount: def __init__(self, balance): self.balance = balance def withdraw(self, amount): self.balance -= amount class SavingsAccount(BankAccount): def __init__(self, balance, interest_rate): BankAccount.__init__(self, balance) self.interest_rate = interest_rate def compute_interest(self, n_periods=1): return self.balance * ((1 + self.interest_rate) ** n_periods - 1) sa = SavingsAccount(10000, .05) print(sa.compute_interest(5)) sa.withdraw(2048) print(sa.compute_interest(5))   2762.8156250000034 2196.9909850000026 You can also modify the method of parent class in the subclass. We now define another subclass of BankAccount. Start by inheriting from the parent class, add a customized constructor that also executes the parent code, a new deposit method, and a withdraw method with a new argument to withdraw fee.\n Notice that we can change the signature of the method in the subclass by adding a parameter, and we again, just like in the constructor, call the parent version of the method directly by using parentclassname.method(self) syntax and passing self. Notice the parentheses, that means we call this function.\n( you can view function signature as number of arguments + type of argument + return value )\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  class BankAccount: def __init__(self, balance): self.balance = balance def withdraw(self, amount): self.balance -= amount class CheckingAccount(BankAccount): def __init__(self, balance, limit): BankAccount.__init__(self, balance) self.limit = limit def deposit(self, amount): self.balance += amount def withdraw(self, amount, fee=0): if fee \u0026lt;= self.limit: BankAccount.withdraw(self, amount+fee) else: BankAccount.withdraw(self, amount+self.limit) ca = CheckingAccount(10000, 50) ba = BankAccount(10000) print(ca.balance) ca.withdraw(6000, 34) ba.withdraw(6000) print(ca.balance) print(ba.balance)   10000 3966 4000 Now when you call withdraw from an object that is a CheckingAccount instance, the new customized version will be used, but when you call it from regular BankAccount, the basic version will be used. For a CheckingAccount instance, we could call the method with 2 parameters. But trying this call for a generic BankAccount instance would cause an error, because the method defined in the BankAccount class was not affected by the changes in the subclass.\nThe interface of the call is the same, and the actual method that is called is determined by the instance class. This is an application of polymorphism.\n1  ba.withdraw(300, 30)   --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_74663/1465132301.py in \u0026lt;module\u0026gt; ----\u0026gt; 1 ba.withdraw(300, 30) TypeError: withdraw() takes 2 positional arguments but 3 were given  withdraw() takes self and amount 2 positional arguments.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # -------- Example -------- # class Employee: def __init__(self, name, salary=30000): self.name = name self.salary = salary def give_raise(self, amount): self.salary += amount class Manager(Employee): def display(self): print(\u0026#34;Manager \u0026#34;, self.name) def __init__(self, name, salary=50000, project=None): Employee.__init__(self, name, salary) self.project = project def give_raise(self, amount, bonus=1.05): Employee.give_raise(self, amount*bonus) mngr = Manager(\u0026#34;Ashta Dunbar\u0026#34;, 78500) mngr.give_raise(1000) print(mngr.salary) mngr.give_raise(2000, bonus=1.03) print(mngr.salary)   79550.0 81610.0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  # -------- Example -------- # import pandas as pd class LoggedDF(pd.DataFrame): def __init__(self, *args, **kwargs): pd.DataFrame.__init__(self, *args, **kwargs) self.created_at = datetime.today() def to_csv(self, *args, **kwargs): temp = self.copy() # \u0026lt;-- `self` refers the object that method follows.  temp[\u0026#34;created_at\u0026#34;] = self.created_at pd.DataFrame.to_csv(temp, *args, **kwargs) LDF = LoggedDF( { \u0026#34;a\u0026#34;: [1,2,3,4,5], \u0026#34;b\u0026#34;: [2,3,4,5,6], \u0026#34;c\u0026#34;: [3,4,5,6,7] } ) LDF.to_csv(\u0026#34;~/Documents/LDF.csv\u0026#34;) !cat ~/Documents/LDF.csv   ,a,b,c,created_at 0,1,2,3,2019-09-04 20:05:50.471649 1,2,3,4,2019-09-04 20:05:50.471649 2,3,4,5,2019-09-09 20:05:50.471649 3,4,5,6,2019-09-09 20:05:50.471649 4,5,6,7,2019-09-09 20:05:50.471649 Integrating with Standard Python object and type* This is a deep-level knowledge, which indeed bothers me for a long time. But reader could just skip it. When we say inheritance previously, It generally follows the rule: class C2(C1):. But if we do not specify what class does class inherits from, it inherits from object class. There are two rules in Python:\n every class (except object) inherits from class object. every class is an instance of the class type.  Furthermore, type is an instance of itself, and inherits from object, thus type is an instance of object. object is an instance of type, and type is inherits from object, thus object is an instance of object.\n1 2 3 4 5 6 7  print(isinstance(object, type)) print(isinstance(type, object)) print(isinstance(object, object)) print(isinstance(type, type), \u0026#34;\\n\u0026#34;) print(issubclass(type, object)) print(issubclass(object, type))   True True True True True False Every class (except object) inherits from object.\n1 2  for x in object, type, int, float, str, list, dict: print(f\u0026#39;{x.__name__:6}: {x.__bases__}\u0026#39;)   object: () type : (\u0026lt;class 'object'\u0026gt;,) int : (\u0026lt;class 'object'\u0026gt;,) float : (\u0026lt;class 'object'\u0026gt;,) str : (\u0026lt;class 'object'\u0026gt;,) list : (\u0026lt;class 'object'\u0026gt;,) dict : (\u0026lt;class 'object'\u0026gt;,) Every class is an instance of the class type.\n1 2  for x in object, type, int, float, str, list, dict: print(f\u0026#39;{x.__name__:6}: {x.__class__}\u0026#39;)   object: \u0026lt;class 'type'\u0026gt; type : \u0026lt;class 'type'\u0026gt; int : \u0026lt;class 'type'\u0026gt; float : \u0026lt;class 'type'\u0026gt; str : \u0026lt;class 'type'\u0026gt; list : \u0026lt;class 'type'\u0026gt; dict : \u0026lt;class 'type'\u0026gt; Since type inherits from object, every class is an instance of the class object too.\n1 2  for x in object, type, int, float, str, list, dict: print(f\u0026#39;{x.__name__:6}is an instance of object : {isinstance(x, object)}\u0026#39;)   object is an instance of object : True type is an instance of object : True int is an instance of object : True float is an instance of object : True str is an instance of object : True list is an instance of object : True dict is an instance of object : True Thus you should know \u0026ldquo;Everything in Python is an object\u0026rdquo; better. Any classes that we define are objects (inherits), and of course, instances of those classes are objects (\u0026rsquo;s instance) as well.\n Tips: print(f\u0026quot;{'aaa':5}bbb\u0026quot;) prints out aaa bbb.\n Comparison Here are two objects of the Customer class that have the same data. If we ask Python if these objects are equal, the answer is \u0026ldquo;False\u0026rdquo;. When an object is created, Python allocates a chunk of memory to that object, and the variable that the object is assigned to actually contains just the reference to the memory chunk.\n1 2 3 4 5 6 7 8 9  class Customer: def __init__(self, name, balance): self.name = name self.balance = balance customer1 = Customer(\u0026#34;haoming\u0026#34;, 20000) customer2 = Customer(\u0026#34;haoming\u0026#34;, 20000) customer1 == customer2   False When we call == to compare variables, Python acquiescently compares references of the memory chunk, not the data. Because customer1 and customer2 point to different chunks in memory, they are not considered equal.\n1 2  print(customer1) print(customer2)   \u0026lt;__main__.Customer object at 0x7fe21895adf0\u0026gt; \u0026lt;__main__.Customer object at 0x7fe21894f430\u0026gt; But we can define a special method for customizing comparison. The __eq__ method is implicitly called whenever two objects are compared to each other. We can re-define this method to execute custom comparison code. The method should accept two arguments, referring to the objects to be compared. They are usually called self and other by convention.\nNow, if we create two objects containing the same data and try to compare them using double equality sign, we see from the diagnostic printout that the __eq__ method is called, and the comparison returns \u0026ldquo;True\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13  class Customer: def __init__(self, name, balance): self.name = name self.balance = balance def __eq__(self, other): print(\u0026#34;__eq__() is called\u0026#34;) return (self.name == other.name) and (self.balance == other.balance) customer1 = Customer(\u0026#34;haoming\u0026#34;, 20000) customer2 = Customer(\u0026#34;haoming\u0026#34;, 20000) customer1 == customer2   __eq__() is called True Notice that the redefined __eq__ method compares instances by only attributes, which means though we\u0026rsquo;re comparing a Phone class instance with a BankAccount class instance, Python still returns True.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  class Phone: def __init__(self, number): self.number = number def __eq__(self, other): return self.number == other.number class BankAccount: def __init__(self, number, balance=0): self.number, self.balance = number, balance def withdraw(self, amount): self.balance -= amount def __eq__(self, other): return (self.number == other.number) acct = BankAccount(873555333) pn = Phone(873555333) print(acct == pn)   True But we can modify it by type function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  class Phone: def __init__(self, number): self.number = number def __eq__(self, other): print(\u0026#34;Phone class\u0026#39;s __eq__\u0026#34;) return self.number == other.number class BankAccount: def __init__(self, number, balance=0): self.number, self.balance = number, balance def withdraw(self, amount): self.balance -= amount def __eq__(self, other): print(\u0026#34;BankAccount class\u0026#39;s __eq__\u0026#34;) return (self.number == other.number) and (type(self) == type(other)) acct = BankAccount(873555333) pn = Phone(873555333) print(acct == pn) print(pn == acct)   BankAccount class's __eq__ False Phone class's __eq__ True Notice that when comparing two object without inheriting relationship, the __eq__ of former object\u0026rsquo;s class will be called. self refers to this object, and other refers the later one, thus acct==pn is not equivalent with pn==acct.\nBut When we comparing two object with inheriting relationship, the subclass\u0026rsquo;s __eq__ will always be called.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  class Parent: def __eq__(self, other): print(\u0026#34;Parent\u0026#39;s __eq__() called\u0026#34;) return True class Child(Parent): def __eq__(self, other): print(\u0026#34;Child\u0026#39;s __eq__() called\u0026#34;) return True p = Parent() c = Child() print(p == c) print(c == p) print(c == c) print(p == p)   Child's __eq__() called True Child's __eq__() called True Child's __eq__() called True Parent's __eq__() called True  Recall, __init__ implicitly called as creating an instance; __eq__ implicitly called as calling ==.\n Python allows you to implement all the comparison operators in your custom class using special methods:\n   Operator Method     == __eq__()   != __ne__()   \u0026gt;= __ge__()   \u0026lt;= __le__()   \u0026gt; __gt__()   \u0026lt; __lt__()    String representation When calling print on an object of a custom class returns the object\u0026rsquo;s address in memory by default. But there are plenty of classes for which the printout is much more informative. For example, if we print a numpy array or a DataFrame, we\u0026rsquo;ll see the actual data contained in the object.\nThere are two special methods that we can define in a class that will return a printable representation of an object.\n__str__ method is executed when we call print or str on an object. __str__ is supposed to give an informal representation, suitable for an end user;\n1 2 3 4 5  import numpy as np npls = np.array([1,2,3]) print(npls) str(npls)   [1 2 3] '[1 2 3]' __repr__ method is executed when we call repr on the object, or when we print it in the console without calling print explicitly. __repr__ is mainly used by developers.\n1  repr(npls)   'array([1, 2, 3])' 1  npls   array([1, 2, 3]) __str__ and __repr__ accept only self argument and return a string. The best practice is to use __repr__ to print a string that can be used to reproduce the object. (reproduce means the return of __repr__ could create the same object directly.)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  class Customer: def __init__(self, name, balance): self.name = name self.balance = balance def __eq__(self, other): print(\u0026#34;__eq__() is called\u0026#34;) return (self.name == other.name) and (self.balance == other.balance) def __repr__(self): return \u0026#34;Customer(\u0026#39;{name}\u0026#39;, {balance})\u0026#34;.format(name=self.name, balance=self.balance) c1 = Customer(\u0026#34;haoming\u0026#34;, 142857) cc1 = eval(repr(c1)) print(type(cc1)) print(cc1 == c1)   \u0026lt;class '__main__.Customer'\u0026gt; __eq__() is called True  eval function accepts the string of an expression and return the value of the expression.\n If you only choose to implement one of them, chose __repr__, because it is also used as a fallback for print when __str__ is not defined.\n1 2 3 4 5 6 7 8 9  class Customer: def __init__(self, name, balance): self.name = name self.balance = balance def __repr__(self): return \u0026#34;Customer(\u0026#39;{name}\u0026#39;, {balance})\u0026#34;.format(name=self.name, balance=self.balance) c1 = Customer(\u0026#34;haoming\u0026#34;, 142857)   1  repr(c1)   Customer('haoming', 142857) 1  print(c1)   Customer('haoming', 142857) In this class we didn\u0026rsquo;t define the __str__ method, so __repr__ will be used as a fallback for the actual print method as well. Notice the single quotes around the name in the return statement, the point of __repr__ is to give the exact call needed to reproduce the the object, where the name should be in quotes.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # -------- Example -------- # class Employee: def __init__(self, name, salary=30000): self.name, self.salary = name, salary def __str__(self): s = \u0026#34;Employee name: {name}\\nEmployee salary: {salary}\u0026#34;.format(name=self.name, salary=self.salary) return s def __repr__(self): return \u0026#34;Employee(\u0026#39;{}\u0026#39;, {})\u0026#34;.format(self.name, self.salary) emp1 = Employee(\u0026#34;Amar Howard\u0026#34;, 30000) emp2 = Employee(\u0026#34;Carolyn Ramirez\u0026#34;, 35000) print(emp1) print(emp2) print(\u0026#34;\\n\u0026#34;) print(repr(emp1)) print(repr(emp2))   Employee name: Amar Howard Employee salary: 30000 Employee name: Carolyn Ramirez Employee salary: 35000 Employee('Amar Howard', 30000) Employee('Carolyn Ramirez', 35000)  Recall, the triple quotes are used in Python to define multi-line strings, and the format method is used on strings to substitute values inside curly brackets with variables.\n Exceptions Some statements in Python will cause an error when you try to execute them. These errors are called exceptions. Many exceptions have special names, like ZeroDivisionError or TypeError. If exceptions not handled correctly, they will stop the execution of your program entirely.\nInstead, you might want to execute special code to handle this case. To catch an exception and handle it, use the try-except-finally code: wrap the code that you\u0026rsquo;re worried about in a try block, then add an except block, followed by the name of the particular exception you want to handle (exceptions names are optional) , and the code that should be executed when the exception is raised. Then if this particular exception does happen, the program will not terminate, but execute the code in the except block instead. You can also have multiple exception blocks. And finally, you can use the optional \u0026ldquo;finally\u0026rdquo; block that will contain the code that runs no matter what. This block is best used for cleaning up like, for example, closing opened files.\n1 2 3 4 5 6 7 8  try: # Try running some code except ZeroDivisionError: # Run this code if ZeroDivisionError happens except TypeError: # Run this code if TypeError happens finally: # \u0026lt;-- optional # Run this code no matter what   Sometimes, you want to raise exceptions yourself, for example when some conditions aren\u0026rsquo;t satisfied. You can use the raise keyword, optionally followed by a specific error message in parentheses. The user of the code can then decide to handle the error using try/except.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  def make_list_of_ones(length): if length \u0026lt;=0 : raise ValueError(\u0026#34;Invalid length!\u0026#34;) return [1]*length def tef(func, arg): try: func(arg) except ValueError: print(\u0026#34;length should be positive!\u0026#34;) finally: print(\u0026#34;program is over.\u0026#34;) tef(make_list_of_ones, -1)   length should be positive! program is over. You can define your own exceptions in Python by inheriting from built-in classes BaseException or Exception. To define a custom exception, just define a class that inherits from the built-in Exception class or one of its subclasses. The class itself can be empty - inheritance alone is enough to ensure that Python will treat this class as an exception class.\nFor example, let\u0026rsquo;s define a BalanceError class that inherits from Exception. Then, in Customer class we raise an exception if a negative balance value is passed to the constructor.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  class BalanceError(Exception): pass class Customer: def __init__(self, name, balance): if balance \u0026lt; 0: raise BalanceError(\u0026#34;balance must be positive\u0026#34;) else: self.name = name self.balance = balance try: c1 = Customer(\u0026#34;haoming\u0026#34;, -1000) except BalanceError: c1 = Customer(\u0026#34;haoming\u0026#34;, 0) print(\u0026#34;balance must be positive, the account with 0 balance value has been created.\u0026#34;)   except block for a parent exception will catch child exceptions, but the opposite is not true:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  class SalaryError(ValueError): pass class BonusError(SalaryError): pass class Employee: MIN_SALARY = 30000 MAX_BONUS = 5000 def __init__(self, name, salary = 30000): self.name = name if salary \u0026lt; Employee.MIN_SALARY: raise SalaryError(\u0026#34;Salary is too low!\u0026#34;) self.salary = salary def give_bonus(self, amount): if amount \u0026gt; Employee.MAX_BONUS: raise BonusError(\u0026#34;The bonus amount is too high!\u0026#34;) elif self.salary + amount \u0026lt; Employee.MIN_SALARY: raise SalaryError(\u0026#34;The salary after bonus is too low!\u0026#34;) else: self.salary += amount emp = Employee(\u0026#34;Katze Rik\u0026#34;, salary=50000)   1 2 3 4  try: emp.give_bonus(7000) # bonus \u0026gt; MAX_BONUS --\u0026gt; BonusError Caught except SalaryError: print(\u0026#34;SalaryError caught!\u0026#34;)   SalaryError caught! 1 2 3 4  try: emp.give_bonus(7000) # bonus \u0026gt; MAX_BONUS --\u0026gt; BonusError Caught except BonusError: print(\u0026#34;BonusError caught!\u0026#34;)   BonusError caught! 1 2 3 4  try: emp.give_bonus(-100000) # salary + amount \u0026lt; MIN_SALARY --\u0026gt; SalaryError Caught except SalaryError: print(\u0026#34;SalaryError caught again!\u0026#34;)   SalaryError caught again! 1 2 3 4  try: emp.give_bonus(-100000) # salary + amount \u0026lt; MIN_SALARY --\u0026gt; SalaryError Caught except BonusError: print(\u0026#34;BonusError caught again!\u0026#34;)   --------------------------------------------------------------------------- SalaryError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_57175/1321339394.py in \u0026lt;module\u0026gt; 1 try: ----\u0026gt; 2 emp.give_bonus(-100000) # salary + amount \u0026lt; MIN_SALARY --\u0026gt; SalaryError Caught 3 except BonusError: 4 print(\u0026quot;BonusError caught again!\u0026quot;) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_57175/1909163033.py in give_bonus(self, amount) 17 18 elif self.salary + amount \u0026lt; Employee.MIN_SALARY: ---\u0026gt; 19 raise SalaryError(\u0026quot;The salary after bonus is too low!\u0026quot;) 20 21 else: SalaryError: The salary after bonus is too low! If you don\u0026rsquo;t specify the exception name behind the except, then Python views it as Exception, thus every exceptions will be caught.\n1 2 3 4  try: emp.give_bonus(-100000) except Exception: # or just `except:`  print(\u0026#34;BonusError caught again!\u0026#34;)   BonusError caught again! It\u0026rsquo;s better to include an except block for a child exception before the block for a parent exception, otherwise the child exceptions will be always be caught in the parent block, and the except block for the child will never be executed.\n1 2 3 4 5 6 7 8  emp = Employee(\u0026#34;Katze Rik\u0026#34;,\\ 50000) try: emp.give_bonus(7000) # sub error except BonusError: print(\u0026#34;BonusError caught\u0026#34;) except SalaryError: print(\u0026#34;SalaryError caught\u0026#34;)   BonusError caught 1 2 3 4 5 6 7 8  emp = Employee(\u0026#34;Katze Rik\u0026#34;,\\ 50000) try: emp.give_bonus(7000) # sub error except SalaryError: print(\u0026#34;SalaryError caught\u0026#34;) except BonusError: print(\u0026#34;BonusError caught\u0026#34;)   SalaryError caught Class Design Liskov substitution principle Polymorphism means using a unified interface to operate on objects of different classes just as We\u0026rsquo;ve already dealt with pd.DataFrame.\nRecall that we defined a BankAccount class, and two classes inherited from it: a CheckingAccount class and a SavingsAccount class. All of them had a withdraw method, but the CheckingAccount\u0026rsquo;s method was executing different code.\nLet\u0026rsquo;s say we defined a function to withdraw the same amount of money from a whole list of accounts at once. This function doesn\u0026rsquo;t know whether the objects passed to it are CheckingAccount, SavingsAccount or just BankAccount. All that matters is that they have a withdraw method that accepts one argument. That is enough to make the function work. It does not check which withdraw it should call \u0026ndash; the original or the modified. When the withdraw method is actually called, Python will dynamically pull the correct method: modified withdraw for whenever a CheckingAccount is being processed,and base withdraw for whenever a SavingAccount or generic BankAccount is processed. So you, as a person writing this batch processing function, don\u0026rsquo;t need to worry about what exactly is being passed to it, only what kind of interface it has. To really make use of this idea, you have to design your classes with inheritance and polymorphism - the uniformity of interface - in mind.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  class BankAccount: def __init__(self, balance): self.balance = balance def withdraw(self, amount): self.balance -= amount class SavingsAccount(BankAccount): pass class CheckingAccount(BankAccount): def __init__(self, balance, limit): BankAccount.__init__(self, balance) self.limit = limit def deposit(self, amount): self.balance += amount def withdraw(self, amount, fee=20): if fee \u0026lt;= self.limit: BankAccount.withdraw(self, amount+fee) else: BankAccount.withdraw(self, amount+self.limit) def batch_withdraw(list_of_accounts, amount): for acct in list_of_accounts: acct.withdraw(amount) a1 = BankAccount(1000) a2 = SavingsAccount(2000) a3 = CheckingAccount(3000, 50) accounts = [a1, a2, a3] batch_withdraw(accounts, 500) repr([ac.balance for ac in accounts])   '[500, 1500, 2480]' There is a fundamental object-oriented design principle of when and how to use inheritance properly, called \u0026ldquo;Liskov substitution principle (LSP)\u0026quot;: A base class should be interchangeable with any of its subclasses without altering any properties of the surrounding program.\nOn the one hand, the method in a subclass should have a signature with parameters and returned values compatible with the method in the parent class. On the other hand, the state of objects also must stay consistent; the subclass method shouldn\u0026rsquo;t rely on stronger input conditions, should not provide weaker output conditions, it should not throw additional exceptions and so on.\n The state of an object is it\u0026rsquo;s attributes with it\u0026rsquo;s values. To modify an object\u0026rsquo;s state, a method need to have access to this object. For ordinary methods, this is provided by the self parameter. Python\u0026rsquo;s classes are objects, so Python has \u0026ldquo;classmethods\u0026rdquo; which can be invoked on either an instance or the class itself, but get the class object itself instead of an instance. Those classmethods can then modify the class\u0026rsquo;s state (class attributes, which are shared by all instances of the class).\n The ultimate rule is that if your class hierarchy violates the Liskov substitution principle, then you should not be using inheritance, because it is likely to cause the code to behave in unpredictable ways somewhere down the road.\nUsing the example of our Account hierarchy, that means that wherever in your application you use a BankAccount object instance, substituting a CheckingAaccount instead should not affect anything in the surrounding program. For example, the batch withdraw functions worked regardless of what kind of account was used.\nLet\u0026rsquo;s illustrate some possible violations of LSP on our account classes: for example,\n the parent\u0026rsquo;s \u0026ndash; or base\u0026rsquo;s \u0026ndash; withdraw method could require 1 parameter, but the subclass method could require 2. Then we couldn\u0026rsquo;t use the subclass\u0026rsquo;s withdraw in place of the parent\u0026rsquo;s. But if the subclass method has a default value for the second parameter, then there is no problem. If the subclass method only accepts certain amounts, unlike the base one, then sometimes the subclass could not be used in place of the base class, if those unsuitable amounts are used. If the base withdraw had a check for whether the resulting balance is positive, and only performed the withdraw in that case, but the subclass did not do that, we wouldn\u0026rsquo;t be able to use subclass in place of the base class, because it\u0026rsquo;s possible that ambient program depends on the fact that the balance is always positive after withdraw.  The classic example of a problem that violates the Liskov Substitution Principle is the Circle-Ellipse problem, sometimes called the Square-Rectangle problem. By all means, it seems like you should be able to define a class Rectangle, with attributes h and w (for height and width), and then define a class Square that inherits from the Rectangle. After all, a square \u0026ldquo;is-a\u0026rdquo; rectangle! Unfortunately, this intuition doesn\u0026rsquo;t apply to object-oriented design.\n1 2 3 4 5 6 7 8 9 10 11 12 13  class Rectangle: def __init__(self, h, w): self.h = h self.w = w class Square(Rectangle): def __init__(self, w): self.h = w self.w = w s1 = Square(4) s1.h = 7 print(s1.w, s1.h)   4 7 For example, we create a Square object with side length 4. Then the 4x4 Square object would no longer be a square if we assign 7 to h. A Square inherited from a Rectangle will always have both the h and w attributes, but we can\u0026rsquo;t allow them to change independently of each other. We can make such modifies:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  class Rectangle: def __init__(self, w,h): self.w, self.h = w,h def set_h(self, h): self.h = h def set_w(self, w): self.w = w class Square(Rectangle): def __init__(self, w): self.w, self.h = w, w def set_h(self, h): self.h = h self.w = h def set_w(self, w): self.h = w self.w = w   But these setter methods violate Liskov Substitution principle. Each of the setter methods of Square change both h and w attributes, while setter methods of Rectangle change only one attribute at a time, so the Square objects cannot be substituted for Rectangle into programs that rely on one attribute staying constant.\nPrivate attributes Any attribute or method of any class in Python can be accessed by anyone. There are a few ways to manage access to data. We can use some universal naming conventions to signal that the data is not for external consumption.\n_xxx When involving the attribute and method, the leading underscore has an conventional meaning. It is a tips to developer that Python community reach a consensus where this attribute or method are for internal use only. This convention is defined in PEP 8.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class MyClass: def my_func(self): print(\u0026#34;I Love Python.\u0026#34;) self.at1 = 1 self._at2 = 2 def _my_func(self): print(\u0026#34;I Love Python too.\u0026#34;) self.at3 = 3 self._at4 = 4 mc = MyClass() mc.my_func() mc._my_func() print(mc.at1, mc._at2, mc.at3, mc._at4)   I Love Python. I Love Python too. 1 2 3 4 You can see that _my_func() or _at2 does not prevent us from entering the class. This because it is not mandatory, just conventional. However, the leading underscore does generate affection when importing from modules.\nNow, if you use wildcards * to import all names from a module, Python does not import names with leading underscores (but the methods and attributes warped by unprefixed object can be imported).\nFor example, we write such a module, and name it as my_module.py.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class MyClass: def my_func(self): self.at1 = 1 self._at2 = 2 def _my_func(self): self.at3 = 3 self._at4 = 4 def my_func3(): def _my_func4(): print(\u0026#34;test\u0026#34;) return _my_func4 def _my_func2(): print(\u0026#34;test\u0026#34;)   import the module with wildcard:\n1 2 3 4 5 6 7  from my_module import * mc = MyClass() mc.my_func() mc._my_func() print(mc.at1, mc._at2, mc.at3, mc._at4) my_func3()()   1 2 3 4 test 1  _my_func2()   --------------------------------------------------------------------------- NameError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_25437/3445096031.py in \u0026lt;module\u0026gt; ----\u0026gt; 1 _my_func2() NameError: name '_my_func2' is not defined Unlike wildcard imports, regular imports are not affected by the leading single underscore naming convention:\n1 2  import my_module as mm mm._my_func2()   test In general, you should avoid wildcard imports and calling the leading underlined methods and attributes in public space.\n__xxx The double leading underscore prefix causes the Python interpreter to override attribute/method names to avoid naming conflicts in subclasses. This is also called name mangling - the interpreter changes the name of a variable so that it is less likely to collide when the class is extended.\n1 2 3 4 5 6 7 8 9 10  class Test: def __init__(self): self.foo = 11 self._bar = 23 self.__baz = 23 def __func(self): print(\u0026#34;test\u0026#34;) t = Test() dir(t)   ['_Test__baz', # \u0026lt;--- '_Test__func', # \u0026lt;--- ... '_bar', 'foo'] You will see that there is a attribute on this object named _Test__baz and a method named _Test_func. This is the name modification that the Python interpreter does. It does this to avoid subclasses overwriting base class attributes and methods.\nLet\u0026rsquo;s create a subclass that extends the Test class and try to override the existing attributes/method added in the constructor:\n1 2 3 4 5 6 7 8 9 10 11 12 13  class ExtendedTest(Test): def __init__(self): super().__init__() self.foo = \u0026#39;overridden\u0026#39; self.__baz = \u0026#39;overridden\u0026#39; def __func(self): print(\u0026#34;test1\u0026#34;) t2 = ExtendedTest() print(t2.foo) print(t2._bar) print(t2.__baz)   overridden 23 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_29055/1866563237.py in \u0026lt;module\u0026gt; 11 print(t2.foo) 12 print(t2._bar) ---\u0026gt; 13 print(t2.__baz) AttributeError: 'ExtendedTest' object has no attribute '__baz' 1  dir(t2)   ['_ExtendedTest__baz', # \u0026lt;-- '_ExtendedTest__func', # \u0026lt;-- '_Test__baz', # \u0026lt;-- '_Test__func', # \u0026lt;-- ... '_bar', 'foo'] You will find the __baz attribute and __func method of subclass has been renemed by Python interpreter as _ExtendedTest__baz and _ExtendedTest__func. Thus __baz/__func of base class has not been overrided by subclass.\n1 2 3 4  print(t2._Test__baz) t2._Test__func() print(t2._ExtendedTest__baz) t2._ExtendedTest__func()   23 test overridden test1  Click here or here for more information.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # -------- Example -------- # class BetterDate: _MAX_DAYS = 30 _MAX_MONTHS = 12 def __init__(self, year, month, day): self.year, self.month, self.day = year, month, day @classmethod def from_str(cls, datestr): year, month, day = map(int, datestr.split(\u0026#34;-\u0026#34;)) return cls(year, month, day) def _is_valid(self): return (self.day \u0026lt;= BetterDate._MAX_DAYS) and (self.month \u0026lt;= BetterDate._MAX_MONTHS) bd1 = BetterDate(2020, 4, 30) print(bd1._is_valid()) bd2 = BetterDate(2020, 6, 45) print(bd2._is_valid())   True False Properties We have worked with an Employee class where we defined methods like set_salary that were used to set the values for attributes. Later, we use the constructor to initialize the attributes . We can access and change the attributes directly by assignment. But this means that with a simple equality we can assign anything to salary: a million, a negative number, or even the word \u0026ldquo;Hello\u0026rdquo;. But salary should only be positive number.\nSo how do we control attribute access, validate it or even make the attribute read-only? We could modify the set_salary method, but that wouldn\u0026rsquo;t help, because we could still use the dot syntax and assignment via equality.\nWe can reach there using the property decorator.\n Start by defining an \u0026ldquo;internal\u0026rdquo; attribute that will store the data, it is recommended to start the name with one leading underscore. Here, we defined a _salary attribute. Next, we define a method whose name is the exact name we\u0026rsquo;d like the restricted attribute to have, and put a decorator \u0026ldquo;property\u0026rdquo; on it. In our case that method is called salary, without underscore, because that\u0026rsquo;s how we\u0026rsquo;d like to refer to it. If we were writing a DataFrame class, this could be \u0026ldquo;columns\u0026rdquo;, or \u0026ldquo;shape\u0026rdquo;. The method just returns the actual internal attribute that is storing the data. To customize how the attribute is set, we implement a method with a decorator \u0026lt;attribute-name\u0026gt;.setter: salary.setter in our case. The method itself is again named exactly the same as the property \u0026ndash; salary - and it will be called when a value is assigned to the property attribute. It has a self argument, and an argument that represents the value to be assigned. Here we raise an exception if the value is negative, otherwise change the internal attribute.  1 2 3 4 5 6 7 8 9 10 11 12 13  class Employer: def __init__(self, name, new_salary): self._salary = new_salary @property def salary(self): return self._salary @salary.setter def salary(self, new_salary): if new_salary \u0026lt; 0: raise ValueError(\u0026#34;Invalid salary\u0026#34;) self._salary = new_salary   So there are two methods called salary \u0026ndash; the name of the property \u0026ndash; that have different decorators. The method with property decorator returns the data, and the method with salary.setter decorator implements validation and sets the attribute.\nWe can use this property just as if it was a regular attribute (remember the only real attribute we have is the internal underscore-salary). Use the dot syntax and equality sign to assign a value to the salary property. Then, the setter method will be called. If we try to assign a negative value to salary, an exception will be raised.\n1 2 3 4 5  emp = Employer(\u0026#34;haoming\u0026#34;, 35000) print(emp.salary) # \u0026lt;-- accessing the property emp.salary = 60000 # \u0026lt;-- @salary.setter print(emp.salary) # \u0026lt;-- accessing the property again emp.salary = -1   35000 60000 --------------------------------------------------------------------------- ValueError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_29055/2378834019.py in \u0026lt;module\u0026gt; ----\u0026gt; 1 emp.salary = -1 /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_29055/3054542886.py in salary(self, new_salary) 10 def salary(self, new_salary): 11 if new_salary \u0026lt; 0: ---\u0026gt; 12 raise ValueError(\u0026quot;Invalid salary\u0026quot;) 13 self._salary = new_salary 14 ValueError: Invalid salary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # -------- Example -------- # class Customer: def __init__(self, name, new_bal): self.name = name if new_bal \u0026lt; 0: raise ValueError(\u0026#34;Invalid balance!\u0026#34;) self._balance = new_bal @property def balance(self): print(\u0026#34;property called\u0026#34;) return self._balance @balance.setter def balance(self, new_bal): if new_bal \u0026lt; 0: raise ValueError(\u0026#34;Invalid balance!\u0026#34;) self._balance = new_bal print(\u0026#34;Setter method called\u0026#34;) cust = Customer(\u0026#34;Belinda Lutz\u0026#34;, 2000) print(cust.balance) cust.balance = 3000 print(cust.balance)   property called 2000 Setter method called property called 3000 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # -------- Example -------- # import pandas as pd from datetime import datetime class LoggedDF(pd.DataFrame): def __init__(self, *args, **kwargs): pd.DataFrame.__init__(self, *args, **kwargs) self._created_at = datetime.today() def to_csv(self, *args, **kwargs): temp = self.copy() temp[\u0026#34;created_at\u0026#34;] = self._created_at pd.DataFrame.to_csv(temp, *args, **kwargs) # Add a read-only property: _created_at @property def created_at(self): return self._created_at # Instantiate a LoggedDF called ldf ldf = LoggedDF({\u0026#34;col1\u0026#34;: [1,2], \u0026#34;col2\u0026#34;:[3,4]}) print(ldf.created_at)   2019-09-10 14:56:45.115035 There are a few other things you can do with properties: if you do not define a setter method, the property will be read-only, like Dataframe shape. A method with an \u0026lt;attribute-name\u0026gt;.getter decorator will be called when the property\u0026rsquo;s value is just retrieved, and the method with the \u0026lt;attribute-name\u0026gt;.deleter \u0026ndash; when an attribute is deleted.\n","date":"2019-09-10T00:00:00Z","image":"https://wanghaoming177.netlify.app/head_img/oop_hi/oo5.png","permalink":"https://wanghaoming177.netlify.app/p/object-oriented-programming-iv-class/","title":"Object Oriented Programming IV (Class)"},{"content":"Intro Docstrings Docstrings, which describes what the arguments are supposed to be, and what it returns, makes code much easier to use, read, and maintain. A docstring is a string enclosed in triple quotes and written as the first line of a function.\n1 2 3 4 5  def func() \u0026#34;\u0026#34;\u0026#34; docstring \u0026#34;\u0026#34;\u0026#34; pass   Every docstring has some (although usually not all) of these five key pieces of information:\n what the function does what the arguments are what the return value or values should be info about any errors raised, anything else you\u0026rsquo;d like to say about the function.  Python community has evolved several standards for how to format your docstrings. Google-style and Numpydoc are the most popular formats, so we\u0026rsquo;ll focus on those.\nIn Google style, the docstring starts with a concise description of what the function does. This should be in imperative language.\n1 2 3 4 5 6 7  def count_letter(content, letter): \u0026#34;\u0026#34;\u0026#34; Count the number of times `letter` appears in `content` \u0026#34;\u0026#34;\u0026#34; if (not isinstance(letter, str)) or len(letter) != 1: raise ValueError(\u0026#39;`letter` must be a single character string.\u0026#39;) return len([char for char in content if char == letter])   Next comes the \u0026ldquo;Args\u0026rdquo; section where you list each argument name, followed by its expected type in parentheses, and then what its role is in the function. If you need extra space, you can break to the next line and indent. If an argument has a default value, mark it as \u0026ldquo;optional\u0026rdquo; when describing the type. If the function does not take any parameters, feel free to leave this section out.\n1 2 3 4 5 6 7 8 9 10  def count_letter(content, letter): \u0026#34;\u0026#34;\u0026#34;Count the number of times `letter` appears in `content`. Args: content (str): The string to search. letter (str): The letter to search for. \u0026#34;\u0026#34;\u0026#34; if (not isinstance(letter, str)) or len(letter) != 1: raise ValueError(\u0026#39;`letter` must be a single character string.\u0026#39;) return len([char for char in content if char == letter])   The next section is the \u0026ldquo;Returns\u0026rdquo; section, where you list the expected type or types of what gets returned. You can also provide some comment about what gets returned, but often the name of the function and the description will make this clear. Finally, if your function intentionally raises any errors, you should add a \u0026ldquo;Raises\u0026rdquo; section. You can also include any additional notes or examples of usage in free form text at the end.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def count_letter(content, letter): \u0026#34;\u0026#34;\u0026#34;Count the number of times `letter` appears in `content`. Args: content (str): The string to search. letter (str): The letter to search for. Returns: int Raises: ValueError: If `letter` is not a one-character string. \u0026#34;\u0026#34;\u0026#34; if (not isinstance(letter, str)) or len(letter) != 1: raise ValueError(\u0026#39;`letter` must be a single character string.\u0026#39;) return len([char for char in content if char == letter])    Personally, I think the Numpydoc format looks better than the Google style.\n you can access the contents of your function\u0026rsquo;s docstring using __doc__ attribute. Notice that the __doc__ attribute contains the raw docstring, including any tabs or spaces.\nTo remove those leading spaces, you can use the getdoc() function from the inspect module. The inspect module contains a lot of useful methods for gathering information about functions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  import inspect def build_tooltip(function): \u0026#34;\u0026#34;\u0026#34;Create a tooltip for any function that shows the function\u0026#39;s docstring. Args: function (callable): The function we want a tooltip for. Returns: str \u0026#34;\u0026#34;\u0026#34; docstring = inspect.getdoc(function) border = \u0026#39;#\u0026#39; * 28 return \u0026#39;{}\\n{}\\n{}\u0026#39;.format(border, docstring, border) print(build_tooltip(count_letter)) print(build_tooltip(range)) print(build_tooltip(print))   ############################ Count the number of times `letter` appears in `content`. Args: content (str): The string to search. letter (str): The letter to search for. Returns: int Raises: ValueError: If `letter` is not a one-character string. ############################ ############################ range(stop) -\u0026gt; range object range(start, stop[, step]) -\u0026gt; range object Return an object that produces a sequence of integers from start (inclusive) to stop (exclusive) by step. range(i, j) produces i, i+1, i+2, ..., j-1. start defaults to 0, and stop is omitted! range(4) produces 0, 1, 2, 3. These are exactly the valid indices for a list of 4 elements. When step is given, it specifies the increment (or decrement). ############################ ############################ print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream. ############################ DRY \u0026amp; Do one thing DRY (also known as \u0026ldquo;don\u0026rsquo;t repeat yourself\u0026rdquo;) and the \u0026ldquo;Do One Thing\u0026rdquo; principle are good ways to ensure that functions are well designed and easy to test.\nWhen you are writing code to look for answers to a research question, it is totally normal to copy and paste a bit of code, tweak it slightly, and return it. But it can lead to real problems. For instance, it is easy to accidentally introduce errors that are hard to spot. Another problem is that if you want to change something, you often have to do it in multiple places.\nWrapping the repeated logic in a function and then calling that function several times makes it much easier to avoid the kind of errors introduced by copying and pasting. And if you ever need to change something, you only have to do it in one or two places.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def standardize(column): \u0026#34;\u0026#34;\u0026#34;Standardize the values in a column. Args: column (pandas Series): The data to standardize. Returns: pandas Series: the values as z-scores \u0026#34;\u0026#34;\u0026#34; z_score = (column - column.mean()) / column.std() return z_score df[\u0026#39;y1_z\u0026#39;] = standardize(df.y1_gpa) df[\u0026#39;y2_z\u0026#39;] = standardize(df.y2_gpa) df[\u0026#39;y3_z\u0026#39;] = standardize(df.y3_gpa) df[\u0026#39;y4_z\u0026#39;] = standardize(df.y4_gpa)   Another software engineering principle: Do One Thing. Every function should have a single responsibility. Instead of one big function, we could have a more nimble function. We get several advantages from splitting the big function into smaller functions. First of all, our code has become more flexible. The code will also be easier for other developers to understand, and it will be more pleasant to test and debug. Finally, if you ever need to update your code, functions that each have a single responsibility make it easier to predict how changes in one place will affect the rest of the code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # split it into two simpler functions: mean() and median() def mean_and_median(values): \u0026#34;\u0026#34;\u0026#34;Get the mean and median of a sorted list of `values` Args: values (iterable of float): A list of numbers Returns: tuple (float, float): The mean and median \u0026#34;\u0026#34;\u0026#34; mean = sum(values) / len(values) midpoint = int(len(values) / 2) if len(values) % 2 == 0: median = (values[midpoint - 1] + values[midpoint]) / 2 else: median = values[midpoint] return mean, median   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  # function mean() def mean(values): \u0026#34;\u0026#34;\u0026#34;Get the mean of a sorted list of values Args: values (iterable of float): A list of numbers Returns: float \u0026#34;\u0026#34;\u0026#34; mean = sum(values)/len(values) return mean # function median() def median(values): \u0026#34;\u0026#34;\u0026#34;Get the median of a sorted list of values Args: values (iterable of float): A list of numbers Returns: float \u0026#34;\u0026#34;\u0026#34; midpoint = int(len(values) / 2) if len(values) % 2 == 0: median = (values[midpoint - 1] + values[midpoint]) / 2 else: median = values[midpoint] return median   Pass by assignment The way that Python passes information to functions is called \u0026ldquo;pass by assignment\u0026rdquo;.\nConsider function foo() that takes a list and sets the first value of the list to 99. Then we set \u0026ldquo;my_list\u0026rdquo; to the value [1, 2, 3] and pass it to foo(). The value of \u0026ldquo;my_list\u0026rdquo; would be [99, 2, 3] after calling foo(). Lists in Python are mutable objects, meaning that they can be changed.\n1 2 3 4 5  def foo(x): x[0] = 99 my_list = [1, 2, 3] foo(my_list) print(my_list)   [99, 2, 3] Consider function bar() that takes an argument and adds 90 to it. Then we assign the value 3 to the variable \u0026ldquo;my_var\u0026rdquo; and call bar() with \u0026ldquo;my_var\u0026rdquo; as the argument. The value of \u0026ldquo;my_var\u0026rdquo; would be 3 after we\u0026rsquo;ve called bar(). In Python, integers are immutable objects, meaning they can\u0026rsquo;t be changed.\n1 2 3 4 5  def bar(x): x = x + 90 my_var = 3 bar(my_var) print(my_var)   3 Imagine that this gray bar is the computer\u0026rsquo;s memory. When we set the variable \u0026ldquo;a\u0026rdquo; equal to the list [1, 2, 3], the Python interpreter makes \u0026lsquo;a\u0026rsquo; points to this location in memory. Then if we type \u0026ldquo;b = a\u0026rdquo;, the interpreter makes \u0026lsquo;b\u0026rsquo; points to whatever \u0026lsquo;a\u0026rsquo; is pointing to. (certainly, \u0026lsquo;a\u0026rsquo; points to whatever \u0026lsquo;b\u0026rsquo; is pointing to correspondingly.)\nIf we append 4 to the end of \u0026ldquo;a\u0026rdquo;, both variables get it because there is only one list. Likewise, if we append 5 to \u0026ldquo;b\u0026rdquo;, both variables get it.\nHowever, if we assign \u0026ldquo;a\u0026rdquo; to a different object (like int) in memory, that does not change where \u0026ldquo;b\u0026rdquo; is pointing. Now, things that happen to \u0026ldquo;a\u0026rdquo; are no longer happening to \u0026ldquo;b\u0026rdquo;, and vice versa.\nWhen we assign a list to the variable \u0026ldquo;my_list\u0026rdquo;, it sets up a location in memory for it. Then, when we pass \u0026ldquo;my_list\u0026rdquo; to the function foo(), the parameter \u0026ldquo;x\u0026rdquo; gets assigned to that same location. So when the function modifies the thing that \u0026ldquo;x\u0026rdquo; points to, it is also modifying the thing that \u0026ldquo;my_list\u0026rdquo; points to.\nIn the other example, we created a variable \u0026ldquo;my_var\u0026rdquo; and assigned it the value 3. Then we passed it to the function bar(), which caused the argument \u0026ldquo;x\u0026rdquo; to point to the same place \u0026ldquo;my_var\u0026rdquo; is pointing. But the bar() function assigns \u0026ldquo;x\u0026rdquo; to a new value , so the \u0026ldquo;my_var\u0026rdquo; variable isn\u0026rsquo;t touched. In fact, there is no way in Python to have changed \u0026ldquo;x\u0026rdquo; or \u0026ldquo;my_var\u0026rdquo; directly, because integers are immutable variables.\n You can comprehend it as: Python interpreter kills the old x, and release the relationship between x and my_var, and then create a new object x which has nothing to do with my_var.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def store_lower(_dict, _string): # \u0026lt;-- d \u0026amp; (associates with) _dict; s \u0026amp; _string \u0026#34;\u0026#34;\u0026#34;Add a mapping between `_string` and a lowercased version of `_string` to `_dict` Args: _dict (dict): The dictionary to update. _string (str): The string to add. \u0026#34;\u0026#34;\u0026#34; orig_string = _string # \u0026lt;-- orig_string \u0026amp; _string \u0026amp; s  _string = _string.lower() # \u0026lt;-- _string # (breaks up with) orig_string \u0026amp; s _dict[orig_string] = _string d = {} s = \u0026#39;Hello\u0026#39; store_lower(d, s) print(d,s)   {'Hello': 'hello'} Hello # {'orig_string': '_string'} s There are only a few immutable data types in Python because almost everything is represented as an object. The only way to tell if something is mutable is to see if there is a function or method that will change the object without assigning it to a new variable.\nNotice, Consider foo() is a function that appends the value 1 to the end of a list with an empty list as a default value.\n1 2 3  def foo(var=[]): var.append(1) return var   When we call foo() the first time, we will get a list with one entry as expected. But, when we call foo() again, the default value has already been modified.\n1 2  foo() foo()   [1] [1, 1] If you want a mutable variable as a default value, consider defaulting to None and setting the argument in the function.\n1 2 3 4 5 6 7  def foo(var=None): if var is None: var = [] var.append(1) return var foo() foo()   [1] [1] I wan to explain exactly what\u0026rsquo;s happening when we call a function with mutable object default value. Consider following example,\n1 2 3 4 5 6 7 8 9  def addend(lt=[]): # 0. lt.append(\u0026#39;end\u0026#39;) return lt lst = [1, 2, 3, 4] # 1. print(addend(lst)) # 2. print(lst) print(addend()) # 3. print(addend()) # 4.   [1, 2, 3, 4, 'end'] [1, 2, 3, 4, 'end'] ['end'] ['end', 'end']  step 0: def addend(lt=[]): As we define the function append(), Python interpreter assigns a memory space block, which stored an empty list, to argument lt. Notice, lt points to this block. step 1: lst = [1,2,3,4] We we define a global list object lst, Python interpreter assigns another memory, which stored a corresponding list, to variable lst. lst points to this block. step 2: print(addend(lst)) When we call function addend with parameter lst, Python interpreter reads this global object in the local scope, and then makes pointer lt point to the block pointed by lst. Thus lt, lst point to the same memory block, which loaded list object ([1,2,3,4]). And then interpreter appends a new element \u0026ldquo;end\u0026rdquo; to this block, which means it changes the value of lt and lst simultaneously. This is the reason why the outcome of print(lst) is [1, 2, 3, 4, 'end'].   Notice that if lt, lst is an immutable object, the python interpreter will kill the older lt pointer, and assigns a new memory block to lt, instead of appending the element to the memory block. Thus it will not modify the value of lst. Collectively,\n immutable object: change the space pointed by variable. mutable object: change the value of space pointed by variable.    step 3/4: print(addend()) When we call addend() without parameter, the Python interpreter will add an element \u0026ldquo;end\u0026rdquo; to the space pointed by lt; when we call addend() again, the space pointed by lt has already been changed.  Above example is adapted from link.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # Modify this func using an immutable variable for the default argument. def add_column(values, df=pandas.DataFrame()): \u0026#34;\u0026#34;\u0026#34;Add a column of `values` to a DataFrame `df`. The column will be named \u0026#34;col_\u0026lt;n\u0026gt;\u0026#34; where \u0026#34;n\u0026#34; is the numerical index of the column. Args: values (iterable): The values of the new column df (DataFrame, optional): The DataFrame to update. If no DataFrame is passed, one is created by default. Returns: DataFrame \u0026#34;\u0026#34;\u0026#34; df[\u0026#39;col_{}\u0026#39;.format(len(df.columns))] = values return df   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  def better_add_column(values, df=None): \u0026#34;\u0026#34;\u0026#34;Add a column of `values` to a DataFrame `df`. The column will be named \u0026#34;col_\u0026lt;n\u0026gt;\u0026#34; where \u0026#34;n\u0026#34; is the numerical index of the column. Args: values (iterable): The values of the new column df (DataFrame, optional): The DataFrame to update. If no DataFrame is passed, one is created by default. Returns: DataFrame \u0026#34;\u0026#34;\u0026#34; if df is None: df = pandas.DataFrame() df[\u0026#39;col_{}\u0026#39;.format(len(df.columns))] = values return df   Context Managers Using context managers A context manager is a type of function that\n sets up a context for code to run in, runs your code, and then removes the context.  The \u0026ldquo;open()\u0026rdquo; function is a context manager. When you write \u0026ldquo;with open()\u0026rdquo;, it opens a file that you can read from or write to. Then, it gives control back to your code so that you can perform operations on the file object. When the code inside the indented block is done, the \u0026ldquo;open()\u0026rdquo; function makes sure that the file is closed before continuing on in the script.\nAny time you use a context manager, it will look like this:\n The keyword \u0026ldquo;with\u0026rdquo; lets Python know that you are trying to enter a context. Then you call a function. You can call any function that is built to work as a context manager. A context manager can take arguments like any normal function. You end the \u0026ldquo;with\u0026rdquo; statement with a colon \u0026lsquo;:\u0026rsquo;, as if you were writing a for loop or an if statement. Any code that you want to run inside the context that the context manager created needs to be indented. When the indented block is done, the context manager gets a chance to clean up anything that it needs to, like when the \u0026ldquo;open()\u0026rdquo; context manager closed the file.  Collectively:\n1 2  with \u0026lt;context-manager\u0026gt;(\u0026lt;args\u0026gt;): \u0026lt;code\u0026gt;    Statements in Python that have an indented block after them, like for loops, if/else statements, function/class definitions def/Class, etc. are called \u0026ldquo;compound statements\u0026rdquo;. The \u0026ldquo;with\u0026rdquo; statement is another type of compound statement.\n Some context managers want to return a value that you can use inside the context. By adding \u0026ldquo;as\u0026rdquo; and a variable name at the end of the \u0026ldquo;with\u0026rdquo; statement, you can assign the returned value to the variable name.\n1 2 3 4 5 6 7 8 9  with open(\u0026#39;alice.txt\u0026#39;) as file: text = file.read() n = 0 for word in text.split(): if word.lower() in [\u0026#39;cat\u0026#39;, \u0026#39;cats\u0026#39;]: n += 1 print(\u0026#39;Lewis Carroll uses the word \u0026#34;cat\u0026#34; {}times\u0026#39;.format(n))   Lewis Carroll uses the word \u0026quot;cat\u0026quot; 24 times Writing context managers There are two ways to define a context manager in Python:\n by using a class that has special __enter__() and __exit__() methods by decorating a certain kind of function.  We will focus on the function-based method here. There are five parts to creating a context manager.\n define a function. (optional) add any setup code your context needs. use the \u0026ldquo;yield\u0026rdquo; keyword to signal to Python that this is a special kind of function. (optional) add any teardown code that your context needs. decorate the function on the line above your context manager function with @contextlib.contextmanager.  1 2 3 4 5 6  import contextlib @contextlib.contextmanager def context_`func()`: # setup code yield # teardown code   When you write yield keyword, it means that you are going to return a value, but you expect to finish the rest of the function at some point in the future. As we say last blog, the \u0026ldquo;yield\u0026rdquo; keyword as a thing that gets used when creating generators. In fact, a context manager function is technically a generator that yields a single value. The value that your context manager yields can be assigned to a variable in the \u0026ldquo;with\u0026rdquo; statement by adding \u0026ldquo;as \u0026lt;variable name\u0026gt;\u0026rdquo;.\nHere, we\u0026rsquo;ve assigned the value 42 that my_context() yields to the variable \u0026ldquo;foo\u0026rdquo;. By running this code, you can see that after the context block is done executing, the rest of the my_context() function gets run, printing \u0026ldquo;goodbye\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import contextlib @contextlib.contextmanager def my_context(): \u0026#34;\u0026#34;\u0026#34;Show the order of the execution of CM code. Yield: a certain value. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;1\u0026#34;) yield 34 print(\u0026#34;2\u0026#34;) with my_context() as foo: print(foo)   1 34 2 This context manager is an example of code that accesses a database. Like most context managers, it has some setup code that runs before the function yields. This context manager uses that setup code to connect to the database. Most context managers also have some teardown or cleanup code when they get control back after yielding.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  @contextlib.contextmanager def database(url): \u0026#34;\u0026#34;\u0026#34;Connect to a database and close the connection after query. Yield: connection object \u0026#34;\u0026#34;\u0026#34; db = postgres.connect(url) yield db db.disconnect() url = \u0026#34;https://xxxx\u0026#34; with database(url) as d: outcome = d.execute(\u0026#34;select * from table\u0026#34;)   This setup/teardown behavior allows a context manager to hide things like connecting and disconnecting from a database so that a programmer using the context manager can just perform operations on the database without worrying about the underlying details.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  @contextlib.contextmanager def open_read_only(filename): \u0026#34;\u0026#34;\u0026#34;Open a file in read-only mode. Args: filename (str): The location of the file to read. Yields: str: the content of the opened file object. \u0026#34;\u0026#34;\u0026#34; f = open(filename, mode=\u0026#39;r\u0026#39;) content = f.read() yield content f.close() with open_read_only(\u0026#39;my_file.txt\u0026#39;) as content: print(content)   Congratulations! You wrote a context manager that acts like `open()` but responses the content of the file directly without `read()` method. The database() context manager that we\u0026rsquo;ve been looking at yields a specific value - the database connection - that can be used in the context block. Some context managers don\u0026rsquo;t yield an explicit value. in_dir() is a context manager that changes the current working directory to a specific path and then changes it back after the context block is done. It does not need to return anything with its \u0026ldquo;yield\u0026rdquo; statement.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  import os @contextlib.contextmanager def in_dir(path=\u0026#34;../\u0026#34;): \u0026#34;\u0026#34;\u0026#34;changes the current working directory to a specific path and then changes it back after the context block is done. Yield: None \u0026#34;\u0026#34;\u0026#34; old_dir = os.getcwd() print(os.getcwd()) os.chdir(path) yield os.chdir(old_dir) print(os.getcwd()) with in_dir(): print(os.getcwd()) print(os.listdir())   /Users/wanghaoming/PycharmProjects/daliy_test/save /Users/wanghaoming/PycharmProjects/daliy_test ['testpdf1.pdf', 'skl.ipynb', 'testpdf.pdf', 'pdf.ipynb', 'save', '.idea'] /Users/wanghaoming/PycharmProjects/daliy_test/save 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  @contextlib.contextmanager def timer(): \u0026#34;\u0026#34;\u0026#34;Time the execution of a context block. Yields: None \u0026#34;\u0026#34;\u0026#34; start = time.time() yield end = time.time() print(\u0026#39;Elapsed: {:.2f}s\u0026#39;.format(end - start)) with timer(): print(\u0026#39;This should take approximately 0.25 seconds\u0026#39;) time.sleep(0.25)   This should take approximately 0.25 seconds Elapsed: 0.25s Advanced topics Nested contexts Imagine you are implementing this copy() function that copies the contents of one file to another file. One way you could write this function would be to open the source file, store the contents of the file in the \u0026ldquo;contents\u0026rdquo; variable, then open the destination file and write the contents to it. This approach works fine until you try to copy a file that is too large to fit in memory.\n1 2 3 4 5 6 7 8 9 10 11  def copy(src, dst): \u0026#34;\u0026#34;\u0026#34;Copy the contents of one file to another. Args: src (str): File name of the file to be copied. dst (str): Where to write the new file. \u0026#34;\u0026#34;\u0026#34; with open(src,\u0026#34;r\u0026#34;) as f_src: contents = f_src.read() with open(dst,\u0026#34;w\u0026#34;) as f_dst: f_dst.write(contents)   A good ideal is that we could open both files at once and copy over one line at a time. The file object that the \u0026ldquo;open()\u0026rdquo; context manager returns can be iterated over in a for loop. In Python, nested \u0026ldquo;with\u0026quot; statements are perfectly legal.\n1 2 3 4 5 6 7 8 9 10 11  def copy(src, dst): \u0026#34;\u0026#34;\u0026#34;Copy the contents of one file to another. Args: src (str): File name of the file to be copied. dst (str): Where to write the new file. \u0026#34;\u0026#34;\u0026#34; with open(src,\u0026#34;r\u0026#34;) as f_src: with open(dst,\u0026#34;w\u0026#34;) as f_dst: for line in f_src: f_dst.write(line)   The context manager stock('NVDA') will connect to the NASDAQ and return an object that you can use to get the latest price by calling its .price() method. We now connect to stock('NVDA') and record 10 timesteps of price data by writing it to the file NVDA.txt. Notice that the object returned by stock() would be very large, thus we use nested context manager:\n1 2 3 4 5 6  with stock(\u0026#34;NVDA\u0026#34;) as nvda: with open(\u0026#39;NVDA.txt\u0026#39;, \u0026#39;w\u0026#39;) as f_out: for _ in range(10): value = nvda.price() print(\u0026#39;Logging ${:.2f}for NVDA\u0026#39;.format(value)) f_out.write(\u0026#39;{:.2f}\\n\u0026#39;.format(value))   Opening stock ticker for NVDA Logging $139.50 for NVDA Logging $139.54 for NVDA Logging $139.61 for NVDA Logging $139.65 for NVDA Logging $139.72 for NVDA Logging $139.73 for NVDA Logging $139.80 for NVDA Logging $139.78 for NVDA Logging $139.73 for NVDA Logging $139.64 for NVDA Closing stock ticker Handling errors If the programmer who uses your context manager writes code that causes an error after setup code but before teardown code, then the file or connection would not be close.\nTo cope with an error, you may be familiar with the \u0026ldquo;try\u0026rdquo; statement. It allows you to write code that might raise an error inside the \u0026ldquo;try\u0026rdquo; block and catch that error inside the \u0026ldquo;except\u0026rdquo; block. You can choose to ignore the error or re-raise it. The \u0026ldquo;try\u0026rdquo; statement also allows you to add a \u0026ldquo;finally\u0026rdquo; block. This is code that runs no matter what an exception occurred or not.\nThe solution is to put a \u0026ldquo;try\u0026rdquo; statement before the \u0026ldquo;yield\u0026rdquo; statement in our context manager function and a \u0026ldquo;finally\u0026rdquo; statement before teardown code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  def in_dir(directory): \u0026#34;\u0026#34;\u0026#34;Change current working directory to `directory`, allow the user to run some code, and change back. Args: directory (str): The path to a directory to work in. \u0026#34;\u0026#34;\u0026#34; current_dir = os.getcwd() os.chdir(directory) try: yield finally: os.chdir(current_dir)   If you notice that your code is following any of these patterns, you might consider using a context manager.\n   setup teardown     open close   lock release   change reset   enter exit   start stop   setup teardown   connect disconnect   Adapted from Dave Brondsema\u0026rsquo;s talk at PyCon 2012.     Decorators Functions are objects Functions are just like any other object in Python. They are not fundamentally different from lists, dictionaries, DataFrames, strings, integers, floats, modules, or anything else in Python. And because functions are just another type of object, you can do anything to or with them that you would do with any other kind of object.\nYou can take a function and assign it to a variable, like assign the print() function to p, and use it as your print() function.\n1 2  p = print p(\u0026#34;test\u0026#34;)   test You can also add functions to a list or dictionary. Here, we\u0026rsquo;ve added the functions my_function(), open(), and print() to the list \u0026ldquo;list_of_functions\u0026rdquo;. Below that, we\u0026rsquo;ve added the same three functions to a dictionary dict_of_functions, under the keys \u0026ldquo;func1\u0026rdquo;, \u0026ldquo;func2\u0026rdquo;, and \u0026ldquo;func3\u0026rdquo;. We can call the element of the list or reference values of the dictionary and use them as if we were calling the functions directly.\n1 2 3 4 5 6 7 8 9 10  def my_function(): return 42 list_of_functions = [my_function, open, print] dict_of_functions = { \u0026#34;func1\u0026#34;: my_function, \u0026#34;func2\u0026#34;: open, \u0026#34;func3\u0026#34;: print } dict_of_functions[\u0026#34;func3\u0026#34;](list_of_functions[0]())   42 Notice that when you assign a function to a variable, you do not include the parentheses after the function name. This is a subtle but very important distinction. When you type my_function() with the parentheses, you are calling that function. It evaluates to the value that the function returns. However, when you type \u0026ldquo;my_function\u0026rdquo; without the parentheses, you are referencing the function itself. It evaluates to a function object.\n1 2  print(my_function()) print(my_function)   42 \u0026lt;function my_function at 0x7fe6c1691b80\u0026gt; Since a function is just an object like anything else in Python, you can pass one as an argument to another function. The has_docstring() function checks to see whether the function that is passed to it has a docstring or not. We could define these two functions, no() and yes(), and pass them as arguments to the has_docstring() function. Since the no() function doesn\u0026rsquo;t have a docstring, the has_docstring() function returns False. Likewise, has_docstring() returns True for the yes() function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  def has_docstring(func): \u0026#34;\u0026#34;\u0026#34;Check to see if the function `func` has a docstring. Args: func (callable): A function. Returns: bool \u0026#34;\u0026#34;\u0026#34; return func.__doc__ is not None def yes(): \u0026#34;\u0026#34;\u0026#34;Return the value 42 \u0026#34;\u0026#34;\u0026#34; return 42 def no(): return 42 print(has_docstring(yes)) print(has_docstring(no)) print(has_docstring(has_docstring))   True False True Functions can also be defined inside other functions. These kinds of functions are called nested functions. A nested function can make your code easier to read. In this example, if x and y are within some bounds, foo() prints x times y. We can make that if statement easier to read by defining an in_range() function.\n1 2 3 4 5 6 7 8 9 10 11 12  def foo1(x,y): if (x\u0026gt;4 and x\u0026lt;10) and (y\u0026gt;4 and y \u0026lt;10): print(x*y) def foo2(x,y): def in_range(v): return v\u0026gt;4 and v\u0026lt;10 if in_range(x) and in_range(y): print(x*y) foo1(5,6) foo2(7,8)   30 56 Function also could return a function. For instance, the function get_function() creates a new function, print_me(), and then returns it. If we assign the result of calling get_function() to the variable \u0026ldquo;new_func\u0026rdquo;, we are assigning the return value, \u0026ldquo;print_me()\u0026rdquo; to \u0026ldquo;new_func\u0026rdquo;. We can then call new_func()`` as if it were the print_me() function.\n1 2 3 4 5 6 7  def get_function(): def print_me(s): print(s) return print_me new_func = get_function() new_func(\u0026#34;test\u0026#34;)   test 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  def create_math_function(func_name): if func_name == \u0026#39;add\u0026#39;: def add(a, b): return a + b return add elif func_name == \u0026#39;subtract\u0026#39;: def subtract(a,b): return a-b return subtract else: print(\u0026#34;I don\u0026#39;t know that one\u0026#34;) add = create_math_function(\u0026#39;add\u0026#39;) print(\u0026#39;5 + 2 = {}\u0026#39;.format(add(5, 2))) subtract = create_math_function(\u0026#39;subtract\u0026#39;) print(\u0026#39;5 - 2 = {}\u0026#39;.format(subtract(5, 2)))   5 + 2 = 7 5 - 2 = 3 Scope We have discussed scope in python before, now we gonna briefly recall it. Python has to have strict rules about which variable you are referring to when using a particular variable name.\n First, the interpreter looks in the Local Scope. When you are inside a function, the local scope is made up of the arguments and any variables defined inside the function. In the case of nested functions, where one function is defined inside another function, Python will check the scope of the parent function after checking the local scope. This is called the Nonlocal Scope to show that it is not the local scope of the child function and not the global scope. If the interpreter can\u0026rsquo;t find the variable in the (non)local scope, it expands its search to the Global Scope. These are the things defined outside the function. Finally, if it can\u0026rsquo;t find the thing it is looking for in the global scope, the interpreter checks the Builtin Scope. These are things that are always available in Python. For instance, the print() function is in the builtin scope, which is why we are able to use it in any scope.  Note that Python only gives you read access, instead of write, to variables defined outside of your current scope. But you can write it indirectly using global or nonlocal keywords.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  x = 50 def one(): x = 10 def two(): global x x = 30 def three(): x = 100 print(x) for func in [one, two, three]: `func()` print(x)   50 30 100 30 Closure A closure in Python is a tuple of variables that are no longer in scope, but that a function needs in order to run.\nConsider an example. The function foo() defines a nested function bar() that prints the value of \u0026ldquo;a\u0026rdquo;. foo() returns this new function, so when we say \u0026ldquo;func = foo()\u0026rdquo; we are assigning the bar() function to the variable \u0026ldquo;func\u0026rdquo;. When we call func(), it prints the value of variable \u0026ldquo;a\u0026rdquo;, which is 5.\n1 2 3 4 5 6 7 8  def foo(): a = 5 def bar(): # Notice, if you set `def bar(a):`, then you must assign it a value when you call bar(). print(a) return bar func = foo() func()   a How does function \u0026ldquo;func()\u0026rdquo; know anything about variable \u0026ldquo;a\u0026rdquo;? \u0026ldquo;a\u0026rdquo; is defined in foo()\u0026rsquo;s scope, not bar()\u0026rsquo;s. You would think that \u0026ldquo;a\u0026rdquo; would not be observable outside of the scope of foo(). That\u0026rsquo;s where closures come in. When foo() returned the new bar() function, Python helpfully attached any nonlocal variable that bar() was going to need to the function object.\nThose variables get stored in a tuple in the \u0026ldquo;__closure__\u0026rdquo; attribute, which could be iterated, of the function. The closure for \u0026ldquo;func\u0026rdquo; has one variable, and you can view the value of that variable by accessing the \u0026ldquo;cell_contents\u0026rdquo; of the item.\n1 2 3 4  print(func.__closure__) print(type(func.__closure__)) print(len(func.__closure__)) print(func.__closure__[0].cell_contents)   (\u0026lt;cell at 0x7fe6b033aa30: int object at 0x7fe70002e9b0\u0026gt;,) \u0026lt;class 'tuple'\u0026gt; 1 5 1 2 3 4 5 6 7 8 9 10 11 12 13  def parent(arg_1, arg_2): value = 22 my_dict = {\u0026#39;chocolate\u0026#39;: \u0026#39;yummy\u0026#39;} def child(): print(2 * value) print(my_dict[\u0026#39;chocolate\u0026#39;]) print(arg_1 + arg_2) return child new_function = parent(3, 4) print([cell.cell_contents for cell in new_function.__closure__])   [3, 4, {'chocolate': 'yummy'}, 22] Let\u0026rsquo;s examine this bit of code. Here, x is defined in the global scope. foo() creates a function bar() that prints whatever argument was passed to foo(). When we call foo() and assign the result to \u0026ldquo;my_func\u0026rdquo;, we pass in \u0026ldquo;x\u0026rdquo;. So, as expected, calling my_func() prints the value of x. Now let\u0026rsquo;s delete x and call my_func() again, we would still print 25. That\u0026rsquo;s because foo()\u0026rsquo;s \u0026ldquo;value\u0026rdquo; argument gets added to the closure attached to the new \u0026ldquo;my_func\u0026rdquo; function. So even though x doesn\u0026rsquo;t exist anymore, the value persists in its closure.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  x = 25 def foo(value=1): def bar(): print(value) return bar my_func = foo(x) # \u0026lt;-- Create closure here my_func() del(x) my_func() print(my_func.__closure__[0].cell_contents) x = 52 my_func() print(my_func.__closure__[0].cell_contents)   25 25 25 25 25 Notice that nothing changes if we overwrite \u0026ldquo;x\u0026rdquo; instead of deleting it. Here we\u0026rsquo;ve passed x into foo() and then assigned the new function to the variable x. The old value of \u0026ldquo;x\u0026rdquo;, 25, is still stored in the new function\u0026rsquo;s closure, even though the new function is now stored in the \u0026ldquo;x\u0026rdquo; variable.\nDecorators Let\u0026rsquo;s say you have a function that takes some inputs and returns some outputs. Decorators are just functions that take a function as an argument and return a modified version of that function that changes that function\u0026rsquo;s behavior. You can modify the inputs, modify the outputs, or even change the behavior of the function itself.\nTo start off, let\u0026rsquo;s define a function double_args does not modify anything of the input function.\n1 2  def double_args(func): return func   It just takes a function and immediately returns it. If we call this version of double_args() that does nothing and pass it the multiply() function and then assign the result to the variable \u0026ldquo;new_multiply\u0026rdquo;, then we can call new_multiply(1, 5) and get the same value we would have gotten from multiply(1, 5).\n1 2 3 4 5 6 7 8  def multiply(a,b): return a * b def double_args(func): return func new_multiply = double_args(multiply) print(new_multiply(1,5) == multiply(1,5))   True In order for your decorator to return a modified function, it is usually helpful for it to define a nested function to return. We\u0026rsquo;ll call that nested function \u0026ldquo;wrapper()\u0026rdquo;. All wrapper() does is take two arguments and passes them on to whatever function was passed to double_args() in the first place, assuming that the function passed to double_args() also takes exactly two arguments.\nSo, double_args() is still not doing anything to actually modify the function it is decorating. Once again, we\u0026rsquo;ll pass multiply() to double_args() and assign the result to new_multiply(). If we then call new_multiply(), which is now equal to the wrapper() function, wrapper() calls multiply() because it is the function that was passed to double_args(). So wrapper() calls multiply() with the arguments 1 and 5, which returns 5.\n1 2 3 4 5 6 7  def double_args(func): def wrapper(a,b): return(func(a,b)) return wrapper new_multiply = double_args(multiply) # ==\u0026gt; **refer** `wrapper` with 2 arguments `a`,`b` and a closure `multiply`. new_multiply(1,5) # ==\u0026gt; **call** wrapper(1,5) return multiply(1,5)   5 Now let\u0026rsquo;s actually modify the function our decorator is decorating. This time, wrapper() will still call whatever function is passed to double_args(), but it will double every argument when it calls the original function. As usual, we will call double_args() on the multiply() function and assign the result to new_multiply. Now, when we call new_multiply() with 1 and 5 as arguments, new_multiply() is equal to wrapper(), which calls multiply() after doubling each argument. So 1 becomes 2 and 5 becomes 10, giving us 2 times 10, which equals 20.\n1 2 3 4 5 6 7  def double_args(func): def wrapper(a,b): return(func(2*a,2*b)) return wrapper new_multiply = double_args(multiply) # ==\u0026gt; e.t. wrapper with closure `multiply` new_multiply(1,5) # ==\u0026gt; e.t. `wrapper(1,5)` returns `multiply(2,10)`   20 This time, instead of assigning the new function to \u0026ldquo;new_multiply\u0026rdquo;, we\u0026rsquo;re going to overwrite the \u0026ldquo;multiply\u0026rdquo; variable. And then calling multiply() with arguments 1 and 5 gives us 20 instead of 5. Remember that we can do this because Python stores the original multiply function in the new function\u0026rsquo;s closure.\n1 2 3  multiply = double_args(multiply) print(multiply(1,5)) print(multiply.__closure__[0].cell_contents)   20 \u0026lt;function multiply at 0x7f0060c9e620\u0026gt; Now we can implement above code in a concise way, using @ symbol followed by the decorator\u0026rsquo;s name on the line directly above the function you are decorating.\n1 2 3 4 5 6 7 8 9 10  def double_args(func): def wrapper(a,b): return func(a*2, b*2) return wrapper @double_args def multiply(a,b): return a * b multiply(1,5)   20 This is just a Python convenience for saying \u0026ldquo;multiply\u0026rdquo; equals the value returned by calling double_args() with \u0026ldquo;multiply\u0026rdquo; as the only argument, i.e. multiply = double_args(multiply). The above code is exactly equivalent to the following code,\n1 2 3 4 5 6 7 8 9 10  def double_args(func): def wrapper(a,b): return func(a*2, b*2) return wrapper def multiply(a,b): return a * b multiply = double_args(multiply) multiply(1,5)   20 More on Decorators Real-world examples count time cost We will walk through some real-world decorators so that we can start to recognize common decorator patterns. The timer() decorator runs the decorated function and then prints how long it took for the function to run.\n you wind up adding some version of this to all of projects because it is a pretty easy way to figure out where your computational bottlenecks are.\n All decorators have fairly similar-looking docstrings because they all take and return a single function. Like most decorators, we\u0026rsquo;ll start off by defining a wrapper() function. This is the function that the decorator will return. wrapper() takes any number of positional and keyword arguments so that it can be used to decorate any function. The first thing the new function will do is record the time that it was called with the time() function. Then wrapper() gets the result of calling the decorated function. We don\u0026rsquo;t return that value yet though. After calling the decorated function, wrapper() checks the time again, and prints a message about how long it took to run the decorated function. Once we\u0026rsquo;ve done that, we need to return the value that the decorated function calculated.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import time def timer(func): \u0026#34;\u0026#34;\u0026#34;A decorator that prints how long a function took to run. Args: func (callable): The function being decorated. Returns: callable: The decorated function. \u0026#34;\u0026#34;\u0026#34; def wrapper(*args, **kargs): start_time = time.time() result = func(*args, **kargs) print(f\u0026#34;time cost: {time.time()-start_time}s\u0026#34;) return result return wrapper @timer def sleep_second(n): time.sleep(n) print(\u0026#34;get up ~ \u0026#34;) sleep_second(3)   get up ~ time cost: 3.0057671070098877s Alternatively, you can also modify it as\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  import time import numpy as np def timer(func): \u0026#34;\u0026#34;\u0026#34;A decorator that prints how long a function took to run. Args: func (callable): The function being decorated. Returns: callable: The decorated function. \u0026#34;\u0026#34;\u0026#34; def wrapper(*args, **kargs): start_time = time.time() result = func(*args, **kargs) cost = time.time()-start_time timecost[func.__name__]=cost return result return wrapper @timer def sleep_second1(n): time.sleep(n) print(\u0026#34;get up ~ \u0026#34;) @timer def sleep_second2(n): time.sleep(n) print(\u0026#34;get up ~ \u0026#34;) @timer def sleep_second3(n): time.sleep(n) print(\u0026#34;get up ~ \u0026#34;) timecost = {} funcls = [sleep_second1,sleep_second2,sleep_second3] [f(np.random.uniform(.5,2.5)) for f in funcls] print(timecost) # \u0026lt;-- Notice, we can accept the desired dict because dict is mutable object.    get up ~ get up ~ get up ~ {'sleep_second1': 0.689359188079834, 'sleep_second2': 1.0407230854034424, 'sleep_second3': 1.763228178024292} store the result Memoizing is the process of storing the results of a function so that the next time the function is called with the same arguments; you can just look up the answer.\nWe start by setting up a dictionary that will map arguments to results. Then, as usual, we create wrapper() to be the new decorated function that this decorator returns. When the new function gets called, we check to see whether we\u0026rsquo;ve ever seen these arguments before. If we haven\u0026rsquo;t, we send them to the decorated function and store the arguments and result in the dictionary. The next time we call this function with those same arguments, the return value will already be in the dictionary, and we can return the value very quickly.\nThere are something deserve attentions:\n Fitstly, when we use @ symbol, Python actions as multiply = memoize(multiply), which means Python packages all objects required by wrapper function, include (original)multiply and result_dict , in to the closure. Secondly, when we call multiply(1,2,3,4,5), the result_dict will be written the arguments and result by Python. But notice that dict is mutable object, which means the closure\u0026rsquo;s second element, result_dict, will be changed simultaneously.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  import time def memoize(func): \u0026#34;\u0026#34;\u0026#34;A decorator that stores the result of decorated function with specific arguments. Args: func (callable): The function being decorated. Returns: callable: The decorated function. \u0026#34;\u0026#34;\u0026#34; result_dict = {} def wrapper(*args): if args not in result_dict: result_dict[args] = func(*args) return result_dict[args] return wrapper @memoize # \u0026lt;--- e.t. multiply = memoize(multiply) def multiply(*args): j = 1 for i in args: j *= i time.sleep(3) print(\u0026#34;time cost: 3s\u0026#34;) return j print([cell.cell_contents for cell in multiply.__closure__]) print(multiply(1,2,3,4,5)) print([cell.cell_contents for cell in multiply.__closure__]) print(multiply(1,2,3,4,5)) print([cell.cell_contents for cell in multiply.__closure__]) print(multiply(1,2,3,4,5,6)) print([cell.cell_contents for cell in multiply.__closure__])   [\u0026lt;function multiply at 0x7fe6d00fe3a0\u0026gt;, {}] time cost: 3s 120 [\u0026lt;function multiply at 0x7fe6d00fe3a0\u0026gt;, {(1, 2, 3, 4, 5): 120}] 120 [\u0026lt;function multiply at 0x7fe6d00fe3a0\u0026gt;, {(1, 2, 3, 4, 5): 120}] time cost: 3s 720 [\u0026lt;function multiply at 0x7fe6d00fe3a0\u0026gt;, {(1, 2, 3, 4, 5): 120, (1, 2, 3, 4, 5, 6): 720}]  You should consider using a decorator when you want to add some common bit of code to multiple functions.\n multi decorators You can use @ symbol folded to make multi decorators, like:\n1 2 3 4  @decorator2 @decorator1 def func(): pass   Recall that when we use @decorator, what Python exactly does is overwrite the decorated function func, like func = decorator(func), which assigns the child function of decorator with the closure containing origin func to new func. Homogeneously, multi decorators just do nestedly overwrite the decorated function with inner-to-outer order. For instance, if we use above codes, then decorator1 overwrites func firstly, assigns child function with closure to func; And then decorator2 overwrites func again, assigns child function with closure to func. There is an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  def decorator1(func): a1 = 1 print(\u0026#34;Get in decorator1\u0026#34;) def wrapper1(*args): print(\u0026#34;Get in wrapper1\u0026#34; ,a1) return func(*args) return wrapper1 def decorator2(func): a2 = 2 print(\u0026#34;Get in decorator2\u0026#34;) def wrapper2(*args): print(\u0026#34;Get in wrapper2\u0026#34; ,a2) return func(*args) return wrapper2 @decorator1 def f(): print(\u0026#34;Ger in f\u0026#34;)   Get in decorator1 Python implements f = decorator1(f), thus print the \u0026ldquo;Get in decorator1\u0026rdquo;.\n1 2  print(f.__closure__) print([cell.cell_contents for cell in f.__closure__])   (\u0026lt;cell at 0x7fe6c168c7f0: int object at 0x7fe70002e930\u0026gt;, \u0026lt;cell at 0x7fe6c168c2b0: function object at 0x7fe6d00fdee0\u0026gt;) [1, \u0026lt;function f at 0x7fe6d00fdee0\u0026gt;] We can see the global object f and nonlocal object a1 has been in closure of decorated f. Now call the multi decorators:\n1 2 3 4 5 6 7  @decorator2 @decorator1 def f(): print(\u0026#34;Ger in f\u0026#34;) print(f.__closure__) print([cell.cell_contents for cell in f.__closure__])   Get in decorator1 Get in decorator2 (\u0026lt;cell at 0x7fe6e0242df0: int object at 0x7fe70002e950\u0026gt;, \u0026lt;cell at 0x7fe6e0242280: function object at 0x7fe7014f6160\u0026gt;) [2, \u0026lt;function decorator1.\u0026lt;locals\u0026gt;.wrapper1 at 0x7fe7014f6160\u0026gt;] we can see Python implements f = decorator1(f) and f = decorator2(f) successively, and print the statement in corresponding order. Notice that the nonlocal object a2 and decorated f (wrapper1) were written into the closure of final f. So where is the closure of wrapper1 that we discussed? We can find them disassembling \u0026lt;function decorator1.\u0026lt;locals\u0026gt;.wrapper1 at 0x7fe7014f6160\u0026gt;\n1 2  print(f.__closure__[1].cell_contents.__closure__) print([cell1.cell_contents for cell1 in f.__closure__[1].cell_contents.__closure__])   (\u0026lt;cell at 0x7fe7014f8af0: int object at 0x7fe70002e930\u0026gt;, \u0026lt;cell at 0x7fe7014f87c0: function object at 0x7fe7014f61f0\u0026gt;) [1, \u0026lt;function f at 0x7fe7014f61f0\u0026gt;]  f.__closure__[1] is a cell object, you should call cell_contents attribute to attain the content of it.\n Everything we discussed above happened before calling the double-decorated function f. Since f is a nested function, like f=decorator2(decorator1(f)), when we call f(), the inner function wrapper2 will conduct firstly, and then wrapper1 and finally original f.\n1  f()   Get in wrapper2 2 Get in wrapper1 1 Ger in f other examples print_return_type() will print out the type of the variable that gets returned from every call of any function it is decorating\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def print_return_type(func): def wrapper(v): result = func(v) print(\u0026#39;{}() returned type {}\u0026#39;.format( func.__name__, type(result) )) return result return wrapper @print_return_type def foo(value): return value print(foo(42)) print(foo([1, 2, 3])) print(foo({\u0026#39;a\u0026#39;: 42}))   foo() returned type \u0026lt;class 'int'\u0026gt; 42 foo() returned type \u0026lt;class 'list'\u0026gt; [1, 2, 3] foo() returned type \u0026lt;class 'dict'\u0026gt; {'a': 42} Counting how many times some functions is called:\n1 2 3 4 5 6 7 8 9 10 11 12 13  def counter(func): # \u0026lt;-- called when use `@` def wrapper(*args, **kwargs): # \u0026lt;-- called everytime calling `foo()` wrapper.count += 1 return func() # \u0026lt;-- Notice the parentheses, we need to implement the passed-in function and attain the outcome, instead of assigning it to something. wrapper.count = 0 return wrapper @counter def foo(): print(\u0026#39;calling foo()\u0026#39;) [foo() for _ in range(5)] print(\u0026#39;foo() was called {}times.\u0026#39;.format(foo.count))   calling foo() calling foo() calling foo() calling foo() calling foo() foo() was called 5 times.  You can define the attribute for function object by func.attr or setattr()/getattr(). (more)\n Metadata One of the problems with decorators is that they obscure the decorated function\u0026rsquo;s metadata. Here we have a function, sleep_n_seconds(), with a docstring that explains exactly what it does. We can access the metadata for the function, like its docstring, name and default arguments.\n1 2 3 4 5 6 7 8 9 10 11  def sleep_n_seconds(n=10): \u0026#34;\u0026#34;\u0026#34;Pause processing for n seconds. Args: n (int): The number of seconds to pause for. \u0026#34;\u0026#34;\u0026#34; time.sleep(n) print(sleep_n_seconds.__doc__) print(sleep_n_seconds.__name__) print(sleep_n_seconds.__defaults__)   Pause processing for n seconds. Args: n (int): The number of seconds to pause for. sleep_n_seconds (10,) But if we decorate sleep_n_seconds() with the timer() decorator, when we try to print the docstring or default, we get nothing back. Furthermore, when we try to look up the function\u0026rsquo;s name, Python tells us that sleep_n_seconds()\u0026rsquo;s name is \u0026ldquo;wrapper\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import time def timer(func): def wrapper(*args, **kargs): start_time = time.time() result = func(*args, **kargs) print(time.time()-start_time) return result return wrapper @timer def sleep_n_seconds(n=10): \u0026#34;\u0026#34;\u0026#34;Pause processing for n seconds. Args: n (int): The number of seconds to pause for. \u0026#34;\u0026#34;\u0026#34; time.sleep(n) print(sleep_n_seconds.__doc__) print(sleep_n_seconds.__name__) print(sleep_n_seconds.__defaults__)   None wrapper None Remember that when we write decorators, we almost always define a nested function to return. Because the decorator overwrites the sleep_n_seconds() function, when you ask for sleep_n_seconds()\u0026rsquo;s docstring or name, you are actually referencing the nested function that was returned by the decorator. In this case, the nested function was called wrapper() and it didn\u0026rsquo;t have a docstring. You can access to this function via the closure, like\n1 2 3  print(sleep_n_seconds.__closure__[0].cell_contents.__doc__) print(sleep_n_seconds.__closure__[0].cell_contents.__name__) print(sleep_n_seconds.__closure__[0].cell_contents.__defaults__)   Pause processing for n seconds. Args: n (int): The number of seconds to pause for. sleep_n_seconds (10,) But there is an easy way to get to it if you need it. The wraps() function from the functools module is a decorator that you use when defining a decorator. If you use it to decorate the wrapper function that your decorator returns, it will modify wrapper()\u0026rsquo;s metadata to look like the function you are decorating. Notice that the wraps() decorator takes the function you are decorating as an argument, we will talked about decorators that take arguments in the next section.\nIf we use this updated version of the timer() decorator to decorate sleep_n_seconds() and then try to print sleep_n_seconds()\u0026rsquo;s metadata, Python gives you the metadata from the function being decorated rather than the metadata of the wrapper() function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  import time from functools import wraps def timer(func): @wraps(func) def wrapper(*args, **kargs): start_time = time.time() result = func(*args, **kargs) print(time.time()-start_time) return result return wrapper @timer def sleep_n_seconds(n=10): \u0026#34;\u0026#34;\u0026#34;Pause processing for n seconds. Args: n (int): The number of seconds to pause for. \u0026#34;\u0026#34;\u0026#34; time.sleep(n) print(f\u0026#34;sleep {n}s\u0026#34;) print(sleep_n_seconds.__doc__) print(sleep_n_seconds.__name__) print(sleep_n_seconds.__defaults__) # \u0026lt;-- why???   Pause processing for n seconds. Args: n (int): The number of seconds to pause for. sleep_n_seconds None # \u0026lt;-- why??? As an added bonus, using wraps() when creating your decorator also gives you easy access to the original undecorated function via the __wrapped__ attribute, or .__closure__[0].cell_content__\n1 2 3 4 5 6 7  orgfuc = sleep_n_seconds.__wrapped__ print(orgfuc) print(sleep_n_seconds.__closure__[0].cell_contents) print(orgfuc.__doc__) print(orgfuc.__name__) print(orgfuc.__defaults__)   \u0026lt;function sleep_n_seconds at 0x7fdc88b33af0\u0026gt; \u0026lt;function sleep_n_seconds at 0x7fdc88b33af0\u0026gt; Pause processing for n seconds. Args: n (int): The number of seconds to pause for. sleep_n_seconds (10,) Notice that __wrapped__ attribute is a function object, thus you can call it directly.\n1  orgfuc(3)   sleep 3s Decorators that take arguments To add arguments to our decorator, we need another level of function nesting.\nLet\u0026rsquo;s consider this run_three_times() decorator. If you use it to decorate a function, it will run that function three times.\n1 2 3 4 5  def run_three_times(func): def wrapper(*args, **kargs): for _ in range(3): func(*args, **kargs) return wrapper   Let\u0026rsquo;s think about what we would need to change if we wanted to write a run_n_times() decorator. A decorator is only supposed to take one argument - the function it is decorating. Also, when you use decorator syntax, you\u0026rsquo;re not supposed to use the parentheses. To make run_n_times() work, we have to turn it into a function that returns a decorator, rather than a function that is a decorator.\nSo run_n_times() takes n as an argument, instead of func. Then, inside of run_n_times(), we\u0026rsquo;ll define a new decorator function. This function takes \u0026ldquo;func\u0026rdquo; as an argument because it is the function that will be acting as our decorator. We start our new decorator with a nested wrapper() function, as usual. Finally, run_n_times() returns the decorator() function we just defined, then we can use that return value as a decorator.\n1 2 3 4 5 6 7 8 9 10 11 12 13  def run_n_times(n): def decorator(func): def wrapper(*arg, **kargs): for _ in range(n): func(*arg, **kargs) return wrapper return decorator @run_n_times(4) def print_sum(a,b): print(a+b) print(1,3)   4 4 4 4 Notice how when we decorate print_sum() with run_n_times(), we use parentheses after @run_n_times. This indicates that we are actually calling run_n_times() and decorating print_sum() with the result of that function call. Since the return value from run_n_times() is a decorator function, we can use it to decorate print_sum().\nNow let\u0026rsquo;s explain exactly how this works without using decorator syntax. Like before, we have a function, run_n_times() that returns a decorator function when you call it. If we call run_n_times() with the argument 3, it will return a decorator. In fact, it returns the decorator just like run_three_times defined above. The argument 3 is in the closure of returned decorator:\n1 2  # run_n_times(3) almost e.t. run_three_times print([cell.cell_contents for cell in run_n_times(3).__closure__])   [3] We could decorate print_sum() with this returned new decorator using decorator syntax. Python makes it convenient to do both of those in a single step though. When we use decorator syntax, the thing that comes after the @ symbol must be a reference to a decorator function. We can use the name of a specific decorator, or we can call a function that returns a decorator.\nNotice that decorating a function almost is equivalent with overwriting it. Thus we can decorate an already has been defined function by overwriting like:\n1 2  print = run_n_times(5)(print) # run_n_times(5) returns a decorator, and then the decorator overwrites `print` and assigns the result to `print`. print(\u0026#34;hello\u0026#34;)   hello hello hello hello hello  Remember implement del print after test.\n There is another application:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  def html(open_tag, close_tag): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): msg = func(*args, **kwargs) return \u0026#39;{}{}{}\u0026#39;.format(open_tag, msg, close_tag) return wrapper return decorator @html(\u0026#34;\u0026lt;b\u0026gt;\u0026#34;, \u0026#34;\u0026lt;/b\u0026gt;\u0026#34;) def hello(name): return \u0026#39;Hello {}!\u0026#39;.format(name) @html(\u0026#34;\u0026lt;i\u0026gt;\u0026#34;, \u0026#34;\u0026lt;/i\u0026gt;\u0026#34;) def goodbye(name): return \u0026#39;Goodbye {}.\u0026#39;.format(name) @html(\u0026#34;\u0026lt;div\u0026gt;\u0026#34;, \u0026#34;\u0026lt;/div\u0026gt;\u0026#34;) def hello_goodbye(name): return \u0026#39;\\n{}\\n{}\\n\u0026#39;.format(hello(name), goodbye(name)) print(hello_goodbye(\u0026#39;Alice\u0026#39;))   \u0026lt;div\u0026gt; \u0026lt;b\u0026gt;Hello Alice!\u0026lt;/b\u0026gt; \u0026lt;i\u0026gt;Goodbye Alice.\u0026lt;/i\u0026gt; \u0026lt;/div\u0026gt; Other real world examples timeout Let\u0026rsquo;s imagine that we have some functions that occasionally either run for longer than we want them to or just hang and never return. It would be nice if we could add some kind of timeout() decorator to those functions that will raise an error if the function runs for longer than expected.\nTo create the timeout() decorator, we are going to use some functions from Python\u0026rsquo;s signal module.\n The signal.signal() function tells Python, \u0026ldquo;When you see the signal whose number is signalnum, call the handler function.\u0026rdquo;. In this case, we tell Python to call raise_timeout() whenever it sees the alarm signal. The raise_timeout() function raises a TimeoutError when it is called. The signal.alarm() function lets us set an alarm for some number of seconds in the future. Passing 0 to the alarm() function cancels the alarm. timeout() is a function that returns a decorator. It just like a decorator factory. When you call timeout(), it cranks out a brand new decorator that times out in 5 seconds, or 20 seconds, or whatever value we pass as n_seconds. wrapper() sets an alarm for 5 seconds in the future. Then it calls the function being decorated. It wraps that call in a try block so that in a finally block we can cancel the alarm. This ensures that the alarm either rings or gets canceled. Remember, when the alarm rings, Python calls the raise_timeout() function.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  import time import signal import functools def raise_timeout(*args, **kwargs): # \u0026lt;-- Notice the arguments, why??? raise TimeoutError(\u0026#34;you timeout\u0026#34;) signal.signal( signalnum=signal.SIGALRM, handler=raise_timeout ) def timeout(n_seconds): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): signal.alarm(n_seconds) try: return func(*args, **kwargs) finally: signal.alarm(0) return wrapper return decorator @timeout(3) def foo(): time.sleep(5) print(\u0026#39;foo!\u0026#39;) @timeout(20) def bar(): time.sleep(5) print(\u0026#39;bar!\u0026#39;) foo()   --------------------------------------------------------------------------- TimeoutError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_38592/2588934869.py in \u0026lt;module\u0026gt; 31 print('bar!') 32 ---\u0026gt; 33 foo() /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_38592/2588934869.py in wrapper(*args, **kwargs) 15 signal.alarm(n_seconds) 16 try: ---\u0026gt; 17 return func(*args, **kwargs) 18 finally: 19 signal.alarm(0) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_38592/2588934869.py in foo() 23 @timeout(3) 24 def foo(): ---\u0026gt; 25 time.sleep(5) 26 print('foo!') 27 /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_38592/2588934869.py in raise_timeout(*args, **kwargs) 2 3 def raise_timeout(*args, **kwargs): ----\u0026gt; 4 raise TimeoutError(\u0026quot;you timeout\u0026quot;) 5 6 signal.signal( TimeoutError: you timeout 1  bar()   bar! Notice that wrapper() returns the result of calling func(), decorator() returns wrapper, and timeout() returns decorator. So when we call foo(), which has a 5-second timeout, it will timeout like before. But bar(), which has a 20-second timeout, prints its message in 10 seconds, so the alarm gets canceled.\ntag your functions 1 2 3 4 5 6 7 8 9 10 11 12 13 14  def tag(*tags): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): return func(*args, **kwargs) wrapper.tags = tags return wrapper return decorator @tag(\u0026#39;test\u0026#39;, \u0026#39;this is a tag\u0026#39;) def foo(): pass print(foo.tags)   ('test', 'this is a tag') Check the return type 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  def returns(return_type): def decorator(func): def wrapper(*args, **kargs): result = func(*args, **kargs) assert type(result) == return_type return result return wrapper return decorator @returns(dict) def foo(value): return value try: print(foo([1,2,3])) except AssertionError: print(\u0026#39;foo() did not return a dict!\u0026#39;)   foo() did not return a dict! assert \u0026lt;condition\u0026gt; return an AssertionError if \u0026lt;condition\u0026gt; is False.\n","date":"2019-08-19T00:00:00Z","image":"https://wanghaoming177.netlify.app/head_img/oop_hi/oo3.png","permalink":"https://wanghaoming177.netlify.app/p/object-oriented-programming-iii-context-managers-decorators/","title":"Object Oriented Programming III (Context Managers \u0026 Decorators)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Iterators in PythonLand Introduction to iterators When you use a for loop to print out each element of a list, you\u0026rsquo;re iterating over the list. You can also use a for loop to iterate over characters in a string. You can also use a for loop to iterate a over a sequence of numbers produced by a special range object. The reason that we can loop over such objects is that they are special objects called iterables: lists, strings and range objects are all iterables, as are many other Python objects, such as dictionaries and file connections.\nThe actual definition of an iterable is an object that has an associated iter() method. Once this iter() method is applied to an iterable, an iterator object is created. An iterator is defined as an object that has an associated next() method that produces the consecutive values. To create an iterator from an iterable, all we need to do is use the function iter() and pass it the iterable. Once we have the iterator defined, we pass it to the function next() and this returns the first value. Calling next() again on the iterator returns the next value until there are no values left to return and then it throws us a StopIteration error.\n iter(iterable) \u0026mdash;-\u0026gt; iterator next(iterator) \u0026mdash;-\u0026gt; iterate the elements in iterator  For example, a list object is an iterable, which means you can transform a list to a iterator calling iter() on it. But you can not call next() on it directly.\n1 2 3 4 5  ls = [1,2,3,4] itls = iter(ls) print(next(itls)) print(next(itls)) print(next(itls))   1 2 3 1  next(ls)   --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /Users/wanghaoming/Documents/doc1/oop/test2.ipynb Cell 35' in \u0026lt;module\u0026gt; ----\u0026gt; 1 next(ls) TypeError: 'list' object is not an iterator  Under the hood, this is actually what a for loop is doing: it takes an iterable, creates the associated iterator object, and iterates over it.\n 1 2 3 4 5 6 7 8 9 10  flash = [\u0026#39;jay garrick\u0026#39;, \u0026#39;barry allen\u0026#39;, \u0026#39;wally west\u0026#39;, \u0026#39;bart allen\u0026#39;] for item in flash: print(item) # e.t. superhero = iter(flash) print(next(superhero)) print(next(superhero)) print(next(superhero)) print(next(superhero))   jay garrick barry allen wally west bart allen jay garrick barry allen wally west bart allen You can also print all values of an iterator in one fell swoop using the * operator. This * operator unpacks all elements of an iterator or an iterable. Be warned, however, once you do so, you cannot do it (the iterator instead of the iterable) again as there are no more values to iterate through. We would have to redefine our iterator to do so.\n1 2 3 4 5 6 7 8 9 10 11 12  def pr(ob): print(\u0026#34;ob: \u0026#34;, ob) print(\u0026#34;*ob: \u0026#34;, *ob) print(\u0026#34;type: \u0026#34;, type(ob), \u0026#34;\\n\u0026#34;) word = \u0026#34;dfghjhgfds\u0026#34; word1 = iter(word) pr(word) pr(word1) pr(word) pr(word1)   ob: dfghjhgfds *ob: d f g h j h g f d s type: \u0026lt;class 'str'\u0026gt; ob: \u0026lt;str_iterator object at 0x7fc5787b2ac0\u0026gt; *ob: d f g h j h g f d s type: \u0026lt;class 'str_iterator'\u0026gt; ob: dfghjhgfds *ob: d f g h j h g f d s type: \u0026lt;class 'str'\u0026gt; ob: \u0026lt;str_iterator object at 0x7fc5787b2ac0\u0026gt; *ob: type: \u0026lt;class 'str_iterator'\u0026gt; You can also assembles the elements, in the iterator, into a list with list() method\n1 2 3 4  word = \u0026#34;dfghjhgfds\u0026#34; word1 = iter(word) print(word1) print(list(word1))   \u0026lt;str_iterator object at 0x7fc5787b2df0\u0026gt; ['d', 'f', 'g', 'h', 'j', 'h', 'g', 'f', 'd', 's'] We mentioned before that dictionaries and file connections are iterables as well. To iterate over the key-value pairs of a Python dictionary, we need to unpack them by applying the items() method to the dictionary.\nw.r.t. file connections, here you can see how to use the iter() and next() methods to return the lines from a file.\n1 2 3 4 5 6 7 8  (base) wanghaoming@localhost oop_ii_slide % touch file.txt (base) wanghaoming@localhost oop_ii_slide % echo \u0026#34;the first line\u0026#34; \u0026gt; file.txt (base) wanghaoming@localhost oop_ii_slide % echo \u0026#34;the second line\u0026#34; \u0026gt;\u0026gt; file.txt (base) wanghaoming@localhost oop_ii_slide % echo \u0026#34;the third line\u0026#34; \u0026gt;\u0026gt; file.txt (base) wanghaoming@localhost oop_ii_slide % cat file.txt the first line the second line the third line   1 2 3 4 5 6 7 8 9 10  def pr(ob): print(\u0026#34;ob: \u0026#34;, ob) print(next(ob)) # Placing it below *ob will cause an error because there are no more elements in the iterator print(\u0026#34;*ob: \u0026#34;, *ob) print(\u0026#34;type: \u0026#34;, type(ob), \u0026#34;\\n\u0026#34;) file = open(\u0026#34;file.txt\u0026#34;) it = iter(file) pr(it)   ob: \u0026lt;_io.TextIOWrapper name='file.txt' mode='r' encoding='UTF-8'\u0026gt; the first line *ob: the second line # \u0026lt;-- Notice here the third line type: \u0026lt;class '_io.TextIOWrapper'\u0026gt; Operating iterator Now, we are going to introduce two useful functions. The first function, enumerate, will allow us to add a counter to any iterable while the second function, zip, will allow us to stitch together an arbitrary number of iterables.\nenumerate is a function that takes any iterable as argument, such as a list, and returns a special enumerate object, which consists of pairs containing the elements of the original iterable, along with their index within the iterable. The enumerate object itself is also an iterable and we can loop over it while unpacking its elements using the clause for index, value in enumerate(avengers). The default behavior of enumerate to begin indexing at 0. However, you can alter this with startargument, such as start=10\n1 2 3 4 5 6 7 8  tx = \u0026#34;asdfghjkll\u0026#34; etx = enumerate(tx) print(*tx) print(list(tx)) print(*etx) print(list(etx))   a s d f g h j k l l ['a', 's', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'l'] (0, 'a') (1, 's') (2, 'd') (3, 'f') (4, 'g') (5, 'h') (6, 'j') (7, 'k') (8, 'l') (9, 'l') [] # \u0026lt;-- lazy evaluation Notice that The enumerate object has the characteristics of lazy evaluation. The elements that have been visited in the enumerate object cannot be accessed again, and the use of square brackets to index the elements is not supported. (Objects like zip, filter, map, etc. have similar characteristics.)\n1 2 3 4 5 6 7 8  tx = \u0026#34;asdfghjkll\u0026#34; mtx = map( lambda x: x+\u0026#34;!\u0026#34;, tx ) print(mtx) print(*mtx) print(list(mtx))   \u0026lt;map object at 0x7fc5a95e7b20\u0026gt; a! s! d! f! g! h! j! k! l! l! [] Now let\u0026rsquo;s move on to zip, which accepts an arbitrary number of iterables and returns an iterator of tuples. We could use a for loop to iterate over the zip object and print the tuples. We could also have used the * operator or list method to print all the elements.\n1 2 3 4 5 6 7 8  s = \u0026#34;asdfghjkl\u0026#34; t = \u0026#34;qwertyuio\u0026#34; st = zip(s,t) print(st) print(next(st), \u0026#34;\\n\u0026#34;) for i in st: print(i)   \u0026lt;zip object at 0x7fc5787fce40\u0026gt; ('a', 'q') ('s', 'w') ('d', 'e') ('f', 'r') ('g', 't') ('h', 'y') ('j', 'u') ('k', 'i') ('l', 'o') Notice the difference between zip(), *zip(), and zip(*).\n zip(it1, it2) can assemble two iterable object it1, it2 to a zip object; zip(it1) assemble a iterable object it with an empty iterable object. *z can show a zip object; zip(*z) can disassemble a zip object z to the original iterable objects.  1 2 3 4 5 6 7 8 9  s = list(\u0026#34;asdfghjkl\u0026#34;) t = list(\u0026#34;qwertyuio\u0026#34;) print(\u0026#34;zip:\u0026#34;, zip(s,t)) print(\u0026#34;*zip:\u0026#34;, *zip(s,t)) print(\u0026#34;zip(*):\u0026#34;, *zip(*zip(s,t))) s1, t1 = zip(*zip(s,t)) print(list(s1) == s and list(t1) == t)   zip: \u0026lt;zip object at 0x7fc5a95b2ec0\u0026gt; *zip: ('a', 'q') ('s', 'w') ('d', 'e') ('f', 'r') ('g', 't') ('h', 'y') ('j', 'u') ('k', 'i') ('l', 'o') zip(*): ('a', 's', 'd', 'f', 'g', 'h', 'j', 'k', 'l') ('q', 'w', 'e', 'r', 't', 'y', 'u', 'i', 'o') True 1 2 3 4 5 6 7  z1 = zip(mutants, powers) print(*z1) z1 = zip(mutants, powers) result1, result2 = zip(*z1) print(result1 == mutants) print(result2 == powers)   ('charles xavier', 'telepathy') ('bobby drake', 'thermokinesis') ('kurt wagner', 'teleportation') ('max eisenhardt', 'magnetokinesis') ('kitty pryde', 'intangibility') True True Using iterators to load large files into memory If you are pulling data that you can\u0026rsquo;t hold it in memory. One solution is to load the data in chunks, perform the desired operation or operations on each chuck, store the result, discard the chunk and then load the next chunk. To surmount this challenge, we can use the pandas function pd.read_csv()and specify the argument chunksize.\nThe object created by the read_csv() call is an iterable so you can can iterate over it, using a for loop, in which each chunk will be a DataFrame.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def count_entries(csv_file, c_size, colname): \u0026#34;\u0026#34;\u0026#34;Return a dictionary with counts of occurrences as value for each key.\u0026#34;\u0026#34;\u0026#34; counts_dict = {} for chunk in pd.read_csv(csv_file, chunksize=c_size): for entry in chunk[colname]: if entry in counts_dict.keys(): counts_dict[entry] += 1 else: counts_dict[entry] = 1 return counts_dict result_counts = count_entries(\u0026#34;tweets.csv\u0026#34;, 10, \u0026#34;lang\u0026#34;) print(result_counts)   {'en': 97, 'et': 1, 'und': 2} List comprehensions and generators List comprehensions You can finish a for loop in one line of code by comprehensions. The syntax is as follows:\n1  [ \u0026lt;expression\u0026gt; for \u0026lt;variable\u0026gt; in \u0026lt;iterable\u0026gt; ]    You can write a list comprehension over any iterable.\n 1 2 3 4 5  matrix = [[i for i in range(6)] for j in range(5)] Print the matrix for row in matrix: print(row)   [0, 1, 2, 3, 4, 5] [0, 1, 2, 3, 4, 5] [0, 1, 2, 3, 4, 5] [0, 1, 2, 3, 4, 5] [0, 1, 2, 3, 4, 5] 1 2 3 4 5  tweet_time = df[\u0026#34;created_at\u0026#34;] print(tweet_time.head()) tweet_clock_time = [entry[12:20] for entry in tweet_time] print(tweet_clock_time)   0 Tue Mar 29 23:40:17 +0000 2016 1 Tue Mar 29 23:40:17 +0000 2016 2 Tue Mar 29 23:40:17 +0000 2016 3 Tue Mar 29 23:40:17 +0000 2016 4 Tue Mar 29 23:40:17 +0000 2016 Name: created_at, dtype: object ['23:40:17', '23:40:17', '23:40:17', '23:40:17', '23:40:17', '23:40:17', '23:40:18', '23:40:17', '23:40:18', '23:40:18', '23:40:18', '23:40:17', '23:40:18', '23:40:18', '23:40:17', '23:40:18', '23:40:18', '23:40:17', '23:40:18', '23:40:17', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:17', '23:40:18', '23:40:18', '23:40:17', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:19', '23:40:18', '23:40:18', '23:40:18', '23:40:19', '23:40:19', '23:40:19', '23:40:18', '23:40:19', '23:40:19', '23:40:19', '23:40:18', '23:40:19', '23:40:19', '23:40:19', '23:40:18', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19'] We can also use list comprehensions to create nested loop. The syntax is as follows:\n1  [\u0026lt;exp.\u0026gt; \u0026lt;outer for loop\u0026gt; \u0026lt;inner for loop\u0026gt;]   1 2  ls = [(x,y) for x in range(0,3) for y in range(3,6)] print(ls)   [(0, 3), (0, 4), (0, 5), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5)] Advanced comprehensions We can filter the output of a list comprehension using a conditional on the iterable, the syntax is as follows:\n1  [\u0026lt;exp.\u0026gt; \u0026lt;for loop\u0026gt; if \u0026lt;con.\u0026gt;]   We can also condition the list comprehension on the output expression, like\n1  [\u0026lt;exp.\u0026gt; if \u0026lt;con.\u0026gt; else \u0026lt;exp.\u0026gt; \u0026lt;for loop\u0026gt;]   1 2 3 4 5 6 7 8 9 10 11 12 13 14  [ \u0026lt;exp.1\u0026gt; if \u0026lt;con.1\u0026gt; else \u0026lt;exp.2\u0026gt; if \u0026lt;con.2\u0026gt; else \u0026lt;exp3\u0026gt; \u0026lt;for loop\u0026gt; ] # e.t. if con.1: exp.1 elif con.2: exp.2 else: exp.3   some example:\n1 2 3 4 5  fellowship = [\u0026#39;frodo\u0026#39;, \u0026#39;samwise\u0026#39;, \u0026#39;merry\u0026#39;, \u0026#39;aragorn\u0026#39;, \u0026#39;legolas\u0026#39;, \u0026#39;boromir\u0026#39;, \u0026#39;gimli\u0026#39;] new_fellowship = [member for member in fellowship if len(member)\u0026gt;=7] print(new_fellowship)   ['samwise', 'aragorn', 'legolas', 'boromir'] 1 2 3 4 5  fellowship = [\u0026#39;frodo\u0026#39;, \u0026#39;samwise\u0026#39;, \u0026#39;merry\u0026#39;, \u0026#39;aragorn\u0026#39;, \u0026#39;legolas\u0026#39;, \u0026#39;boromir\u0026#39;, \u0026#39;gimli\u0026#39;] new_fellowship = [member if len(member)\u0026gt;=7 else \u0026#34;\u0026#34; for member in fellowship] print(new_fellowship)   ['', 'samwise', '', 'aragorn', 'legolas', 'boromir', ''] 1 2 3 4 5  tweet_time = df[\u0026#34;created_at\u0026#34;] tweet_clock_time = [entry[11:19] for entry in tweet_time if entry[17:19] == \u0026#34;19\u0026#34;] print(tweet_clock_time)   ['23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19'] Dictionary comprehensions Now we can also write dictionary comprehensions to create new dictionaries from iterables. The syntax is almost the same as in list comprehensions and there are 2 differences.\n One, we use curly braces instead of square brackets. Two, the key and value are separated by a colon in the output expression.  We can generate a dictionary from two list:\n1 2 3 4 5 6  keys = list(\u0026#34;abcdefg\u0026#34;) vals = list(\u0026#34;1234567\u0026#34;) dic = { key: value for key, value in zip(keys, vals) } print(dic)   {'a': '1', 'b': '2', 'c': '3', 'd': '4', 'e': '5', 'f': '6', 'g': '7'} 1 2 3 4 5 6  import numpy as np import pandas as pd ks = list(\u0026#34;abcde\u0026#34;) vs = [np.random.randn(10) for i in range(5)] df = pd.DataFrame({k:v for k,v in zip(ks,vs)}) print(df)    a b c d e 0 0.108696 0.405694 0.018458 0.529013 0.284126 1 0.645469 0.937125 -0.420953 0.210035 0.151663 2 1.414896 1.298583 1.485383 -0.096663 0.036326 3 0.883876 -3.009644 1.328881 -0.127821 -0.111707 4 1.888317 -2.507632 0.157957 2.036903 2.164004 5 0.144894 1.298034 0.160900 -0.022732 0.080893 6 -0.233574 0.076446 -0.146595 0.842054 1.818775 7 0.509977 -0.424521 0.657196 -0.913793 0.131405 8 -0.049865 -0.816738 -0.975876 0.170173 -0.569519 9 -0.269802 0.755459 0.021207 0.102335 0.119438 1 2 3 4 5 6 7 8 9 10 11  import numpy as np import pandas as pd ks = list(\u0026#34;abcde\u0026#34;) vs = [ np.random.rand(10) if i \u0026lt;2 else np.random.uniform(5,6,10) if i\u0026lt;4 else np.random.randint(0,9,10) for i in range(5) ] df = pd.DataFrame({k:v for k,v in zip(ks,vs)}) print(df)    a b c d e 0 0.797270 0.079468 5.167152 5.205353 7 1 0.036326 0.752136 5.421250 5.275327 4 2 0.698624 0.703501 5.322341 5.295758 8 3 0.306367 0.092034 5.659722 5.430591 8 4 0.217272 0.251593 5.353020 5.078669 2 5 0.404077 0.882906 5.145223 5.866817 3 6 0.677442 0.012514 5.186332 5.221120 8 7 0.080858 0.226505 5.724067 5.976189 7 8 0.366588 0.884782 5.561158 5.070010 6 9 0.694882 0.928411 5.180124 5.449207 2 1 2 3 4 5 6 7  fellowship = [\u0026#39;frodo\u0026#39;, \u0026#39;samwise\u0026#39;, \u0026#39;merry\u0026#39;, \u0026#39;aragorn\u0026#39;, \u0026#39;legolas\u0026#39;, \u0026#39;boromir\u0026#39;, \u0026#39;gimli\u0026#39;] new_fellowship = { member: len(member) for member in fellowship } print(new_fellowship)   {'frodo': 5, 'samwise': 7, 'merry': 5, 'aragorn': 7, 'legolas': 7, 'boromir': 7, 'gimli': 5} Generator expressions Recall that this list comprehension will create a list. Now lets replace the square brackets with round parentheses. Something called a generator object has been created.\nA generator is like a list comprehension except it does not store the list in memory: it does not construct the list, but is an object we can iterate over to produce elements of the list as required. Like any other iterator, we can pass a generator to the function next() in order to iterate through its elements.\n1 2 3 4 5 6 7 8 9 10 11  result = (num for num in range(0,10)) print(next(result)) print(next(result)) print(next(result)) print(next(result)) print(next(result), \u0026#34;\\n\u0026#34;) # Print the rest of the values for value in result: print(value)   0 1 2 3 4 5 6 7 8 9 Generator is an example of something called lazy evaluation, whereby the evaluation of the expression is delayed until its value is needed. (This means that you can\u0026rsquo;t index values by square brackets.) This can help a great deal when working with extremely large sequences as you don\u0026rsquo;t want to store the entire list in memory, which is what comprehensions would do; you can generate elements of the sequence on the fly. Let\u0026rsquo;s say that we wanted to iterate over a very large sequence of numbers, such as from 0 up to 10 to the power of a million.\n1  [i for i in range(10 ** 1000000)]    DO NOT try this on your personal computer.\n This can cause outages, since the list we are trying to create are so large that can\u0026rsquo;t even be stored in memory. however: We can easily create the analogous generator object because it does not yet create the entire list.\n1 2  g = (i for i in range(10 ** 1000000)) print(next(g))   0 The thing we can do in a list comprehension such as filtering and applying conditionals, can also be done in a generator expression.\nGenerator functions are functions that, when called, produce generator objects. Generator functions are written with the syntax of any other user-defined function, however instead of returning values using the keyword return, they yield sequences of values using the keyword yield.\n1 2 3 4 5 6 7 8 9 10 11  lannister = [\u0026#39;cersei\u0026#39;, \u0026#39;jaime\u0026#39;, \u0026#39;tywin\u0026#39;, \u0026#39;tyrion\u0026#39;, \u0026#39;joffrey\u0026#39;] def get_lengths(input_list): \u0026#34;\u0026#34;\u0026#34;Generator function that yields the length of the strings in input_list.\u0026#34;\u0026#34;\u0026#34; for person in input_list: yield len(person) for value in get_lengths(lannister): print(value)   6 5 5 6 7 World Bank Data Analysis Case Here, we will use the skills of writing user-defined functions, iterators, list comprehensions and generators to wrangle and extract meaningful information from a real-world dataset, the World Bank World Development Indicators dataset.\nThis dataset contains data on 217 world economies for over half a century, from 1960 up until 2015. The data contains hundreds of indicators from population, electricity consumption and CO2 emissions to literacy rates, unemployment and mortality rates.\n The first list feature_names contains header names of the dataset and the second list row_vals contains actual values of a row from the dataset, corresponding to each of the header names. Create a zip object by calling zip() and passing to it feature_names and row_vals. Assign the result to zipped_lists. Create a dictionary from the zipped_lists zip object by calling dict() with zipped_lists. Assign the resulting dictionary to rs_dict.\n1 2 3  zipped_lists = zip(feature_names, row_vals) rs_dict = dict(zipped_lists) # e.t. rs_dict = {k:v for k,v in zipped_lists} print(rs_dict)   {'CountryName': 'Arab World', 'CountryCode': 'ARB', 'IndicatorName': 'Adolescent fertility rate (births per 1,000 women ages 15-19)', 'IndicatorCode': 'SP.ADO.TFRT', 'Year': '1960', 'Value': '133.56090740552298'} Define the function lists2dict() with two parameters: first is list1 and second is list2. Return the resulting dictionary rs_dict in lists2dict(). Call the lists2dict() function with the arguments feature_names and row_vals. Assign the result of the function call to rs_fxn.\n1 2 3 4 5 6 7 8 9  def lists2dict(list1, list2): \u0026#34;\u0026#34;\u0026#34;Return a dictionary where list1 provides the keys and list2 provides the values.\u0026#34;\u0026#34;\u0026#34; zipped_lists = zip(list1, list2) rs_dict = dict(zipped_lists) return rs_dict rs_fxn = lists2dict(feature_names, row_vals) print(rs_fxn)   {'CountryName': 'Arab World', 'CountryCode': 'ARB', 'IndicatorName': 'Adolescent fertility rate (births per 1,000 women ages 15-19)', 'IndicatorCode': 'SP.ADO.TFRT', 'Year': '1960', 'Value': '133.56090740552298'} row_lists is a list of lists, where each sublist is a list of actual values of a row from the dataset. Inspect the contents of row_lists by printing the first two lists in row_lists. Create a list comprehension that generates a dictionary using lists2dict() for each sublist in row_lists. The keys are from the feature_names list and the values are the row entries in row_lists. Use sublist as your iterator variable and assign the resulting list of dictionaries to list_of_dicts. Look at the first two dictionaries in list_of_dicts by printing them out.\n1 2 3 4 5 6 7  print(row_lists[0]) print(row_lists[1]) list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists] print(list_of_dicts[0]) print(list_of_dicts[1])   ['Arab World', 'ARB', 'Adolescent fertility rate (births per 1,000 women ages 15-19)', 'SP.ADO.TFRT', '1960', '133.56090740552298'] ['Arab World', 'ARB', 'Age dependency ratio (% of working-age population)', 'SP.POP.DPND', '1960', '87.7976011532547'] {'CountryName': 'Arab World', 'CountryCode': 'ARB', 'IndicatorName': 'Adolescent fertility rate (births per 1,000 women ages 15-19)', 'IndicatorCode': 'SP.ADO.TFRT', 'Year': '1960', 'Value': '133.56090740552298'} {'CountryName': 'Arab World', 'CountryCode': 'ARB', 'IndicatorName': 'Age dependency ratio (% of working-age population)', 'IndicatorCode': 'SP.POP.DPND', 'Year': '1960', 'Value': '87.7976011532547'} To use the DataFrame() function, first import the pandas package with the alias pd. Create a DataFrame from the list of dictionaries in list_of_dicts by calling pd.DataFrame(). Assign the resulting DataFrame to df. Inspect the contents of df printing the head of the DataFrame. Head of the DataFrame df can be accessed by calling df.head().\n1 2 3 4 5  import pandas as pd list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists] df = pd.DataFrame(list_of_dicts) df.head()    CountryName CountryCode IndicatorName IndicatorCode Year Value 0 Arab World ARB Adolescent fertility rate (births per 1,000 wo... SP.ADO.TFRT 1960 133.56090740552298 1 Arab World ARB Age dependency ratio (% of working-age populat... SP.POP.DPND 1960 87.7976011532547 2 Arab World ARB Age dependency ratio, old (% of working-age po... SP.POP.DPND.OL 1960 6.634579191565161 3 Arab World ARB Age dependency ratio, young (% of working-age ... SP.POP.DPND.YG 1960 81.02332950839141 4 Arab World ARB Arms exports (SIPRI trend indicator values) MS.MIL.XPRT.KD 1960 3000000.0 In the function read_large_file(), read a line from file_object by using the method readline(). Assign the result to data. In the function read_large_file(), yield the line read from the file data. In the context manager, create a generator object gen_file by calling your generator function read_large_file() and passing file to it. Print the first three lines produced by the generator object gen_file using next().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def read_large_file(file_object): \u0026#34;\u0026#34;\u0026#34;A generator function to read a large file lazily.\u0026#34;\u0026#34;\u0026#34; while True: data = file_object.readline() if not data: # \u0026lt;-- Break if this is the end of the file break yield data # \u0026lt;-- After calling read_large_file(), in each loop, programme will freeze here until calling next() with open(\u0026#39;world_dev_ind.csv\u0026#39;) as file: gen_file = read_large_file(file) print(next(gen_file)) print(next(gen_file)) print(next(gen_file))   CountryName,CountryCode,IndicatorName,IndicatorCode,Year,Value Arab World,ARB,\u0026quot;Adolescent fertility rate (births per 1,000 women ages 15-19)\u0026quot;,SP.ADO.TFRT,1960,133.56090740552298 Arab World,ARB,Age dependency ratio (% of working-age population),SP.POP.DPND,1960,87.7976011532547 Bind the file \u0026lsquo;world_dev_ind.csv\u0026rsquo; to file in the context manager with open(). Complete the for loop so that it iterates over the generator from the call to read_large_file() to process all the rows of the file.\n1 2 3 4 5 6 7 8 9 10 11 12 13  counts_dict = {} with open(\u0026#34;world_dev_ind.csv\u0026#34;) as file: for line in read_large_file(file): # \u0026lt;-- Each loop is a call to next() row = line.split(\u0026#39;,\u0026#39;) # \u0026lt;-- Separate each line of the CSV with a comma and generate a list first_col = row[0] if first_col in counts_dict.keys(): counts_dict[first_col] += 1 else: counts_dict[first_col] = 1 print(counts_dict)   {'CountryName': 1, 'Arab World': 80, 'Caribbean small states': 77, 'Central Europe and the Baltics': 71, 'East Asia \u0026amp; Pacific (all income levels)': 122, 'East Asia \u0026amp; Pacific (developing only)': 123, 'Euro area': 119, 'Europe \u0026amp; Central Asia (all income levels)': 109, 'Europe \u0026amp; Central Asia (developing only)': 89, 'European Union': 116, 'Fragile and conflict affected situations': 76, 'Heavily indebted poor countries (HIPC)': 99, 'High income': 131, 'High income: nonOECD': 68, 'High income: OECD': 127, 'Latin America \u0026amp; Caribbean (all income levels)': 130, 'Latin America \u0026amp; Caribbean (developing only)': 133, 'Least developed countries: UN classification': 78, 'Low \u0026amp; middle income': 138, 'Low income': 80, 'Lower middle income': 126, 'Middle East \u0026amp; North Africa (all income levels)': 89, 'Middle East \u0026amp; North Africa (developing only)': 94, 'Middle income': 138, 'North America': 123, 'OECD members': 130, 'Other small states': 63, 'Pacific island small states': 66, 'Small states': 69, 'South Asia': 36} Use pd.read_csv() to read in \u0026lsquo;ind_pop.csv\u0026rsquo; in chunks of size 10. Assign the result to df_reader. Print the first two chunks from df_reader.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import pandas as pd df_reader = pd.read_csv(\u0026#34;ind_pop.csv\u0026#34;, chunksize=10) print(next(df_reader)) print(next(df_reader)) # e.t. # t=0 # for i in df_reaedr: # \u0026lt;-- Each loop is a next() # t += 1 # print(i) # if t == 2: # break    CountryName CountryCode IndicatorName IndicatorCode Year Value 0 Arab World ARB Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 31.285 1 Caribbean small states CSS Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 31.597 2 Central Europe and the Baltics CEB Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 44.508 3 East Asia \u0026amp; Pacific (all income levels) EAS Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 22.471 4 East Asia \u0026amp; Pacific (developing only) EAP Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 16.918 5 Euro area EMU Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 62.097 6 Europe \u0026amp; Central Asia (all income levels) ECS Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 55.379 7 Europe \u0026amp; Central Asia (developing only) ECA Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 38.066 8 European Union EUU Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 61.213 9 Fragile and conflict affected situations FCS Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 17.892 CountryName CountryCode IndicatorName IndicatorCode Year Value 10 Heavily indebted poor countries (HIPC) HPC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 12.236 11 High income HIC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 62.680 12 High income: nonOECD NOC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 56.108 13 High income: OECD OEC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 64.285 14 Latin America \u0026amp; Caribbean (all income levels) LCN Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 49.285 15 Latin America \u0026amp; Caribbean (developing only) LAC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 44.863 16 Least developed countries: UN classification LDC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 9.616 17 Low \u0026amp; middle income LMY Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 21.273 18 Low income LIC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 11.498 19 Lower middle income LMC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 19.811 Use pd.read_csv() to read in the file in \u0026lsquo;ind_pop_data.csv\u0026rsquo; in chunks of size 1000. Assign the result to urb_pop_reader. Get the first DataFrame chunk from the iterable urb_pop_reader and assign this to df_urb_pop.\nSelect only the rows of df_urb_pop that have a \u0026lsquo;CountryCode\u0026rsquo; of \u0026lsquo;CEB\u0026rsquo;. Using zip(), zip together the \u0026lsquo;Total Population\u0026rsquo; and \u0026lsquo;Urban population (% of total)\u0026rsquo; columns of df_pop_ceb. Assign the resulting zip object to pops.\n1 2 3  urb_pop_reader = pd.read_csv(\u0026#34;ind_pop_data.csv\u0026#34;, chunksize=1000) df_urb_pop = next(urb_pop_reader) print(df_urb_pop.head())    CountryName CountryCode Year Total Population Urban population (% of total) 0 Arab World ARB 1960 9.250e+07 31.285 1 Caribbean small states CSS 1960 4.191e+06 31.597 2 Central Europe and the Baltics CEB 1960 9.140e+07 44.508 3 East Asia \u0026amp; Pacific (all income levels) EAS 1960 1.042e+09 22.471 4 East Asia \u0026amp; Pacific (developing only) EAP 1960 8.965e+08 16.918 1 2 3 4  df_pop_ceb = df_urb_pop[df_urb_pop[\u0026#34;CountryCode\u0026#34;]==\u0026#34;CEB\u0026#34;] pops = zip(df_pop_ceb[\u0026#39;Total Population\u0026#39;], df_pop_ceb[\u0026#39;Urban population (% of total)\u0026#39;]) pops_list = list(pops) print(pops_list)   [(91401583.0, 44.5079211390026), (92237118.0, 45.206665319194), (93014890.0, 45.866564696018), (93845749.0, 46.5340927663649), (94722599.0, 47.2087429803526)] Write a list comprehension to generate a list of values from pops_list for the new column \u0026lsquo;Total Urban Population\u0026rsquo;. The output expression should be the product of the first and second element in each tuple in pops_list. Because the 2nd element is a percentage, you also need to either multiply the result by 0.01 or divide it by 100. In addition, note that the column \u0026lsquo;Total Urban Population\u0026rsquo; should only be able to take on integer values. To ensure this, make sure you cast the output expression to an integer with int().\nCreate a scatter plot where the x-axis are values from the \u0026lsquo;Year\u0026rsquo; column and the y-axis are values from the \u0026lsquo;Total Urban Population\u0026rsquo; column.\n1 2 3 4 5 6 7 8 9 10  urb_pop_reader = pd.read_csv(\u0026#39;ind_pop_data.csv\u0026#39;, chunksize=1000) df_urb_pop = next(urb_pop_reader) df_pop_ceb = df_urb_pop[df_urb_pop[\u0026#39;CountryCode\u0026#39;] == \u0026#39;CEB\u0026#39;] pops = zip(df_pop_ceb[\u0026#39;Total Population\u0026#39;], df_pop_ceb[\u0026#39;Urban population (% of total)\u0026#39;]) pops_list = list(pops) df_pop_ceb[\u0026#39;Total Urban Population\u0026#39;] = [int(x[0]*x[1]*0.01) for x in pops_list ] df_pop_ceb.plot(kind=\u0026#34;scatter\u0026#34;, x=\u0026#34;Year\u0026#34;, y=\u0026#34;Total Urban Population\u0026#34;) plt.show()   Now let\u0026rsquo;s show the panoramic view. Initialize an empty DataFrame data using pd.DataFrame(). In the for loop, iterate over urb_pop_reader to be able to process all the DataFrame chunks in the dataset. Using the method append() of the DataFrame data, append df_pop_ceb to data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  urb_pop_reader = pd.read_csv(\u0026#39;ind_pop_data.csv\u0026#39;, chunksize=1000) data = pd.DataFrame() for df_urb_pop in urb_pop_reader: df_pop_ceb = df_urb_pop[df_urb_pop[\u0026#39;CountryCode\u0026#39;] == \u0026#39;CEB\u0026#39;] pops = zip( df_pop_ceb[\u0026#39;Total Population\u0026#39;], df_pop_ceb[\u0026#39;Urban population (% of total)\u0026#39;] ) pops_list = list(pops) df_pop_ceb[\u0026#39;Total Urban Population\u0026#39;] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list] data = data.append(df_pop_ceb) data.plot(kind=\u0026#39;scatter\u0026#39;, x=\u0026#39;Year\u0026#39;, y=\u0026#39;Total Urban Population\u0026#39;) plt.show()   Now we encapsulate the above code into a function. Define the function plot_pop() that has two arguments: first is filename for the file to process and second is country_code for the country to be processed in the dataset. Call plot_pop() to process the data for country code \u0026lsquo;CEB\u0026rsquo; in the file \u0026lsquo;ind_pop_data.csv\u0026rsquo;. Call plot_pop() to process the data for country code \u0026lsquo;ARB\u0026rsquo; in the file \u0026lsquo;ind_pop_data.csv\u0026rsquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  def plot_pop(filename, country_code): urb_pop_reader = pd.read_csv(filename, chunksize=1000) data = pd.DataFrame() for df_urb_pop in urb_pop_reader: df_pop_ceb = df_urb_pop[df_urb_pop[\u0026#39;CountryCode\u0026#39;] == country_code] pops = zip( df_pop_ceb[\u0026#39;Total Population\u0026#39;], df_pop_ceb[\u0026#39;Urban population (% of total)\u0026#39;] ) pops_list = list(pops) df_pop_ceb[\u0026#39;Total Urban Population\u0026#39;] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list] data = data.append(df_pop_ceb) data.plot(kind=\u0026#39;scatter\u0026#39;, x=\u0026#39;Year\u0026#39;, y=\u0026#39;Total Urban Population\u0026#39;) plt.show() fn = \u0026#39;ind_pop_data.csv\u0026#39; # plot_pop(fn, \u0026#34;CEB\u0026#34;) plot_pop(fn, \u0026#34;ARB\u0026#34;)   ","date":"2019-08-16T00:00:00Z","image":"https://wanghaoming177.netlify.app/head_img/oop_hi/oo2.png","permalink":"https://wanghaoming177.netlify.app/p/object-oriented-programming-ii-iterators-comprehensions-generators/","title":"Object Oriented Programming II (Iterators, Comprehensions \u0026 Generators)"},{"content":"Writing Functions1 Define function To define the function, We begin with the keyword def, followed by the function name; this is then followed by a set of parentheses and a colon. This piece of code is called a function header. To complete the function definition, We write the function body inside the indentation.\nThere\u0026rsquo;s an essential aspect of writing functions in Python: docstrings. Docstrings serve as documentation for your function so that anyone who reads your function\u0026rsquo;s docstring understands what your function does, without having to trace through all the code in the function definition. Function docstrings are placed in the immediate line after the function header and are placed in between triple quotation marks.\n1 2 3 4 5 6  def shout(): \u0026#34;\u0026#34;\u0026#34;Print a string with three exclamation marks\u0026#34;\u0026#34;\u0026#34; shout_word = \u0026#34;congratulations\u0026#34; + \u0026#34;!!!\u0026#34; print(shout_word) shout()   congratulations!!! You can add a parameter to the function definition in between the parentheses. When you define a function, you write parameters in the function header. When you call a function, you pass arguments into the function.\nWhat if we don\u0026rsquo;t want to print that outcome directly and instead we want to return the it and assign it to some variable? You can have your function return the new value by adding the return keyword, followed by the outcome to return.\n1 2 3 4 5 6 7  def shout(word): \u0026#34;\u0026#34;\u0026#34;Return a string with three exclamation marks\u0026#34;\u0026#34;\u0026#34; shout_word = word + \u0026#34;!!!\u0026#34; return shout_word yell = shout(\u0026#34;congratulations\u0026#34;) print(yell)   congratulations!!! Multiple parameters and return values We can imput multi parameters to the function.\n You should also change your function name and docstrings to reflect this new behavior.\n 1 2 3 4 5 6 7 8 9  def shout(word1, word2): \u0026#34;\u0026#34;\u0026#34;Concatenate strings with three exclamation marks\u0026#34;\u0026#34;\u0026#34; shout1 = word1 + \u0026#34;!!!\u0026#34; shout2 = word2 + \u0026#34;!!!\u0026#34; new_shout = shout1 + shout2 return new_shout yell = shout(\u0026#34;congratulations\u0026#34;, \u0026#34;you\u0026#34;) print(yell)   congratulations!!!you!!! You can also make your function return multiple values. You can do that by constructing objects known as tuples in your functions. In the function body, we construct a tuple consisting of the values we want the function to return ,and also we return the tuple.\n1 2 3 4 5 6 7 8 9 10  def shout_all(word1, word2): shout1 = word1 + \u0026#34;!!!\u0026#34; shout2 = word2 + \u0026#34;!!!\u0026#34; shout_words = (shout1, shout2) return shout_words yell1, yell2 = shout_all(\u0026#34;congratulations\u0026#34;, \u0026#34;you\u0026#34;) print(yell1) print(yell2)   congratulations!!! you!!! 1 2 3 4 5 6 7 8 9 10 11 12 13 14  def count_entries(df, col_name): \u0026#34;\u0026#34;\u0026#34;Return a dictionary with counts of occurrences as value for each key.\u0026#34;\u0026#34;\u0026#34; langs_count = {} col = df[col_name] for entry in col: if entry in langs_count.keys(): langs_count[entry] += 1 else: langs_count[entry] = 1 return langs_count result = count_entries(tweets_df, \u0026#34;lang\u0026#34;) print(result)   {'en': 97, 'et': 1, 'und': 2} Default arguments, variable-length arguments and scope Scope We\u0026rsquo;ll now talk about the idea of scope in the context of user-defined functions, which tells you which part of a program an object or a name may be accessed.\nGenerally, there are three types of scope:\n global scope. A name that is in the global scope means that it is defined in the main body of a script or a Python program. local scope. A name that is in a local scope means that it is defined within a function. Once the execution of a function is done, any name inside the local scope ceases to exist, which means you cannot access those names anymore outside of the function definition. built-in scope: this consists of names in the pre-defined built-ins module Python provides, such as print and sum.  The rule for referencing global variable inside the function is to search for local scope first, and then global scope if not. Notice that\n if global variable was not defined in the function body, you can refer it directly:  1 2 3 4  v = 3 def func0(): print(v) func0()   3 if global variable was defined in the function body after you call the variable, you need use global key word te declare te variable:  1 2 3 4 5 6 7  v = 3 def func1(): global v print(v) v = 4 print(v) func1()   3 4 otherwise python will report Errors:\n1 2 3 4 5 6  v = 3 def func1(): print(v) v = 4 print(v) func1()   UnboundLocalError: local variable 'v' referenced before assignment if the variable was defined in the function before you call it, python will use local variable:  1 2 3 4 5  v = 3 def func2(): v=4 print(v) func2()   4 In general, we cannot refer to a local scope outside a function, unless we declare a local scope as global using the global keyword. Notice that once you use the global keyword for a variable, any changes made to that variable within the function are reflected in the global variable. For example\n1 2 3 4 5 6 7 8 9 10 11 12 13  n = 5 def func1(): n=3 print(\u0026#34;n1:\u0026#34;, n) def func2(): global n print(\u0026#34;n2:\u0026#34;, n) n = 4 print(\u0026#34;n3:\u0026#34;, n) func1() func2() print(\u0026#34;n4:\u0026#34;, n)   n1: 3 n2: 5 n3: 4 n4: 4 1 2 3 4 5 6 7 8 9  team = \u0026#34;teen titans\u0026#34; def change_team(): \u0026#34;\u0026#34;\u0026#34;Change the value of the global variable team.\u0026#34;\u0026#34;\u0026#34; global team team = \u0026#34;justice league\u0026#34; print(team) change_team() print(team)   teen titans justice league Nested functions There are a number of good reasons to do write nested function. For example, we want a function that takes 3 numbers as parameters and performs the same function on each of them. One way would be to write out the computation 3 times.\n1 2 3 4 5 6  def func(v1, v2, v3): v1 = v1 + 5 v2 = v2 + 5 v3 = v3 + 5 return (v1, v2, v3) func(1,2,3)   (6,7,8) but this definitely does not scale if you need to perform the computation many times. What we can do instead is define an inner function within our function definition, such that\n1 2 3 4 5  def func1(v1,v2,v3): def func2(x): return x + 5 return(func2(v1), func2(v2), func2(v3)) func1(1,2,3)   (6,7,8) This is called a nested function. The syntax for the inner function is exactly the same as that for any other function.\n1 2 3 4 5 6 7 8 9  def three_shouts(word1, word2, word3): \u0026#34;\u0026#34;\u0026#34;Returns a tuple of strings concatenated with \u0026#39;!!!\u0026#39;.\u0026#34;\u0026#34;\u0026#34; def inner(word): \u0026#34;\u0026#34;\u0026#34;Returns a string concatenated with \u0026#39;!!!\u0026#39;.\u0026#34;\u0026#34;\u0026#34; return word + \u0026#39;!!!\u0026#39; return (inner(word1), inner(word2), inner(word3)) print(three_shouts(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;))    ('a!!!', 'b!!!', 'c!!!') 1 2 3 4 5 6 7 8 9 10 11 12  def echo(n): \u0026#34;\u0026#34;\u0026#34;Return the inner_echo function.\u0026#34;\u0026#34;\u0026#34; def inner_echo(word1): \u0026#34;\u0026#34;\u0026#34;Concatenate n copies of word1.\u0026#34;\u0026#34;\u0026#34; echo_word = word1 * n return echo_word return inner_echo twice = echo(2) thrice = echo(3) print(twice(\u0026#39;hello\u0026#39;), thrice(\u0026#39;hello\u0026#39;))   hellohello hellohellohello What if we have a function inner defined within another function outer and we reference a name x in the inner function? The answer is intuitive: Python searches the local scope of the function inner, then if it doesn\u0026rsquo;t find x, it searches the scope of the function outer, which is called an enclosing function because it encloses the function inner. If Python can\u0026rsquo;t find x in the scope of the enclosing function, it only then searches the global scope and then the built-in scope. This is known as the LEGB rule, where L is for local, E for enclosing, G for global and B for built-ins\n1 2 3 4 5 6 7 8 9 10 11  n = 1 # \u0026lt;-- G scope def outter(): n = 2 # \u0026lt;-- E scope def inner(): n = 3 # \u0026lt;-- L scope print(n) # \u0026lt;-- L scope inner() print(n) # \u0026lt;-- E scope outter() print(n) # \u0026lt;-- G scope   3 2 1 1 2 3 4 5 6 7 8 9 10  n = 1 # \u0026lt;-- G scope def outter(): n = 2 # \u0026lt;-- E scope def inner(): print(n) # \u0026lt;-- L scope inner() print(n) # \u0026lt;-- E scope outter() print(n) # \u0026lt;-- G scope   2 2 1 1 2 3 4 5 6 7 8 9  n = 1 # \u0026lt;-- G scope def outter(): def inner(): print(n) # \u0026lt;-- L scope inner() print(n) # \u0026lt;-- E scope outter() print(n) # \u0026lt;-- G scope   1 1 1  Notice that The E scope variable can reference the values of L and G scope variables, but cannot change the values of L and G scope variables, unless declare keyword global or nonlocol.\n Let\u0026rsquo;s now look at another important use case of nested functions. In this example, we define a function func1, which contains an inner function called func2. func1 returns the inner function func2. func1 takes an argument x and creates a function inner that returns the yth power of x.\n1 2 3 4 5 6 7  def func1(x): def func2(y): return x ** y return func2 f = func1(2) print(type(f)) f(3)   \u0026lt;class 'function'\u0026gt; 8 One interesting detail: when we call the function f, it remembers the value x=2, although the enclosing scope defined by func1 and to which x=2 is local, has finished execution. This is a subtlety referred to as a closure in Computer Science.\nRecall that you can use the keyword global in function definitions to create and change global names; similarly, in a nested function, you can use the keyword nonlocal to create and changes names in an enclosing scope. In simple terms, global: L -\u0026gt; G or global: E -\u0026gt; G; and nonlocal: L -\u0026gt; E.\n outer + inner(nonlocal)  1 2 3 4 5 6 7 8 9 10 11 12  n=0 # \u0026lt;-- G scope def func1(): n=1 # \u0026lt;-- E scope def func2(): nonlocal n # \u0026lt;== [L -\u0026gt; E] n=2 # \u0026lt;-- E scope print(\u0026#34;n1:\u0026#34;,n) # \u0026lt;-- E scope print(\u0026#34;n2:\u0026#34;,n) # \u0026lt;-- E scope func2() print(\u0026#34;n3:\u0026#34;,n) # \u0026lt;-- E scope func1() print(\u0026#34;n4:\u0026#34;,n) # \u0026lt;-- G scope   n2: 1 n1: 2 n3: 2 n4: 0 outer(global) + inner  1 2 3 4 5 6 7 8 9 10 11 12  n=0 # \u0026lt;-- G scope def func1(): global n # \u0026lt;== [E -\u0026gt; G] n=1 # \u0026lt;-- G scope def func2(): n=2 # \u0026lt;-- L scope print(\u0026#34;n1:\u0026#34;,n) # \u0026lt;-- L scope print(\u0026#34;n2:\u0026#34;,n) # \u0026lt;-- G scope func2() print(\u0026#34;n3:\u0026#34;,n) # \u0026lt;-- G scope func1() print(\u0026#34;n4:\u0026#34;,n) # \u0026lt;-- G scope   n2: 1 n1: 2 n3: 1 n4: 1 outer + inner(global)  1 2 3 4 5 6 7 8 9 10 11 12 13 14  n=0 def func1(): n=1 # \u0026lt;-- E scope def func2(): global n # \u0026lt;== [L -\u0026gt; G] print(\u0026#34;n0:\u0026#34;,n) # \u0026lt;-- G scope n=2 # \u0026lt;-- G scope print(\u0026#34;n1:\u0026#34;,n) # \u0026lt;-- G scope print(\u0026#34;n2:\u0026#34;,n) # \u0026lt;-- E scope func2() print(\u0026#34;n3:\u0026#34;,n) # \u0026lt;-- E scope func1() print(\u0026#34;n4:\u0026#34;,n) # \u0026lt;-- G scope   n2: 1 n0: 0 n1: 2 n3: 1 # \u0026lt;-- Notice, E scope n4: 2 outer(global) + inner(global)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  n=0 # \u0026lt;-- G scope def func1(): global n # \u0026lt;== [E -\u0026gt; G] n=1 # \u0026lt;-- G scope def func2(): global n # \u0026lt;== [L -\u0026gt; G] print(\u0026#34;n0:\u0026#34;,n) # \u0026lt;-- G scope n=2 # \u0026lt;-- G scope print(\u0026#34;n1:\u0026#34;,n) # \u0026lt;-- G scope print(\u0026#34;n2:\u0026#34;,n) # \u0026lt;-- G scope func2() print(\u0026#34;n3:\u0026#34;,n) # \u0026lt;-- G scope func1() print(\u0026#34;n4:\u0026#34;,n) # \u0026lt;-- G scope   n2: 1 n0: 1 n1: 2 n3: 2 n4: 2 outer(global) + inner(nonlocal)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  n=0 # \u0026lt;-- G scope def func1(): global n # \u0026lt;== [E -\u0026gt; G] n=1 # \u0026lt;-- G scope def func2(): nonlocal n # \u0026lt;== [L -\u0026gt; E] ERROR! print(\u0026#34;n0:\u0026#34;,n) n=2 print(\u0026#34;n1:\u0026#34;,n) print(\u0026#34;n2:\u0026#34;,n) func2() print(\u0026#34;n3:\u0026#34;,n) func1() print(\u0026#34;n4:\u0026#34;,n)   SyntaxError: no binding for nonlocal 'n' found This leads an error, since E scope variable n has been transform to be G scope variable, which means there is no E scope variable n, and nonlocal n cannot match n.\n1 2 3 4 5 6 7 8 9 10 11 12 13  def echo_shout(word): \u0026#34;\u0026#34;\u0026#34;Change the value of a nonlocal variable\u0026#34;\u0026#34;\u0026#34; echo_word = word * 2 print(echo_word) def shout(): \u0026#34;\u0026#34;\u0026#34;Alter a variable in the enclosing scope\u0026#34;\u0026#34;\u0026#34; nonlocal echo_word echo_word = echo_word + \u0026#34;!!!\u0026#34; shout() print(echo_word) echo_shout(\u0026#34;hello\u0026#34;)   hellohello hellohello!!! Default and flexible arguments To define a function with a default argument value, in the function header we follow the parameter of interest with an equals sign and the default argument value.\n1 2 3 4 5 6 7 8 9 10 11 12 13  # Define shout_echo def shout_echo(word1, echo=1): \u0026#34;\u0026#34;\u0026#34;Concatenate echo copies of word1 and three exclamation marks at the end of the string.\u0026#34;\u0026#34;\u0026#34; echo_word = word1 * echo shout_word = echo_word + \u0026#39;!!!\u0026#39; return shout_word no_echo = shout_echo(\u0026#34;Hey\u0026#34;) with_echo = shout_echo(\u0026#34;Hey\u0026#34;, echo=5) print(no_echo) print(with_echo)   Hey!!! HeyHeyHeyHeyHey!!! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def shout_echo(word1, echo=1, intense=False): \u0026#34;\u0026#34;\u0026#34;Concatenate echo copies of word1 and three exclamation marks at the end of the string.\u0026#34;\u0026#34;\u0026#34; echo_word = word1 * echo if intense is True: echo_word_new = echo_word.upper() + \u0026#39;!!!\u0026#39; else: echo_word_new = echo_word + \u0026#39;!!!\u0026#39; return echo_word_new with_big_echo = shout_echo(\u0026#34;Hey\u0026#34;, echo=5, intense=True) big_no_echo = shout_echo(\u0026#34;Hey\u0026#34;, intense=True) print(with_big_echo) print(big_no_echo)   HEYHEYHEYHEYHEY!!! HEY!!! In the function definition, we use the parameter *arg: this then turns all the arguments passed to a function call into a tuple called args in the function body;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  def gibberish(*args): \u0026#34;\u0026#34;\u0026#34;Concatenate strings in *args together.\u0026#34;\u0026#34;\u0026#34; hodgepodge=\u0026#34;\u0026#34; for word in args: hodgepodge += word return hodgepodge one_word = gibberish(\u0026#34;luke\u0026#34;) many_words = gibberish(\u0026#34;luke\u0026#34;, \u0026#34;leia\u0026#34;, \u0026#34;han\u0026#34;, \u0026#34;obi\u0026#34;, \u0026#34;darth\u0026#34;) ls = [\u0026#34;luke\u0026#34;, \u0026#34;leia\u0026#34;, \u0026#34;han\u0026#34;, \u0026#34;obi\u0026#34;, \u0026#34;darth\u0026#34;] many_words2 = gibberish(*ls) print(one_word) print(many_words) print(many_words2)   luke lukeleiahanobidarth lukeleiahanobidarth You can also use the parameter kwargs preceded by a double star. This turns the identifier-keyword pairs into a dictionary within the function body.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def report_status(**kwargs): \u0026#34;\u0026#34;\u0026#34;Print out the status of a movie character.\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;\\nBEGIN: REPORT\\n\u0026#34;) for key, value in kwargs.items(): print(key + \u0026#34;: \u0026#34; + value) print(\u0026#34;\\nEND REPORT\u0026#34;) report_status(name=\u0026#34;luke\u0026#34;, affiliation=\u0026#34;jedi\u0026#34;, status=\u0026#34;missing\u0026#34;) params = { \u0026#34;name\u0026#34;:\u0026#34;anakin\u0026#34;, \u0026#34;affiliation\u0026#34;:\u0026#34;sith lord\u0026#34;, \u0026#34;status\u0026#34;:\u0026#34;deceased\u0026#34; } report_status(**params)   BEGIN: REPORT name: luke affiliation: jedi status: missing END REPORT BEGIN: REPORT name: anakin affiliation: sith lord status: deceased END REPORT  Note that it is NOT the names args and kwargs that are important when using flexible arguments, but rather that they\u0026rsquo;re preceded by a single and double star, respectively.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  def count_entries(df, *args): \u0026#34;\u0026#34;\u0026#34;Return a dictionary with counts of occurrences as value for each key.\u0026#34;\u0026#34;\u0026#34; cols_count = {} for col_name in args: col = df[col_name] for entry in col: if entry in cols_count.keys(): cols_count[entry] += 1 else: cols_count[entry] = 1 return cols_count result1 = count_entries(tweets_df, \u0026#34;lang\u0026#34;) result2 = count_entries(tweets_df, \u0026#34;lang\u0026#34;, \u0026#34;source\u0026#34;) print(result1) print(result2)   {'en': 97, 'et': 1, 'und': 2} {'en': 97, 'et': 1, 'und': 2, '\u0026lt;a href=\u0026quot;http://twitter.com\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Twitter Web Client\u0026lt;/a\u0026gt;': 24, '\u0026lt;a href=\u0026quot;http://www.facebook.com/twitter\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Facebook\u0026lt;/a\u0026gt;': 1, '\u0026lt;a href=\u0026quot;http://twitter.com/download/android\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Twitter for Android\u0026lt;/a\u0026gt;': 26, '\u0026lt;a href=\u0026quot;http://twitter.com/download/iphone\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Twitter for iPhone\u0026lt;/a\u0026gt;': 33, '\u0026lt;a href=\u0026quot;http://www.twitter.com\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Twitter for BlackBerry\u0026lt;/a\u0026gt;': 2, '\u0026lt;a href=\u0026quot;http://www.google.com/\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Google\u0026lt;/a\u0026gt;': 2, '\u0026lt;a href=\u0026quot;http://twitter.com/#!/download/ipad\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Twitter for iPad\u0026lt;/a\u0026gt;': 6, '\u0026lt;a href=\u0026quot;http://linkis.com\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Linkis.com\u0026lt;/a\u0026gt;': 2, '\u0026lt;a href=\u0026quot;http://rutracker.org/forum/viewforum.php?f=93\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;newzlasz\u0026lt;/a\u0026gt;': 2, '\u0026lt;a href=\u0026quot;http://ifttt.com\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;IFTTT\u0026lt;/a\u0026gt;': 1, '\u0026lt;a href=\u0026quot;http://www.myplume.com/\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Plume\\xa0for\\xa0Android\u0026lt;/a\u0026gt;': 1} Lambda functions and error-handling Lambda functions There\u0026rsquo;s a quicker way to write functions on the fly and these are called lambda functions because you use the keyword lambda. To do so, after the keyword lambda, we specify the names of the arguments; then we use a colon followed by the expression that specifies what we wish the function to return, such as\n1  lambda x,y: x + y   1 2  f = lambda x: x ** 2 f(3)   9 1 2 3 4  echo_word = (lambda word1, echo: word1 * echo) result = echo_word(\u0026#34;hey\u0026#34;, 5) print(result)   heyheyheyheyhey Here we introduce three useful function: map(), filter() and reduce().\nFirstly, check out the map function, which takes two arguments, a function and a sequence such as a list and applies the function over all elements of the sequence. We can pass lambda functions to map without even naming them and in this case we refer to them as anonymous functions.\n1 2 3 4 5 6  ls = [1,2,3,4,5] ls1 = map( lambda x: x ** 2, ls ) print(ls1, list(ls1))   \u0026lt;map object at 0x7fc5a95e76d0\u0026gt; [1, 4, 9, 16, 25] 1 2 3 4 5 6 7 8 9  spells = [\u0026#34;protego\u0026#34;, \u0026#34;accio\u0026#34;, \u0026#34;expecto patronum\u0026#34;, \u0026#34;legilimens\u0026#34;] shout_spells = map( lambda item: item + \u0026#34;!!!\u0026#34;, spells ) shout_spells_list = list(shout_spells) print(shout_spells_list)   ['protego!!!', 'accio!!!', 'expecto patronum!!!', 'legilimens!!!'] The function filter() offers a way to filter out elements from a list that don\u0026rsquo;t satisfy certain criteria. We now use filter() to create, from an input list of strings, a new list that contains only strings that have more than 6 characters.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  fellowship = [\u0026#39;frodo\u0026#39;, \u0026#39;samwise\u0026#39;, \u0026#39;merry\u0026#39;, \u0026#39;pippin\u0026#39;, \u0026#39;aragorn\u0026#39;, \u0026#39;boromir\u0026#39;, \u0026#39;legolas\u0026#39;, \u0026#39;gimli\u0026#39;, \u0026#39;gandalf\u0026#39;] result_f = filter( lambda x: len(x) \u0026gt; 6, fellowship ) result_m = map( lambda x: len(x) \u0026gt; 6, fellowship ) result_f_list = list(result_f) result_m_list = list(result_m) print(result_f_list) print(result_m_list)   ['samwise', 'aragorn', 'boromir', 'legolas', 'gandalf'] [False, True, False, False, True, True, True, False, True] The reduce() function is useful for performing some computation on a list and, unlike map() and filter(), returns a single value as a result. The operation rule of reduce() is to use the input function (with 2 arguments) to operate on the first and second elements in the iterator (e.g. list), and then use the function to operate the result with the third data, and finally get a result. To use reduce(), you must import it from the functools module.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  from functools import reduce stark = [\u0026#39;robb\u0026#39;, \u0026#39;sansa\u0026#39;, \u0026#39;arya\u0026#39;, \u0026#39;brandon\u0026#39;, \u0026#39;rickon\u0026#39;] nums = [1,1,1,1,1,1] result_s = reduce( lambda x, y: x + y, stark ) result_n = reduce( lambda x, y: x + y, nums ) print(result_s) print(result_n)   robbsansaaryabrandonrickon 6 Error handling A error caught during execution, commonly called exceptions. The main way to catch exceptions is the try-except clause, in which Python tries to run the code following try and if it can, all is well. If it cannot due to an exception, it runs the code following except.\nWe may also wish to only catch specific type of error and let other errors pass through, in which case we would use except \u0026lt;error type\u0026gt;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  def count_entries(df, col_name=\u0026#39;lang\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Return a dictionary with counts of occurrences as value for each key.\u0026#34;\u0026#34;\u0026#34; cols_count = {} try: col = df[col_name] for entry in col: if entry in cols_count.keys(): cols_count[entry] += 1 else: cols_count[entry] = 1 return cols_count except: print(\u0026#39;The DataFrame does not have a \u0026#39; + col_name + \u0026#39; column.\u0026#39;) result1 = count_entries(tweets_df, \u0026#39;lang\u0026#39;) print(result1)   {'en': 97, 'et': 1, 'und': 2} More often than not, instead of merely printing an error message, we\u0026rsquo;ll want to actually raise an error by clause raise \u0026lt;error type\u0026gt;(\u0026quot;xxx\u0026quot;). For example\n1 2 3  a = 4 if a \u0026lt; 5: raise TypeError(\u0026#34;ddd\u0026#34;)   TypeError: ddd The other method to catch error for the last example is that\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  def count_entries(df, col_name=\u0026#39;lang\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Return a dictionary with counts of occurrences as value for each key.\u0026#34;\u0026#34;\u0026#34; if col_name not in df.columns: raise ValueError(\u0026#39;The DataFrame does not have a \u0026#39; + col_name + \u0026#39; column.\u0026#39;) cols_count = {} col = df[col_name] for entry in col: if entry in cols_count.keys(): cols_count[entry] += 1 else: cols_count[entry] = 1 return cols_count result1 = count_entries(tweets_df, \u0026#34;lang\u0026#34;) print(result1)    {'en': 97, 'et': 1, 'und': 2} ","date":"2019-08-13T00:00:00Z","image":"https://wanghaoming177.netlify.app/head_img/oop_hi/oo1.png","permalink":"https://wanghaoming177.netlify.app/p/object-oriented-programming-i-function/","title":"Object Oriented Programming I (Function)"}]