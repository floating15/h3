[{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Introduction to PySpark Spark is a platform for cluster computing. Spark spreads data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up data makes it easier to work with very large datasets because each node only works with a small amount of data.\nAs each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\nConnection The first step in using Spark is connecting to a cluster. In practice, the cluster will be hosted on a remote machine that\u0026rsquo;s connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called worker. The master sends the workers data and calculations to run, and they send their results back to the master.\nCreating the connection is as simple as creating an instance of the pyspark.context.SparkContext class.\n1 2  from pyspark.context import SparkContext as sc print(sc)   \u0026lt;class 'pyspark.context.SparkContext'\u0026gt; The class constructor takes a few optional arguments that allow you to specify the attributes of the cluster you\u0026rsquo;re connecting to. An object holding all these attributes can be created with the SparkConf() constructor. (documentation)\nSpark\u0026rsquo;s core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so we need use the Spark DataFrame abstraction built on top of RDDs.\nThe Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs. When using RDDs, it\u0026rsquo;s up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in.\nTo start working with Spark DataFrames, we first have to create a SparkSession object from SparkContext. You can think of the SparkContext as the connection to the cluster and the SparkSession as the interface with that connection.\nBecause creating multiple SparkSessions and SparkContexts can cause issues. So if you\u0026rsquo;re not sure there already exists one SparkSession, it\u0026rsquo;s better to use the SparkSession.builder.getOrCreate() method. This returns an existing SparkSession if there\u0026rsquo;s already one in the environment, or creates a new one if necessary.\n1 2 3 4  from pyspark.sql import SparkSession my_spark = SparkSession.builder.getOrCreate() print(my_spark)   \u0026lt;pyspark.sql.session.SparkSession object at 0x7f60900800a0\u0026gt; Once you\u0026rsquo;ve created a SparkSession, you can start poking around to see what data is in your cluster. The SparkSession has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information. One of the most useful is the .listTables() method, which returns the names of all the tables in the cluster as a list.\n1 2  # Here spark is a SparkSession object print(spark.catalog.listTables())   [Table(name='flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True)] As you can see, one of the tables in the cluster is the flights table. This table contains a row for every flight that left Portland International Airport (PDX) or Seattle-Tacoma International Airport (SEA) in 2014 and 2015.\nQuery and Manipulate One of the advantages of the DataFrame interface is that you can run SQL queries on the tables in the Spark cluster. Running a query on flights table is as easy as using the .sql() method on the SparkSession spark. This method takes a string containing the query and returns a DataFrame with the results. And you can call .show() method on the DataFrame to see the query result.\n1 2 3 4  # Get the first 10 rows of flights query = \u0026#34;FROM flights SELECT * LIMIT 10\u0026#34; flights10 = spark.sql(query) flights10.show()   +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+ |year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute| +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+ |2014| 12| 8| 658| -7| 935| -5| VX| N846VA| 1780| SEA| LAX| 132| 954| 6| 58| |2014| 1| 22| 1040| 5| 1505| 5| AS| N559AS| 851| SEA| HNL| 360| 2677| 10| 40| |2014| 3| 9| 1443| -2| 1652| 2| VX| N847VA| 755| SEA| SFO| 111| 679| 14| 43| |2014| 4| 9| 1705| 45| 1839| 34| WN| N360SW| 344| PDX| SJC| 83| 569| 17| 5| |2014| 3| 9| 754| -1| 1015| 1| AS| N612AS| 522| SEA| BUR| 127| 937| 7| 54| |2014| 1| 15| 1037| 7| 1352| 2| WN| N646SW| 48| PDX| DEN| 121| 991| 10| 37| |2014| 7| 2| 847| 42| 1041| 51| WN| N422WN| 1520| PDX| OAK| 90| 543| 8| 47| |2014| 5| 12| 1655| -5| 1842| -18| VX| N361VA| 755| SEA| SFO| 98| 679| 16| 55| |2014| 4| 19| 1236| -4| 1508| -7| AS| N309AS| 490| SEA| SAN| 135| 1050| 12| 36| |2014| 11| 19| 1812| -3| 2352| -4| AS| N564AS| 26| SEA| ORD| 198| 1721| 18| 12| +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+ You can see that the table flights is only mentioned in the query, not as an argument to any of the methods. This is because there isn\u0026rsquo;t a local object in the environment that holds that data, so it wouldn\u0026rsquo;t make sense to pass the table as an argument.\nspark -\u0026gt; pd We usually need run a query on the huge dataset and aggregated it down to something a little more manageable. Sometimes it makes sense to then take that table and work with it locally using a tool like pandas. Spark DataFrames make that easy with the .toPandas() method. Calling this method on a Spark DataFrame returns the corresponding pandas DataFrame.\n1 2 3 4 5  # query counts the number of flights to each airport from SEA and PDX. query = \u0026#34;SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\u0026#34; flight_counts = spark.sql(query) pd_counts = flight_counts.toPandas() print(pd_counts.head())    origin dest N 0 SEA RNO 8 1 SEA DTW 98 2 SEA CLE 2 3 SEA LAX 450 4 PDX SEA 144 pd -\u0026gt; spark We can move data from Spark to pandas by calling .toPandas() on the DateFrame object (which is returned by SparkSession.sql()). It is also possible to put a pandas DataFrame into a Spark cluster. The SparkSession class has .createDataFrame() method takes a pandas DataFrame and returns a Spark DataFrame.\nThe output of this method is stored locally, not in the SparkSession catalog. This means that you can use all the Spark DataFrame methods on it, but you can\u0026rsquo;t access the data in other contexts. For example, a SQL query (using the .sql() method) that references your DataFrame will throw an error.\nTo access the data in this way, you have to save it as a temporary table. You can do this using the .createTempView() Spark DataFrame method, which takes as its only argument the name of the temporary table you\u0026rsquo;d like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark DataFrame. There is also the method .createOrReplaceTempView(). This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. Using this method can avoid running into problems with duplicate tables.\n1 2 3 4 5 6 7  pd_temp = pd.DataFrame(np.random.random(10)) # spark is the SparkSession object spark_temp = spark.createDataFrame(pd_temp) # convert pd.df -\u0026gt; spark.df print(spark.catalog.listTables()) spark_temp.createOrReplaceTempView(\u0026#34;temp\u0026#34;) # register spark.df -\u0026gt; spark.catalog print(spark.catalog.listTables())   [] [Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)] csv -\u0026gt; spark SparkSession has a .read attribute which has several methods for reading different data sources into Spark DataFrames. Using these we can create a DataFrame from a .csv file just like with regular pandas DataFrames.\n1 2 3 4 5  file_path = \u0026#34;/usr/local/share/datasets/airports.csv\u0026#34; airports = spark.read.csv(file_path, header=True) airports.show() print(type(airports))   +---+--------------------+----------------+-----------------+----+---+---+ |faa| name| lat| lon| alt| tz|dst| +---+--------------------+----------------+-----------------+----+---+---+ |04G| Lansdowne Airport| 41.1304722| -80.6195833|1044| -5| A| |06A|Moton Field Munic...| 32.4605722| -85.6800278| 264| -5| A| |06C| Schaumburg Regional| 41.9893408| -88.1012428| 801| -6| A| |06N| Randall Airport| 41.431912| -74.3915611| 523| -5| A| |09J|Jekyll Island Air...| 31.0744722| -81.4277778| 11| -4| A| |0A9|Elizabethton Muni...| 36.3712222| -82.1734167|1593| -4| A| |0G6|Williams County A...| 41.4673056| -84.5067778| 730| -5| A| |0G7|Finger Lakes Regi...| 42.8835647| -76.7812318| 492| -5| A| |0P2|Shoestring Aviati...| 39.7948244| -76.6471914|1000| -5| U| |0S9|Jefferson County ...| 48.0538086| -122.8106436| 108| -8| A| |0W3|Harford County Ai...| 39.5668378| -76.2024028| 409| -5| A| |10C| Galt Field Airport| 42.4028889| -88.3751111| 875| -6| U| |17G|Port Bucyrus-Craw...| 40.7815556| -82.9748056|1003| -5| A| |19A|Jackson County Ai...| 34.1758638| -83.5615972| 951| -4| U| |1A3|Martin Campbell F...| 35.0158056| -84.3468333|1789| -4| A| |1B9| Mansfield Municipal| 42.0001331| -71.1967714| 122| -5| A| |1C9|Frazier Lake Airpark|54.0133333333333|-124.768333333333| 152| -8| A| |1CS|Clow Internationa...| 41.6959744| -88.1292306| 670| -6| U| |1G3| Kent State Airport| 41.1513889| -81.4151111|1134| -4| A| |1OH| Fortman Airport| 40.5553253| -84.3866186| 885| -5| U| +---+--------------------+----------------+-----------------+----+---+---+ only showing top 20 rows \u0026lt;class 'pyspark.sql.dataframe.DataFrame'\u0026gt; Operations on spark\u0026rsquo;s DataFrame In the following, the SparkSession is called spark, along with the Spark DataFrame flights.\nCreate columns Use the spark.table() method with the argument \u0026quot;xxx\u0026quot; to create a DataFrame containing the values of the xxx table in the .catalog.\nIn Spark you can perform column-wise operations using the .withColumn() method, which takes two arguments.\n First, a string with the name of the new column, Second the new column itself whose form is an expression (operational expression or logical judgment expression). The new column must be an object of class Column. Creating one of these is as easy as extracting a column from your DataFrame using df.colName.  Updating a Spark DataFrame is somewhat different than working in pandas because the Spark DataFrame is immutable. This means that it can\u0026rsquo;t be changed, and so columns can\u0026rsquo;t be updated in place. Thus, all these methods return a new DataFrame. To overwrite the original DataFrame you must reassign the returned DataFrame using the method like so:\n1 2  df = df.withColumn(\u0026#34;newCol\u0026#34;, df.oldCol + 1) # operational expression df = df.withColumn(\u0026#34;newCol\u0026#34;, df.oldCol \u0026gt; 0) # logical judgment expression   The above code plus 1 to every rows of the \u0026ldquo;oldCol\u0026rdquo; column of the df, and then name the new column as \u0026ldquo;newCol\u0026rdquo;\n1 2 3 4 5 6 7  print(spark.catalog.listTables()) flights = spark.table(\u0026#34;flights\u0026#34;) flights.show() flights = flights.withColumn(\u0026#34;duration_hrs\u0026#34;, flights.air_time/60) flights.show()   [Table(name='flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True)] +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+ |year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute| +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+ |2014| 12| 8| 658| -7| 935| -5| VX| N846VA| 1780| SEA| LAX| 132| 954| 6| 58| |2014| 1| 22| 1040| 5| 1505| 5| AS| N559AS| 851| SEA| HNL| 360| 2677| 10| 40| |2014| 3| 9| 1443| -2| 1652| 2| VX| N847VA| 755| SEA| SFO| 111| 679| 14| 43| |2014| 4| 9| 1705| 45| 1839| 34| WN| N360SW| 344| PDX| SJC| 83| 569| 17| 5| |2014| 3| 9| 754| -1| 1015| 1| AS| N612AS| 522| SEA| BUR| 127| 937| 7| 54| |2014| 1| 15| 1037| 7| 1352| 2| WN| N646SW| 48| PDX| DEN| 121| 991| 10| 37| |2014| 7| 2| 847| 42| 1041| 51| WN| N422WN| 1520| PDX| OAK| 90| 543| 8| 47| |2014| 5| 12| 1655| -5| 1842| -18| VX| N361VA| 755| SEA| SFO| 98| 679| 16| 55| |2014| 4| 19| 1236| -4| 1508| -7| AS| N309AS| 490| SEA| SAN| 135| 1050| 12| 36| |2014| 11| 19| 1812| -3| 2352| -4| AS| N564AS| 26| SEA| ORD| 198| 1721| 18| 12| |2014| 11| 8| 1653| -2| 1924| -1| AS| N323AS| 448| SEA| LAX| 130| 954| 16| 53| |2014| 8| 3| 1120| 0| 1415| 2| AS| N305AS| 656| SEA| PHX| 154| 1107| 11| 20| |2014| 10| 30| 811| 21| 1038| 29| AS| N433AS| 608| SEA| LAS| 127| 867| 8| 11| |2014| 11| 12| 2346| -4| 217| -28| AS| N765AS| 121| SEA| ANC| 183| 1448| 23| 46| |2014| 10| 31| 1314| 89| 1544| 111| AS| N713AS| 306| SEA| SFO| 129| 679| 13| 14| |2014| 1| 29| 2009| 3| 2159| 9| UA| N27205| 1458| PDX| SFO| 90| 550| 20| 9| |2014| 12| 17| 2015| 50| 2150| 41| AS| N626AS| 368| SEA| SMF| 76| 605| 20| 15| |2014| 8| 11| 1017| -3| 1613| -7| WN| N8634A| 827| SEA| MDW| 216| 1733| 10| 17| |2014| 1| 13| 2156| -9| 607| -15| AS| N597AS| 24| SEA| BOS| 290| 2496| 21| 56| |2014| 6| 5| 1733| -12| 1945| -10| OO| N215AG| 3488| PDX| BUR| 111| 817| 17| 33| +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+ only showing top 20 rows +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+ |year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute| duration_hrs| +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+ |2014| 12| 8| 658| -7| 935| -5| VX| N846VA| 1780| SEA| LAX| 132| 954| 6| 58| 2.2| |2014| 1| 22| 1040| 5| 1505| 5| AS| N559AS| 851| SEA| HNL| 360| 2677| 10| 40| 6.0| |2014| 3| 9| 1443| -2| 1652| 2| VX| N847VA| 755| SEA| SFO| 111| 679| 14| 43| 1.85| |2014| 4| 9| 1705| 45| 1839| 34| WN| N360SW| 344| PDX| SJC| 83| 569| 17| 5|1.3833333333333333| |2014| 3| 9| 754| -1| 1015| 1| AS| N612AS| 522| SEA| BUR| 127| 937| 7| 54|2.1166666666666667| |2014| 1| 15| 1037| 7| 1352| 2| WN| N646SW| 48| PDX| DEN| 121| 991| 10| 37|2.0166666666666666| |2014| 7| 2| 847| 42| 1041| 51| WN| N422WN| 1520| PDX| OAK| 90| 543| 8| 47| 1.5| |2014| 5| 12| 1655| -5| 1842| -18| VX| N361VA| 755| SEA| SFO| 98| 679| 16| 55|1.6333333333333333| |2014| 4| 19| 1236| -4| 1508| -7| AS| N309AS| 490| SEA| SAN| 135| 1050| 12| 36| 2.25| |2014| 11| 19| 1812| -3| 2352| -4| AS| N564AS| 26| SEA| ORD| 198| 1721| 18| 12| 3.3| |2014| 11| 8| 1653| -2| 1924| -1| AS| N323AS| 448| SEA| LAX| 130| 954| 16| 53|2.1666666666666665| |2014| 8| 3| 1120| 0| 1415| 2| AS| N305AS| 656| SEA| PHX| 154| 1107| 11| 20| 2.566666666666667| |2014| 10| 30| 811| 21| 1038| 29| AS| N433AS| 608| SEA| LAS| 127| 867| 8| 11|2.1166666666666667| |2014| 11| 12| 2346| -4| 217| -28| AS| N765AS| 121| SEA| ANC| 183| 1448| 23| 46| 3.05| |2014| 10| 31| 1314| 89| 1544| 111| AS| N713AS| 306| SEA| SFO| 129| 679| 13| 14| 2.15| |2014| 1| 29| 2009| 3| 2159| 9| UA| N27205| 1458| PDX| SFO| 90| 550| 20| 9| 1.5| |2014| 12| 17| 2015| 50| 2150| 41| AS| N626AS| 368| SEA| SMF| 76| 605| 20| 15|1.2666666666666666| |2014| 8| 11| 1017| -3| 1613| -7| WN| N8634A| 827| SEA| MDW| 216| 1733| 10| 17| 3.6| |2014| 1| 13| 2156| -9| 607| -15| AS| N597AS| 24| SEA| BOS| 290| 2496| 21| 56| 4.833333333333333| |2014| 6| 5| 1733| -12| 1945| -10| OO| N215AG| 3488| PDX| BUR| 111| 817| 17| 33| 1.85| +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+ only showing top 20 rows Filter .filter() method is the Spark counterpart of SQL\u0026rsquo;s WHERE clause. The .filter() method takes either an expression that follows the WHERE clause of a SQL expression as a string, or a Spark Column of boolean (True/False) values.\nFor example, the following two expressions will produce the same output:\n1 2  flights.filter(\u0026#34;air_time \u0026gt; 120\u0026#34;).show() flights.filter(flights.air_time \u0026gt; 120).show()   In the first case, we pass a string to .filter(). In SQL, we would write this filtering task as SELECT * FROM flights WHERE air_time \u0026gt; 120. Notice that in this case, we do not reference the name of the table in the string.\nIn the second case, we actually pass a column of boolean values to .filter(). Remember that flights.air_time \u0026gt; 120 returns a column of boolean values that has True in place of those records in flights.air_time that are over 120, and False otherwise.\n1 2 3 4 5 6  long_flights1 = flights.filter(\u0026#34;distance \u0026gt; 1000\u0026#34;) long_flights2 = flights.filter(flights.distance \u0026gt; 1000) print(type(flights), type(long_flights1)) long_flights1.show() long_flights2.show()   \u0026lt;class 'pyspark.sql.dataframe.DataFrame'\u0026gt; \u0026lt;class 'pyspark.sql.dataframe.DataFrame'\u0026gt; +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+ |year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute| +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+ |2014| 1| 22| 1040| 5| 1505| 5| AS| N559AS| 851| SEA| HNL| 360| 2677| 10| 40| |2014| 4| 19| 1236| -4| 1508| -7| AS| N309AS| 490| SEA| SAN| 135| 1050| 12| 36| |2014| 11| 19| 1812| -3| 2352| -4| AS| N564AS| 26| SEA| ORD| 198| 1721| 18| 12| |2014| 8| 3| 1120| 0| 1415| 2| AS| N305AS| 656| SEA| PHX| 154| 1107| 11| 20| |2014| 11| 12| 2346| -4| 217| -28| AS| N765AS| 121| SEA| ANC| 183| 1448| 23| 46| |2014| 8| 11| 1017| -3| 1613| -7| WN| N8634A| 827| SEA| MDW| 216| 1733| 10| 17| |2014| 1| 13| 2156| -9| 607| -15| AS| N597AS| 24| SEA| BOS| 290| 2496| 21| 56| |2014| 9| 26| 610| -5| 1523| 65| US| N127UW| 616| SEA| PHL| 293| 2378| 6| 10| |2014| 12| 4| 954| -6| 1348| -17| HA| N395HA| 29| SEA| OGG| 333| 2640| 9| 54| |2014| 6| 4| 1115| 0| 1346| -3| AS| N461AS| 488| SEA| SAN| 133| 1050| 11| 15| |2014| 6| 26| 2054| -1| 2318| -6| B6| N590JB| 907| SEA| ANC| 179| 1448| 20| 54| |2014| 6| 7| 1823| -7| 2112| -28| AS| N512AS| 815| SEA| LIH| 335| 2701| 18| 23| |2014| 4| 30| 801| 1| 1757| 90| AS| N407AS| 18| SEA| MCO| 342| 2554| 8| 1| |2014| 11| 29| 905| 155| 1655| 170| DL| N824DN| 1598| SEA| ATL| 229| 2182| 9| 5| |2014| 6| 2| 2222| 7| 55| 15| AS| N402AS| 99| SEA| ANC| 190| 1448| 22| 22| |2014| 11| 15| 1034| -6| 1414| -26| AS| N589AS| 794| SEA| ABQ| 139| 1180| 10| 34| |2014| 10| 20| 1328| -1| 1949| 4| UA| N68805| 1212| SEA| IAH| 228| 1874| 13| 28| |2014| 12| 16| 1500| 0| 1906| 19| US| N662AW| 500| SEA| PHX| 151| 1107| 15| 0| |2014| 11| 19| 1319| -6| 1821| -14| DL| N309US| 2164| PDX| MSP| 169| 1426| 13| 19| |2014| 5| 21| 515| 0| 757| 0| US| N172US| 593| SEA| PHX| 143| 1107| 5| 15| +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+ only showing top 20 rows +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+ |year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute| +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+ |2014| 1| 22| 1040| 5| 1505| 5| AS| N559AS| 851| SEA| HNL| 360| 2677| 10| 40| |2014| 1| 13| 2156| -9| 607| -15| AS| N597AS| 24| SEA| BOS| 290| 2496| 21| 56| |2014| 9| 26| 610| -5| 1523| 65| US| N127UW| 616| SEA| PHL| 293| 2378| 6| 10| |2014| 12| 4| 954| -6| 1348| -17| HA| N395HA| 29| SEA| OGG| 333| 2640| 9| 54| |2014| 6| 7| 1823| -7| 2112| -28| AS| N512AS| 815| SEA| LIH| 335| 2701| 18| 23| |2014| 4| 30| 801| 1| 1757| 90| AS| N407AS| 18| SEA| MCO| 342| 2554| 8| 1| |2014| 11| 29| 905| 155| 1655| 170| DL| N824DN| 1598| SEA| ATL| 229| 2182| 9| 5| |2014| 3| 13| 2201| 1| 554| -14| UA| N37468| 1275| SEA| EWR| 268| 2402| 22| 1| |2014| 9| 4| 830| -5| 1647| -23| AS| N583AS| 12| SEA| BOS| 285| 2496| 8| 30| |2014| 4| 1| 1010| -5| 1258| -17| HA| N381HA| 25| PDX| HNL| 328| 2603| 10| 10| |2014| 12| 26| 2337| 0| 741| -1| DL| N3760C| 1358| PDX| JFK| 274| 2454| 23| 37| |2014| 8| 23| 831| -4| 1639| -3| AS| N435AS| 32| SEA| PHL| 285| 2378| 8| 31| |2014| 10| 15| 2244| 5| 607| 3| DL| N816DN| 2497| PDX| ATL| 233| 2172| 22| 44| |2014| 7| 28| 1047| -8| 1921| 11| DL| N713TW| 1473| SEA| JFK| 269| 2422| 10| 47| |2014| 7| 27| 925| 25| 1232| 27| AS| N513AS| 875| SEA| LIH| 343| 2701| 9| 25| |2014| 4| 6| 1329| 4| 2159| NA| DL| N130DL| 1929| SEA| ATL| NA| 2182| 13| 29| |2014| 8| 16| 853| 3| 1709| -21| AS| N597AS| 774| SEA| TPA| 294| 2520| 8| 53| |2014| 8| 18| 2116| -4| 541| 18| B6| N536JB| 264| SEA| JFK| 304| 2422| 21| 16| |2014| 9| 10| 1308| -2| 2109| 2| US| N558UW| 1857| SEA| CLT| 271| 2279| 13| 8| |2014| 11| 3| 840| 0| 1703| -17| AS| N462AS| 38| SEA| FLL| 307| 2717| 8| 40| +----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+ only showing top 20 rows Select The Spark variant of SQL\u0026rsquo;s SELECT is the .select() method. This method takes multiple arguments - one for each column you want to select. These arguments can either be the column name as a string (one for each column) or a column object (using the df.colName syntax).\nWhen you pass a column object, you can perform operations like addition or subtraction on the column to change the data contained in it, much like inside .withColumn(). The difference between .select() and .withColumn() methods is that .select() returns only the columns you specify, while .withColumn() returns all the columns of the DataFrame in addition to the one you defined.\n1 2 3 4 5 6 7 8 9  selected1 = flights.select(\u0026#34;tailnum\u0026#34;, \u0026#34;origin\u0026#34;, \u0026#34;dest\u0026#34;) filterA = flights.origin == \u0026#34;SEA\u0026#34; filterB = flights.dest == \u0026#34;PDX\u0026#34; temp = flights.select(flights.origin, flights.dest, flights.carrier) selected2 = temp.filter(filterA).filter(filterB) selected1.show() selected2.show()   +-------+------+----+ |tailnum|origin|dest| +-------+------+----+ | N846VA| SEA| LAX| | N559AS| SEA| HNL| | N847VA| SEA| SFO| | N360SW| PDX| SJC| | N612AS| SEA| BUR| | N646SW| PDX| DEN| | N422WN| PDX| OAK| | N361VA| SEA| SFO| | N309AS| SEA| SAN| | N564AS| SEA| ORD| | N323AS| SEA| LAX| | N305AS| SEA| PHX| | N433AS| SEA| LAS| | N765AS| SEA| ANC| | N713AS| SEA| SFO| | N27205| PDX| SFO| | N626AS| SEA| SMF| | N8634A| SEA| MDW| | N597AS| SEA| BOS| | N215AG| PDX| BUR| +-------+------+----+ only showing top 20 rows +------+----+-------+ |origin|dest|carrier| +------+----+-------+ | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| AS| | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| AS| | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| OO| | SEA| PDX| OO| +------+----+-------+ only showing top 20 rows Similar to SQL, you can also use the .select() method to perform column-wise operations. When you\u0026rsquo;re selecting a column using the df.colName notation, you can perform any column operation and the .select() method will return the transformed column. For example,\n1  flights.select(flights.air_time/60)   returns a column of flight durations in hours instead of minutes. You can also use the .alias() method to rename a column you\u0026rsquo;re selecting. So if you wanted to .select() the column duration_hrs (which isn\u0026rsquo;t in your DataFrame) you could do\n1  flights.select((flights.air_time/60).alias(\u0026#34;duration_hrs\u0026#34;))   The equivalent Spark DataFrame method .selectExpr() takes SQL expressions as a string:\n1  flights.selectExpr(\u0026#34;air_time/60 as duration_hrs\u0026#34;)   with the SQL as keyword being equivalent to the .alias() method. To select multiple columns, you can pass multiple strings.\n1 2 3 4 5 6 7 8 9 10 11 12 13  avg_speed = (flights.distance/(flights.air_time/60)).alias(\u0026#34;avg_speed\u0026#34;) air_time_h = (flights.air_time/60).alias(\u0026#34;air_time_h\u0026#34;) speed1 = flights.select(\u0026#34;origin\u0026#34;, \u0026#34;dest\u0026#34;, \u0026#34;tailnum\u0026#34;, avg_speed, air_time_h) speed2 = flights.selectExpr( \u0026#34;origin\u0026#34;, \u0026#34;dest\u0026#34;, \u0026#34;tailnum\u0026#34;, \u0026#34;distance/(air_time/60) as avg_speed\u0026#34;, \u0026#34;air_time/60 as air_time_h\u0026#34; ) speed1.show() speed2.show()   +------+----+-------+------------------+------------------+ |origin|dest|tailnum| avg_speed| air_time_h| +------+----+-------+------------------+------------------+ | SEA| LAX| N846VA| 433.6363636363636| 2.2| | SEA| HNL| N559AS| 446.1666666666667| 6.0| | SEA| SFO| N847VA|367.02702702702703| 1.85| | PDX| SJC| N360SW| 411.3253012048193|1.3833333333333333| | SEA| BUR| N612AS| 442.6771653543307|2.1166666666666667| | PDX| DEN| N646SW|491.40495867768595|2.0166666666666666| | PDX| OAK| N422WN| 362.0| 1.5| | SEA| SFO| N361VA| 415.7142857142857|1.6333333333333333| | SEA| SAN| N309AS| 466.6666666666667| 2.25| | SEA| ORD| N564AS| 521.5151515151515| 3.3| | SEA| LAX| N323AS| 440.3076923076923|2.1666666666666665| | SEA| PHX| N305AS|431.29870129870125| 2.566666666666667| | SEA| LAS| N433AS| 409.6062992125984|2.1166666666666667| | SEA| ANC| N765AS|474.75409836065575| 3.05| | SEA| SFO| N713AS| 315.8139534883721| 2.15| | PDX| SFO| N27205| 366.6666666666667| 1.5| | SEA| SMF| N626AS|477.63157894736844|1.2666666666666666| | SEA| MDW| N8634A|481.38888888888886| 3.6| | SEA| BOS| N597AS| 516.4137931034483| 4.833333333333333| | PDX| BUR| N215AG| 441.6216216216216| 1.85| +------+----+-------+------------------+------------------+ only showing top 20 rows +------+----+-------+------------------+------------------+ |origin|dest|tailnum| avg_speed| air_time_h| +------+----+-------+------------------+------------------+ | SEA| LAX| N846VA| 433.6363636363636| 2.2| | SEA| HNL| N559AS| 446.1666666666667| 6.0| | SEA| SFO| N847VA|367.02702702702703| 1.85| | PDX| SJC| N360SW| 411.3253012048193|1.3833333333333333| | SEA| BUR| N612AS| 442.6771653543307|2.1166666666666667| | PDX| DEN| N646SW|491.40495867768595|2.0166666666666666| | PDX| OAK| N422WN| 362.0| 1.5| | SEA| SFO| N361VA| 415.7142857142857|1.6333333333333333| | SEA| SAN| N309AS| 466.6666666666667| 2.25| | SEA| ORD| N564AS| 521.5151515151515| 3.3| | SEA| LAX| N323AS| 440.3076923076923|2.1666666666666665| | SEA| PHX| N305AS|431.29870129870125| 2.566666666666667| | SEA| LAS| N433AS| 409.6062992125984|2.1166666666666667| | SEA| ANC| N765AS|474.75409836065575| 3.05| | SEA| SFO| N713AS| 315.8139534883721| 2.15| | PDX| SFO| N27205| 366.6666666666667| 1.5| | SEA| SMF| N626AS|477.63157894736844|1.2666666666666666| | SEA| MDW| N8634A|481.38888888888886| 3.6| | SEA| BOS| N597AS| 516.4137931034483| 4.833333333333333| | PDX| BUR| N215AG| 441.6216216216216| 1.85| +------+----+-------+------------------+------------------+ only showing top 20 rows Aggregate All of the common aggregation methods, like .min(), .max(), and .count() are GroupedData methods. These are created by calling the .groupBy() DataFrame method. For example, to find the minimum value of a column, col, in a DataFrame, df, you could do\n1  df.groupBy().min(\u0026#34;col\u0026#34;).show()   .groupBy() creates a GroupedData object, then you can use the .min() method, and then return a DataFrame object, so you can call .show() on it.\n1 2 3  flights.filter(flights.origin == \u0026#34;PDX\u0026#34;).groupBy().min(\u0026#34;distance\u0026#34;).show() flights.filter(flights.origin == \u0026#34;SEA\u0026#34;).groupBy().max(\u0026#34;air_time\u0026#34;).show()   +-------------+ |min(distance)| +-------------+ | 106| +-------------+ +-------------+ |max(air_time)| +-------------+ | 409| +-------------+ 1 2 3 4 5 6 7 8 9  flights.filter( flights.carrier == \u0026#34;DL\u0026#34; ).filter( flights.origin == \u0026#34;SEA\u0026#34; ).groupBy().avg(\u0026#34;air_time\u0026#34;).show() flights.withColumn( \u0026#34;duration_hrs\u0026#34;, flights.air_time/60 ).groupBy().sum(\u0026#34;duration_hrs\u0026#34;).show()   +------------------+ | avg(air_time)| +------------------+ |188.20689655172413| +------------------+ +------------------+ | sum(duration_hrs)| +------------------+ |25289.600000000126| +------------------+ Group PySpark has a whole class devoted to grouped data frames: pyspark.sql.GroupedData.\nYou can create a GroupedData by calling the .groupBy() method on a DataFrame with no arguments. You can also pass the name of one or more columns in your DataFrame to the .groupBy() method, the aggregation methods behave like when using a GROUP BY statement in a SQL query.\n1 2 3 4 5  flights.groupBy(\u0026#34;tailnum\u0026#34;).count().show() # same as the above by_origin = flights.groupBy(\u0026#34;origin\u0026#34;) by_origin.avg(\u0026#34;air_time\u0026#34;).show()   +-------+-----+ |tailnum|count| +-------+-----+ | N442AS| 38| | N102UW| 2| | N36472| 4| | N38451| 4| | N73283| 4| | N513UA| 2| | N954WN| 5| | N388DA| 3| | N567AA| 1| | N516UA| 2| | N927DN| 1| | N8322X| 1| | N466SW| 1| | N6700| 1| | N607AS| 45| | N622SW| 4| | N584AS| 31| | N914WN| 4| | N654AW| 2| | N336NW| 1| +-------+-----+ only showing top 20 rows +------+------------------+ |origin| avg(air_time)| +------+------------------+ | SEA| 160.4361496051259| | PDX|137.11543248288737| +------+------------------+ There is also an .agg() method. This method lets you pass an aggregate column expression that uses any of the aggregate functions from the pyspark.sql.functions submodule.\nThis submodule contains many useful functions for computing things like standard deviations. All the aggregation functions in this submodule take the name of a column in a GroupedData table, just like .min(), max() and avg() we have talked about.\n1 2 3 4 5 6  import pyspark.sql.functions as F by_month_dest = flights.groupBy(\u0026#34;month\u0026#34;, \u0026#34;dest\u0026#34;) by_month_dest.avg(\u0026#34;dep_delay\u0026#34;).show() by_month_dest.agg(F.stddev(\u0026#34;dep_delay\u0026#34;)).show()   +-----+----+--------------------+ |month|dest| avg(dep_delay)| +-----+----+--------------------+ | 11| TUS| -2.3333333333333335| | 11| ANC| 7.529411764705882| | 1| BUR| -1.45| | 1| PDX| -5.6923076923076925| | 6| SBA| -2.5| | 5| LAX|-0.15789473684210525| | 10| DTW| 2.6| | 6| SIT| -1.0| | 10| DFW| 18.176470588235293| | 3| FAI| -2.2| | 10| SEA| -0.8| | 2| TUS| -0.6666666666666666| | 12| OGG| 25.181818181818183| | 9| DFW| 4.066666666666666| | 5| EWR| 14.25| | 3| RDM| -6.2| | 8| DCA| 2.6| | 7| ATL| 4.675675675675675| | 4| JFK| 0.07142857142857142| | 10| SNA| -1.1333333333333333| +-----+----+--------------------+ only showing top 20 rows +-----+----+----------------------+ |month|dest|stddev_samp(dep_delay)| +-----+----+----------------------+ | 11| TUS| 3.0550504633038935| | 11| ANC| 18.604716401245316| | 1| BUR| 15.22627576540667| | 1| PDX| 5.677214918493858| | 6| SBA| 2.380476142847617| | 5| LAX| 13.36268698685904| | 10| DTW| 5.639148871948674| | 6| SIT| null| | 10| DFW| 45.53019017606675| | 3| FAI| 3.1144823004794873| | 10| SEA| 18.70523227029577| | 2| TUS| 14.468356276140469| | 12| OGG| 82.64480404939947| | 9| DFW| 21.728629347782924| | 5| EWR| 42.41595968929191| | 3| RDM| 2.16794833886788| | 8| DCA| 9.946523680831074| | 7| ATL| 22.767001039582183| | 4| JFK| 8.156774303176903| | 10| SNA| 13.726234873756304| +-----+----+----------------------+ only showing top 20 rows Join In PySpark, joins are performed using the DataFrame method .join(). This method takes three arguments.\n The first is the second DataFrame that you want to join with the first one. The second argument, on, is the name of the key column(s) as a string. The names of the key column(s) must be the same in each table. The third argument, how, specifies the kind of join to perform. For example, how=\u0026quot;leftouter\u0026quot;.  1 2 3 4 5 6 7  print(airports.show()) # Rename the faa column to keep the name of key same airports = airports.withColumnRenamed(\u0026#34;faa\u0026#34;, \u0026#34;dest\u0026#34;) flights_with_airports = flights.join(airports, on=\u0026#34;dest\u0026#34;, how=\u0026#34;leftouter\u0026#34;) print(flights_with_airports.show())   +---+--------------------+----------------+-----------------+----+---+---+ |faa| name| lat| lon| alt| tz|dst| +---+--------------------+----------------+-----------------+----+---+---+ |04G| Lansdowne Airport| 41.1304722| -80.6195833|1044| -5| A| |06A|Moton Field Munic...| 32.4605722| -85.6800278| 264| -5| A| |06C| Schaumburg Regional| 41.9893408| -88.1012428| 801| -6| A| |06N| Randall Airport| 41.431912| -74.3915611| 523| -5| A| |09J|Jekyll Island Air...| 31.0744722| -81.4277778| 11| -4| A| |0A9|Elizabethton Muni...| 36.3712222| -82.1734167|1593| -4| A| |0G6|Williams County A...| 41.4673056| -84.5067778| 730| -5| A| |0G7|Finger Lakes Regi...| 42.8835647| -76.7812318| 492| -5| A| |0P2|Shoestring Aviati...| 39.7948244| -76.6471914|1000| -5| U| |0S9|Jefferson County ...| 48.0538086| -122.8106436| 108| -8| A| |0W3|Harford County Ai...| 39.5668378| -76.2024028| 409| -5| A| |10C| Galt Field Airport| 42.4028889| -88.3751111| 875| -6| U| |17G|Port Bucyrus-Craw...| 40.7815556| -82.9748056|1003| -5| A| |19A|Jackson County Ai...| 34.1758638| -83.5615972| 951| -4| U| |1A3|Martin Campbell F...| 35.0158056| -84.3468333|1789| -4| A| |1B9| Mansfield Municipal| 42.0001331| -71.1967714| 122| -5| A| |1C9|Frazier Lake Airpark|54.0133333333333|-124.768333333333| 152| -8| A| |1CS|Clow Internationa...| 41.6959744| -88.1292306| 670| -6| U| |1G3| Kent State Airport| 41.1513889| -81.4151111|1134| -4| A| |1OH| Fortman Airport| 40.5553253| -84.3866186| 885| -5| U| +---+--------------------+----------------+-----------------+----+---+---+ only showing top 20 rows None +----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+--------------------+---------+-----------+----+---+---+ |dest|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|air_time|distance|hour|minute| name| lat| lon| alt| tz|dst| +----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+--------------------+---------+-----------+----+---+---+ | LAX|2014| 12| 8| 658| -7| 935| -5| VX| N846VA| 1780| SEA| 132| 954| 6| 58| Los Angeles Intl|33.942536|-118.408075| 126| -8| A| | HNL|2014| 1| 22| 1040| 5| 1505| 5| AS| N559AS| 851| SEA| 360| 2677| 10| 40| Honolulu Intl|21.318681|-157.922428| 13|-10| N| | SFO|2014| 3| 9| 1443| -2| 1652| 2| VX| N847VA| 755| SEA| 111| 679| 14| 43| San Francisco Intl|37.618972|-122.374889| 13| -8| A| | SJC|2014| 4| 9| 1705| 45| 1839| 34| WN| N360SW| 344| PDX| 83| 569| 17| 5|Norman Y Mineta S...| 37.3626|-121.929022| 62| -8| A| | BUR|2014| 3| 9| 754| -1| 1015| 1| AS| N612AS| 522| SEA| 127| 937| 7| 54| Bob Hope|34.200667|-118.358667| 778| -8| A| | DEN|2014| 1| 15| 1037| 7| 1352| 2| WN| N646SW| 48| PDX| 121| 991| 10| 37| Denver Intl|39.861656|-104.673178|5431| -7| A| | OAK|2014| 7| 2| 847| 42| 1041| 51| WN| N422WN| 1520| PDX| 90| 543| 8| 47|Metropolitan Oakl...|37.721278|-122.220722| 9| -8| A| | SFO|2014| 5| 12| 1655| -5| 1842| -18| VX| N361VA| 755| SEA| 98| 679| 16| 55| San Francisco Intl|37.618972|-122.374889| 13| -8| A| | SAN|2014| 4| 19| 1236| -4| 1508| -7| AS| N309AS| 490| SEA| 135| 1050| 12| 36| San Diego Intl|32.733556|-117.189667| 17| -8| A| | ORD|2014| 11| 19| 1812| -3| 2352| -4| AS| N564AS| 26| SEA| 198| 1721| 18| 12| Chicago Ohare Intl|41.978603| -87.904842| 668| -6| A| | LAX|2014| 11| 8| 1653| -2| 1924| -1| AS| N323AS| 448| SEA| 130| 954| 16| 53| Los Angeles Intl|33.942536|-118.408075| 126| -8| A| | PHX|2014| 8| 3| 1120| 0| 1415| 2| AS| N305AS| 656| SEA| 154| 1107| 11| 20|Phoenix Sky Harbo...|33.434278|-112.011583|1135| -7| N| | LAS|2014| 10| 30| 811| 21| 1038| 29| AS| N433AS| 608| SEA| 127| 867| 8| 11| Mc Carran Intl|36.080056| -115.15225|2141| -8| A| | ANC|2014| 11| 12| 2346| -4| 217| -28| AS| N765AS| 121| SEA| 183| 1448| 23| 46|Ted Stevens Ancho...|61.174361|-149.996361| 152| -9| A| | SFO|2014| 10| 31| 1314| 89| 1544| 111| AS| N713AS| 306| SEA| 129| 679| 13| 14| San Francisco Intl|37.618972|-122.374889| 13| -8| A| | SFO|2014| 1| 29| 2009| 3| 2159| 9| UA| N27205| 1458| PDX| 90| 550| 20| 9| San Francisco Intl|37.618972|-122.374889| 13| -8| A| | SMF|2014| 12| 17| 2015| 50| 2150| 41| AS| N626AS| 368| SEA| 76| 605| 20| 15| Sacramento Intl|38.695417|-121.590778| 27| -8| A| | MDW|2014| 8| 11| 1017| -3| 1613| -7| WN| N8634A| 827| SEA| 216| 1733| 10| 17| Chicago Midway Intl|41.785972| -87.752417| 620| -6| A| | BOS|2014| 1| 13| 2156| -9| 607| -15| AS| N597AS| 24| SEA| 290| 2496| 21| 56|General Edward La...|42.364347| -71.005181| 19| -5| A| | BUR|2014| 6| 5| 1733| -12| 1945| -10| OO| N215AG| 3488| PDX| 111| 817| 17| 33| Bob Hope|34.200667|-118.358667| 778| -8| A| +----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+--------------------+---------+-----------+----+---+---+ only showing top 20 rows Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  ################################################################ ## Operations on pyspark.sql.dataframe.DataFrame object ## ################################################################ # Show DataFrame.show() # Convert to pandas DataFrame DataFrame.toPandas() # Column-wise operation DataFrame.withColumn() # Filter rows DataFrame.filter() DataFrame.filter().filter() # Select columns DataFrame.select() DataFrame.selectExpr() # Rename a column (DataFrame.Column + 1).alias() # in df.select() method DataFrame.withColumnRenamed(\u0026#34;oldColName\u0026#34;, \u0026#34;newColName\u0026#34;) # General situation # Create a GroupedData object DataFrame.groupBy() # Aggregate (return a DataFrame object) DataFrame.groupBy().min() import import pyspark.sql.functions as F DataFrame.groupBy().agg(F.func())   Machine Learning Pipelines Data type Spark only handles numeric data. That means all of the columns in the DataFrame must be either integers or decimals (called \u0026lsquo;doubles\u0026rsquo; in Spark).\nYou can see that some of the columns in our DataFrame are strings containing numbers as opposed to actual numeric values. To remedy this, we can use the .cast() method in combination with the .withColumn() method, that is, we can call .cast() inside a call to .withColumn() to overwrite the already existing column, like col. For example,\n1  dataframe = dataframe.withColumn(\u0026#34;col\u0026#34;, dataframe.col.cast(\u0026#34;new_type\u0026#34;))   The only argument we need to pass to .cast() is the kind of value we want to create, in string form. For example, to create integers, you can pass the argument \u0026ldquo;integer\u0026rdquo; and for decimal numbers you\u0026rsquo;ll use \u0026ldquo;double\u0026rdquo;.\n It\u0026rsquo;s important to note that .cast() works on columns, while .withColumn() works on DataFrames.\n 1 2 3 4  model_data = model_data.withColumn(\u0026#34;arr_delay\u0026#34;, model_data.arr_delay.cast(\u0026#34;integer\u0026#34;)) model_data = model_data.withColumn(\u0026#34;air_time\u0026#34;, model_data.air_time.cast(\u0026#34;integer\u0026#34;)) model_data = model_data.withColumn(\u0026#34;month\u0026#34;, model_data.month.cast(\u0026#34;integer\u0026#34;)) model_data = model_data.withColumn(\u0026#34;plane_year\u0026#34;, model_data.plane_year.cast(\u0026#34;integer\u0026#34;))   Convert to Boolean:\n1 2 3 4  model_data = model_data.withColumn(\u0026#34;is_late\u0026#34;, model_data.arr_delay \u0026gt; 0) model_data = model_data.withColumn(\u0026#34;label\u0026#34;, model_data.is_late.cast(\u0026#34;integer\u0026#34;)) model_data.show()   +--------+----+------+----------+--------------------+--------------+-----------+-------+-----+-----+---------+---------+-------+-----+ |tailnum|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|flight|origin|dest|air_time|distance|hour|minute|plane_year| type| manufacturer| model|engines|seats|speed| engine|plane_age|is_late|label| +-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+--------------+-----------+-------+-----+-----+---------+---------+-------+-----+ | N846VA|2014| 12| 8| 658| -7| 935| -5| VX| 1780| SEA| LAX| 132| 954| 6| 58| 2011|Fixed wing multi ...| AIRBUS| A320-214| 2| 182| NA|Turbo-fan| 3.0| false| 0| | N559AS|2014| 1| 22| 1040| 5| 1505| 5| AS| 851| SEA| HNL| 360| 2677| 10| 40| 2006|Fixed wing multi ...| BOEING| 737-890| 2| 149| NA|Turbo-fan| 8.0| true| 1| | N847VA|2014| 3| 9| 1443| -2| 1652| 2| VX| 755| SEA| SFO| 111| 679| 14| 43| 2011|Fixed wing multi ...| AIRBUS| A320-214| 2| 182| NA|Turbo-fan| 3.0| true| 1| | N360SW|2014| 4| 9| 1705| 45| 1839| 34| WN| 344| PDX| SJC| 83| 569| 17| 5| 1992|Fixed wing multi ...| BOEING| 737-3H4| 2| 149| NA|Turbo-fan| 22.0| true| 1| | N612AS|2014| 3| 9| 754| -1| 1015| 1| AS| 522| SEA| BUR| 127| 937| 7| 54| 1999|Fixed wing multi ...| BOEING| 737-790| 2| 151| NA|Turbo-jet| 15.0| true| 1| | N646SW|2014| 1| 15| 1037| 7| 1352| 2| WN| 48| PDX| DEN| 121| 991| 10| 37| 1997|Fixed wing multi ...| BOEING| 737-3H4| 2| 149| NA|Turbo-fan| 17.0| true| 1| | N422WN|2014| 7| 2| 847| 42| 1041| 51| WN| 1520| PDX| OAK| 90| 543| 8| 47| 2002|Fixed wing multi ...| BOEING| 737-7H4| 2| 140| NA|Turbo-fan| 12.0| true| 1| | N361VA|2014| 5| 12| 1655| -5| 1842| -18| VX| 755| SEA| SFO| 98| 679| 16| 55| 2013|Fixed wing multi ...| AIRBUS| A320-214| 2| 182| NA|Turbo-fan| 1.0| false| 0| | N309AS|2014| 4| 19| 1236| -4| 1508| -7| AS| 490| SEA| SAN| 135| 1050| 12| 36| 2001|Fixed wing multi ...| BOEING| 737-990| 2| 149| NA|Turbo-jet| 13.0| false| 0| | N564AS|2014| 11| 19| 1812| -3| 2352| -4| AS| 26| SEA| ORD| 198| 1721| 18| 12| 2006|Fixed wing multi ...| BOEING| 737-890| 2| 149| NA|Turbo-fan| 8.0| false| 0| | N323AS|2014| 11| 8| 1653| -2| 1924| -1| AS| 448| SEA| LAX| 130| 954| 16| 53| 2004|Fixed wing multi ...| BOEING| 737-990| 2| 149| NA|Turbo-jet| 10.0| false| 0| | N305AS|2014| 8| 3| 1120| 0| 1415| 2| AS| 656| SEA| PHX| 154| 1107| 11| 20| 2001|Fixed wing multi ...| BOEING| 737-990| 2| 149| NA|Turbo-jet| 13.0| true| 1| | N433AS|2014| 10| 30| 811| 21| 1038| 29| AS| 608| SEA| LAS| 127| 867| 8| 11| 2013|Fixed wing multi ...| BOEING| 737-990ER| 2| 222| NA|Turbo-fan| 1.0| true| 1| | N765AS|2014| 11| 12| 2346| -4| 217| -28| AS| 121| SEA| ANC| 183| 1448| 23| 46| 1992|Fixed wing multi ...| BOEING| 737-4Q8| 2| 149| NA|Turbo-fan| 22.0| false| 0| | N713AS|2014| 10| 31| 1314| 89| 1544| 111| AS| 306| SEA| SFO| 129| 679| 13| 14| 1999|Fixed wing multi ...| BOEING| 737-490| 2| 149| NA|Turbo-jet| 15.0| true| 1| | N27205|2014| 1| 29| 2009| 3| 2159| 9| UA| 1458| PDX| SFO| 90| 550| 20| 9| 2000|Fixed wing multi ...| BOEING| 737-824| 2| 149| NA|Turbo-fan| 14.0| true| 1| | N626AS|2014| 12| 17| 2015| 50| 2150| 41| AS| 368| SEA| SMF| 76| 605| 20| 15| 2001|Fixed wing multi ...| BOEING| 737-790| 2| 151| NA|Turbo-jet| 13.0| true| 1| | N8634A|2014| 8| 11| 1017| -3| 1613| -7| WN| 827| SEA| MDW| 216| 1733| 10| 17| 2014|Fixed wing multi ...| BOEING| 737-8H4| 2| 140| NA|Turbo-fan| 0.0| false| 0| | N597AS|2014| 1| 13| 2156| -9| 607| -15| AS| 24| SEA| BOS| 290| 2496| 21| 56| 2008|Fixed wing multi ...| BOEING| 737-890| 2| 149| NA|Turbo-fan| 6.0| false| 0| | N215AG|2014| 6| 5| 1733| -12| 1945| -10| OO| 3488| PDX| BUR| 111| 817| 17| 33| 2001|Fixed wing multi ...|BOMBARDIER INC|CL-600-2C10| 2| 80| NA|Turbo-fan| 13.0| false| 0| +-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+--------------+-----------+-------+-----+-----+---------+---------+-------+-----+ only showing top 20 rows one-hot vectors Spark requires numeric data for modeling. But we\u0026rsquo;ll also the string as features in the model like the airline and the plane\u0026rsquo;s destination. PySpark has functions for handling this built into the pyspark.ml.features submodule. we can create \u0026lsquo;one-hot vectors\u0026rsquo; to represent the carrier and the destination of each flight.\nThe first step to encoding the categorical feature is to create a StringIndexer. Members of this class are Estimators that take a DataFrame with a column of strings and map each unique string to a number. Then, the Estimator returns a Transformer that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column.\nThe second step is to encode this numeric column as a one-hot vector using a OneHotEncoder. This works exactly the same way as the StringIndexer by creating an Estimator and then a Transformer. The end result is a column that encodes the categorical feature as a vector that\u0026rsquo;s suitable for machine learning routines!\nTo create a StringIndexer and a OneHotEncoder to code the carrier column, we\u0026rsquo;ll call the class constructors with the arguments inputCol and outputCol. The inputCol is the name of the column we want to index or encode, and the outputCol is the name of the new column that the Transformer should create.\n1 2 3 4 5 6 7 8 9  # Step1: create a StringIndexer carr_indexer = StringIndexer(inputCol=\u0026#34;carrier\u0026#34;, outputCol=\u0026#34;carrier_index\u0026#34;) # Step2: create a OneHotEncoder carr_encoder = OneHotEncoder(inputCol=\u0026#34;carrier_index\u0026#34;, outputCol=\u0026#34;carrier_fact\u0026#34;) dest_indexer = StringIndexer(inputCol=\u0026#34;dest\u0026#34;, outputCol=\u0026#34;dest_index\u0026#34;) dest_encoder = OneHotEncoder(inputCol=\u0026#34;dest_index\u0026#34;, outputCol=\u0026#34;dest_fact\u0026#34;) print(type(carr_indexer), type(carr_encoder))   \u0026lt;class 'pyspark.ml.feature.StringIndexer'\u0026gt; \u0026lt;class 'pyspark.ml.feature.OneHotEncoder'\u0026gt; Assemble a vector The last step in the Pipeline is to combine all of the columns containing our features into a single column. This has to be done before modeling can take place because every Spark modeling routine expects the data to be in this form. We can do this by storing each of the values from a column as an entry in a vector. Then, from the model\u0026rsquo;s point of view, every observation is a vector that contains all of the information about it and a label that tells the modeler what value that observation corresponds to.\nBecause of this, the pyspark.ml.feature submodule contains a class called VectorAssembler. This Transformer takes all of the columns you specify and combines them into a new vector column. Create a VectorAssembler by calling VectorAssembler() with the inputCols names as a list and the outputCol names \u0026ldquo;feature\u0026rdquo;.\n1 2 3 4 5 6  # Make a VectorAssembler vec_assembler = VectorAssembler( inputCols=[\u0026#34;month\u0026#34;, \u0026#34;air_time\u0026#34;, \u0026#34;carrier_fact\u0026#34;, \u0026#34;dest_fact\u0026#34;, \u0026#34;plane_age\u0026#34;], outputCol=\u0026#34;features\u0026#34; ) print(type(vec_assembler))   \u0026lt;class 'pyspark.ml.feature.VectorAssembler'\u0026gt; Create a Pipeline Pipeline is a class in the pyspark.ml module that combines all the Estimators and Transformers that We\u0026rsquo;ve already created. This lets we reuse the same modeling process over and over again by wrapping it up in one simple object.\nTo create a Pipeline, we need import Pipeline from pyspark.ml. Then call the Pipeline() constructor with the argument stages. stages should be a list holding all the stages we want our data to go through in the pipeline.\n1 2 3 4 5 6 7 8 9 10 11  from pyspark.ml import Pipeline flights_pipe = Pipeline( stages=[ dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler ] ) print(type(flights_pipe))   \u0026lt;class 'pyspark.ml.pipeline.Pipeline'\u0026gt; Notice that so far we have only specified the column name to operate on, not the DataFrame to operate on. Figuratively speaking, we\u0026rsquo;ve built the whole pipe system up, but we haven\u0026rsquo;t connected the source yet. Now we\u0026rsquo;re finally ready to pass our data, that is the DataFrame, through the Pipeline we created.\n1 2 3  # Fit and transform the data piped_data = flights_pipe.fit(model_data).transform(model_data) piped_data.show()   +-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+--------------+-----------+-------+-----+-----+---------+---------+-------+-----+----------+---------------+-------------+--------------+--------------------+ |tailnum|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|flight|origin|dest|air_time|distance|hour|minute|plane_year| type| manufacturer| model|engines|seats|speed| engine|plane_age|is_late|label|dest_index| dest_fact|carrier_index| carrier_fact| features| +-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+--------------+-----------+-------+-----+-----+---------+---------+-------+-----+----------+---------------+-------------+--------------+--------------------+ | N846VA|2014| 12| 8| 658| -7| 935| -5| VX| 1780| SEA| LAX| 132| 954| 6| 58| 2011|Fixed wing multi ...| AIRBUS| A320-214| 2| 182| NA|Turbo-fan| 3.0| false| 0| 1.0| (68,[1],[1.0])| 7.0|(10,[7],[1.0])|(81,[0,1,3,77,80]...| | N559AS|2014| 1| 22| 1040| 5| 1505| 5| AS| 851| SEA| HNL| 360| 2677| 10| 40| 2006|Fixed wing multi ...| BOEING| 737-890| 2| 149| NA|Turbo-fan| 8.0| true| 1| 19.0|(68,[19],[1.0])| 0.0|(10,[0],[1.0])|(81,[0,1,21,70,80...| | N847VA|2014| 3| 9| 1443| -2| 1652| 2| VX| 755| SEA| SFO| 111| 679| 14| 43| 2011|Fixed wing multi ...| AIRBUS| A320-214| 2| 182| NA|Turbo-fan| 3.0| true| 1| 0.0| (68,[0],[1.0])| 7.0|(10,[7],[1.0])|(81,[0,1,2,77,80]...| | N360SW|2014| 4| 9| 1705| 45| 1839| 34| WN| 344| PDX| SJC| 83| 569| 17| 5| 1992|Fixed wing multi ...| BOEING| 737-3H4| 2| 149| NA|Turbo-fan| 22.0| true| 1| 7.0| (68,[7],[1.0])| 1.0|(10,[1],[1.0])|(81,[0,1,9,71,80]...| | N612AS|2014| 3| 9| 754| -1| 1015| 1| AS| 522| SEA| BUR| 127| 937| 7| 54| 1999|Fixed wing multi ...| BOEING| 737-790| 2| 151| NA|Turbo-jet| 15.0| true| 1| 22.0|(68,[22],[1.0])| 0.0|(10,[0],[1.0])|(81,[0,1,24,70,80...| | N646SW|2014| 1| 15| 1037| 7| 1352| 2| WN| 48| PDX| DEN| 121| 991| 10| 37| 1997|Fixed wing multi ...| BOEING| 737-3H4| 2| 149| NA|Turbo-fan| 17.0| true| 1| 2.0| (68,[2],[1.0])| 1.0|(10,[1],[1.0])|(81,[0,1,4,71,80]...| | N422WN|2014| 7| 2| 847| 42| 1041| 51| WN| 1520| PDX| OAK| 90| 543| 8| 47| 2002|Fixed wing multi ...| BOEING| 737-7H4| 2| 140| NA|Turbo-fan| 12.0| true| 1| 8.0| (68,[8],[1.0])| 1.0|(10,[1],[1.0])|(81,[0,1,10,71,80...| | N361VA|2014| 5| 12| 1655| -5| 1842| -18| VX| 755| SEA| SFO| 98| 679| 16| 55| 2013|Fixed wing multi ...| AIRBUS| A320-214| 2| 182| NA|Turbo-fan| 1.0| false| 0| 0.0| (68,[0],[1.0])| 7.0|(10,[7],[1.0])|(81,[0,1,2,77,80]...| | N309AS|2014| 4| 19| 1236| -4| 1508| -7| AS| 490| SEA| SAN| 135| 1050| 12| 36| 2001|Fixed wing multi ...| BOEING| 737-990| 2| 149| NA|Turbo-jet| 13.0| false| 0| 10.0|(68,[10],[1.0])| 0.0|(10,[0],[1.0])|(81,[0,1,12,70,80...| | N564AS|2014| 11| 19| 1812| -3| 2352| -4| AS| 26| SEA| ORD| 198| 1721| 18| 12| 2006|Fixed wing multi ...| BOEING| 737-890| 2| 149| NA|Turbo-fan| 8.0| false| 0| 11.0|(68,[11],[1.0])| 0.0|(10,[0],[1.0])|(81,[0,1,13,70,80...| | N323AS|2014| 11| 8| 1653| -2| 1924| -1| AS| 448| SEA| LAX| 130| 954| 16| 53| 2004|Fixed wing multi ...| BOEING| 737-990| 2| 149| NA|Turbo-jet| 10.0| false| 0| 1.0| (68,[1],[1.0])| 0.0|(10,[0],[1.0])|(81,[0,1,3,70,80]...| | N305AS|2014| 8| 3| 1120| 0| 1415| 2| AS| 656| SEA| PHX| 154| 1107| 11| 20| 2001|Fixed wing multi ...| BOEING| 737-990| 2| 149| NA|Turbo-jet| 13.0| true| 1| 4.0| (68,[4],[1.0])| 0.0|(10,[0],[1.0])|(81,[0,1,6,70,80]...| | N433AS|2014| 10| 30| 811| 21| 1038| 29| AS| 608| SEA| LAS| 127| 867| 8| 11| 2013|Fixed wing multi ...| BOEING| 737-990ER| 2| 222| NA|Turbo-fan| 1.0| true| 1| 3.0| (68,[3],[1.0])| 0.0|(10,[0],[1.0])|(81,[0,1,5,70,80]...| | N765AS|2014| 11| 12| 2346| -4| 217| -28| AS| 121| SEA| ANC| 183| 1448| 23| 46| 1992|Fixed wing multi ...| BOEING| 737-4Q8| 2| 149| NA|Turbo-fan| 22.0| false| 0| 5.0| (68,[5],[1.0])| 0.0|(10,[0],[1.0])|(81,[0,1,7,70,80]...| | N713AS|2014| 10| 31| 1314| 89| 1544| 111| AS| 306| SEA| SFO| 129| 679| 13| 14| 1999|Fixed wing multi ...| BOEING| 737-490| 2| 149| NA|Turbo-jet| 15.0| true| 1| 0.0| (68,[0],[1.0])| 0.0|(10,[0],[1.0])|(81,[0,1,2,70,80]...| | N27205|2014| 1| 29| 2009| 3| 2159| 9| UA| 1458| PDX| SFO| 90| 550| 20| 9| 2000|Fixed wing multi ...| BOEING| 737-824| 2| 149| NA|Turbo-fan| 14.0| true| 1| 0.0| (68,[0],[1.0])| 4.0|(10,[4],[1.0])|(81,[0,1,2,74,80]...| | N626AS|2014| 12| 17| 2015| 50| 2150| 41| AS| 368| SEA| SMF| 76| 605| 20| 15| 2001|Fixed wing multi ...| BOEING| 737-790| 2| 151| NA|Turbo-jet| 13.0| true| 1| 9.0| (68,[9],[1.0])| 0.0|(10,[0],[1.0])|(81,[0,1,11,70,80...| | N8634A|2014| 8| 11| 1017| -3| 1613| -7| WN| 827| SEA| MDW| 216| 1733| 10| 17| 2014|Fixed wing multi ...| BOEING| 737-8H4| 2| 140| NA|Turbo-fan| 0.0| false| 0| 31.0|(68,[31],[1.0])| 1.0|(10,[1],[1.0])|(81,[0,1,33,71],[...| | N597AS|2014| 1| 13| 2156| -9| 607| -15| AS| 24| SEA| BOS| 290| 2496| 21| 56| 2008|Fixed wing multi ...| BOEING| 737-890| 2| 149| NA|Turbo-fan| 6.0| false| 0| 24.0|(68,[24],[1.0])| 0.0|(10,[0],[1.0])|(81,[0,1,26,70,80...| | N215AG|2014| 6| 5| 1733| -12| 1945| -10| OO| 3488| PDX| BUR| 111| 817| 17| 33| 2001|Fixed wing multi ...|BOMBARDIER INC|CL-600-2C10| 2| 80| NA|Turbo-fan| 13.0| false| 0| 22.0|(68,[22],[1.0])| 2.0|(10,[2],[1.0])|(81,[0,1,24,72,80...| +-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+--------------+-----------+-------+-----+-----+---------+---------+-------+-----+----------+---------------+-------------+--------------+--------------------+ only showing top 20 rows Test and train After we\u0026rsquo;ve cleaned our data and gotten it ready for modeling, one of the most important steps is to split the data into a test set and a train set. After that, don\u0026rsquo;t touch the test data until we have a good model. Once we\u0026rsquo;ve got the favorite model, we can see how well it predicts the new data in the test set. This never-before-seen data will give us a much more realistic idea of our model\u0026rsquo;s performance in the real world when we\u0026rsquo;re trying to predict or classify new data.\nIn Spark it\u0026rsquo;s important to make sure we split the data after all the transformations. This is because operations like StringIndexer don\u0026rsquo;t always produce the same index even when given the same list of strings.\nUse the DataFrame method .randomSplit() to split piped_data into two pieces, training with 60% of the data, and test with 40% of the data by passing the list [.6, .4] to the .randomSplit() method.\n1 2  training, test = piped_data.randomSplit([.6, .4]) print(type(training), type(test))   \u0026lt;class 'pyspark.sql.dataframe.DataFrame'\u0026gt; \u0026lt;class 'pyspark.sql.dataframe.DataFrame'\u0026gt; Model tuning and selection Create the modeler To create a Logistic regressor modeler lr, just import the LogisticRegression class from pyspark.ml.classification, and then call LogisticRegression() with no arguments.\n1 2 3  from pyspark.ml.classification import LogisticRegression lr = LogisticRegression()   Tune In the next section, we will tune our logistic regression model using k-fold cross validation. This is a method of estimating the model\u0026rsquo;s performance on unseen data (like the test DataFrame).\nIt works by splitting the training data into a few different partitions. The PySpark\u0026rsquo;s default number of partitions is 3. Once the data is split up, one of the partitions is set aside, and the model is fit to the others. Then the error is measured against the held out partition. This is repeated for each of the partitions, so that every block of data is held out and used as a test set exactly once. Then the error on each of the partitions is averaged. This is called the cross validation error of the model, and is a good estimate of the actual error on the held out data.\nWe\u0026rsquo;ll use cross validation to choose the hyperparameters by creating a grid of the possible pairs of values for the two hyperparameters, elasticNetParam and regParam, and using the cross validation error to compare all the different models so we can choose the best one.\nEvaluator The first thing we need when doing cross validation for model selection is a way to compare different models. The pyspark.ml.evaluation submodule has classes for evaluating different kinds of models. Logistic regression model is a binary classification model, so we\u0026rsquo;ll be using the BinaryClassificationEvaluator from the pyspark.ml.evaluation module.\nTo create an evaluator, we need import BinaryClassificationEvaluator from the submodule pyspark.ml.evaluation, and then call BinaryClassificationEvaluator() with the argument metricName, which specifies the metric method (loss function). Here we use AUC metric areaUnderROC.\nThis metric calculates the Area Under Curve (here the curve is ROC), which combines the two kinds of errors a binary classifier can make (false positives and false negatives) into a simple number.\n1 2 3  import pyspark.ml.evaluation as evals evaluator = evals.BinaryClassificationEvaluator(metricName=\u0026#34;areaUnderROC\u0026#34;)   Grid Next, we need to create a grid of values to search over when looking for the optimal hyperparameters. The submodule pyspark.ml.tuning includes a class called ParamGridBuilder that does just that.\nWe\u0026rsquo;ll need to use the .addGrid() and .build() methods to create a grid that we can use for cross validation. The .addGrid() method takes a model parameter (an attribute of the model Estimator, lr, that we have created) and a list of values that we want to try. The .build() method takes no arguments, it just returns the grid that we\u0026rsquo;ll use later.\nSo to make a grid, we need\n Import the submodule pyspark.ml.tuning first. Then call the class constructor ParamGridBuilder() with no arguments save the return as grid. Call the .addGrid() method on grid with lr.regParam as the first argument and a list of parameters as the second argument. Overwrite grid with the result. Update grid again by calling the .addGrid() method a second time create a grid for lr.elasticNetParam that includes only the values [0, 1]. Call the .build() method on grid and overwrite it with the output.  1 2 3 4 5 6 7  import pyspark.ml.tuning as tune import numpy as np grid = tune.ParamGridBuilder() grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01)) grid = grid.addGrid(lr.elasticNetParam, [0, 1]) grid = grid.build()   Validator The submodule pyspark.ml.tuning also has a class called CrossValidator for performing cross validation. This Estimator takes the modeler we want to fit, the grid of hyperparameters we created, and the evaluator we want to use to compare our models.\nTo create the CrossValidator, we call CrossValidator() with 3 arguments:\n estimator: the logistic regression Estimator lr estimatorParamMaps: the parameter grid evaluator: the evaluator  1 2 3 4 5  cv = tune.CrossValidator( estimator=lr, estimatorParamMaps=grid, evaluator=evaluator )   Fit We\u0026rsquo;re finally ready to fit the models and select the best one. To do this call .fit() method with the training data as argument on the CrossValidator object cv we have just created. The best model is the bestModel attribute of the outcome returned by .fit() method.\n1 2 3 4 5  # Fit cross validation models models = cv.fit(training) # Extract the best model best_lr = models.bestModel   Remember, the training data is called training and we\u0026rsquo;re using lr to fit a logistic regression model.\nNote that if you do not want to make cross verify, you can call .fit() method on Estimator lr directly:\n1  model = lr.fit(training)   Evaluate Use our (best) model to generate predictions by applying best_lr.transform() to the test data. Save this the result returned. Call evaluator.evaluate() and pass the result saved as argument to compute the AUC. Note that the closer the AUC is to 1, the better the model is.\n1 2 3 4 5  # Use the model to predict the test set test_results = best_lr.transform(test) # Evaluate the predictions print(evaluator.evaluate(test_results))   0.7123313100891033 ","date":"2022-02-01T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/pyspark-and-large-dataset-i/","title":"PySpark and Large Dataset I"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Tensor Conception of Tensor Tensor is a multi-dimensions array, which id embodied as tensor object in Pytorch. Tensor is an extension of scalar, vector and matrix, where scalar is 0-dimension tensor, vector is 1-dimension tensor and matric is 2-dimension tensor.\nGenerally, torch.tensor object has 8 commonly-used attributes:\n data: the array encapsuled in tensor; dtype: data type of the tensor; Pytorch provides 9 kinds of data type, which can be divided in 3 categories:  float (16-bit, 32-bit, 64-bit) integer (unsigned-8-bit ,8-bit, 16-bit, 32-bit, 64-bit) Boolean;   shape: shape of the tensor. Notice that shape=[4] is 1D tensor, while shape=[1,4] is 2D tensor; grad: gradient of data; grad_fn: The Function that you use when you create a Tensor, that\u0026rsquo;s the key to automatically taking derivatives; requires_grad: Specify whether or not the gradient is required, not all tensors need to compute gradients; is_leaf: Indicates whether leaf nodes (must be tensors object)  Create Tensor Specify data directly torch.tensor() 1 2 3 4  def tensor(data: Any, dtype: dtype | None = None, device: device | str | None = None, requires_grad: bool = False) -\u0026gt; Tensor    data: array, it could be list , np.ndarray or pd.Series. Notice that data can not be pd.DataFrame, but pd.DataFrame.values works, because that is np.ndarray. dtype: data type, consistent with data\u0026rsquo;s type, defaultly. device: cpu(default) or cuda requires_grad: whether gradients is required. pin_memory: Whether to store in pin memory.  1 2 3 4 5 6 7 8 9 10 11 12 13 14  import torch import numpy as np n = np.random.random((4,4)) t = torch.tensor(n) print(\u0026#34;t: \u0026#34;, t) print(\u0026#34;dtype: \u0026#34;, t.dtype) print(\u0026#34;shape: \u0026#34;, t.shape) print(\u0026#34;device: \u0026#34;,t.device) print(\u0026#34;data: \u0026#34;, t.data) print(\u0026#34;grad: \u0026#34;, t.grad) print(\u0026#34;requires_grad: \u0026#34;, t.requires_grad) print(\u0026#34;is_leaf: \u0026#34;, t.is_leaf)   t: tensor([[0.7134, 0.9249, 0.0549, 0.6584], [0.5203, 0.9346, 0.2586, 0.1961], [0.9640, 0.5363, 0.4325, 0.5146], [0.8006, 0.4149, 0.9814, 0.9949]], dtype=torch.float64) dtype: torch.float64 shape: torch.Size([4, 4]) device: cpu data: tensor([[0.7134, 0.9249, 0.0549, 0.6584], [0.5203, 0.9346, 0.2586, 0.1961], [0.9640, 0.5363, 0.4325, 0.5146], [0.8006, 0.4149, 0.9814, 0.9949]], dtype=torch.float64) grad: None requires_grad: False is_leaf: True torch.from_numpy(ndarray) 1  def from_numpy(ndarray: Any) -\u0026gt; Tensor   The essence of this function is if you create tensor object form ndarray object by torch.from_numpy, then these two objects will share the memory.\n1 2 3 4 5 6 7 8 9 10  n = np.arange(5) t1 = torch.tensor(n) t2 = torch.from_numpy(n) print(\u0026#34;n: {}\\nt1: {}\\nt2: {}\u0026#34;.format(n, t1, t2), \u0026#34;\\n\u0026#34;) n[2] = -5 print(\u0026#34;n: {}\\nt1: {}\\nt2: {}\u0026#34;.format(n, t1, t2), \u0026#34;\\n\u0026#34;) t2[3] = -6 print(\u0026#34;n: {}\\nt1: {}\\nt2: {}\u0026#34;.format(n, t1, t2))   n: [0 1 2 3 4] t1: tensor([0, 1, 2, 3, 4]) t2: tensor([0, 1, 2, 3, 4]) n: [ 0 1 -5 3 4] t1: tensor([0, 1, 2, 3, 4]) t2: tensor([ 0, 1, -5, 3, 4]) n: [ 0 1 -5 -6 4] t1: tensor([0, 1, 2, 3, 4]) t2: tensor([ 0, 1, -5, -6, 4]) Batch fill torch.zeros() 1 2 3 4 5 6 7 8 9  @overload def zeros(size: Size | list[int] | tuple[int, ...], *, out: Tensor | None = None, dtype: dtype | None = None, layout: layout | None = strided, device: device | str | None = None, pin_memory: bool = False, requires_grad: bool = False) -\u0026gt; Tensor   You can create a tensor whose elements are all zero by this function.\n size: the shape of tensor out: if you specify this arguments, then the return of torch.zeros() share memory with this parameter. layout: memory layout form, such as strided, sparse_coo, etc. When the matrix is sparse, set to sparse_coo to reduce memory footprint. device: cpu/cuda requires_grad: whether need gradients.  1 2 3 4 5 6 7 8 9 10  t1 = torch.tensor([1]) print(f\u0026#34;t1: {t1}\\n\u0026#34;) t2 = torch.zeros((3, 3), out=t1) print(f\u0026#34;t1: {t1}\\nt2: {t2}\\n\u0026#34;) torch.zeros((3, 4), out=t1) print(f\u0026#34;t1: {t1}\\nt2: {t2}\\n\u0026#34;) print(hex(id(t1)), hex(id(t2)), id(t1) == id(t2))   t1: tensor([1]) t1: tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) t2: tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) t1: tensor([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]) t2: tensor([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]) 0x7fb5cae243b0 0x7fb5cae243b0 True torch.eye() 1 2 3 4 5 6 7 8 9  @overload def eye(n: int, *, out: Tensor | None = None, dtype: dtype | None = None, layout: layout | None = strided, device: device | str | None = None, pin_memory: bool = False, requires_grad: bool = False) -\u0026gt; Tensor   Creates the unit diagonal matrix (2-dimensional tensor), which defaults to square matrices.\n1 2 3 4  t1 = torch.eye(3) t2 = torch.eye(3,4) t3 = torch.eye(4,3) print(f\u0026#34;t1: {t1}\\nt2: {t2}\\nt3: {t3}\u0026#34;)   t1: tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) t2: tensor([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.]]) t3: tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 0., 0.]]) torch.full() 1 2 3 4 5 6 7 8 9  @overload def full(size: Size | list[int] | tuple[int, ...], fill_value: int | float | bool, *, out: Tensor | None = None, layout: layout = strided, dtype: dtype | None = None, device: device | str | None = None, requires_grad: bool = False) -\u0026gt; Tensor   You can specify the fill element by fill_value argument. Like torch.zero, is you appoint the out argument, then the return will share memory with it.\n1 2  t = torch.full((3,3), np.pi) print(t)   tensor([[3.1416, 3.1416, 3.1416], [3.1416, 3.1416, 3.1416], [3.1416, 3.1416, 3.1416]]) torch.arange(), torch.linspace() 1 2 3 4 5 6 7 8 9  @overload def arange(start: int | float | bool, end: int | float | bool, step: int | float | bool, *, out: Tensor | None = None, dtype: dtype | None = None, device: device | str | None = None, requires_grad: bool = False) -\u0026gt; Tensor   Create an arithmetic 1-dimensional tensor whose start value is start, end value is end and tolerance is step. Notice that the interval is [start, end).\n1 2 3 4 5 6 7 8  def linspace(start: int | float | bool, end: int | float | bool, steps: int | None = None, *, out: Tensor | None = None, dtype: dtype | None = None, device: device | str | None = None, requires_grad: bool = False) -\u0026gt; Tensor   Create a 1-dimensional tensor that is steps equally divided. Value range: [start, end].\n1 2 3  t1 = torch.arange(0, 10, 0.01) t2 = torch.linspace(0, 10, 1000) print(t1[-1], t2[-1])   tensor(9.9900) tensor(10.) torch.logspace() 1 2 3 4 5 6 7 8 9  def logspace(start: int | float | bool, end: int | float | bool, steps: int | None = None, base: float = 10.0, *, out: Tensor | None = None, dtype: dtype | None = None, device: device | str | None = None, requires_grad: bool = False) -\u0026gt; Tensor   Creates a 1-dimensional tensor of logarithmic equipartition. The value range is [start, end], and the base is base.\n1 2 3 4 5 6  t1 = torch.logspace(1, 5, steps=15) t2 = torch.linspace(1, 5, 15) t3 = 10 ** t2 print(t1) print(t3)   tensor([1.0000e+01, 1.9307e+01, 3.7276e+01, 7.1969e+01, 1.3895e+02, 2.6827e+02, 5.1795e+02, 1.0000e+03, 1.9307e+03, 3.7276e+03, 7.1969e+03, 1.3895e+04, 2.6827e+04, 5.1795e+04, 1.0000e+05]) tensor([1.0000e+01, 1.9307e+01, 3.7276e+01, 7.1969e+01, 1.3895e+02, 2.6827e+02, 5.1795e+02, 1.0000e+03, 1.9307e+03, 3.7276e+03, 7.1969e+03, 1.3895e+04, 2.6827e+04, 5.1795e+04, 1.0000e+05]) With random torch.normal() 1 2 3 4 5 6  @overload def normal(mean: Tensor, std: Tensor, *, generator: Generator | None = None, out: Tensor | None = None) -\u0026gt; Tensor   Generating a normal distribution, mean and std could be scalar or tensor, tensor\u0026rsquo;s dtype must be torch.float:\n1 2 3 4 5 6 7  t1 = torch.normal(0, 1, size=([4])) # \u0026lt;-- Notice here `size` aa = torch.tensor([1,2,3,4], dtype=torch.float) # Must set dtype as torch.float bb = 1 t2 = torch.normal(mean=aa, std=bb) t3 = torch.normal(mean=bb, std=aa) t4 = torch.normal(mean=aa, std=aa) print(f\u0026#34;t1: {t1}\\nt2: {t2}\\nt3: {t3}\\nt4: {t4}\u0026#34;)   t1: tensor([-0.3097, 0.3981, -1.0884, -0.8362]) t2: tensor([-0.3892, 1.4906, 2.2902, 3.7142]) t3: tensor([ 1.3407, 0.8870, 2.2898, -3.8842]) t4: tensor([2.2082, 5.1530, 5.5105, 1.4939]) torch.randn(), torch.rand() and torch.randint() 1 2 3 4 5 6 7 8 9 10  @overload def randn(size: Size | list[int] | tuple[int, ...], *, generator: Generator | None, names: Sequence[str | ellipsis | None] | None, dtype: dtype | None = None, layout: layout | None = strided, device: device | str | None = None, pin_memory: bool = False, requires_grad: bool = False) -\u0026gt; Tensor   Generating a standard normal distribution.\n1 2 3 4 5 6 7 8 9  @overload def rand(size: Size | list[int] | tuple[int, ...], *, generator: Generator | None, names: Sequence[str | ellipsis | None] | None, dtype: dtype | None = None, layout: layout | None = strided, device: device | str | None = None, pin_memory: bool = False, requires_grad: bool = False) -\u0026gt; Tensor   A uniform distribution is generated on the interval [0, 1).\n1 2 3 4 5 6 7 8 9  @overload def randint(low: int, high: int, size: Size | list[int] | tuple[int, ...], *, generator: Generator | None = None, dtype: dtype | None = None, device: device | str | None = None, requires_grad: bool = False) -\u0026gt; Tensor   Generates integer uniform distribution on interval [low, high).\ntorch.randperm() 1 2 3 4 5 6 7 8 9 10  @overload def randperm(n: int, *, generator: Generator | None, out: Tensor | None = None, dtype: dtype | None = None, layout: layout | None = strided, device: device | str | None = None, pin_memory: bool = False, requires_grad: bool = False) -\u0026gt; Tensor   Generate a random permutation from 0 to n-1. Often used to generate indexes.\ntorch.bernoulli() 1 2 3 4 5  @overload def bernoulli(input: Tensor, *, generator: Generator | None = None, out: Tensor | None = None) -\u0026gt; Tensor   Use input as probability, generate Bernoulli distribution:\n1 2 3  t1 = torch.full(size=[100000], fill_value=0.4) t2 = torch.bernoulli(input=t1) print(sum(t2)/len(t2))   tensor(0.4009) torch.*_like() You can create tensor whose shape is equal to the other one, with torch.*like():\nfor example:\n1 2 3 4 5 6 7 8  def zeros_like(input: Tensor, *, memory_format: memory_format | None = None, dtype: dtype | None = None, layout: layout | None = strided, device: device | str | None = None, pin_memory: bool = False, requires_grad: bool = False) -\u0026gt; Tensor   The arguments has been discussed before, but notice input must be tensor object.\n1 2 3 4  n = np.random.random((2,2)) t0 = torch.tensor(n) t1 = torch.empty_like(t0) print(t1)   tensor([[2.9644e-323, 4.9407e-324], [1.2882e-231, 2.3204e+77]], dtype=torch.float64) Operations on Tensor Concatenate torch.cat() 1 2 3 4  @overload def cat(tensors: tuple[Tensor, ...] | list[Tensor], dim: int = 0, *, out: Tensor | None = None) -\u0026gt; Tensor   torch.cat() concatenates tensors according to dimension dim. We need introduce the dimension regulation in tensor. The dimension rules in tensor are just like list: It counts layers as dimension from outer to inner, and the number of blocks in the layer is the value of this dimension:\nYou can concatenate list of tensors in dimension dim, like:\n1 2 3 4 5 6 7 8 9 10  n1 = np.arange(0,6).reshape((2,3)) n2 = np.arange(7,13).reshape((2,3)) t1 = torch.tensor(n1) t2 = torch.tensor(n2) print(f\u0026#34;t1: {t1}\\nt2: {t2}\\n\u0026#34;) t3 = torch.cat([t1,t2], dim=0) t4 = torch.cat([t1,t2], dim=1) print(f\u0026#34;t3: {t3}\\nt4: {t4}\u0026#34;)   t1: tensor([[0, 1, 2], [3, 4, 5]]) t2: tensor([[ 7, 8, 9], [10, 11, 12]]) t3: tensor([[ 0, 1, 2], [ 3, 4, 5], [ 7, 8, 9], [10, 11, 12]]) t4: tensor([[ 0, 1, 2, 7, 8, 9], [ 3, 4, 5, 10, 11, 12]]) torch.stack() 1 2 3 4  def stack(tensors: tuple[Tensor, ...] | list[Tensor], dim: int = 0, *, out: Tensor | None = None) -\u0026gt; Tensor   Create a new dimension at dim, and concatenate list of tensors at this dimension.\nFor example, if we specify dim=0, then Pytorch will set 0-dimension of tensor as 1, and move the later dimensions one step behind, for example (2,3) --\u0026gt; (1,2,3). You can understand it\u0026rsquo;s mechanism better by the top of the image below:\n1 2 3 4 5 6 7 8 9 10 11  n1 = np.arange(0,6).reshape((2,3)) n2 = np.arange(7,13).reshape((2,3)) t1 = torch.tensor(n1) t2 = torch.tensor(n2) print(f\u0026#34;t1: {t1}\\nt2: {t2}\\n\u0026#34;) t3 = torch.stack([t1,t2], dim=0) t4 = torch.stack([t1,t2], dim=1) t5 = torch.stack([t1,t2], dim=2) print(f\u0026#34;t3: {t3}\\nt4: {t4}\\nt5: {t5}\u0026#34;)   t1: tensor([[0, 1, 2], [3, 4, 5]]) t2: tensor([[ 7, 8, 9], [10, 11, 12]]) t3: tensor([[[ 0, 1, 2], [ 3, 4, 5]], [[ 7, 8, 9], [10, 11, 12]]]) t4: tensor([[[ 0, 1, 2], [ 7, 8, 9]], [[ 3, 4, 5], [10, 11, 12]]]) t5: tensor([[[ 0, 7], [ 1, 8], [ 2, 9]], [[ 3, 10], [ 4, 11], [ 5, 12]]]) Split torch.chunk() 1 2 3  def chunk(input: Tensor, chunks: int, dim: int = 0) -\u0026gt; tuple[Tensor, ...] | list[Tensor]   Divide the tensor input equally into chunk parts by dimension dim. If it is not divisible, the last tensor is smaller than the other tensors\n1 2 3 4  t = torch.range(1,24).reshape((2,3,4)) print(t) for i in torch.chunk(t, 3, 1): print(i)   tensor([[[ 1., 2., 3., 4.], [ 5., 6., 7., 8.], [ 9., 10., 11., 12.]], [[13., 14., 15., 16.], [17., 18., 19., 20.], [21., 22., 23., 24.]]]) tensor([[[ 1., 2., 3., 4.]], [[13., 14., 15., 16.]]]) tensor([[[ 5., 6., 7., 8.]], [[17., 18., 19., 20.]]]) tensor([[[ 9., 10., 11., 12.]], [[21., 22., 23., 24.]]]) torch.split() 1 2 3  def split(tensor: Tensor, split_size_or_sections: Any, dim: int = 0) -\u0026gt; Any   Splits the tensor into chunks. Each chunk is a view of the original tensor.\nIf split_size_or_sections is an integer type, it represents the length of each chunks. Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size.\nIf split_size_or_sections is a list, pytorch splits according to the split_size_or_sections element as the length of each chunks. If the sum of the split_size_or_sections elements is not equal to the value of the slice dimension (dim), an error will be reported.\n1 2 3 4 5 6 7 8  t = torch.arange(10).reshape(5,2) print(t) print(\u0026#34;--------\u0026#34;) for i in torch.split(t, 2): print(i) print(\u0026#34;--------\u0026#34;) for i in torch.split(t,[1,1,3]): print(i)   tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]) -------- tensor([[0, 1], [2, 3]]) tensor([[4, 5], [6, 7]]) tensor([[8, 9]]) -------- tensor([[0, 1]]) tensor([[2, 3]]) tensor([[4, 5], [6, 7], [8, 9]]) Index torch.index_select() 1 2 3 4 5 6  @overload def index_select(input: Tensor, dim: int, index: Tensor, *, out: Tensor | None = None) -\u0026gt; Tensor   On the dimension dim, Pytorch takes out the data according to the index, and returns the spliced tensor of these data.\nNotice that index need to be tensor with dtype=torch.long\n1 2 3 4 5 6 7 8 9  t = torch.range(1,27).reshape((3,3,3)) idx = torch.tensor([0,2], dtype=torch.long) print(t) print(\u0026#34;----- dim=0 -----\u0026#34;) print(torch.index_select(t, dim=0, index=idx)) print(\u0026#34;----- dim=1 -----\u0026#34;) print(torch.index_select(t, dim=1, index=idx)) print(\u0026#34;----- dim=2 -----\u0026#34;) print(torch.index_select(t, dim=2, index=idx))   tensor([[[ 1., 2., 3.], [ 4., 5., 6.], [ 7., 8., 9.]], [[10., 11., 12.], [13., 14., 15.], [16., 17., 18.]], [[19., 20., 21.], [22., 23., 24.], [25., 26., 27.]]]) ----- dim=0 ----- tensor([[[ 1., 2., 3.], [ 4., 5., 6.], [ 7., 8., 9.]], [[19., 20., 21.], [22., 23., 24.], [25., 26., 27.]]]) ----- dim=1 ----- tensor([[[ 1., 2., 3.], [ 7., 8., 9.]], [[10., 11., 12.], [16., 17., 18.]], [[19., 20., 21.], [25., 26., 27.]]]) ----- dim=2 ----- tensor([[[ 1., 3.], [ 4., 6.], [ 7., 9.]], [[10., 12.], [13., 15.], [16., 18.]], torch.mask_select() 1 2 3 4  def masked_select(input: Tensor, mask: Tensor, *, out: Tensor | None = None) -\u0026gt; Tensor   Index and concatenate according to True in mask to get a one-dimensional tensor and return\n1 2 3 4  t = torch.range(1,27).reshape((3,3,3)) m = t.le(13) print(f\u0026#34;t: {t}\\nm: {m}\u0026#34;) print(torch.masked_select(t,m))   t: tensor([[[ 1., 2., 3.], [ 4., 5., 6.], [ 7., 8., 9.]], [[10., 11., 12.], [13., 14., 15.], [16., 17., 18.]], [[19., 20., 21.], [22., 23., 24.], [25., 26., 27.]]]) m: tensor([[[ True, True, True], [ True, True, True], [ True, True, True]], [[ True, True, True], [ True, False, False], [False, False, False]], [[False, False, False], [False, False, False], [False, False, False]]]) tensor([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13.]) Transform torch.reshape() 1 2  def reshape(input: Tensor, shape: Size | list[int] | tuple[int, ...]) -\u0026gt; Tensor   Transform the shape of the tensor. When the tensor is contiguous in memory, the returned tensor shares the data memory with the original tensor, and when one variable is changed, the other variable is also changed.\n1 2 3 4 5 6  t1 = torch.range(1,8) t2 = torch.reshape(t1,(-1,4)) # \u0026lt;-- -1 means this dimension is calculated from other dimensions print(f\u0026#34;t1: {t1}\\nt2: {t2}\u0026#34;) t1[0] = 256 print(f\u0026#34;t1: {t1}\\nt2: {t2}\u0026#34;) print(f\u0026#34;t1id: {id(t1)}\\nt2id: {id(t2)}\u0026#34;)   t1: tensor([1., 2., 3., 4., 5., 6., 7., 8.]) t2: tensor([[1., 2., 3., 4.], [5., 6., 7., 8.]]) t1: tensor([256., 2., 3., 4., 5., 6., 7., 8.]) t2: tensor([[256., 2., 3., 4.], [ 5., 6., 7., 8.]]) t1id: 140159708076368 t2id: 140159155889232  Pytorch variable\u0026rsquo;s continuous: link\n torch.transpose() 1 2 3  @overload def transpose(input: Tensor, dim0: int, dim1: int) -\u0026gt; Tensor   Swap the two dimensions of a tensor. Commonly used for image transformation, such as converting c*h*w to h*w*c\n1 2 3  t1 = torch.randint(0,100,size=(2,2,3)) t2 = torch.transpose(t1, dim0=2, dim1=1) print(f\u0026#34;t1: {t1}\\nt2: {t2}\u0026#34;)   t1: tensor([[[16, 72, 15], [43, 0, 0]], [[17, 31, 84], [36, 44, 80]]]) t2: tensor([[[16, 43], [72, 0], [15, 0]], [[17, 36], [31, 44], [84, 80]]]) For 2-dimension tensor t, torch.transpose(t, dim0=0, dim1=1) means matrix transpose.\n1 2 3  t1 = torch.randint(0,100,size=(2,2)) t2 = torch.transpose(t1, dim0=0, dim1=1) print(f\u0026#34;t1: {t1}\\nt2: {t2}\u0026#34;)   t1: tensor([[29, 8], [53, 52]]) t2: tensor([[29, 53], [ 8, 52]]) torch.t() 1  def t(input: Tensor) -\u0026gt; Tensor   2D tensor transpose, equivalent to torch.transpose(input, 0, 1) for 2D matrices input\n1 2 3  t1 = torch.randint(0,100,size=(2,2)) t2 = torch.t(t1) print(f\u0026#34;t1: {t1}\\nt2: {t2}\u0026#34;)   t1: tensor([[75, 88], [82, 87]]) t2: tensor([[75, 82], [88, 87]]) torch.squeeze() 1 2 3 4 5  @overload def squeeze(input: Tensor) -\u0026gt; Tensor def unsqueeze(input: Tensor, dim: int) -\u0026gt; Tensor   torch.squeeze() will eliminate the dimension whose value is 1. dim: If None, all dimensions of length 1 are removed; if a dim is specified, it can be removed if and only if the dimension is of length 1. torch.unsqueeze() will extend the dimension according to dim.\n1 2 3 4  t1 = torch.randint(1,100,size=(1,1,1,4)) t2 = torch.squeeze(t1) t3 = torch.unsqueeze(t2,dim=0) print(f\u0026#34;t1: {t1}\\nt2: {t2}\\nt3: {t3}\u0026#34;)   t1: tensor([[[[91, 4, 72, 40]]]]) t2: tensor([91, 4, 72, 40]) t3: tensor([[91, 4, 72, 40]]) Calculate on Tensor Regular Calculation torch.add() 1 2 3 4 5 6  @overload def add(input: Tensor | int | float | bool, other: Tensor | int | float | bool, *, alpha: int | float | bool | None = 1, out: Tensor | None = None) -\u0026gt; Tensor   Vector computation: input + alpha * other:\n1 2 3  t1 = torch.range(1,9).reshape((3,3)) t2 = torch.range(1,9).reshape((3,3)) print(torch.add(t1,t2,alpha=0.1))   tensor([[1.1000, 2.2000, 3.3000], [4.4000, 5.5000, 6.6000], [7.7000, 8.8000, 9.9000]]) torch.mul() 1 2 3 4  def mul(input: Tensor | int | float | bool, other: Tensor | int | float | bool, *, out: Tensor | None = None) -\u0026gt; Tensor   Vector computation (instead of matrix computation):\n1 2 3  t1 = torch.range(1,9) t2 = torch.range(1,9) print(torch.mul(t1,t2))   tensor([ 1., 4., 9., 16., 25., 36., 49., 64., 81.]) torch.addcdiv() 1 2 3 4 5  @overload def addcdiv(self: Tensor, value: int | float | bool, tensor1: Tensor, tensor2: Tensor) -\u0026gt; Tensor   Vector computation: self + value * (tensor1/tensor2)\n1 2 3 4  t1 = torch.range(2,18, step=2).reshape((3,3)) t2 = torch.range(1,9).reshape((3,3)) t3 = torch.range(1,9).reshape((3,3)) print(t3.addcdiv(value=0.1, tensor1=t1, tensor2=t2))   tensor([[1.2000, 2.2000, 3.2000], [4.2000, 5.2000, 6.2000], [7.2000, 8.2000, 9.2000]]) torch.addcmul() 1 2 3 4 5  @overload def addcmul(self: Tensor, value: int | float | bool, tensor1: Tensor, tensor2: Tensor) -\u0026gt; Tensor   Vector computation: self + value * tensor1 * tensor2:\n1 2 3 4  t1 = torch.range(1,9).reshape((3,3)) t2 = torch.range(1,9).reshape((3,3)) t3 = torch.range(1,9).reshape((3,3)) print(t3.addcmul(value=0.1, tensor1=t1, tensor2=t2))   tensor([[ 1.1000, 2.4000, 3.9000], [ 5.6000, 7.5000, 9.6000], [11.9000, 14.4000, 17.1000]])  ","date":"2021-08-16T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/pytorch-deep-learning-i-basic-conception/","title":"Pytorch \u0026 Deep Learning I (Basic Conception)"},{"content":" ASHRAEAmerican Society of Heating, Refrigerating and Air-Conditioning Engineers1894HVAC54,000ASHRAE\n\n\n1000\nnotebooknotebook\n Step 1:  Step 2:  Step 3:  Step 4: LightGBM Step 5:   \n  1:   2:   3: LightGBMCatBoost  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # import pandas as pd import numpy as np import os import gc import copy import warnings import lightgbm as lgb from lightgbm import LGBMRegressor from sklearn.metrics import mean_squared_log_error from sklearn.model_selection import StratifiedKFold, KFold from tqdm.notebook import tqdm import matplotlib.pyplot as plt import seaborn as sns warnings.filterwarnings(\u0026#39;ignore\u0026#39;) pd.set_option(\u0026#34;max_columns\u0026#34;, 500) %matplotlib inline   1.  ASHRAE6csv51\n[train/test].csv\n building_id meter : id, {0:  , 1: , 2: , 3: } timestamp meter_reading, site0  building_meta.csv\n site_id:  building_id: training.csv primary_use: EnergyStar property type definitionseducation, office) square_feet:  year_built:  floor_count:   weather_[train/test].csv,\n site_id:  air_temperature:  cloud_coverage: oktas dew_temperature:  precip_depth_1_hr:  sea_level_pressure: / wind_direction: 0-360 wind_speed:   1 2 3 4 5  train = pd.read_csv(\u0026#34;../input/ashrae-energy-prediction/train.csv\u0026#34;, parse_dates=[\u0026#34;timestamp\u0026#34;]) test = pd.read_csv(\u0026#34;../input/ashrae-energy-prediction/test.csv\u0026#34;, parse_dates=[\u0026#34;timestamp\u0026#34;]) building = pd.read_csv(\u0026#39;../input/ashrae-energy-prediction/building_metadata.csv\u0026#39;) weather_train = pd.read_csv(\u0026#39;../input/ashrae-energy-prediction/weather_train.csv\u0026#39;, parse_dates=[\u0026#34;timestamp\u0026#34;]) weather_test = pd.read_csv(\u0026#34;../input/ashrae-energy-prediction/weather_test.csv\u0026#34;, parse_dates=[\u0026#34;timestamp\u0026#34;])   1 2 3 4  df = pd.DataFrame( weather_train[\u0026#39;site_id\u0026#39;].value_counts() ) df   2.   \u0026ldquo;meter\u0026quot; {0:  , 1: , 2: , 3: } 0, site,meter_type,primary_use  trainbuilding\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  train_plot = train.merge(building, on=\u0026#39;building_id\u0026#39;, how=\u0026#39;left\u0026#39;) site = 0 # meter_type = 1 # primary_use = \u0026#39;Education\u0026#39; # r = int( np.ceil( len( train_plot[ (train_plot[\u0026#39;site_id\u0026#39;] == site) \u0026amp; (train_plot[\u0026#39;primary_use\u0026#39;] == primary_use) \u0026amp; (train_plot[\u0026#39;meter\u0026#39;] == meter_type) ][\u0026#39;building_id\u0026#39;].value_counts(dropna=False).index.to_list() ) / 2 ) )   1 2 3 4 5 6 7  s = enumerate( train_plot[ (train_plot[\u0026#39;site_id\u0026#39;] == site) \u0026amp; (train_plot[\u0026#39;primary_use\u0026#39;] == primary_use) \u0026amp; (train_plot[\u0026#39;meter\u0026#39;] == meter_type) ][\u0026#39;building_id\u0026#39;].value_counts(dropna=False).index.to_list() )   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  fig, axes = plt.subplots(r,2,figsize=(14, 36), dpi=100) for i, building_id in s: train_plot[ (train_plot[\u0026#39;site_id\u0026#39;] == site) \u0026amp; (train_plot[\u0026#39;primary_use\u0026#39;] == primary_use) \u0026amp; (train_plot[\u0026#39;meter\u0026#39;] == meter_type) \u0026amp; (train_plot[\u0026#39;building_id\u0026#39;] == building_id) ][ [\u0026#39;timestamp\u0026#39;, \u0026#39;meter_reading\u0026#39;] ].set_index(\u0026#39;timestamp\u0026#39;).resample(\u0026#39;H\u0026#39;).mean()[\u0026#39;meter_reading\u0026#39;].plot( ax=axes[i%r][i//r], alpha=0.8, label=\u0026#39;By hour\u0026#39;, color=\u0026#39;tab:blue\u0026#39; ).set_ylabel(\u0026#39;Mean meter reading\u0026#39;, fontsize=13); train_plot[ (train_plot[\u0026#39;site_id\u0026#39;] == site) \u0026amp; (train_plot[\u0026#39;primary_use\u0026#39;] == primary_use) \u0026amp; (train_plot[\u0026#39;meter\u0026#39;] == meter_type) \u0026amp; (train_plot[\u0026#39;building_id\u0026#39;] == building_id ) ][ [\u0026#39;timestamp\u0026#39;, \u0026#39;meter_reading\u0026#39;] ].set_index(\u0026#39;timestamp\u0026#39;).resample(\u0026#39;D\u0026#39;).mean()[\u0026#39;meter_reading\u0026#39;].plot( ax=axes[i%r][i//r], alpha=1, label=\u0026#39;By day\u0026#39;, color=\u0026#39;tab:orange\u0026#39; ).set_xlabel(\u0026#39;\u0026#39;); axes[i%r][i//r].legend(); axes[i%r][i//r].set_title(\u0026#39;building_id: \u0026#39; + str(building_id ), fontsize=13); plt.subplots_adjust(hspace=0.45) del train_plot,fig,axes,r gc.collect();   3.   1: \n:\n3.1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def compress_dataframe(df): \u0026#39;\u0026#39;\u0026#39;\u0026#39;\u0026#39;\u0026#39; result = df.copy() for col in result.columns: col_data = result[col] dn = col_data.dtype.name if dn == \u0026#34;object\u0026#34;: result[col] = pd.to_numeric(col_data.astype(\u0026#34;category\u0026#34;).cat.codes, downcast=\u0026#34;integer\u0026#34;) elif dn == \u0026#34;bool\u0026#34;: result[col] = col_data.astype(\u0026#34;int8\u0026#34;) elif dn.startswith(\u0026#34;int\u0026#34;) or (col_data.round() == col_data).all(): result[col] = pd.to_numeric(col_data, downcast=\u0026#34;integer\u0026#34;) else: result[col] = pd.to_numeric(col_data, downcast=\u0026#39;float\u0026#39;) return result   3.2  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  def set_time(df): df.timestamp = (df.timestamp - pd.to_datetime(\u0026#34;2016-01-01\u0026#34;)).dt.total_seconds() // 3600 #timestamp16110// return df # site # https://www.kaggle.com/patrick0302/locate-cities-according-weather-temperature site_GMT_offsets = [-5, 0, -7, -5, -8, 0, -5, -5, -5, -6, -7, -5, 0, -6, -5, -5] #, def weather_set_time(df,time_zone): df.timestamp = (df.timestamp - pd.to_datetime(\u0026#34;2016-01-01\u0026#34;)).dt.total_seconds() // 3600 GMT_offset_map = {site: offset for site, offset in enumerate(site_GMT_offsets)} df.timestamp = df.timestamp + df.site_id.map(GMT_offset_map) # site_dfs = [] for site_id in df.site_id.unique(): #  site_df = df[df.site_id == site_id].set_index(\u0026#34;timestamp\u0026#34;).reindex(time_zone) site_df.site_id = site_id for col in [c for c in site_df.columns if c != \u0026#34;site_id\u0026#34;]: site_df[f\u0026#34;had_{col}\u0026#34;] = ~site_df[col].isna() site_df[col] = site_df[col].interpolate(limit_direction=\u0026#39;both\u0026#39;, method=\u0026#39;linear\u0026#39;) #  site_df[col] = site_df[col].fillna(df[col].median()) site_dfs.append(site_df) df = pd.concat(site_dfs).reset_index() # make timestamp back into a regular column for col in df.columns: if df[col].isna().any(): df[f\u0026#34;had_{col}\u0026#34;] = ~df[col].isna() #had_xxx xxx return df # def _add_time_features(X): return X.assign(tm_day_of_week=((X.timestamp // 24) % 7), tm_hour_of_day=(X.timestamp % 24)) building = compress_dataframe(building.fillna(-1)).set_index(\u0026#34;building_id\u0026#34;) train = compress_dataframe(set_time(train)) test = compress_dataframe(set_time(test)).set_index(\u0026#34;row_id\u0026#34;) weather_train = compress_dataframe(weather_set_time(weather_train,range(8784))).set_index([\u0026#34;site_id\u0026#34;, \u0026#34;timestamp\u0026#34;]) weather_test = compress_dataframe(weather_set_time(weather_test,range(8784,26304))).set_index([\u0026#34;site_id\u0026#34;, \u0026#34;timestamp\u0026#34;])   3.3  1 2 3 4  def combined_data(df,weather): df = compress_dataframe(df.join(building, on=\u0026#34;building_id\u0026#34;).join(weather, on=[\u0026#34;site_id\u0026#34;, \u0026#34;timestamp\u0026#34;]).fillna(-1)) return df.drop(columns=[\u0026#34;meter_reading\u0026#34;]),df.meter_reading   3.4  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  def make_is_bad_zero(Xy_subset, min_interval=48, summer_start=3000, summer_end=7500): #3000/24=1257500/24=312.5,125312.5 meter = Xy_subset.meter_id.iloc[0] is_zero = Xy_subset.meter_reading == 0 #0indices if meter == 0: #0meter00training dataframedrop return is_zero transitions = (is_zero != is_zero.shift(1))#00 all_sequence_ids = transitions.cumsum()#pd.Seires ids = all_sequence_ids[is_zero].rename(\u0026#34;ids\u0026#34;)#0 if meter in [2, 3]: #  keep = set(ids[(Xy_subset.timestamp \u0026lt; summer_start) | (Xy_subset.timestamp \u0026gt; summer_end)].unique())#indices is_bad = ids.isin(keep) \u0026amp; (ids.map(ids.value_counts()) \u0026gt;= min_interval) #48 elif meter == 1: time_ids = ids.to_frame().join(Xy_subset.timestamp).set_index(\u0026#34;timestamp\u0026#34;).ids#idstimestamp is_bad = ids.map(ids.value_counts()) \u0026gt;= min_interval#48 #  jan_id = time_ids.get(0, False)#id dec_id = time_ids.get(8283, False)#id if (jan_id and dec_id and jan_id == time_ids.get(500, False) and dec_id == time_ids.get(8783, False)): #5005000 is_bad = is_bad \u0026amp; (~(ids.isin(set([jan_id, dec_id])))) #is_bad else: raise Exception(f\u0026#34;Unexpected meter type: {meter}\u0026#34;) result = is_zero.copy() result.update(is_bad) return result def find_bad_zeros(X, y): \u0026#34;\u0026#34;\u0026#34;Index\u0026#34;\u0026#34;\u0026#34; Xy = X.assign(meter_reading=y, meter_id=X.meter) is_bad_zero = Xy.groupby([\u0026#34;building_id\u0026#34;, \u0026#34;meter\u0026#34;]).apply(make_is_bad_zero) return is_bad_zero[is_bad_zero].index.droplevel([0, 1]) def find_bad_sitezero(X): \u0026#34;\u0026#34;\u0026#34;Site 0 index.\u0026#34;\u0026#34;\u0026#34; return X[(X.timestamp \u0026lt; 3378) \u0026amp; (X.site_id == 0) \u0026amp; (X.meter == 0)].index def find_bad_building1099(X, y): \u0026#34;\u0026#34;\u0026#34;1099index .\u0026#34;\u0026#34;\u0026#34; return X[(X.building_id == 1099) \u0026amp; (X.meter == 2) \u0026amp; (y \u0026gt; 3e4)].index def find_bad_rows(X, y): return find_bad_zeros(X, y).union(find_bad_sitezero(X)).union(find_bad_building1099(X, y))   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  X, y = combined_data(train,weather_train) bad_rows = find_bad_rows(X, y) #index pd.Series(bad_rows.sort_values()).to_csv(\u0026#34;rows_to_drop.csv\u0026#34;, header=False, index=False) X = X.drop(index=bad_rows) y = y.reindex_like(X) X = _add_time_features(X) X = compress_dataframe(X) X = X.drop(columns=\u0026#34;timestamp\u0026#34;) # droptimestamp del bad_rows,train,weather_train gc.collect();   3.5  Kaggle RMSLERoot Mean Squared Logarithmic Error, RMSLE)RMSLERMSLE\n$${\\rm RMSLE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 }$$\n\n $n$public/private, $p_i$  $a_i$ i. $\\log(x)$   $y = \\log(y+1)$RMSEnumpylog1p\n$y = e^y-1$ y = np.exp1m(y)\n1 2  # y = np.log1p(y)    2: \n:\n4. LightGBM 4.1  LightGBM \n learning_rate, num_leavesLightGBMleaf-wisenum_leaves subsample0-1 lambda_l2L2 num_trees  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  params = { \u0026#39;task\u0026#39;: \u0026#39;train\u0026#39;, \u0026#39;boosting_type\u0026#39;: \u0026#39;gbdt\u0026#39;, \u0026#39;objective\u0026#39;: \u0026#39;regression\u0026#39;, \u0026#39;metric\u0026#39;: \u0026#39;rmse\u0026#39;, \u0026#39;num_leaves\u0026#39;: 40, \u0026#39;subsample\u0026#39;:0.8, \u0026#39;learning_rate\u0026#39;: 0.03, \u0026#39;verbose\u0026#39;: 1, \u0026#39;lambda_l2\u0026#39;:3 } num_trees = 1000 # categorical_features=[\u0026#39;building_id\u0026#39;, \u0026#39;site_id\u0026#39;, \u0026#39;primary_use\u0026#39;, \u0026#39;had_air_temperature\u0026#39;, \u0026#39;had_cloud_coverage\u0026#39;, \u0026#39;had_dew_temperature\u0026#39;, \u0026#39;had_precip_depth_1_hr\u0026#39;,\u0026#39;had_sea_level_pressure\u0026#39;, \u0026#39;had_wind_direction\u0026#39;, \u0026#39;had_wind_speed\u0026#39;, \u0026#39;tm_day_of_week\u0026#39;, \u0026#39;tm_hour_of_day\u0026#39;]   4.2  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  n_splits = 3 for val in X[\u0026#39;meter\u0026#39;].unique(): X1 = X[X[\u0026#39;meter\u0026#39;] == val].drop(columns=[\u0026#39;meter\u0026#39;]) kf = StratifiedKFold(n_splits=n_splits,random_state=42) #StratifiedKFoldfold3fold t = 0 for train_index, test_index in kf.split(X1, X1[\u0026#39;tm_hour_of_day\u0026#39;]): #fold[\u0026#39;tm_hour_of_day\u0026#39;] train_features = X1.iloc[train_index] train_target = y[X1.iloc[train_index].index] test_features = X1.iloc[test_index] test_target = y[X1.iloc[test_index].index] d_train = lgb.Dataset(train_features, train_target, categorical_feature=categorical_features) d_eval = lgb.Dataset(test_features,test_target, categorical_feature=categorical_features) print(\u0026#34;Building model meter :\u0026#34;,val,\u0026#39;fold:\u0026#39;,t) md = lgb.train(params, d_train, num_boost_round=num_trees, valid_sets=(d_train, d_eval), early_stopping_rounds=200,verbose_eval=20) md.save_model(\u0026#39;lgb_val{}_fold{}.bin\u0026#39;.format(val,t)) t += 1 del X1 del d_train, d_eval, train_features, test_features, md gc.collect();    3: LightGBMCatBoost\n:\n5.  1 2 3 4 5 6  X = compress_dataframe(test.join(building, on=\u0026#34;building_id\u0026#34;).join(weather_test, on=[\u0026#34;site_id\u0026#34;, \u0026#34;timestamp\u0026#34;]).fillna(-1)) X = compress_dataframe(_add_time_features(X)) X = X.drop(columns=\u0026#34;timestamp\u0026#34;) # droptimestamp del test, weather_test gc.collect();   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # result = np.zeros(len(X)) for val in X[\u0026#39;meter\u0026#39;].unique(): ix = np.nonzero((X[\u0026#39;meter\u0026#39;] == val).to_numpy()) for i in tqdm(range(n_splits)): # model = lgb.Booster(model_file=\u0026#39;lgb_val{}_fold{}.bin\u0026#39;.format(val, i)) result[ix] += model.predict(X.iloc[ix].drop(columns=[\u0026#39;meter\u0026#39;]), num_iteration=model.best_iteration)/n_splits del model gc.collect(); predictions = pd.DataFrame({ \u0026#34;row_id\u0026#34;: X.index, \u0026#34;meter_reading\u0026#34;: np.clip(np.expm1(result), 0, None) }) # float_format predictions.to_csv(\u0026#34;submission.csv\u0026#34;, index=False, float_format=\u0026#34;%.4f\u0026#34;)   1    ","date":"2021-07-23T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/kaggle-ashrae-great-energy-predictor-iii/","title":"Kaggle - ASHRAE, Great Energy Predictor III"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\nAll operations in this series of blogs (Linux Operation System) are done in AWS Elastic Compute Cloud (EC2).\n File System Intro A file system controls how data is stored and retrieved. Without a file system, data placed on a storage medium, like a hard disk drive or USB stick, would be one large body of data with no way of telling where one piece of it stops and where another one begins. By separating the data into pieces and giving each piece a name, the data is easily isolated and identified.\nEach group of data is called a file and the structure and the logic is used to manage the files and their names are called file systems. In other words, a file system is a logical collection of files on a partition or disk. The hard drive can have multiple partitions, which usually contain only one file system. On a Linux system, everything is considered to be a file, including physical devices like a USB stick. Even a directory is also a special kind of file and, if something is not a file, then it\u0026rsquo;s a process.\nOn Linux the structure of the file system is hierarchical, much like an upside down three, the root directory, which is a /, lies at the base of the file system and all other directories are spreading from there.\nFilesystem Hierarchy Standard (FHS) defines the directory structure and contents for Linux distributions. For example, this is the contents of the root directly:\n1 2 3  ubuntu@ip-172-31-87-26:~$ ls / bin dev home lib32 libx32 media opt root sbin srv tmp var boot etc lib lib64 lost+found mnt proc run snap sys usr   In the FHS all files and directories appear under the root directory, even if they are stored on different physical devices like on different disks or partitions. When you want to access an additional file system, like the one on a USB stick, you need to mount it or logically attach it to an existing directory of the unique file system. That directory is called Mount Point.\n Mount is the process by which the operating system makes the file system on one storage device (such as a hard disk, CD-ROM, or shared resource) accessible to users through the other (computer\u0026rsquo;s) file system.\n To see the mount point, you can run df -h, i will make it on my laptop: You can see that my mobile HDD space has been automatically mounted in /Volumes/space, and we can access its contents in that directory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  (base) wanghaoming@localhost ~ % df -h Filesystem Size Used Avail Capacity iused ifree %iused Mounted on /dev/disk3s1s1 228Gi 15Gi 45Gi 25% 575614 474126600 0% / devfs 203Ki 203Ki 0Bi 100% 702 0 100% /dev /dev/disk3s6 228Gi 3.0Gi 45Gi 7% 3 474126600 0% /System/Volumes/VM /dev/disk3s2 228Gi 755Mi 45Gi 2% 2972 474126600 0% /System/Volumes/Preboot /dev/disk3s4 228Gi 22Mi 45Gi 1% 67 474126600 0% /System/Volumes/Update /dev/disk1s2 500Mi 6.0Mi 481Mi 2% 3 4926240 0% /System/Volumes/xarts /dev/disk1s1 500Mi 7.5Mi 481Mi 2% 91 4926240 0% /System/Volumes/iSCPreboot /dev/disk1s3 500Mi 636Ki 481Mi 1% 39 4926240 0% /System/Volumes/Hardware /dev/disk3s5 228Gi 163Gi 45Gi 79% 1911877 474126600 0% /System/Volumes/Data map auto_home 0Bi 0Bi 0Bi 100% 0 0 100% /System/Volumes/Data/home /dev/disk4s1 466Gi 175Gi 290Gi 38% 1437326 2378046 38% /Volumes/space /Applications/Mi Home.app/Wrapper 228Gi 164Gi 46Gi 78% 1919113 487182760 0% /private/var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/X/81C72F62-6D4D-5490-B104-2ECBB5B21B26 (base) wanghaoming@localhost ~ % (base) wanghaoming@localhost ~ % (base) wanghaoming@localhost ~ % ls /Volumes/space $RECYCLE.BIN 01.9 5.14 ASUS  Kg MATLAB MATLAB_R2017a.app System Volume Information media old pro ssd flie ~$2021 .docx ~$.docx    FHS This is the contents of the root directory. Most Linux distributions will show the same or a very similar layout to this output.\n1 2 3  ubuntu@ip-172-31-87-26:~$ ls / bin dev home lib32 libx32 media opt root sbin srv tmp var boot etc lib lib64 lost+found mnt proc run snap sys usr   /bin, /sbin The /bin directory contains binaries or user executable files which are available to all users. Recall that ls is an executable file, as you see, ls resides in this directory.\n1 2  ubuntu@ip-172-31-87-26:~$ ls -l /bin/ls -rwxr-xr-x 1 root root 142144 Sep 5 2019 /bin/ls   Theres another directory called /sbin which is similar to /bin, but it contains applications that only the superuser (hence the initial s) will need. These executables are used for system administration. For example, ifconfig used to configure the network adapter or fdisk used to manipulate disk partitions.\n1 2 3  ubuntu@ip-172-31-87-26:~$ ls -l /sbin/ifconfig /sbin/fdisk -rwxr-xr-x 1 root root 153880 Feb 7 13:33 /sbin/fdisk -rwxr-xr-x 1 root root 87152 Feb 1 2019 /sbin/ifconfig   /boot, /home and /root The boot directory contains files required for starting the system. You normally do not touch it.\n/home stores the users home directories. Under this directory there is another directory for each user, if that particular user has a home directory. It\u0026rsquo;s common for the name of the home directory to be the same as the user\u0026rsquo;s name.\n1 2 3  ubuntu@ip-172-31-87-26:~$ ls /home ubuntu   Note that ~ (tilde) represents the current user\u0026rsquo;s home directory. So if you run ls ~ , it\u0026rsquo;s the same as is running ls /home/ubuntu. But when you switch to the root user, ~ will denote to /root, which is the home directory of root user.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ ls ~ d1 d2 d3 d4 f1 f2 f3 f4 ubuntu@ip-172-31-87-26:~$ sudo ls ~ d1 d2\td3 d4\tf1 f2\tf3 f4 ubuntu@ip-172-31-87-26:~$ sudo su root@ip-172-31-87-26:/home/ubuntu# ls ~ dir snap t0   /usr /usr was the initial users home directory in the early days of Unix. However these days /home took its place and /usr contains many other subdirectories with binary files, shared libraries and so on.\nOn some distributions like CentOS many commands are saved in /usr/bin or in /usr/sbin.\n/dev /dev contains device files; many of these are generated at boot time or even on the fly. This directory contains the device files for every hardware device attached to the system, like a hard disk, partition, keyboard, stereo and printer.\nThese are not device drivers rather they are files that represent each device on the computer and facilitate the access to those devices. Remember that in Linux everything is a file and the disk partition is logically represented as a special file as well.\n/media, /mnt The /media directory is where the external storage will be automatically mounted when you plug it and try to access it. When you insert an external hard disk or USB stick they will be automatically mounted and made available for you under /media.\n On mac OS, this directory is like to /Volumes.\n /mnt is like /media but its not very often used these days. It was the mount point for the cdroms or the floppy disks. However, when you need to quickly mount a partition and don\u0026rsquo;t know where you always have /mnt available.\n/etc /etc gets its name from the earliest Unix operating systems and it was literally etc. because it was the dumping ground for system files, the system administrators were not sure where to put. On a modern Linux distribution, it contains not all system wide configuration files.\nFor example, the files that contain the users and their passwords, the groups, how the network is configured, or how a network service like a web or an e-mail server is configured are all of there.\n It\u0026rsquo;s always a good idea to make a copy or a backup of this directory.\n /lib, /tmp /lib contains shared library files used by different applications. You don\u0026rsquo;t talk to this directly. The package manager can update it accordingly when you install, remove or update an application.\n/tmp contains temporary files, usually saved there by applications that are running. Non-privileged users may also store files there temporarily. Note that files stored in this directory may be deleted at any time without prior notice.\n/proc /proc is a virtual directory. It contains information about your computer hardware, such as information about CPU, RAM memory or Kernel. The files and directories are generated when the computer starts or on the fly as the system is running and things change.\nFor example if I want to see information about the cpu, I can run cat /proc/cpuinfo :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  ubuntu@ip-172-31-87-26:~$ cat /proc/cpuinfo processor\t: 0 vendor_id\t: GenuineIntel cpu family\t: 6 model\t: 63 model name\t: Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz stepping\t: 2 microcode\t: 0x46 cpu MHz\t: 2399.816 cache size\t: 30720 KB physical id\t: 0 siblings\t: 1 core id\t: 0 cpu cores\t: 1 apicid\t: 0 initial apicid\t: 0 fpu\t: yes fpu_exception\t: yes cpuid level\t: 13 wp\t: yes flags\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm cpuid_fault invpcid_single pti fsgsbase bmi1 avx2 smep bmi2 erms invpcid xsaveopt bugs\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit bogomips\t: 4800.01 clflush size\t: 64 cache_alignment\t: 64 address sizes\t: 46 bits physical, 48 bits virtual power management:   And it has displayed information about the cpu. Or if I want to see information about the memory, then i can run cat /proc/meminfo:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  ubuntu@ip-172-31-87-26:~$ cat /proc/meminfo MemTotal: 992204 kB MemFree: 220400 kB MemAvailable: 644932 kB Buffers: 62684 kB Cached: 462020 kB SwapCached: 0 kB Active: 336272 kB Inactive: 255416 kB Active(anon): 1128 kB Inactive(anon): 78276 kB Active(file): 335144 kB Inactive(file): 177140 kB Unevictable: 23064 kB Mlocked: 18528 kB SwapTotal: 0 kB SwapFree: 0 kB Dirty: 0 kB Writeback: 0 kB AnonPages: 90084 kB Mapped: 65292 kB Shmem: 832 kB KReclaimable: 79780 kB Slab: 119504 kB SReclaimable: 79780 kB SUnreclaim: 39724 kB KernelStack: 2576 kB PageTables: 2388 kB NFS_Unstable: 0 kB Bounce: 0 kB WritebackTmp: 0 kB CommitLimit: 496100 kB Committed_AS: 767944 kB VmallocTotal: 34359738367 kB VmallocUsed: 17472 kB VmallocChunk: 0 kB Percpu: 13632 kB HardwareCorrupted: 0 kB AnonHugePages: 0 kB ShmemHugePages: 0 kB ShmemPmdMapped: 0 kB FileHugePages: 0 kB FilePmdMapped: 0 kB HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB Hugetlb: 0 kB DirectMap4k: 104448 kB DirectMap2M: 944128 kB   /sys, /srv, /run, /var /sys contains information about devices, drivers and some Kernel features and /srv contains data for servers; We use neither /sys nor /srv directly.\n/run directory is fairly new and different distributions use it in different ways. Its a temporary filesystem which runs in RAM so its contents will vanish when the system reboots. This directory is used only by processes.\n/var contains variable-length files such as log which are files that register events that happen on the system.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ ls /var/log alternatives.log cloud-init-output.log kern.log syslog.1 amazon cloud-init.log kern.log.1 syslog.2.gz apt dist-upgrade landscape syslog.3.gz auth.log dmesg lastlog ubuntu-advantage-timer.log auth.log.1 dpkg.log private unattended-upgrades btmp journal syslog wtmp   Path A path is a unique location to a file or a directory in the file system of an operating system. In Linux there are two types of path names: absolute and relative path names.\nAbsolute path An absolute path is defined by specifying the location of a file or directly from the root directory /. It\u0026rsquo;s a complete path and always starts with /.\nTo write an absolute path, start at the root directory / and go deep into the filesystem, writing a slash / after every directory name. Remember that ~ means the user\u0026rsquo;s home directory, that is /home/student in this case. So you can add the directory name behind ~.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ ls /home/ubuntu/ d1 d2 d3 d4 f1 f2 f3 f4 ubuntu@ip-172-31-87-26:~$ ls ~ d1 d2 d3 d4 f1 f2 f3 f4   Relative path A relative path is defined as the path related to the current working directly. It begins at the current directory and never starts with a forward slash. The absolute path to the directory always remains the same, but the relative path will be changed, according to the current working directory.\nA single dot (.) represents the directory you are in and two successive dots or double dot (..) represents the parent directory. On a Linux system these items are automatically created in every directory, and can be seen by using the ls command with the -a option, which shows all hidden files. (any file or directories that starts with a dot is hidden)\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ ls -a . .bash_logout .lesshst .sudo_as_admin_successful d3 f2 .. .bashrc .profile d1 d4 f3 .bash_history .cache .ssh d2 f1 f4 ubuntu@ip-172-31-87-26:~$ ls d1 d2 d3 d4 f1 f2 f3 f4   When you use relative path, you can just start from the file\u0026rsquo;s or directory\u0026rsquo;s name in the currently working directory, it is just the same as ./balbal:\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ ls -a ./d1 . .. ubuntu@ip-172-31-87-26:~$ ls -a d1 . ..   A more useful symbol is .., you can string multi .. together, (separate by slash /), or you can even string .. with the absolute path together. So you can access to any directory or file in Linux File System by .., theoretically.\n1 2 3 4 5 6 7 8 9 10 11  ubuntu@ip-172-31-87-26:~$ ls -a .. . .. ubuntu ubuntu@ip-172-31-87-26:~$ ls -a ../.. . bin dev home lib32 libx32 media opt root sbin srv tmp var .. boot etc lib lib64 lost+found mnt proc run snap sys usr ubuntu@ip-172-31-87-26:~$ ls ~/../.. bin dev home lib32 libx32 media opt root sbin srv tmp var boot etc lib lib64 lost+found mnt proc run snap sys usr ubuntu@ip-172-31-87-26:~$ ls /home/.. bin dev home lib32 libx32 media opt root sbin srv tmp var boot etc lib lib64 lost+found mnt proc run snap sys usr   Tree Another nice command we can use to get used to paths is tree. Tree is a recursive directory listing tool that helps us see easier and clearer the parent and children directories. We need to install it before using:\n1  sudo apt install tree   If you run it with no arguments it will display the contents of the current working directory recursively. showing sub-directories and files in tree-like format.\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ tree .  d1  d2  d3  d4  f1  f2  f3  f4 4 directories, 4 files   When a directory is given as argument, tree lists all the files and directories found in the given directory.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ tree ~/.. /home/ubuntu/..  ubuntu  d1  d2  d3  d4  f1  f2  f3  f4 5 directories, 4 files   Some useful options -d to print out only the directories and -f to print out the absolute path for each file and directory.\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ tree -df ~ /home/ubuntu  /home/ubuntu/d1  /home/ubuntu/d2  /home/ubuntu/d3  /home/ubuntu/d4 4 directories   Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  ########################## ## Linux Paths ########################## . # =\u0026gt; the current working directory .. # =\u0026gt; the parent directory ~ # =\u0026gt; the user\u0026#39;s home directory cd # =\u0026gt; changing the current directory to user\u0026#39;s home directory cd ~ # =\u0026gt; changing the current directory to user\u0026#39;s home directory cd - # =\u0026gt; changing the current directory to the last directory cd /path_to_dir # =\u0026gt; changing the current directory to path_to_dir  pwd # =\u0026gt; printing the current working directory # installing tree sudo apt install tree tree directory/ # =\u0026gt; Ex: tree . tree -d . # =\u0026gt; prints only directories tree -f . # =\u0026gt; prints absolute paths   ls command The syntax of the ls command is as follows:\n1  ls [OPTION]... [FILE]...   option and file are optional, and can be repeated. If you simply ran a ls with no options or arguments, it will print the contents of the current working directory at the terminal. The files are listed in alphabetical order. To list the files in a specific directory, pass the path to that directory as argument to the ls command.\nYou can specify as many arguments you want. For example, ls will display the contents of the root directory /; the /home directory, the home directory of the user ~; and the current working directory .\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ ls / /home ~ . .: d1 d2 d3 d4 f1 f2 f3 f4 /: bin dev home lib32 libx32 media opt root sbin srv tmp var boot etc lib lib64 lost+found mnt proc run snap sys usr /home: ubuntu /home/ubuntu: d1 d2 d3 d4 f1 f2 f3 f4   -1 option The real power of the ls command comes from its options. If you want to have your listing produced in a single column, use the -1 option; 1 means one file per line.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ ls -1 d1 d2 d3 d4 f1 f2 f3 f4   -l, -d option -l will display the output in a long listing format which means lots of information about files and directories.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ ls -l total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4   So let\u0026rsquo;s clear out the meaning of each columns.\n The first character indicates the file type: A hyphen (-) as the 1st character indicates a normal file, d indicates a directory and l indicates a symbolic link which is sort of a shortcut. The nine characters that follow indicate the file permissions for the owner, the group owner and the others. The second column indicates the number of hardlinks. The third and the fourth columns indicates the owner, and the group owner of the file; The fifth is the size and then the modification time and finally the file name.  If you want to see information about the directory, instead of its contents, then use the -d option. It\u0026rsquo;s displaying information about the directory that you specify, and not about its contents;\n1 2  ubuntu@ip-172-31-87-26:~$ ls -ld ~ drwxr-xr-x 8 ubuntu ubuntu 4096 Apr 16 11:19 /home/ubuntu   -h, -a option ls with -lh option will show the file size in human readable format.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  ubuntu@ip-172-31-87-26:~$ ls -lh /var total 44K drwxr-xr-x 2 root root 4.0K Apr 20 06:25 backups drwxr-xr-x 13 root root 4.0K Apr 15 02:41 cache drwxrwxrwt 2 root root 4.0K Nov 29 23:33 crash drwxr-xr-x 41 root root 4.0K Apr 15 03:17 lib drwxrwsr-x 2 root staff 4.0K Apr 15 2020 local lrwxrwxrwx 1 root root 9 Nov 29 23:30 lock -\u0026gt; /run/lock drwxrwxr-x 9 root syslog 4.0K Apr 20 00:00 log drwxrwsr-x 2 root mail 4.0K Nov 29 23:30 mail drwxr-xr-x 2 root root 4.0K Nov 29 23:30 opt lrwxrwxrwx 1 root root 4 Nov 29 23:30 run -\u0026gt; /run drwxr-xr-x 7 root root 4.0K Nov 29 23:36 snap drwxr-xr-x 4 root root 4.0K Nov 29 23:31 spool drwxrwxrwt 6 root root 4.0K Apr 20 09:53 tmp   By default, the ls command will not show hidden files. In Linux a hidden file or directory is any file or director whose name begins with a dot. To display all files, including the hidden ones, use the -a option.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ ls -a . .bash_logout .lesshst .sudo_as_admin_successful d3 f2 .. .bashrc .profile d1 d4 f3 .bash_history .cache .ssh d2 f1 f4   -S, -X, --hide option Another useful option is capital -S which sorts the files by size and the largest file will come first.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  ubuntu@ip-172-31-87-26:~$ ls -lhS /var total 44K drwxr-xr-x 2 root root 4.0K Apr 20 06:25 backups drwxr-xr-x 13 root root 4.0K Apr 15 02:41 cache drwxrwxrwt 2 root root 4.0K Nov 29 23:33 crash drwxr-xr-x 41 root root 4.0K Apr 15 03:17 lib drwxrwsr-x 2 root staff 4.0K Apr 15 2020 local drwxrwxr-x 9 root syslog 4.0K Apr 20 00:00 log drwxrwsr-x 2 root mail 4.0K Nov 29 23:30 mail drwxr-xr-x 2 root root 4.0K Nov 29 23:30 opt drwxr-xr-x 7 root root 4.0K Nov 29 23:36 snap drwxr-xr-x 4 root root 4.0K Nov 29 23:31 spool drwxrwxrwt 6 root root 4.0K Apr 20 09:53 tmp lrwxrwxrwx 1 root root 9 Nov 29 23:30 lock -\u0026gt; /run/lock lrwxrwxrwx 1 root root 4 Nov 29 23:30 run -\u0026gt; /run   So backups is the largest file in /var. Note that ls does not display the real size of a directory as we expect. It displays only the size of the inode structure. To see the size of a directory, use the du command instead. du comes from disk usage.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo du -sh /var 867M\t/var   By the way, in /var, there are some files that cannot be read by an unprivileged user, so we add sudp prefix to du command.\nTo sort by extension, use the -X option, written in uppercase.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  ubuntu@ip-172-31-87-26:~$ ls -X /etc NetworkManager networks gai.conf PackageKit newt hdparm.conf X11 opt host.conf gshadow- terminfo pam.d gss timezone profile.d hostname ubuntu-advantage rc0.d hosts udev rc1.d initramfs-tools udisks2 rc2.d inputrc ufw rc3.d iproute2 update-manager rc4.d iscsi update-notifier rc5.d issue vim rc6.d kernel vmware-tools rcS.d landscape vtrgb rsyslog.d ldap wgetrc sudoers.d legal xdg sysctl.d libblockdev zsh_command_not_found tmpfiles.d localtime python3.8 update-motd.d logcheck locale.alias cron.daily lsb-release hosts.allow login.defs lvm bash.bashrc at.deny machine-id bindresvport.blacklist hosts.deny   (Delete some content for easy display)\nIf you want to omit certain files from the listing, use the --hide option.\nFor example, if you don\u0026rsquo;t want to see the files which end in that .conf and .d in the listing, you can run this command: ls --hide=*.conf --hide=*.d /etc/, here * is wildcards.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  ubuntu@ip-172-31-87-26:~$ ls --hide=*.conf --hide=*.d /etc/ NetworkManager hibinit-config.cfg perl PackageKit hostname pki calendar legal shadow cloud libblockdev shadow- console-setup locale.alias shells cron.daily locale.gen skel cron.hourly localtime sos cron.monthly logcheck ssh cron.weekly login.defs ssl crontab lsb-release subgid cryptsetup-initramfs lvm subgid- crypttab machine-id subuid dbus-1 magic subuid- dconf magic.mime sudoers   (Delete some content for easy display)\n-R, --color option, , \\\u0026lt;char\u0026gt; With -R option you can list directories recursively. Recursively means that ls works its way through the entire directory tree below the starting directory, and lists the files in each subdirectory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  ubuntu@ip-172-31-87-26:~$ ls -R /home /home: ubuntu /home/ubuntu: d1 d2 d3 d4 f1 f2 f3 f4 /home/ubuntu/d1: /home/ubuntu/d2: /home/ubuntu/d3: /home/ubuntu/d4:   If you run type ls, you\u0026rsquo;ll see that there\u0026rsquo;s an alias called ls to ls --color=auto\n1 2  ubuntu@ip-172-31-87-26:~$ type ls ls is aliased to `ls --color=auto\u0026#39;   The --color=auto is the option that provides the different colors for the different file types in the listing. Thats why when you run ls -l you see directories in blue, regular files in black, symlinks in cyan, or device files in yellow on a black background.\nIf you want to run the original ls command without adding --color=auto, run \\ls and youll see everything displayed in black. The \\char cancels the alias.\n1 2  ubuntu@ip-172-31-87-26:~$ \\ls d1 d2\td3 d4\tf1 f2\tf3 f4   Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  ########################## ## The ls Command ## ls [OPTIONS] [FILES] ########################## # listing the current directory # ~ =\u0026gt; user\u0026#39;s home directory # . =\u0026gt; current directory # .. =\u0026gt; parent directory ls ls . # listing more directories ls ~ /var / # -l =\u0026gt; long listing ls -l ~ # -a =\u0026gt; listing all files and directories including hidden ones ls -la ~ # -1 =\u0026gt; listing on a single column ls -1 /etc # -d =\u0026gt; displaying information about the directory, not about its contents ls -ld /etc # -h =\u0026gt; displaying the size in human readable format ls -h /etc # -S =\u0026gt; displaying sorting by size ls -Sh /var/log # Note: ls does not display the size of a directory and all its contents. Use du instead du -sh ~ # -X =\u0026gt; displaying sorting by extension ls -lX /etc # --hide =\u0026gt; hiding some files ls --hide=*.log /var/log # -R =\u0026gt; displaying a directory recursively ls -lR ~ # -i =\u0026gt; displaying the inode number ls -li /etc   Timestamps Every file on Linux has three timestamps:\n The access time or atime is the last time the file was read. This means the time when someone used a program to display the contents of the file, it doesn\u0026rsquo;t matter if the file contains were changed or not; the modification timestamp or mtime signifies the last time the contents of the file was modified. If someone used a text editor and changed something inside, the file mtime was updated. the change time or ctime is the time when some metadata related to the file, not to the file contents, was changed.   Linux timestamps hold an integer number rather than a date and time. This number is the number of seconds since the unique epoch, which was midnight on January 1st, 1970 UTC. When Linux needs to display the date, it translates that number of seconds into a date and time. This makes it easier for humans to understand.\n To view these timestamps, use the stat or ls with options.\nstat command The stat command is showing statistics about the file including the timestamps. You can see the access time, the modification time and the changed time.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ stat ~ File: /home/ubuntu Size: 4096 Blocks: 8 IO Block: 4096 directory Device: ca01h/51713d\tInode: 256109 Links: 8 Access: (0755/drwxr-xr-x) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2021-04-20 07:53:14.829663366 +0000 Modify: 2021-04-16 11:19:59.303335040 +0000 Change: 2021-04-16 11:19:59.303335040 +0000 Birth: -   But in most cases, this has not been implemented yet. In this example, file creation or birth hasn\u0026rsquo;t been implemented yet. That\u0026rsquo;s there is a hyphen instead.\nls with options -u To see the timestamps with the ls command run ls -lu to see the access time:\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ ls -lu total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:38 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4   -t ls -l, or ls -lt to see the modification time:\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ ls -lt total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3   -c and ls -lc to see the changed time.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ ls -lc total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4   --full-time Sometimes the ls-l is not displaying the timestamp with a maximum precision. In above example, we only know the month, the day, the hour and the minute of the timestamps. To see the entire timestamp that was saved, which includs the year and the second, use the --full-time option like this: ls -l --full-time\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  ubuntu@ip-172-31-87-26:~$ ls -l --full-time total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:29.215918658 +0000 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:29.215918658 +0000 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:29.215918658 +0000 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:38.811846868 +0000 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:36:14.736027002 +0000 f4 ubuntu@ip-172-31-87-26:~$ ls -lu --full-time total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-20 09:38:09.745767696 +0000 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-20 09:50:48.340039341 +0000 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-20 09:50:48.344039311 +0000 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-20 09:50:48.348039281 +0000 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:36:14.736027002 +0000 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:36:14.736027002 +0000 f4 ubuntu@ip-172-31-87-26:~$ ls -lc --full-time total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:29.215918658 +0000 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:29.215918658 +0000 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:29.215918658 +0000 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 2021-04-14 07:36:38.811846868 +0000 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:35:56.624162448 +0000 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 2021-04-14 07:36:14.736027002 +0000 f4   And we are seeing the entire timestamp.\n-t, -u We can sort the output of the ls command by timestamp. By default, the ls -l command is showing the modification time, this is the modification time for each file, and sorting the output by name and the ls command is listing the files in alphabetical order.\nIf you want to sort by modification time at the -lt option, it will display the output sorted by modification time, newest files first.\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ ls -lt total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 linux.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 ubuntu.txt   -ltu option will sort and show the outcome by access time.\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ ls -ltu total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d4 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:38 d1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 linux.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 ubuntu.txt   -r, --reverse If you want to reverse the order of sorting add the -r or the --reverse option like:\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ ls -ltur total 16 -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 ubuntu.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 linux.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:38 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 20 09:50 d4   -r option can be grouped with the other option like -S, -X, ot just -l (reverse order in alphabeta order)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  ubuntu@ip-172-31-87-26:~$ ls -l total 16 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 linux.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 ubuntu.txt ubuntu@ip-172-31-87-26:~$ ls -lr total 16 -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 ubuntu.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 linux.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1   touch command This is useful when you want a back up program to include or exclude some files or when you simply do not want other users to know that you\u0026rsquo;ve read or modified the file; you can change the timestamp on a file using the touch command.\nThe touch command is also used to create an empty file. So if the file, which is the argument of the touch command, does not exist, touch will create it. But if it exists, touch will update the file timestamps to the computer\u0026rsquo;s current time.\nSo if we run touch linux.txt it will create the file since the file does not already exist.\n1 2 3 4 5  ubuntu@ip-172-31-87-26:~$ ls d1 d2 d3 d4 f1 f2 f3 f4 ubuntu@ip-172-31-87-26:~$ touch linux.txt ubuntu@ip-172-31-87-26:~$ ls d1 d2 d3 d4 f1 f2 f3 f4 linux.txt   Run stat linux.txt; these are the timestamps of the file.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2021-04-20 12:06:57.257973914 +0000 Modify: 2021-04-20 12:06:57.257973914 +0000 Change: 2021-04-20 12:06:57.257973914 +0000 Birth: -    By the way, in Linux, it\u0026rsquo;s not recommended to use spaces or other special characters in filenames; separate the multiple words with an underscore or a hyphen. But if for some reason you want to include a whitespace in the filename, then enclose the file name in a pair of double quotes.\n Now, if we run touch linux.txt again, it will update all three timestamps of the file to the current time:\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ touch linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2021-04-20 12:11:07.872070244 +0000 Modify: 2021-04-20 12:11:07.872070244 +0000 Change: 2021-04-20 12:11:07.872070244 +0000 Birth: -   -a If you want to change only the access time to the current time, use the -a option.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ touch -a linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2021-04-20 12:12:36.715395475 +0000 Modify: 2021-04-20 12:11:07.872070244 +0000 Change: 2021-04-20 12:12:36.715395475 +0000 Birth: -   Note that the access time was updated to the current time, the change time was also updated because metadata information of the file like atime was changed.\n But if you run cat linux.txt, then only atime is changed, why ???\n -m To change the modification timestamp, you can use the same -m option :\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ touch -m linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2021-04-20 12:12:36.715395475 +0000 Modify: 2021-04-20 12:13:53.902809262 +0000 Change: 2021-04-20 12:13:53.902809262 +0000 Birth: -   The mtime and chnge time were modified.\n-t If you want to set the atime or mtime or both to a specific time, not the system current time, you can use -t yyyymmddhhmm.ss option.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  ubuntu@ip-172-31-87-26:~$ touch -mt 199903050708.09 linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2021-04-20 12:12:36.715395475 +0000 Modify: 1999-03-05 07:08:09.000000000 +0000 Change: 2021-04-20 12:23:05.730619023 +0000 Birth: - ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ touch -at 197903050708.09 linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 1979-03-05 07:08:09.000000000 +0000 Modify: 1999-03-05 07:08:09.000000000 +0000 Change: 2021-04-20 12:23:48.706292726 +0000 Birth: - ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ touch -amt 200803050708.09 linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2008-03-05 07:08:09.000000000 +0000 Modify: 2008-03-05 07:08:09.000000000 +0000 Change: 2021-04-20 12:25:26.953546776 +0000 Birth: -   -d If you want to change simultaneously both atime and mtime, you can also use the -d \u0026quot;yyyy-mm-dd hh:mm:ss\u0026quot; option.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ touch -d \u0026#34;2020-02-20 12:12:12\u0026#34; linux.txt ubuntu@ip-172-31-87-26:~$ stat linux.txt File: linux.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 256056 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2020-02-20 12:12:12.000000000 +0000 Modify: 2020-02-20 12:12:12.000000000 +0000 Change: 2021-04-20 12:29:26.847378845 +0000 Birth: -   Note that -t and -d options accept different data formats.\n-r You can use the file1 -r file2 option to set the timestamps of file1 equal to file2\u0026rsquo;s. For example, we create a new file ubuntu.txt:\n1 2 3 4 5 6 7 8 9 10 11  ubuntu@ip-172-31-87-26:~$ touch ubuntu.txt ubuntu@ip-172-31-87-26:~$ touch ubuntu.txt -r linux.txt ubuntu@ip-172-31-87-26:~$ stat ubuntu.txt File: ubuntu.txt Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: ca01h/51713d\tInode: 285473 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2020-02-20 12:12:12.000000000 +0000 Modify: 2020-02-20 12:12:12.000000000 +0000 Change: 2021-04-20 12:33:27.865283951 +0000 Birth: -    Note that it\u0026rsquo;s not possible to modify only the change time to a specific timestamp, but here is a trick. If you want to modify only the change time to a specific time, then you set the system date and time to the desired change time run the command touch and the name of the file to change all the timestamps to the system current time and then change back the access and modification times to the initial values. Change time will remain at the desired value.\n Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67  ########################## ## File Timestamps and Date ########################## # displaying atime ls -lu # displaying mtime ls -l ls -lt # displaying ctime ls -lc # displaying all timestamps stat file.txt # displaying the full timestamp ls -l --full-time /etc/ # creating an empty file if it does not exist, update the timestamps if the file exists touch file.txt # changing only the access time to current time touch -a file # changing only the modification time to current time touch -m file # changing the modification time to a specific date and time touch -m -t 201812301530.45 a.txt # changing both atime and mtime to a specific date and time touch -d \u0026#34;2010-10-31 15:45:30\u0026#34; a.txt # changing the timestamp of a.txt to those of b.txt touch a.txt -r b.txt # displaying the date and time date # showing this month\u0026#39;s calendar cal # showing the calendar of a specific year cal 2021 # showing the calendar of a specific month and year cal 7 2021 # showing the calendar of previous, current and next month cal -3 # setting the date and time date --set=\u0026#34;2 OCT 2020 18:00:00\u0026#34; # displaying the modification time and sorting the output by name. ls -l # displaying the output sorted by modification time, newest files first ls -lt # displaying and sorting by atime ls -ltu # reversing the sorting order ls -ltu --reverse   File Types Linux determines the type of a file via a code in the file header. It doesn\u0026rsquo;t depend on the file extension. In fact, it\u0026rsquo;s quite common in Linux for files to not have any extension at all. For example, the passwd, group do not have extension. Also all executable files or commands do not have extension. (In Windows, they would be .exe files.)\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ ls -l /bin/ls -rwxr-xr-x 1 root root 142144 Sep 5 2019 /bin/ls ubuntu@ip-172-31-87-26:~$ file /bin/ls /bin/ls: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=2f15ad836be3339dec0e2e6a3c637e08e48aacbd, for GNU/Linux 3.2.0, stripped   This is one of the differences between Linux and Windows. In Linux file extensions do not matter so much. They are mostly for humans to find out visually what the specific file could represent. When you see a file name which ends in .log, then it\u0026rsquo;s probably a log file or a file which ends in .conf it\u0026rsquo;s probably a configuration file.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ file /var/log/auth.log /var/log/auth.log: ASCII text ubuntu@ip-172-31-87-26:~$ file /etc/host.conf /etc/host.conf: ASCII text   But they have the same type in Linux\u0026rsquo;s perspective. There is another example:\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ echo \u0026#34;.\u0026#34; \u0026gt; dog.txt ubuntu@ip-172-31-87-26:~$ file dog.txt dog.txt: UTF-8 Unicode text ubuntu@ip-172-31-87-26:~$ mv dog.txt dog.pdf ubuntu@ip-172-31-87-26:~$ ls d1 d2 d3 d4 dog.pdf f1 f2 f3 f4 linux.txt ubuntu.txt ubuntu@ip-172-31-87-26:~$ file dog.pdf dog.pdf: UTF-8 Unicode text ubuntu@ip-172-31-87-26:~$ cat dog.pdf .   So you can see the extension doesn\u0026rsquo;t matter at all. However, graphical applications might require the file to have a specific extension in order to open it. If you have a .txt file and change its extension to .pdf, then you cannot open that file any more by double clicking with the mouse on it. So extensions are useful only for graphical applications.\nls -l again Let\u0026rsquo;s take a closer look at the output of ls-l command, which is one way of finding what type of specific fileis; this is indicated by the first character.\nIf you see a hyphen (-), it means that\u0026rsquo;s a regular file; d indicates a directory, which is a special type of file.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ ls -l total 20 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d2 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d3 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 14 07:36 d4 -rw-rw-r-- 1 ubuntu ubuntu 46 Apr 20 13:17 dog.pdf -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f1 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f2 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:35 f3 -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 14 07:36 f4 -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 linux.txt -rw-rw-r-- 1 ubuntu ubuntu 0 Feb 20 2020 ubuntu.txt   l indicates a symbolic link, which is like a shortcut; b means a block device; and c indicates a char device.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ ls -l /dev/rtc /dev/xvda /dev/vcsu lrwxrwxrwx 1 root root 4 Apr 14 01:53 /dev/rtc -\u0026gt; rtc0 crw-rw---- 1 root tty 7, 64 Apr 14 01:53 /dev/vcsu brw-rw---- 1 root disk 202, 0 Apr 14 01:53 /dev/xvda   Device files represent logical hardware devices like a disk, a partition or a serial console port. If you see an s the file is a type of socket.\n1 2  ubuntu@ip-172-31-87-26:~$ ls -l /run/snapd-snap.socket srw-rw-rw- 1 root root 0 Apr 15 06:08 /run/snapd-snap.socket   This is used by processes to communicate and has nothing to do with the socket term used in TCP/IP Networking.\nAnd the last possible option for the first character is a p which indicates a named pipe.\n1 2  ubuntu@ip-172-31-87-26:~$ ls -l /run/initctl prw------- 1 root root 0 Apr 14 01:53 /run/initctl   A named pipe, like a socket, provides an easy way for the processes to communicate. Using named pipes, you could send the output from one command to another and end up with only the data you want to see without selecting and formatting the entire output.\nls -F Another way to classify a file is to run ls with the capital -F option. This will add the symbol after the filename according to its type.\nA / indicates a directory; no symbol added indicates a regular non-executable file; an @ sign indicates a symlink; a * means executable file.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ ls -F /etc/vtrgb /etc/xdg /etc/pam.conf /etc/pam.conf /etc/vtrgb@ /etc/xdg: autostart/ systemd/ user-dirs.conf user-dirs.defaults ubuntu@ip-172-31-87-26:~$ ls -F /bin/ls /bin/ls*   and the = sign indicates a socket; And a pipe |, a vertical bar, indicates a named pipe.\n1 2 3  ubuntu@ip-172-31-87-26:~$ ls -lF /run/snapd-snap.socket /run/initctl prw------- 1 root root 0 Apr 14 01:53 /run/initctl| srw-rw-rw- 1 root root 0 Apr 15 06:08 /run/snapd-snap.socket=   file file command show type information about file or directory specified:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  ubuntu@ip-172-31-87-26:~$ file ~ /home/ubuntu: directory ubuntu@ip-172-31-87-26:~$ file ~/ /home/ubuntu/: directory ubuntu@ip-172-31-87-26:~$ file ~/* /home/ubuntu/d1: directory /home/ubuntu/d2: directory /home/ubuntu/d3: directory /home/ubuntu/d4: directory /home/ubuntu/dog.pdf: UTF-8 Unicode text /home/ubuntu/f1: empty /home/ubuntu/f2: empty /home/ubuntu/f3: empty /home/ubuntu/f4: empty /home/ubuntu/linux.txt: empty /home/ubuntu/ubuntu.txt: empty   View Files The entire configuration of the Linux system is saved in text files. User accounts and their passwords groups, the network configuration, the security policy, the configuration of any network service like a Web, FTP or an email server are all saved in text files. So read text files on a Linux system is essential.\ncat command The cat command displays the contents of the file in the terminal window.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ cat /etc/hosts 127.0.0.1 localhost # The following lines are desirable for IPv6 capable hosts ::1 ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts   The syntax of the cat command is cat \u0026lt;filename\u0026gt;; if you want, you can give more files as arguments and cat will display all of them in the terminal.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ cat /etc/hosts /etc/host.conf 127.0.0.1 localhost # The following lines are desirable for IPv6 capable hosts ::1 ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts # The \u0026#34;order\u0026#34; line is only used by old versions of the C library. order hosts,bind multi on   Using option -n to print out the number of the line:\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ cat /etc/hosts /etc/host.conf -n 1\t127.0.0.1 localhost 2\t3\t# The following lines are desirable for IPv6 capable hosts 4\t::1 ip6-localhost ip6-loopback 5\tfe00::0 ip6-localnet 6\tff00::0 ip6-mcastprefix 7\tff02::1 ip6-allnodes 8\tff02::2 ip6-allrouters 9\tff02::3 ip6-allhosts 10\t# The \u0026#34;order\u0026#34; line is only used by old versions of the C library. 11\torder hosts,bind 12\tmulti on    note that cat is not interactive and if the file is too long, the contents scroll off or exceed the screen. So cat is the tool of choice when you want to display small files whose entire contents fit to the screen.\n cat\u0026rsquo;s short for word concatenated, so besides displaying the contents on the screen, it\u0026rsquo;s also concatenating files into a single one.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  ubuntu@ip-172-31-87-26:~$ cat /etc/hosts /etc/host.conf \u0026gt; cathost ubuntu@ip-172-31-87-26:~$ ls cathost d1 d2 d3 d4 dog.pdf f1 f2 f3 f4 linux.txt ubuntu.txt ubuntu@ip-172-31-87-26:~$ cat -n cathost 1\t127.0.0.1 localhost 2\t3\t# The following lines are desirable for IPv6 capable hosts 4\t::1 ip6-localhost ip6-loopback 5\tfe00::0 ip6-localnet 6\tff00::0 ip6-mcastprefix 7\tff02::1 ip6-allnodes 8\tff02::2 ip6-allrouters 9\tff02::3 ip6-allhosts 10\t# The \u0026#34;order\u0026#34; line is only used by old versions of the C library. 11\torder hosts,bind 12\tmulti on   less command If the output of a text file exceeds the terminal window you probably should use less commands instead of cat. less is probably the most advanced and used reading tool that Linux provides. To view the contents of a file with less, you write less \u0026lt;filename\u0026gt;:\n1  ubuntu@ip-172-31-87-26:~$ less /var/log/dmesg   This is the contents of the file. We\u0026rsquo;ve already discussed some rules and options when we\u0026rsquo;ve talked about man pages in the previous blog. But now we will recall them: When you are in a less window:\n To get additional help by typing the letter h. To exit the file type q. To move a line up or down use the arrow keys. To forward one window press on control + f or on the space key. To move backwards a screen or a window, you press control + b. To go to the very beginning of the file typed lowercase g. To go to the very end type uppercase G. To search in the current file from the current line to the end of file using /\u0026lt;string\u0026gt; and then typing the enter. To search in the current file from the current line to the begining of file using ?\u0026lt;string\u0026gt; and then typing the enter. To navigate between matches with n which means next occurrence; navigate to the previous occurrence by N.  tail, head, watch tail -n Another useful command used for viewing files is tail. Using tail you can view the last lines of a file and by default it shows the last 10 lines. If you want to read a custom number of lines of a file, use -n and the number.\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ tail -n 4 /etc/passwd ec2-instance-connect:x:112:65534::/nonexistent:/usr/sbin/nologin systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false ubuntu@ip-172-31-87-26:~$ tail -n 2 /etc/group ubuntu:x:1000: mlocate:x:119:   If you want to read the last lines, starting with a specific line number, for example 7 line, then you can use\n1 2 3  tail -6 # or tail -n +7   for example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  ubuntu@ip-172-31-87-26:~$ cat -n cathost 1\t127.0.0.1 localhost 2\t3\t# The following lines are desirable for IPv6 capable hosts 4\t::1 ip6-localhost ip6-loopback 5\tfe00::0 ip6-localnet 6\tff00::0 ip6-mcastprefix 7\tff02::1 ip6-allnodes 8\tff02::2 ip6-allrouters 9\tff02::3 ip6-allhosts 10\t# The \u0026#34;order\u0026#34; line is only used by old versions of the C library. 11\torder hosts,bind 12\tmulti on ubuntu@ip-172-31-87-26:~$ tail -6 cathost ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts # The \u0026#34;order\u0026#34; line is only used by old versions of the C library. order hosts,bind multi on ubuntu@ip-172-31-87-26:~$ tail -n +7 cathost ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts # The \u0026#34;order\u0026#34; line is only used by old versions of the C library. order hosts,bind multi on   So tail will display the contents from line 7 to the end.\ntail -f tail is really useful for reading a file in real time, like when you want to view the last n lines of a log file for important system messages. Using the -f option, tail automatically prints new messages from an open file to the screen in real time (it will not return the prompt untill you press control + c)\nFor example, let\u0026rsquo;s open two AWS EC2 terminals, and run:\n1  tail -n 5 -f /var/log/auth.log   in one of the terminal, and then run sudo su in the other terminal to switch to the root user, this exectuation will be record in the /var/log/auth.log file, and the tail window will update the contents automatically.\n Remember that log file generally be stored in /var/log directory. Amd the authentication tries are saved in /var/log/auth.log\n the head command, its the complementary of tail command, prints the top n number of lines from a given file.\nwatch command If you want to run a command repeatedly displaying its output, you use the watch command. It allows you to watch the program output change over time. What watch does is implement the following command every 2 seconds, and print the outcome to the window. The process will go on untill you press control + c.\nFor example, it can accomplish the above task of monitoring logs in real time (actually 2 seconds) in a way if you run\n1  watch -n 1 tail -n 5 /var/log/auth.log   Here the first -n 1 is the option of watch command, it control the update time interval; the second -n 5 is tail command\u0026rsquo;s.\nAnother interesting option is -d to highlight the differences between successive updates. For example, I want to see how packets are sent and received by executing the ifconfig repeatedly. So run\n1  watch -n 1 -d ifconfig   Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  ########################## ## Viewing files (cat, less, more, head, tail, watch) ########################## # displaying the contents of a file cat filename # displaying more files cat filename1 filename2 # displaying the line numbers can -n filename # concatenating 2 files cat filename1 filename2 \u0026gt; filename3 # viewing a file using less less filename # less shortcuts: # h =\u0026gt; getting help # q =\u0026gt; quit # enter =\u0026gt; show next line # space =\u0026gt; show next screen # /string =\u0026gt; search forward for a string # ?string =\u0026gt; search backwards for a string # n / N =\u0026gt; next/previous appearance # showing the last 10 lines of a file tail filename # showing the last 15 lines of a file tail -n 15 filename # showing the last lines of a file starting with line no. 5 tail -n +5 filename # showing the last 10 lines of the file in real-time tail -f filename # showing the first 10 lines of a file head filename # showing the first 15 lines of a file head -n 15 filename # running repeatedly a command with refresh of 3 seconds watch -n 3 ls -l   Create, Copy and Move (Remove) touch, mkdir command To create a new file in Linux, you have to use the touch command. We\u0026rsquo;ve already discussed it in a previous section. So now we\u0026rsquo;ll just recap it quickly.\nThe touch command takes as argument one or more files. If those files already exist touch will update the files timestamps to the current system time. But if the files do not exist, then touch will create them.\nIt\u0026rsquo;s important to note that the user executing this command must have enough permissions to create the file in the parent directory or he will get a permission denied error. For example, We are now a non privileged user, and if I try touch /abc. I will get a permission denied error, because non-privileged user is not allowed to create the file called abc in /.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm),20(dialout),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),117(netdev),118(lxd) ubuntu@ip-172-31-87-26:~$ touch /abc touch: cannot touch \u0026#39;/abc\u0026#39;: Permission denied   To create a new directories using the mkdir command. mkdir comes from make directory. It\u0026rsquo;s syntax is mkdir [option]... [directory].... In case you want mkdir to display some details about the operation is performing, then add the -v option, it displays this message and created the directory:\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ mkdir -v ddd mkdir: created directory \u0026#39;ddd\u0026#39; ubuntu@ip-172-31-87-26:~$ ls cathost d1 d2 d3 d4 d5 d6 d7 ddd dog.pdf f1 f2 f3 f4 linux.txt ubuntu.txt   And if you want to create more directories in one command, then give all directories, you want to create as arguments. In this example, We create three directories at once.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ mkdir -v ddd1 ddd2 ddd3 mkdir: created directory \u0026#39;ddd1\u0026#39; mkdir: created directory \u0026#39;ddd2\u0026#39; mkdir: created directory \u0026#39;ddd3\u0026#39;   Now, let\u0026rsquo;s suppose I want to create a complete directory structure: ~/first/second/third. Note that first/, second/, third do not exist.\n1 2  ubuntu@ip-172-31-87-26:~$ mkdir first/second/third mkdir: cannot create directory first/second/third: No such file or directory   We get an error. That because mkdir wanted to create the directories from right to left. So when it wanted to create a directory called third/, it noticed that its parent directory called second/ doesn\u0026rsquo;t exist. To solve this issue just at the -p option, which comes from parents. And it will make the parent directories as needed.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ mkdir -p first/second/third ubuntu@ip-172-31-87-26:~$ tree first/ first/  second  third 2 directories, 0 files   -p option is also good when the directory already exists and you don\u0026rsquo;t want an error. For example. we have just created first directory. If you mkdir first again, you will get an error.\n1 2  ubuntu@ip-172-31-87-26:~$ mkdir first mkdir: cannot create directory first: File exists   Remember that a directory is a special type of file. But if I add the -p option, there will be no error. Notice that mkdir -p just closet the error, instead of overwritting the first directoty.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ mkdir -p first ubuntu@ip-172-31-87-26:~$ tree first/ first/  second  third 2 directories, 0 files   cp command To copy files and directories, you use the cp command, which stands for copy. cp has three principal modes of operation, depending on the number and the type of arguments passed.\ncp \u0026lt;file\u0026gt; \u0026lt;file\u0026gt; If the command has two arguments of type files, it copies the contents of the first file to the second file. If the second file does not exist, cp will create it. But if it already exists, cp will overwrite the destination file.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ cp /etc/passwd user.txt ubuntu@ip-172-31-87-26:~$ ls cathost d2 d4 d6 ddd ddd2 dog.pdf f2 f4 linux.txt user.txt d1 d3 d5 d7 ddd1 ddd3 f1 f3 first ubuntu.txt ubuntu@ip-172-31-87-26:~$ tail -n 5 user.txt pollinate:x:111:1::/var/cache/pollinate:/bin/false ec2-instance-connect:x:112:65534::/nonexistent:/usr/sbin/nologin systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false   To show files that are being copied pass the -v option to the cp command. This prints the file and folders that are being copied to standard output. And if you wanted to prompt before overwriting add the -i option.\n1 2 3  ubuntu@ip-172-31-87-26:~$ cp -vi /etc/passwd user.txt cp: overwrite \u0026#39;user.txt\u0026#39;? y \u0026#39;/etc/passwd\u0026#39; -\u0026gt; \u0026#39;user.txt\u0026#39;   cp \u0026lt;file\u0026gt;... \u0026lt;dir\u0026gt; The second mode of operation is when cp has one or more arguments of type file, which is the source and the last argument of typed directory, which is the destination. In this case, it copies the source files to the destination directory, creating any files that do not already exist in the destination. And you can use -i and -v here.\n1 2 3 4 5 6 7 8 9 10 11  ubuntu@ip-172-31-87-26:~$ cp -vi linux.txt user.txt ubuntu.txt d1 \u0026#39;linux.txt\u0026#39; -\u0026gt; \u0026#39;d1/linux.txt\u0026#39; \u0026#39;user.txt\u0026#39; -\u0026gt; \u0026#39;d1/user.txt\u0026#39; \u0026#39;ubuntu.txt\u0026#39; -\u0026gt; \u0026#39;d1/ubuntu.txt\u0026#39; ubuntu@ip-172-31-87-26:~$ tree d1 d1  linux.txt  ubuntu.txt  user.txt 0 directories, 3 files   Notice, the destation directory must exist before, otherwise cp will view the directory name as a filename.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ cp /etc/passwd user.txt ubuntu@ip-172-31-87-26:~$ cp /etc/passwd user ubuntu@ip-172-31-87-26:~$ ls -l total 8 -rw-r--r-- 1 ubuntu ubuntu 1825 Apr 21 11:22 user -rw-r--r-- 1 ubuntu ubuntu 1825 Apr 21 11:22 user.txt   cp -r \u0026lt;dir\u0026gt;... And the third and the last mode of operation is when all arguments are of type directory. In this case, cp copies all files in the source directory to the destination directoy creating any files or directories needed. The destination directory must exist before. This requires -r, an option that indicates the recursive copying of directories. In these mode, option -i is invalid.\nThe user that runs the cp command becomes the files owner and that\u0026rsquo;s not good in some cases. To preserve file attributes, timestamps, permissions, group and user ownership pass the -p option, which stands for a preserve.\nTimestamp:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  ubuntu@ip-172-31-87-26:~$ stat d1 File: d1 Size: 4096 Blocks: 8 IO Block: 4096 directory Device: ca01h/51713d\tInode: 256334 Links: 3 Access: (0775/drwxrwxr-x) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2022-04-21 11:26:00.967959343 +0000 Modify: 2022-04-21 11:23:42.937078490 +0000 Change: 2022-04-21 11:23:42.937078490 +0000 Birth: - ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ cp -r d1 d6 ubuntu@ip-172-31-87-26:~$ cp -rp d1 d7 ubuntu@ip-172-31-87-26:~$ stat d6/d1 File: d6/d1 Size: 4096 Blocks: 8 IO Block: 4096 directory Device: ca01h/51713d\tInode: 307832 Links: 3 Access: (0775/drwxrwxr-x) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2022-04-21 11:30:52.237590828 +0000 Modify: 2022-04-21 11:30:52.237590828 +0000 Change: 2022-04-21 11:30:52.237590828 +0000 Birth: - ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ ubuntu@ip-172-31-87-26:~$ stat d7/d1 File: d7/d1 Size: 4096 Blocks: 8 IO Block: 4096 directory Device: ca01h/51713d\tInode: 307836 Links: 3 Access: (0775/drwxrwxr-x) Uid: ( 1000/ ubuntu) Gid: ( 1000/ ubuntu) Access: 2022-04-21 11:26:00.967959343 +0000 Modify: 2022-04-21 11:23:42.937078490 +0000 Change: 2022-04-21 11:31:06.249476726 +0000 Birth: -   User and Group:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  ubuntu@ip-172-31-87-26:~$ sudo cp -r /var/log ~ ubuntu@ip-172-31-87-26:~$ ls -l total 20 drwxrwxr-x 3 ubuntu ubuntu 4096 Apr 22 12:47 dir1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 11:40 dir2 drwxrwxr-x 9 ubuntu ubuntu 4096 Apr 22 13:42 log -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 22 09:51 ls.txt -rw-rw-r-- 1 ubuntu ubuntu 61 Apr 22 10:04 mac.txt drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 22 12:40 projects ubuntu@ip-172-31-87-26:~$ sudo rm -rf log ubuntu@ip-172-31-87-26:~$ sudo cp -rp /var/log ~ ubuntu@ip-172-31-87-26:~$ ls -l total 20 drwxrwxr-x 3 ubuntu ubuntu 4096 Apr 22 12:47 dir1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 11:40 dir2 drwxrwxr-x 9 root syslog 4096 Apr 22 00:00 log -rw-rw-r-- 1 ubuntu ubuntu 0 Apr 22 09:51 ls.txt -rw-rw-r-- 1 ubuntu ubuntu 61 Apr 22 10:04 mac.txt drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 22 12:40 projects   mv command move The mv command is a command line utility that moves one or more file directories from one place to another. This command has two distinct functions. It moves files and directories or it renames files and directories.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  ubuntu@ip-172-31-87-26:~$ mkdir -p dir1/dir2 ubuntu@ip-172-31-87-26:~$ touch dir1/a.txt dir1/dir2/b.txt ubuntu@ip-172-31-87-26:~$ tree dir1 dir1  a.txt  dir2  b.txt 1 directory, 2 files ubuntu@ip-172-31-87-26:~$ mv dir1/dir2/b.txt dir1/ ubuntu@ip-172-31-87-26:~$ tree dir1 dir1  a.txt  b.txt  dir2 1 directory, 2 files   The file was moved. You can move multiple files into a directory as well: all the files you want to move as arguments and the last argument will be the destination directory.\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ mv dir1/a.txt dir1/b.txt dir1/dir2/ ubuntu@ip-172-31-87-26:~$ tree dir1 dir1  dir2  a.txt  b.txt 1 directory, 2 files   You can also use a pattern matching using the wildcard; And if you want to move a directory, you move it as you would any other file.\n1 2 3 4 5 6 7 8 9 10 11 12  ubuntu@ip-172-31-87-26:~$ mv dir1/dir2/*.txt dir1 ubuntu@ip-172-31-87-26:~$ mv dir1/dir2 ~ ubuntu@ip-172-31-87-26:~$ tree .  dir1  a.txt  b.txt  dir2  user  user.txt 2 directories, 4 files   -i If the destination file already exists, the destination file will be overwritten without prompting; to prompt before overwriting a file, the -i option can be used; -i cmes from interactive.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  ubuntu@ip-172-31-87-26:~$ touch a.txt b.txt ubuntu@ip-172-31-87-26:~$ mv a.txt dir1 ubuntu@ip-172-31-87-26:~$ mv -i b.txt dir1 mv: overwrite \u0026#39;dir1/b.txt\u0026#39;? y ubuntu@ip-172-31-87-26:~$ tree .  dir1  a.txt  b.txt  dir2  user  user.txt 2 directories, 4 files   -n To prevent the existing file being overwritten, use the -n option. You can see that the a.txt, b.txt in ~ does not move because we set the opetion -n.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  ubuntu@ip-172-31-87-26:~$ touch a.txt b.txt ubuntu@ip-172-31-87-26:~$ mv -n a.txt b.txt dir1/ ubuntu@ip-172-31-87-26:~$ tree .  a.txt  b.txt  dir1  a.txt  b.txt  dir2  user  user.txt 2 directories, 6 files   -u Another useful option is -u or --update When using this option, it will move only when the source file is newer than the destination file or when the destination file is missing. This guarantees that only newer files will be moved.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ mv -u dir1/a.txt ~ ubuntu@ip-172-31-87-26:~$ mv -u b.txt dir1/ ubuntu@ip-172-31-87-26:~$ tree .  a.txt  dir1  a.txt  b.txt  dir2  user  user.txt 2 directories, 5 files   You can see that ~/dir1/a.txt does not move to the ~ be cause ~/a.txt is newer than ~/dir1/a.txt; and ~/b.txt moves in to dir1, because it is newer than ~/dir1/b.txt, and ~/dir1/b.txt has been overwritten.\nrename mv can rename the file and directory.\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ ls a.txt dir1 dir2 user user.txt ubuntu@ip-172-31-87-26:~$ mv a.txt aa.txt ubuntu@ip-172-31-87-26:~$ mv dir1 dir11 ubuntu@ip-172-31-87-26:~$ ls aa.txt dir11 dir2 user user.txt   You can even move the file to a different directory and rename it at the same time.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  ubuntu@ip-172-31-87-26:~$ tree .  aa.txt  dir11  a.txt  b.txt  dir2  user  user.txt 2 directories, 5 files ubuntu@ip-172-31-87-26:~$ mv user dir11/c.txt ubuntu@ip-172-31-87-26:~$ tree .  aa.txt  dir11  a.txt  b.txt  c.txt  dir2  user.txt 2 directories, 5 files   rm, shred command In Linux to remove files or directories from the command line use the rm command. Be extra careful when removing files or directories, because once the file is deleted with the rm command it cannot be easily recovered. There isn\u0026rsquo;t such a concept as the recycle bin or trash for the files that were removed from the command line.\nTo remove one or more files, give them as arguments to the rm command.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  ubuntu@ip-172-31-87-26:~$ tree .  aa.txt  dir11  a.txt  b.txt  c.txt  dir2  user.txt 2 directories, 5 files ubuntu@ip-172-31-87-26:~$ rm aa.txt dir11/b.txt ubuntu@ip-172-31-87-26:~$ tree .  dir11  a.txt  c.txt  dir2  user.txt 2 directories, 3 files   Note that the files were removed immediately without prompting. To prompt for confirmation when removing a file use the -i option; it is the same option we\u0026rsquo;ve seen at cp and mv comments and comes from interactive. It will cause rm to ask for confirmation.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ rm -i dir11/a.txt rm: remove regular empty file \u0026#39;dir11/a.txt\u0026#39;? y ubuntu@ip-172-31-87-26:~$ tree .  dir11  c.txt  dir2  user.txt 2 directories, 2 files   To show more information when removing a file add the -v option, which comes from verbose:\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ rm -v dir11/c.txt user.txt removed \u0026#39;dir11/c.txt\u0026#39; removed \u0026#39;user.txt\u0026#39; ubuntu@ip-172-31-87-26:~$ tree .  dir11  dir2 2 directories, 0 files   To remove the directorie, use the -r or --recursive option to make removal recursive.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ rm dir11 rm: cannot remove \u0026#39;dir11\u0026#39;: Is a directory ubuntu@ip-172-31-87-26:~$ rm -rv dir11 removed directory \u0026#39;dir11\u0026#39; ubuntu@ip-172-31-87-26:~$ tree .  dir2 1 directory, 0 files   protect You can protect a file by creating it as root. And if you you try to remove a protected file with rm by default, it will prompt for your confirmation.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ sudo mkdir dir1 ubuntu@ip-172-31-87-26:~$ sudo touch dir1/a.txt ubuntu@ip-172-31-87-26:~$ ls -l ~ dir1/a.txt -rw-r--r-- 1 root root 0 Apr 21 12:29 dir1/a.txt /home/ubuntu: total 8 drwxr-xr-x 2 root root 4096 Apr 21 12:29 dir1 drwxrwxr-x 2 ubuntu ubuntu 4096 Apr 21 11:40 dir2   you can see that dir1 and dir1/a.txt are belong to root, which means thay are under protection. And when you remove them, rm will ask for confirmation.\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ rm dir1/a.txt rm: remove write-protected regular empty file \u0026#39;dir1/a.txt\u0026#39;? Y rm: cannot remove \u0026#39;dir1/a.txt\u0026#39;: Permission denied ubuntu@ip-172-31-87-26:~$ rm -r dir1 rm: descend into write-protected directory \u0026#39;dir1\u0026#39;? y rm: remove write-protected regular empty file \u0026#39;dir1/a.txt\u0026#39;? y rm: cannot remove \u0026#39;dir1/a.txt\u0026#39;: Permission denied   -f, wildcards Just imagine that you have to remove hundreds of files from a directory and you have to confirm each removal. To force removal and not receive a prompt pass the -f option. -f stands for force.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  ubuntu@ip-172-31-87-26:~$ tree .  dir1  a1  a2  a3  a4  a5  dir2  dir3  dir2 4 directories, 5 files ubuntu@ip-172-31-87-26:~$ rm -rf dir1/*2 ubuntu@ip-172-31-87-26:~$ tree .  dir1  a1  a3  a4  a5  dir2 2 directories, 4 files    Notice, when using pattern matches and wildcards like asterisks, dots or question marks, you should be careful. Before running a command like rm *.txt execute echo and the pattern, to see exactly what are the files that will be matched. Sometimes these patterns expand in names you don\u0026rsquo;t expect to. So before executing this command execute echo and the pattern to see which files will be matched. This is also applicable to other possible destructive commands, not only rm.\n shred command The rm command does not actually delete a file. Instead, it unlinks it, meaning the data is still on disk. The space on disk occupied by the deleted file is marked as being free and can be used by other file or process. Now depending on different factors that are professional solutions that make the recovery of the deleted files possible. This is available to any other operating system, including Windows, Mac or Android. So if you remove a file, you can\u0026rsquo;t be 100 percent sure that no one will ever recover it.\nWhat you can do is use the shred command instead. shred overwrites a file to hide its contents many times before removing. Let\u0026rsquo;s see an example. We copy the password file from /etc to the current directory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  ubuntu@ip-172-31-87-26:~$ cp /etc/passwd passwd ubuntu@ip-172-31-87-26:~$ ls dir1 dir2 passwd ubuntu@ip-172-31-87-26:~$ shred -vu -n 10 passwd shred: passwd: pass 1/10 (random)... shred: passwd: pass 2/10 (b6db6d)... shred: passwd: pass 3/10 (ffffff)... shred: passwd: pass 4/10 (db6db6)... shred: passwd: pass 5/10 (000000)... shred: passwd: pass 6/10 (random)... shred: passwd: pass 7/10 (555555)... shred: passwd: pass 8/10 (492492)... shred: passwd: pass 9/10 (aaaaaa)... shred: passwd: pass 10/10 (random)... shred: passwd: removing shred: passwd: renamed to 000000 shred: 000000: renamed to 00000 shred: 00000: renamed to 0000 shred: 0000: renamed to 000 shred: 000: renamed to 00 shred: 00: renamed to 0 shred: passwd: removed ubuntu@ip-172-31-87-26:~$ ls dir1 dir2   here:\n -v means verbose or showing the process -u means removing the files after overwriting. -n and an integer number N, overwrites the file N times instead of the default, which is 3.  You can see what shred did to the target file by this\n Even though shred is good enough in the secure removal of files, if you want to keep your confidential data secure, you have to encrypt it. You cannot rely on simply deleting a file and suppose that no one could ever recover and access it.\n Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82  ########################## ## Working with files and directory (touch, mkdir, cp, mv, rm, shred) ########################## # creating a new file or updating the timestamps if the file already exists touch filename # creating a new directory mkdir dir1 # creating a directory and its parents as well mkdir -p mydir1/mydir2/mydir3 ###################### ### The cp command ### ###################### # copying file1 to file2 in the current directory cp file1 file2 # copying file1 to dir1 as another name (file2) cp file1 dir1/file2 # copying a file prompting the user if it overwrites the destination cp -i file1 file2 # preserving the file permissions, group and ownership when copying cp -p file1 file2 # being verbose cp -v file1 file2 # recursively copying dir1 to dir2 in the current directory cp -r dir1 dir2/ # copy more source files and directories to a destination directory cp -r file1 file2 dir1 dir2 destination_directory/ ###################### ### The mv command ### ###################### # renaming file1 to file2 mv file1 file2 # moving file1 to dir1  mv file1 dir1/ # moving a file prompting the user if it overwrites the destination file mv -i file1 dir1/ # preventing a existing file from being overwritten mv -n file1 dir1/ # moving only if the source file is newer than the destination file or when the destination file is missing mv -u file1 dir1/ # moving file1 to dir1 as file2 mv file1 dir1/file2 # moving more source files and directories to a destination directory mv file1 file2 dir1/ dir2/ destination_directory/ ###################### ### The rm command ### ###################### # removing a file rm file1 # being verbose when removing a file rm -v file1 # removing a directory rm -r dir1/ # removing a directory without prompting rm -rf dir1/ # removing a file and a directory prompting the user for confirmation rm -ri fil1 dir1/ # secure removal of a file (verbose with 100 rounds of overwriting) shred -vu -n 100 file1   ","date":"2021-04-21T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/linux-operation-system-ii-file-system-1/","title":"Linux Operation System II (File System 1)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\nAll operations in this series of blogs (Linux Operation System) are done in AWS Elastic Compute Cloud (EC2).\n Getting Help There are two common kinds of commands' kind in Linux, the executable file, or build-in command. You can use type command to check out. For example, cp is an executable file:\n1 2  ubuntu@ip-172-31-87-26:~$ type cp cp is /usr/bin/cp   while cd is a build-in command:\n1 2  ubuntu@ip-172-31-87-26:~$ type cd cd is a shell builtin   For executable file, You can use man command to fetch ones documentation.\n by the way, man is also an executable file, which means you can use man man command to get its documentation.\n Executable file Structure of man page man is short for a manual and a man page is simply the documentation of a program or command. You can use the man command to read the documentation of a specific command. To open the man page of the ls command. use\n1  ubuntu@ip-172-31-87-26:~$ man ls   By default, man uses a program called \u0026ldquo;less\u0026rdquo; to display the output. While you are in the man page, you can get additional help by typing the letter h. Then the page will return to \u0026ldquo;summary of less commands\u0026rdquo;. You can get back to man page by typing the letter q. You can also quit the man page by q.\nYou\u0026rsquo;ll find the same structure for all man pages.\nThe first line contains the name of the command you are reading about, along with a very short description. Then comes the synopsis of the command, and a formal description with all its available options. At the end of the man page, there are the author, the copyright and see also sections. For example:\nIn a man page everything that appears directly before ..., which are also called ellipsis, can be repeated, and everything with in a square of bracket [] is optional. This means that you can specify more than one option or file to ls, or you can can run it without options and arguments.\nWhen you see an option in short format with single minus -, a comma and an option in long format with double minus --, it means that you can use any form you wish.\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ ls d1 d2 d3 d4 f1 f2 f3 f4 ubuntu@ip-172-31-87-26:~$ ls -a . .bash_history .bashrc .lesshst .ssh d1 d3 f1 f3 .. .bash_logout .cache .profile .sudo_as_admin_successful d2 d4 f2 f4 ubuntu@ip-172-31-87-26:~$ ls --all . .bash_history .bashrc .lesshst .ssh d1 d3 f1 f3 .. .bash_logout .cache .profile .sudo_as_admin_successful d2 d4 f2 f4   Navigation of man page Use the arrow keys to move a line up or down. To forward a screen, press on control and F or on the space key. To move backwards a screen, you press control and B.\nTo go to the very beginning of the man page type lower case g, and to go to the very end type upper case G.\nSometimes you want to search in the current man page for a specific option; To search in the below pages, you could type:\n1  \\\u0026lt;string\u0026gt;   and then press enter. Any matches in the current man page will be highlighted. And then you can go to the next match by typing n or go to the prior match by typing N.\nTo search in the above pages, you could type:\n1  ?\u0026lt;string\u0026gt;   and press enter. Also, you can use n or N to navigate.\nBuild-in command Most of the time shell built in commands don\u0026rsquo;t have a dedicated man page. To see the help of such a command you can use help command:\n1 2 3 4 5 6 7  ubuntu@ip-172-31-87-26:~$ help cd cd: cd [-L|[-P [-e]] [-@]] [dir] Change the shell working directory. Change the current directory to DIR. The default DIR is the value of the HOME shell variable. ...   Note that you can use the help command only for a shell built in commands and not for commands that are executable files on the disk. But another way to get some help is the option --help, this is available for both shell builtin and executable files commands.\nTo search for a command, feature keyword in all man pages, you can use man -k and the keyword you are searching for. For example:\n1 2 3  ubuntu@ip-172-31-87-26:~$ man -k ifconfig ifcfg (8) - simplistic script which replaces ifconfig IP management ifconfig (8) - configure a network interface   You can also pass a string enclosed in double quotes as argument:\n1 2 3 4 5  ubuntu@ip-172-31-87-26:~$ man -k \u0026#34;copy files\u0026#34;. cp (1) - copy files and directories cpio (1) - copy files to and from archives git-checkout-index (1) - Copy files from the index to the working tree install (1) - copy files and set attributes   It will search for the string copy files in all man pages. Note that man -k is similar to the apropos command:\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ apropos ifconfig ifcfg (8) - simplistic script which replaces ifconfig IP management ifconfig (8) - configure a network interface ubuntu@ip-172-31-87-26:~$ apropos \u0026#39;copy files\u0026#39; cp (1) - copy files and directories cpio (1) - copy files to and from archives git-checkout-index (1) - Copy files from the index to the working tree install (1) - copy files and set attributes   Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  ########################## ## Getting Help in Linux ########################## # MAN Pages man command # For executable file =\u0026gt; Ex: man ls # The man page is displayed with the less command # SHORTCUTS: # h =\u0026gt; getting help # q =\u0026gt; quit # enter =\u0026gt; show next line # space =\u0026gt; show next screen # control + f =\u0026gt; show next screen # control + b =\u0026gt; show last screen # /string =\u0026gt; search forward for a string # ?string =\u0026gt; search backwards for a string # n / N =\u0026gt; next/previous appearance # checking if a command is shell built-in or executable file type rm # =\u0026gt; rm is /usr/bin/rm type cd # =\u0026gt; cd is a shell builtin # getting help for shell built-in commands help command # For build-in command =\u0026gt; Ex: help cd command --help # For both executable file and build-in command =\u0026gt; Ex: rm --help # searching for a command, feature or keyword in all man Pages man -k uname man -k \u0026#34;copy files\u0026#34; apropos passwd   Keyboard Shoutcuts Autocomplete The TAB key for auto-completion will not only save you much time at the Linux command line, but it will keep you from making mistakes as well.\nTAB is a feature supported by Bash and other shells as well. When typing at the Linux command line, just use TAB to autocomplete commands, filenames or folder names.\nIf a single match is found, Bash will automatically complete the filename; if multiple matches are found, you have to press the TAB key twice and it will show you all results and you should continue typing until there is only one match.\n1 2  ubuntu@ip-172-31-87-26:~$ ls f f1 f2 f3 f4   If TAB does not complete automatically a command or a filename when it should, then there is a mistake. Do not continue writing the command or the file name, but stop and analyze what you have done up to that point.\nFor example, let\u0026rsquo;s suppose that by mistake we write a whitespace before the file path.\n1  ubuntu@ip-172-31-87-26:~$ rm /home/ubuntu/ f   Then when you press TAB, there is nothing happens, and you should notice it. Otherwise rm command will delete your home directory and leads to the system corrupt. Such a mistake is avoided if you always use tab completion.\nCompendium There are also some important shortcuts:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  ########################## ## Keyboard Shortcuts ########################## TAB # autocompletes the command or the filename if its unique TAB TAB (press twice) # displays all commands or filenames that start with those letters # clearing the terminal CTRL + L # closing the shell (you can do this by command `exit`) CTRL + D # cutting (removing) the current line  CTRL + U # moving the cursor to the start of the line CTRL + A # moving the cursor to the end of the line Ctrl + E # stopping the current command CTRL + C # sleeping a the running program CTRL + Z # opening a terminal  CTRL + ALT + T   History command history $HISTFILESIZE and $HISTSIZE The bash shell stores historical commands in the ~/.bash_history by default.\n1 2 3  ubuntu@ip-172-31-87-26:~$ ls -a ~/ . .bash_history .bashrc .lesshst .ssh d1 d3 f1 f3 .. .bash_logout .cache .profile .sudo_as_admin_successful d2 d4 f2 f4    Every user has his history file in his home directory.\n The number of commands that will be stored in the this file is controlled by an environment variable called HISTFILESIZE . To print the value of a variable run:\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ echo $HISTFILESIZE 2000 ubuntu@ip-172-31-87-26:~$ cat .bash_history | tail -n 5 locate \u0026#39;f2\u0026#39; find / \u0026#39;f2\u0026#39; cd ~ ls exit   We can see that Linux system saves 2000 commands into the history file. Remember that these commands are stored in disk, which means these commands will not update until you log out. So to recall the commands you just used, you can use history command, it will return the commands that stored in the memory:\n1 2 3 4 5 6  ubuntu@ip-172-31-87-26:~$ history | tail -n 5 149 ls ~/ -a 150 cat .bash_history | tail -n 5 151 cd .. 152 cd ~ 153 history | tail -n 5   Note that these running the history commands doesnt mean listing the entire contents of the history file. There is another variable called HISTSIZE that controls how many commands from your bash history will be stored in the memory:\n1 2  ubuntu@ip-172-31-87-26:~$ echo $HISTSIZE 1000   Thus bash will save maximum 2000 commands in the ~/.bash_history file, and maximum 1000 commands in the memory.\n! To run a specific command from the history, you can use an exclamation mark ! and the command number, without any space. For example:\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ history | tail -n 5 154 echo $HISTSIZE 155 history | tail -n 5 156 ! 154 157 history 158 history | tail -n 5 ubuntu@ip-172-31-87-26:~$ !154 echo $HISTSIZE 1000   !! And to rerun the last command type two exclamation marks. It has the same effect as pressing the Up arrow once to view the previous command and then pressing Enter.\n1 2 3  ubuntu@ip-172-31-87-26:~$ !! echo $HISTSIZE 1000   !- You can also refer to a command from a certain number of lines, back in the bash history. For example, !-7 would run the last 7th command.\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ history | tail -n5 156 ! 154 157 history 158 history | tail -n 5 159 echo $HISTSIZE 160 history | tail -n5 ubuntu@ip-172-31-87-26:~$ !-2 echo $HISTSIZE 1000   !\u0026lt;command_name\u0026gt;:p To search for a command in your history and run it, type !command_name. For example, if I want to run the last echo command and to do that:\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ history | tail -n 5 159 echo $HISTSIZE 160 history | tail -n5 161 echo $HISTSIZE 162 ls ~/ -a 163 history | tail -n 5 ubuntu@ip-172-31-87-26:~$ !echo echo $HISTSIZE 1000   Note that this can be also dangerous because you don\u0026rsquo;t know exactly what command was executed and there\u0026rsquo;s the risk of executing another command. What you can do is append :p behind the \u0026lt;command_name\u0026gt; and bash will print out the command to the terminal instead of running it.\n1 2  ubuntu@ip-172-31-87-26:~$ !echo:p echo $HISTSIZE   It\u0026rsquo;s just printing the command, it\u0026rsquo;s not running it. And if the command is correct, You can call it by just pressing up-arrow, because Linux has written the matched command into the history:\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:/$ cd home ubuntu@ip-172-31-87-26:/home$ cd ~ ubuntu@ip-172-31-87-26:~$ !echo:p echo $HISTSIZE ubuntu@ip-172-31-87-26:~$ history | tail -n 5 174 ls 175 cd home 176 cd ~ 177 echo $HISTSIZE 178 history | tail -n 5   reverse-i-search mode Bash also has a recall mode you can use to search for commands you\u0026rsquo;ve previously run; press control and r and then start typing a command to search your Bash history for it. If you do not want to run the matched command, you can press control and g to leave the history searching mode and clear the line.\nremove history If you want to delete a particular command you can add a -d option with the line number of that command:\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ history | tail -n 5 174 ls 175 cd home 176 cd ~ 177 echo $HISTSIZE 178 history | tail -n 5 ubuntu@ip-172-31-87-26:~$ history -d 176 ubuntu@ip-172-31-87-26:~$ history | tail -n 5 175 cd home 176 echo $HISTSIZE 177 history | tail -n 5 178 history -d 176 179 history | tail -n 5   To clear the entire content of the history file, execute history -c. The entire history will be removed:\n1 2 3  ubuntu@ip-172-31-87-26:~$ history -c ubuntu@ip-172-31-87-26:~$ history 1 history   $HISTCONTROL As we introduced last section, you can use option -d or -c to remove the commands histiory. But a deficiency of that is the removing trace will be stored by history at the same time.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ubuntu@ip-172-31-87-26:~$ history | tail -n5 110 history 111 exit 112 history 113 ls 114 history | tail -n5 ubuntu@ip-172-31-87-26:~$ history -d 113 ubuntu@ip-172-31-87-26:~$ history | tail -n5 111 exit 112 history 113 history | tail -n5 114 history -d 113 115 history | tail -n5   The variable HISTCONTROL will help to solve this problem. This variable has three statuses : ignorespace, ignoredups and ignoreboth.\nignorespace If $HISTCONTROL=ignorespace, shell will ignore the command that begins with a space. The command will be executed normally, except being stored by history\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  ubuntu@ip-172-31-87-26:~$ HISTCONTROL=ignorespace # \u0026lt;-- Assign value to variable without symbol `$` ubuntu@ip-172-31-87-26:~$ echo $HISTCONTROL # \u0026lt;-- Print value of variable with symbol `$` ignorespace ubuntu@ip-172-31-87-26:~$ ls d1 d2 d3 d4 f1 f2 f3 f4 ubuntu@ip-172-31-87-26:~$ ls -a . .bash_logout .lesshst .sudo_as_admin_successful d3 f2 .. .bashrc .profile d1 d4 f3 .bash_history .cache .ssh d2 f1 f4 ubuntu@ip-172-31-87-26:~$ history | tail -n 5 117 $HISTCONTROL=ignorespace # \u0026lt;-- Yield error 118 HISTCONTROL=ignorespace 119 echo $HISTCONTROL 120 ls 121 history | tail -n 5   You can see that the command ls -a is not stored by history.\nignoredups If $HISTCONTROL=ignoredups, then shell will only save the repeated same commands as one command. For example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  ubuntu@ip-172-31-87-26:~$ HISTCONTROL=ignoredups ubuntu@ip-172-31-87-26:~$ echo $HISTCONTROL ignoredups ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ history | tail -n 5 122 HISTCONTROL=ignoredups 123 echo $HISTCONTROL 124 cd 125 cd 126 history | tail -n 5   Notice that if you set $HISTCONTROL=ignoredups, then shell will store the commands beginning with a space. Also Notice in shell\u0026rsquo;s perspective, cd is distinct with cd with a space in front.\nignoreboth If you want incorporate the features on ignorespace and ignoredups, you can use ignoreboth.\n.bashrc file Note that bash variables you set will only persist for the current session. After you log out or restart your system the settings will be lost. To save the settings after the system reboots, you have to add the settings to ~/.bashrc file in the user\u0026rsquo;s home directory.\nThe easiest way to do that is by running echo:\n1  ubuntu@ip-172-31-87-26:~$ echo \u0026#34;HISTCONTROL=ignoreboth\u0026#34; \u0026gt;\u0026gt; .bashrc   Then this string will be appended to the end of the file.\n Every user has his .bashrc file in his home directory, and everytime bash shell is opened, .bashrc file will be loaded.\n $HISTTIMEFORMAT history command will persist the command order index defaultly, but you can store the timestamp of the command when it was executed by setting the HISTTIMEFORMAT variable:\n1 2 3 4 5 6 7 8 9  ubuntu@ip-172-31-87-26:~$ HISTTIMEFORMAT=\u0026#34;%y/%m/%d %T \u0026#34; ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ cd ubuntu@ip-172-31-87-26:~$ history | tail -n 5 132 20/04/18 07:09:00 cd 133 20/04/18 07:09:27 history | tail -n 5 134 20/04/18 07:10:18 HISTTIMEFORMAT=\u0026#34;%y/%m/%d %T \u0026#34; 135 20/04/18 07:10:21 cd 136 20/04/18 07:10:34 history | tail -n 5   Similarly, the variable setting will be eliminated after the log out. Thus you can append this setting into .bashrc file by echo:\n1  ubuntu@ip-172-31-87-26:~$ echo \u0026#39;HISTTIMEFORMAT=\u0026#34;%y/%m/%d %T \u0026#34;\u0026#39; \u0026gt;\u0026gt; .bashrc   Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  ########################## ## Bash History ########################## # showing the history history # removing a line (ex: 100) from the history history -d 100 # removing the entire history history -c # printing the no. of commands saved in the history file (~/.bash_history) echo $HISTFILESIZE # printing the no. of history commands saved in the memory echo $HISTSIZE # rerunning the last command from the history !! # running a specific command from the history (ex: the 20th command) !20 # running the last nth (10th) command from the history !-10 # running the last command starting with abc  !abc # printing the last command starting with abc  !abc:p # reverse searching into the history CTRL + R # recording the date and time of each command in the history HISTTIMEFORMAT=\u0026#34;%d/%m/%y %T\u0026#34; # making it persistent after reboot echo \u0026#34;HISTTIMEFORMAT=\\\u0026#34;%d/%m/%y %T\\\u0026#34;\u0026#34; \u0026gt;\u0026gt; ~/.bashrc # or echo \u0026#39;HISTTIMEFORMAT=\u0026#34;%d/%m/%y %T\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc   Root user There are two main categories of Linux users:\n non-privileged users; root. It is also referred to as the superuser or the administrator.  The root account has absolute power on Linux system. It can do anything on a Linux system, such as installing a new software or updating the existing ones, setting up the network, changing the ownership of files, managing other users accounts, or administering any service that\u0026rsquo;s running.\nOn the other hand, all other users, which are called non privileged or normal users, have no special rights on the system. They can only work in the user home directory (which is located in the /home/) and cannot perform any administrative tasks like installing new software, creating other users or running administrative commands. For example:\n1 2 3  ubuntu@ip-172-31-87-26:~$ apt install tree E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied) E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?   Because the root is so powerful, it\u0026rsquo;s not recommended to use it for ordinary tasks such as browsing the web, writing e-mails or documents. Even a simple mistake can cause problems to the entire system. So for ordinary tasks, it\u0026rsquo;s highly recommended to use a normal user account. And when root permissions are needed, you simply become root only to perform that particular administrative task. There are more approaches to to gain root access in the terminal.\nsudo su command The first way to temporarily become root in a terminal is to run: sudo su. By the way sudo stands for super user do and su stands for substitute user. It will ask for the user\u0026rsquo;s password (instead of the root password).\nIf the password is right and the user is part of a specific group which is called sudo on Ubuntu and wheel on CentOs, then a new shell with root privileges starts and the user can run there administrative commands as root. To see the user you are currently logged in run the id command. This command prints out the logged in user and the groups it belongs to. To log out, you type exit or press Ctrl + D.\n1 2 3 4 5 6 7 8  ubuntu@ip-172-31-87-26:~$ sudo su password: root@ip-172-31-87-26:/home/ubuntu# id uid=0(root) gid=0(root) groups=0(root) root@ip-172-31-87-26:/home/ubuntu# exit exit ubuntu@ip-172-31-87-26:~$ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm),20(dialout),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),117(netdev),118(lxd)   Note that when becoming root, if you add a white space and a hyphen after su like sudo su -, then the working environment is activated. The current working directory was changed to the root home directory, i.e. /root/.\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ sudo su root@ip-172-31-87-26:/home/ubuntu# pwd /home/ubuntu root@ip-172-31-87-26:/home/ubuntu# exit exit ubuntu@ip-172-31-87-26:~$ sudo su - root@ip-172-31-87-26:~# pwd /root root@ip-172-31-87-26:~# exit logout   This executed all of the root users shall initialization scripts and set all environmental variables as if the root user was logged into a fresh shell session.\n By convention on Linux, all users prompt ends in a dollar sign $, but root prompt ends in a hash #.\n sudo \u0026lt;command\u0026gt; A second way to run commands as root is without logging into the root account, but by using the sudo command and the command you want to run as root.\nIf you prefix a command with sudo, you are prompted for your password (not the root password), and if your user belongs to a specific group (sudo on Ubuntu and wheel on CentOS) the command will run with root privileges. For example:\n1 2 3 4 5 6 7 8 9 10  ubuntu@ip-172-31-87-26:~$ mkdir /root/dir mkdir: cannot create directory /root/dir: Permission denied ubuntu@ip-172-31-87-26:~$ sudo mkdir /root/dir Password: ubuntu@ip-172-31-87-26:~$ ls /root ls: cannot open directory \u0026#39;/root\u0026#39;: Permission denied ubuntu@ip-172-31-87-26:~$ sudo ls /root dir snap t0 ubuntu@ip-172-31-87-26:~$ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm),20(dialout),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),117(netdev),118(lxd)   Notice that sudo policy caches for 5 minutes, the credentials I\u0026rsquo;ve entered in the first place, and any time I ran the sudo command the timestamp is updated to five minutes. In this example, after I ran the sudo mkdir command, I could run other commands (like sudo ls) as root without being asked for any password for exactly 5 minutes.\nBy running sudo with the -v option a user can update the cached credentials without running a command. And you can invalidate the user\u0026rsquo;s cached credentials running sudo-k, and the next time sudo is run, a passport will be required.\n1 2 3  ubuntu@ip-172-31-87-26:~$ sudo -k ubuntu@ip-172-31-87-26:~$ sudo ls / Password:   su The last method of running commands as root is to temporarily become root in that terminal by running the su command and entering the root password.\nNote that on some distributions like Ubuntu based, this method is by default disabled because direct access to root is locked. Remember that you have installed Ubuntu in the first place without setting a password for the root account.\nTo unlock the account on such a distribution, you simply have to set its password. Note that not having a password doesn\u0026rsquo;t mean that without it you can login as root. The command used to change a password in Linux is passwd \u0026lt;user name\u0026gt;, you will be prompted for password so that the command will run as root.\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ sudo passwd root New password: Retype new password: passwd: password updated successfully    A user can change its own password by running the passwd command and root can change the password of any user by running the passwd command and the username whose password will be changed.\n And then you can temporarily login as root by running su and entering the root password( you just setted ):\n1 2 3 4  ubuntu@ip-172-31-87-26:~$ su Password: root@ip-172-31-87-26:/home/ubuntu# id uid=0(root) gid=0(root) groups=0(root)   At the end of this section, we will briefly discuss 3 \u0026ldquo;root\u0026rdquo; in Linux: root directory, root\u0026rsquo;s home directory and root user:\n root directory: / directory root\u0026rsquo;s home directory: /root/  1 2 3  ubuntu@ip-172-31-87-26:~$ sudo su root@ip-172-31-87-26:/home/ubuntu# echo ~ /root    root user: privilege user on Linux  Compendium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  ########################## ## Running commands as root (sudo, su) ########################## # running a command as root (only users that belong to sudo group [Ubuntu] or wheel [CentOS]) sudo command # =\u0026gt; enter the user\u0026#39;s password # becoming root temporarily in the terminal sudo su # =\u0026gt; enter the user\u0026#39;s password # setting the root password sudo passwd root # changing a user\u0026#39;s password passwd username # becoming root temporarily in the terminal su # =\u0026gt; enter the root password       sudo su sudo \u0026lt;command\u0026gt; su     password user\u0026rsquo;s password user\u0026rsquo;s password root\u0026rsquo;s password   user only users that belong to sudo group [Ubuntu] or wheel [CentOS] only users that belong to sudo group [Ubuntu] or wheel [CentOS] -    ","date":"2021-04-13T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/linux-operation-system-i-terminal/","title":"Linux Operation System I (Terminal)"},{"content":"Overview 123\n1 2 3 4 5 6 7  df.head(x) df.info() df.describe() df.shape df.values df.columns df.index   1 2  df.sort_values(by=\u0026#34;col\u0026#34;).tail(n) # n largest, e.t. df.nlargest(n, \u0026#34;col\u0026#34;)   1 2 3  df.sample(n) # Return n random samples df.sample(frac=0.3) # Return a random sample of 30 percent df.sample(frac=1) # Shuffle   1 2  df.corr() df.diff()   Sort 1 2 3 4 5 6 7 8 9 10 11  df.sort_values( by=\u0026#34;col_name\u0026#34;, axis=0, ascending=True, inplace=False ) df.sort_values( by=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;, \u0026#34;col3\u0026#34;], ascending=[True, False, True] # sort col1 by ascending; sort col2 by descending; sort col3 by ascending )   1 2 3 4  df.sort_index( level=[\u0026#34;outter_index\u0026#34;, \u0026#34;inner_index\u0026#34;], ascending=False )   Set index 1 2 3 4 5 6 7  df.set_index(\u0026#34;col\u0026#34;) df.set_index(\u0026#34;outter_index\u0026#34;, \u0026#34;inner_index\u0026#34;) df.reset_index() df.reset_index(drop=False) df = pd.read_csv(\u0026#34;xxxx.csv\u0026#34;, index_col=[\u0026#34;col\u0026#34;])   Subset and Slice  By row 1 2 3 4 5 6 7 8 9 10 11  # Subset by row df[df[\u0026#34;col\u0026#34;] \u0026gt; 0] df[ (df.col1 \u0026gt; 0) \u0026amp; (df.col2 \u0026gt;0) ] df[ df.col.isin([\u0026#34;xxx\u0026#34;, \u0026#34;yyy\u0026#34;]) ]   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # With loc method ## The first input to .loc[] accessor allows you to select the rows it returns. df.loc[ [\u0026#34;outer_index1\u0026#34;, \u0026#34;outer_index_2\u0026#34;] ] df.loc[ [ (\u0026#34;outer_index_1\u0026#34;, \u0026#34;inner_index_1\u0026#34;), (\u0026#34;outer_index_2\u0026#34;, \u0026#34;inner_index_2\u0026#34;), (\u0026#34;outer_index_3\u0026#34;, \u0026#34;inner_index_3\u0026#34;), ] ] df.loc[ \u0026#34;outer_index_i\u0026#34; : \u0026#34;outer_index_j\u0026#34; ] df.loc[ (\u0026#34;outer_index_i\u0026#34;, \u0026#34;inner_index_i\u0026#34;): (\u0026#34;outer_index_j\u0026#34;, \u0026#34;inner_index_j\u0026#34;) ]   1 2  # With iloc method df[df[\u0026#34;y\u0026#34;]==+1][np.dot(df[df[\u0026#34;y\u0026#34;]==+1].iloc[:,0:df.shape[1]-1], w) \u0026lt;= 0] # subset by row and then by row   1 2 3 4 5 6 7  # With query method  # df.query(\u0026#34;SQL Statements string\u0026#34;) df.query( \u0026#34;col1 == \u0026#39;xxx\u0026#39; or (col1 == \u0026#39;yyy\u0026#39; and col2 \u0026lt; 90)\u0026#34; ) df.query(\u0026#39;date \u0026gt;= \u0026#34;1991-01-01\u0026#34;\u0026#39;) # date could be DateFrame index name.   By col 1 2 3 4 5 6 7  # Subset \u0026amp; Slice by col df[\u0026#34;col\u0026#34;] df.col df[df.col == df.col.max()].col # equal to df.col.max() df[ [\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;] ]   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # With loc method df_sort_id.loc[ [ (\u0026#34;outer_index_1\u0026#34;, \u0026#34;inner_index_1\u0026#34;), (\u0026#34;outer_index_2\u0026#34;, \u0026#34;inner_index_2\u0026#34;), (\u0026#34;outer_index_3\u0026#34;, \u0026#34;inner_index_3\u0026#34;), ] , [\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;] ] df_sort_id.loc[ (\u0026#34;Julia\u0026#34;, \u0026#34;1\u0026#34;): (\u0026#34;Julia\u0026#34;, \u0026#34;3\u0026#34;) , \u0026#34;col_i\u0026#34;: \u0026#34;col_j\u0026#34; ] df.loc[ df.col1 == \u0026#34;xxx\u0026#34;, # the first input of .loc select the rows \u0026#34;col2\u0026#34; # the second input of .loc select the columns ]   1 2 3  # Add a col df[\u0026#34;newcol\u0026#34;] = scalar or list df.insert(0, \u0026#34;newcol\u0026#34;, scalar or list) # 0 is the location of the new column   Aggregate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  df.col.mean() df[[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;]].mean() def pct30(col): return col.quantile(0.3) df.col.agg(min) df[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;].agg([min, max, np.mean, np.median, pct30]) df[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;].agg([cumsum, cummin, cumprod]) df.agg( { \u0026#34;col1\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;col2\u0026#34;: \u0026#34;mean\u0026#34;, # There is no need to call the numpy module at this time. \u0026#34;col3\u0026#34;: \u0026#34;median\u0026#34;, \u0026#34;col4\u0026#34;: \u0026#34;count\u0026#34; } )   1 2  df.mean(axis=\u0026#34;columns\u0026#34;) # calculate for every rows df.mean(axis=\u0026#34;index\u0026#34;) # calculate for every columns   Duplicate Value 1 2 3 4 5 6 7 8 9  df.drop_duplicates( subset=\u0026#34;col\u0026#34; ) df.drop_duplicates( subset=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;] ) df.col.unique()   Counting 1 2 3 4 5 6 7 8  df.col.value_counts( sort=True, normalize=False ) df.col.value_counts() # e.t. df.groupby(\u0026#34;col\u0026#34;).count()   1 2 3 4 5  # count missing value of each columns df.isna() df.isna().any() df.isna().sum() df.isna().sum().plot(kind=\u0026#34;bar\u0026#34;)   Group Groupby 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Groupby method df.groupby( \u0026#34;col\u0026#34; ) df.groupby( [\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;] ) # e.t. df.groupby( [\u0026#34;col1\u0026#34;, df[\u0026#34;col2\u0026#34;]] ) df.groupby(\u0026#34;col1\u0026#34;).agg(\u0026#34;sum\u0026#34;)[\u0026#34;col2\u0026#34;].plot(kind=\u0026#34;bar\u0026#34;) # e.t. df.groupby(\u0026#34;col1\u0026#34;).agg(\u0026#34;sum\u0026#34;).plot(kind=\u0026#34;bar\u0026#34;, y=\u0026#34;col2\u0026#34;)   Argument level = 0 separates rows with the same (outest) index.\n1 2 3 4  ind = [1, 2, 3, 1, 2, 3, 4] s = pd.Series([1, 2, 3, 10, 20, 30, 40], index=ind) sg = s.groupby(level=0) print(sg.first(), sg.last())   1 1 2 2 3 3 4 40 dtype: int64 1 10 2 20 3 30 4 40 dtype: int64 1 2 3 4 5  df1_2_3 = pd.concat( [df1, df2, df3], keys=[\u0026#39;7Jul\u0026#39;, \u0026#39;8Aug\u0026#39;, \u0026#39;9Sep\u0026#39;] ) print(df1_2_3.sample(5))    iid cid invoice_date total bill_ctry 8Aug 18 220 6 2011-08-22 5.94 Czech Republic 7Jul 28 371 8 2013-07-02 1.98 Belgium 8Aug 28 378 46 2013-08-02 1.98 Ireland 9Sep 27 386 27 2013-09-02 1.98 USA 23 309 22 2012-09-26 3.98 USA # \u0026lt;-- there is a Multi indexs 1 2 3 4  sumdf = df_1_2_3.groupby(level=0).agg(sum) print(sumdf) sumdf.plot(kind=\u0026#34;bar\u0026#34;) plt.show()    iid cid total 7Jul 7385 961 190.1 8Aug 7630 1170 198.1 9Sep 7417 961 196.2 Pivot 1 2 3 4 5 6 7 8 9 10 11 12 13  # Pivot table df.pivot_table( values=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;], index=[\u0026#34;col3\u0026#34;, \u0026#34;col4\u0026#34;], columns=[\u0026#34;col5\u0026#34;, \u0026#34;col6\u0026#34;], aggfunc=\u0026#39;mean\u0026#39;, fill_value=None, margins=False, dropna=True, margins_name=\u0026#39;All\u0026#39; ) df.pivot_table(\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;, \u0026#34;col3\u0026#34;) # values = col1, index = col2, columns = col3   Melt melt method will unpivot a table from wide to long format. This is often a much more computer-friendly format. Imagine a situation where you have merged many columns, making your table very wide. The merge() method can then be used to reshape that table into a more computer-friendly format.\n Recall that we call each row of the table an observation and each column a variable. The melt method integrates certain columns (name) into a variable column and the values of those columns into the value column.\n The first input argument to the method is id_vars. These are columns to be used as identifier variables. We can also think of them as columns in our original dataset that we do not want to change.\n1 2 3 4 5 6 7 8 9  df = pd.DataFrame( { \u0026#34;a\u0026#34; : np.random.randint(0,5, 20), \u0026#34;b\u0026#34; : np.random.randint(0,5, 20), \u0026#34;c\u0026#34; : np.random.randint(0,5, 20), \u0026#34;d\u0026#34; : np.random.randint(0,5, 20), } ) print(df.melt(id_vars=\u0026#34;a\u0026#34;).sample(5))    a variable value 56 4 d 2 17 3 b 3 27 4 c 2 47 4 d 2 10 0 b 1 1  print(df.melt(id_vars=[\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;]).sample(5))    a b variable value 12 3 1 c 1 19 2 1 c 0 4 3 1 c 1 32 3 1 d 1 5 3 1 c 3 The argument value_vars with the melt() will allow us to control which columns are unpivoted. If you set this argument, the others columns (except for id_vars and value_vars) will be ignored.\n1  print(df.melt(id_vars=\u0026#34;a\u0026#34;, value_vars=[\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;]).sample(10))    a variable value 2 3 b 2 32 3 c 1 18 2 b 3 16 4 b 2 3 2 b 2 37 3 c 1 21 2 c 1 19 2 b 1 34 4 c 0 28 3 c 1 The var_name argument will allow us to set the name of the variable column in the output. Similarly, the value_name argument will allow us to set the name of the value column in the output.\n1  print(df.melt(id_vars=\u0026#34;a\u0026#34;, value_vars=[\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;], var_name=\u0026#34;bc\u0026#34;, value_name=\u0026#34;va\u0026#34;).sample(5))    a bc va 13 2 b 3 7 4 b 4 36 4 c 1 38 2 c 0 9 3 b 3 Plot Histograms We can create a histogram of the variable by selecting the column and calling .hist().\nWe can adjust the number of bars, or bins, using the bins argument. Plots can also be layered on top of one another. We can use plt.legend(), passing in a list of labels, and then call show. We can use hist\u0026rsquo;s alpha argument, which takes a number. 0 means completely transparent that is, invisible, and 1 means completely opaque.\n1 2 3 4 5 6 7  # Histogram of conventional avg_price  avo[avo[\u0026#34;type\u0026#34;] == \u0026#34;conventional\u0026#34;][\u0026#34;avg_price\u0026#34;].hist(bins= 30, alpha=0.5) # Histogram of organic avg_price avo[avo[\u0026#34;type\u0026#34;] == \u0026#34;organic\u0026#34;][\u0026#34;avg_price\u0026#34;].hist(bins=30, alpha=0.5) # Add a legend plt.legend([\u0026#34;conventional\u0026#34;, \u0026#34;organic\u0026#34;])   You can also plot histograms for multiple variables at a time as follows: df[[\u0026quot;col1\u0026quot;, \u0026quot;col2\u0026quot;]].hist()\n1  avo[[\u0026#34;avg_price\u0026#34;, \u0026#34;nb_sold\u0026#34;]].hist()   Bar plots Bar plots can reveal relationships between a categorical variable and a numeric variable, like lang and rand1. we group by lang, select the rand1 column, and take the mean, giving us the average rand1 values of each lang.\nNow we can create a bar plot from the mean using the .plot() method, setting kind equal to bar. To add a title to our plot, we can use the title argument of the plot method. We may want to rotate the x-axis labels to make the text easier to read. This can be done by passing an angle in degrees with the rot argument\n1 2 3 4 5  # Get the total number of avocados sold of each size nb_sold_by_size = avo.groupby(by=\u0026#34;size\u0026#34;)[\u0026#34;nb_sold\u0026#34;].sum() # Create a bar plot of the number of avocados sold by size nb_sold_by_size.plot(kind=\u0026#34;bar\u0026#34;, rot=45)   You can set the color of the graph by setting the color argument. Color accepts scalar or lists.\n1 2 3  is_recession = [\u0026#39;r\u0026#39; if s==\u0026#39;recession\u0026#39; else \u0026#39;g\u0026#39; for s in gdp_recession[\u0026#39;econ_status\u0026#39;]] gdp_recession.plot(kind=\u0026#34;bar\u0026#34;, y=\u0026#34;gdp\u0026#34;, x=\u0026#34;date\u0026#34;, color=is_recession, rot=90) plt.show()   Line plots Line plots are great for visualizing changes in numeric variables over time. We can use the .plot() method again, but this time, we pass in three arguments: date as x, rand2 as y, and kind equals line\n1 2 3 4 5  # Get the total number of avocados sold on each date nb_sold_by_date = avo.groupby(by=\u0026#34;date\u0026#34;)[\u0026#34;nb_sold\u0026#34;].sum() # \u0026lt;-- note groupby date # Create a line plot of the number of avocados sold by date nb_sold_by_date.plot(kind=\u0026#34;line\u0026#34;, rot=45)   You can also draw multiple lines by setting a list of columns for the y argument:\n1 2  df.plot(y=[\u0026#34;close_jpm\u0026#34;, \u0026#34;close_wells\u0026#34;, \u0026#34;close_bac\u0026#34;]) plt.show()   Scatter plots Scatter plots are great for visualizing relationships between two numeric variables. we call the .plot() method with x equal to rand2, y equal to rand3, and kind equal to scatter.\n1 2 3 4 5 6 7  # Scatter plot of nb_sold vs avg_price with title avo.plot( x=\u0026#34;nb_sold\u0026#34;, y=\u0026#34;avg_price\u0026#34;, kind=\u0026#34;scatter\u0026#34;, title=\u0026#34;Number of avocados sold vs. average price\u0026#34; )   10. Merge  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # Inner Join df1_df2 = df1.merge(df2, on=\u0026#34;col\u0026#34;, suffixes=(\u0026#34;_l\u0026#34;, \u0026#34;_r\u0026#34;)) df1_df2 = df1.merge(df2, on=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;]) df1_df2 = df1.merge(df2, left_on=\u0026#34;col1\u0026#34;, right_on=\u0026#34;col2\u0026#34;) # The columns to be merged in the two tables have different names # Left Join df1_2 = df1.merge(df2, on=\u0026#34;col\u0026#34;, how=\u0026#34;left\u0026#34;) # default argument how is \u0026#34;inner\u0026#34; # Right Join df1_2 = df1.merge(df2, on=\u0026#34;col\u0026#34;, how=\u0026#34;right\u0026#34;) # Outer Join df1_2 = df1.merge(df2, on=\u0026#34;col\u0026#34;, how=\u0026#34;outer\u0026#34;) # Symmetric Difference df1_2 = df1.merge(df2, on=\u0026#34;col\u0026#34;, how=\u0026#34;outer\u0026#34;) m = (df1_2[\u0026#34;col1_x\u0026#34;].isna()) | (df1_2[\u0026#34;col1_y\u0026#34;].isna()) df1_2[m] # Self Join df_1s = df1.merge(df1, left_on=\u0026#34;c1\u0026#34;, right_on=\u0026#34;c3\u0026#34;, suffixes=(\u0026#34;_x\u0026#34;, \u0026#34;_y\u0026#34;))   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # Merge on Index df1.merge(df2, left_index=True, right_index=True) df1.merge(df2, on=\u0026#34;index_name\u0026#34;) df1.merge( df2, left_on=\u0026#34;left_index_name\u0026#34;, left_index=True, right_on=\u0026#34;right_index_name\u0026#34;, right_index=True, ) df1.merge( df2, left_on=\u0026#34;col_name\u0026#34;, right_on=\u0026#34;right_index_name\u0026#34;, right_index=True, )   1 2 3 4 5  # Merge multiple DataFrame df1_df2_df3 = df1.merge(df2, on=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;]).merge(df3, on=\u0026#34;col3\u0026#34;, suffixes=(\u0026#34;_2\u0026#34;, \u0026#34;_3\u0026#34;)) # e.t. (equal to) df1_df2_df3 = df1.merge(df2, on=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;])\\ # \u0026lt;-- note this backslash .merge(df3, on=\u0026#34;col3\u0026#34;, suffixes=(\u0026#34;_2\u0026#34;, \u0026#34;_3\u0026#34;))   1 2 3 4 5 6 7 8  # Merge with other operation df_1_2_3 = df_1.merge(df_2, on=\u0026#34;col1\u0026#34;).merge(df_3, on=\u0026#34;col2\u0026#34;) df_1_2_3.groupby(\u0026#34;col3\u0026#34;).agg({\u0026#39;col4\u0026#39;: \u0026#39;sum\u0026#39;}).plot(kind=\u0026#34;bar\u0026#34;) # e.t. df_1_2_3.groupby(\u0026#34;col3\u0026#34;).col4.agg(sum).plot(kind=\u0026#34;bar\u0026#34;) # e.t. df_1_2_3.groupby(\u0026#34;col3\u0026#34;).agg(sum).plot(kind=\u0026#34;bar\u0026#34;, y=\u0026#34;col4\u0026#34;)   11. Advanced Merge Semi Join A semi join filters the left table down to those observations that have a match in the right table. It is similar to an inner join where only the intersection between the tables is returned, but unlike an inner join, only the columns from the left table are shown. Finally, no duplicate rows from the left table are returned, even if there is a one-to-many relationship.\nSemi Join Three Steps:\n Merge the left and right tables on key column using an inner join; Search if the key column in the left table is in the merged tables using the .isin() method creating a Boolean Series; Subset the rows of left table.  1 2 3 4 5 6 7 8  # 1. df_merged = df1.merge(df2, on=\u0026#34;col1\u0026#34;) # 2. bs = df1[\u0026#34;col2\u0026#34;].isin(df_merged[\u0026#34;col2\u0026#34;]) # bs means boolean series # 3. df3 = df1[bs]   Anti Join An anti join returns the observations in the left table that do not have a matching observation in the right table. It also only returns the columns from the left table.\nWith indicator set to True, the merge method adds a column called \u0026ldquo;_merge\u0026rdquo; to the output. This column tells the source of each row. For example, if rows found a match in both tables, the _merge column shows both; if rows can only be found in the left table, the _merge column shows left_only. Obviously, right_only will not appear in a left join.\n1 2 3 4 5 6 7 8 9 10 11  # 1. df_merge = df1.merge(df2, on=\u0026#34;col1\u0026#34;, how=\u0026#34;left\u0026#34;, indicater=True) # 2. ls = df_merge.loc[ df_merge[\u0026#34;_merge\u0026#34;]==\u0026#34;left_only\u0026#34;, \u0026#34;col1\u0026#34; ] # 3. df3 = df1[df1[\u0026#34;col1\u0026#34;].isin(ls)]   Concatenate So far, we have only discussed how to merge two tables, which mainly grows them horizontally. Now we concern how to grow them vertically.\n1 2 3  pd.concat( [df1, df2, df3] )   Notice the column headers are the same. The result is a vertically combined table. Notice each table\u0026rsquo;s index value was retained.\n1 2 3 4  pd.concat( [df1, df2, df3], ignore_index=True )   If the index contains no valuable information, then we can ignore it in the concat method by setting ignore_index to True. The result is that the index will go from 0 to n-1.\nNow, suppose we wanted to associate specific keys with each of the pieces of our three original tables. We can provide a list of labels to the keys argument. Make sure that ignore_index argument is False, since you can\u0026rsquo;t add a key and ignore the index at the same time. This results in a table with a multi-index, with the label on the first level.\n1 2 3 4 5  pd.concat( [df1, df2, df3], ignore_index=False, keys=[\u0026#34;df1\u0026#34;, \u0026#34;df2\u0026#34;, \u0026#34;df3\u0026#34;] )   When we need to combine tables that have different column names, the concat method by default will include all of the columns in the different tables it\u0026rsquo;s combining. If we only want the matching columns between tables, we set the join argument to \u0026ldquo;inner\u0026rdquo;. Its default value is equal to \u0026ldquo;outer\u0026rdquo;\n1 2 3 4  pd.concat( [df1, df2, df3], join=\u0026#34;inner\u0026#34; )   Append is a simplified concat method. It supports the ignore_index argument. However, it does not support keys or join. Join is always set to outer.\n1 2 3 4  df1.append( [df2, df3], ignore_index=True )   Verifying Integrity Both the merge and concat methods have special features that allow us to verify the structure of our data. The validate and verify_integrity arguments of the merge and concat methods respectively will allow us to verify the data.\nWhen merging two tables, we might expect the tables to have a one-to-one relationship. However, one of the columns we are merging on may have a duplicated value, which will turn the relationship into a one-to-many.\nIf we provide the validate argument one of these key strings:\n one_to_one one_to_many many_to_one many_to_many  it will validate the relationship between the two tables. For example, if we specify we want a one-to-one relationship, but it turns out the relationship is not one-to-one, then an error is raised.\n1 2 3 4 5 6 7 8 9 10 11 12 13  df1 = pd.DataFrame( { \u0026#34;a\u0026#34;: np.arange(5), \u0026#34;b\u0026#34;: list(\u0026#34;adfgh\u0026#34;) } ) df2 = pd.DataFrame( { \u0026#34;a\u0026#34;: [0, 1, 2, 3, 3], \u0026#34;b\u0026#34;: list(\u0026#34;tvdsv\u0026#34;) } ) df1.merge(df2, on=\u0026#34;a\u0026#34;, validate=\u0026#34;one_to_one\u0026#34;)   MergeError: Merge keys are not unique in right dataset; not a one-to-one merge When concatenating tables vertically, we might unintentionally create duplicate records if a record exists in both tables.\n Notice, the recode represent index, not columns values.\n concat method has the argument verify_integrity, which by default is False. However, if set to True, it will check if there are duplicate values in the index and raise an error if there are. It will only check the index values and not the columns.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  df1 = pd.DataFrame( { \u0026#34;a\u0026#34;: np.random.rand(5), \u0026#34;b\u0026#34;: np.random.rand(5), \u0026#34;c\u0026#34;: np.random.rand(5), }, index=np.arange(5) ) df2 = pd.DataFrame( { \u0026#34;a\u0026#34;: np.random.rand(5), \u0026#34;b\u0026#34;: np.random.rand(5), \u0026#34;c\u0026#34;: np.random.rand(5), }, index=np.arange(4,9) ) print(df1, \u0026#39;\\n\u0026#39;, df2)    a b c 0 0.131960 0.559110 0.803359 1 0.862007 0.349336 0.594149 2 0.805661 0.313103 0.937367 3 0.893554 0.633904 0.475403 4 0.381487 0.096156 0.219184 a b c 4 0.469545 0.142471 0.906317 5 0.035282 0.481996 0.207780 6 0.241841 0.355997 0.599351 7 0.684347 0.592812 0.833834 8 0.036764 0.347563 0.895287 1 2 3 4  pd.concat( [df1, df2], verify_integrity=True )   ValueError: Indexes have overlapping values: Int64Index([4], dtype='int64') .merge_ordered() The merge_ordered method are similar to the standard merge method with an outer join, but the results are sorted. The sorted results make this a useful method for ordered or time-series data.\nIt has many of the same arguments we have already covered with the merge method:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  pd.merge_ordered( df1, df2, # \u0026lt;-- no bracket  on=\u0026#34;col\u0026#34;, suffixes(\u0026#34;_l\u0026#34;, \u0026#34;_r\u0026#34;) ) pd.merge_ordered( df1, df2, left_on=\u0026#34;col1\u0026#34;, right_on=\u0026#34;col2\u0026#34;, how=\u0026#34;left\u0026#34;) pd.merge_ordered( df1, df2, on=[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;] # Note the difference with `on=[\u0026#34;col2\u0026#34;, \u0026#34;col1\u0026#34;]` )   # on=[\u0026quot;date\u0026quot;, \u0026quot;country\u0026quot;] date country gdp series_code_x pop series_code_y 0 1990-01-01 Australia 158051.132 NYGDPMKTPSAKD 17065100 SP.POP.TOTL 1 1990-01-01 Sweden 79837.846 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 2 1990-04-01 Australia 158263.582 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 3 1990-04-01 Sweden 80582.286 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 4 1990-07-01 Australia 157329.279 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 5 1990-07-01 Sweden 79974.360 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 6 1990-09-01 Australia 158240.678 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 7 1990-09-01 Sweden 80106.497 NYGDPMKTPSAKD 8558835 SP.POP.TOTL 8 1991-01-01 Australia 156195.954 NYGDPMKTPSAKD 17284000 SP.POP.TOTL 9 1991-01-01 Sweden 79524.242 NYGDPMKTPSAKD 8617375 SP.POP.TOTL # on=[\u0026quot;country\u0026quot;, \u0026quot;date\u0026quot;] date country gdp series_code_x pop series_code_y 0 1990-01-01 Australia 158051.132 NYGDPMKTPSAKD 17065100 SP.POP.TOTL 1 1990-04-01 Australia 158263.582 NYGDPMKTPSAKD 17065100 SP.POP.TOTL 2 1990-07-01 Australia 157329.279 NYGDPMKTPSAKD 17065100 SP.POP.TOTL 3 1990-09-01 Australia 158240.678 NYGDPMKTPSAKD 17065100 SP.POP.TOTL 4 1991-01-01 Australia 156195.954 NYGDPMKTPSAKD 17284000 SP.POP.TOTL 5 1991-04-01 Australia 155989.033 NYGDPMKTPSAKD 17284000 SP.POP.TOTL 6 1991-07-01 Australia 156635.858 NYGDPMKTPSAKD 17284000 SP.POP.TOTL 7 1991-09-01 Australia 156744.057 NYGDPMKTPSAKD 17284000 SP.POP.TOTL 8 1992-01-01 Australia 157916.081 NYGDPMKTPSAKD 17495000 SP.POP.TOTL 9 1992-04-01 Australia 159047.827 NYGDPMKTPSAKD 17495000 SP.POP.TOTL We can fill in the missing data by setting the fill_method argument to \u0026ldquo;ffill\u0026rdquo; for forward fill. It will interpolate missing data by filling the missing values with the previous value (the upper row).\n1 2 3 4 5  pd.merge_ordered( df1, df2, on=\u0026#34;col\u0026#34;, fill_method=\u0026#34;ffill\u0026#34; )   .merge_asof() The merge_asof() method is similar to an ordered left join. It has similar features as merge_ordered(). However, unlike an ordered left join, merge_asof() will match on the nearest value columns rather than equal values.\nFor each row in the left DataFrame, merge_asof() select the last row in the right DataFrame where the value for the key is less than or equal the value for the left key. This brings up an important point - whatever columns you merge on must be sorted. (left and right key columns must be sorted)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  df1 = pd.DataFrame( { \u0026#34;a\u0026#34;: [1, 3, 4, 7, 5], \u0026#34;b\u0026#34;:np.arange(5) } ) df2 = pd.DataFrame( { \u0026#34;a\u0026#34;: [1, 2, 3, 8, 5], \u0026#34;b\u0026#34;:np.arange(5) } ) df_1_2 = pd.merge_asof( df1.sort_values(by=\u0026#34;a\u0026#34;), df2.sort_values(by=\u0026#34;a\u0026#34;), on=\u0026#34;a\u0026#34; ) print(df1.sort_values(by=\u0026#34;a\u0026#34;),\u0026#39;\\n\u0026#39;, df2.sort_values(by=\u0026#34;a\u0026#34;), \u0026#39;\\n\u0026#39;,df_1_2)    a b 0 1 0 1 3 1 2 4 2 4 5 4 3 7 3 a b 0 1 0 1 2 1 2 3 2 4 5 4 3 8 3 a b_x b_y 0 1 0 0 1 3 1 2 2 4 2 2 3 5 4 4 4 7 3 4 Setting the direction argument as \u0026ldquo;forward\u0026rdquo; will change the behavior of the method to select the row in the right table. The default value for the direction argument is \u0026ldquo;backward\u0026rdquo;. Setting the direction argument as nearest to match rows with the nearest times in either direction.\n1 2 3 4 5  pd.merge_asof( df1, df2, on=\u0026#34;col1\u0026#34;, direction=\u0026#34;forward\u0026#34; )   ","date":"2020-08-14T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/feature-engineering-cleaning/","title":"Feature Engineering (Cleaning)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Importing data from the Internet Importing flat files from the web ","date":"2020-05-01T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/importing-data-in-python-ii/","title":"Importing Data in Python II"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n 1. Importing data from flat files We\u0026rsquo;ll discuss how to import data from 3 kind of data sources\n flat files such as dot txts and dot csvs; files native to other software such as Excel spreadsheets, Stata, SAS and MATLAB files; relational databases such as SQLite \u0026amp; PostgreSQL.  1.1 Importing flat files using open() To check out any plain text file, you can use Pythons basic open() function to open a connection to the file. To do so, you pass the filename to the function open() and also pass it the argument mode=\u0026quot;r\u0026quot;, which makes sure that we can only read it (we wouldn\u0026rsquo;t want to accidentally write to it!), assign the text from the file to a variable by applying the method read() to the connection to the file. After you do this, make sure that you close the connection to the file using the close() method.\n1 2 3 4 5  file = open(\u0026#34;moby_dick.txt\u0026#34;, mode=\u0026#34;r\u0026#34;) print(file.read()[0:200]) print(file.closed) # Check whether the file is closed file.close() print(file.closed)   CHAPTER 1. Loomings. Call me Ishmael. Some years ago--never mind how long precisely--having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail abou False True  If you wanted to open a file in order to write to it, you would pass it the argument mode=\u0026quot;w\u0026quot;.\n using a with statement. This allows you to create a context in which you can execute commands with the file open. Once out of this context, the file will be closed, for this reason, it is called a Context Manager.\n1 2 3 4 5 6  with open(\u0026#39;moby_dick.txt\u0026#39;) as file: print(file.readline()) print(file.readline()) print(file.readline()) print(file.readline()) print(file.readline())   CHAPTER 1. Loomings. Call me Ishmael. Some years ago--never mind how long precisely--having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of 1.2 Importing flat files using NumPy If all the data are numerical, you can use the package numpy to import the data as a numpy array with the NumPy functions loadtxt( ). We call loadtxt() and pass it the filename as the first argument, along with the delimiter as the 2nd argument. Note that the default delimiter is any white space so well usually need to specify it explicitly.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  import numpy as np file = \u0026#39;digits.csv\u0026#39; digits = np.loadtxt(file, delimiter=\u0026#34;,\u0026#34;) print(type(digits), digits.shape) fig, ax = plt.subplots(5,5) for i in range(5): for j in range(5): im = digits[np.random.randint(0,100), 1:] im_sq = np.reshape(im, (28, 28)) ax[i][j].imshow( im_sq, cmap=\u0026#39;Greys\u0026#39;, interpolation=\u0026#39;nearest\u0026#39; ) plt.show()   \u0026lt;class 'numpy.ndarray'\u0026gt; (100, 785) You can skip the first row by calling loadtxt with the argument skiprows=1; You can select the 1st and 3rd columns of the data, by setting usecols=[0,2]. You can also import different datatypes into NumPy arrays: for example, setting the argument dtype=\u0026quot;str\u0026quot; will ensure that all entries are imported as strings.\n1 2 3 4 5  import numpy as np file = \u0026#39;digits_header.txt\u0026#39; data = np.loadtxt(file, delimiter=\u0026#34;\\t\u0026#34;, skiprows=1, usecols=[0,2]) print(data[:5])   [[1. 0.] [0. 0.] [1. 0.] [4. 0.] [0. 0.]] Notice that loadtxt tends to break down when we have mixed datatypes, for example, columns consisting of floats and columns consisting of strings. And you should import in different datatypes respectively.\n1 2 3 4 5 6 7 8 9 10 11  file = \u0026#39;seaslug.txt\u0026#39; data = np.loadtxt(file, delimiter=\u0026#39;\\t\u0026#39;, dtype=str) print(data[0]) data_float = np.loadtxt(file, delimiter=\u0026#34;\\t\u0026#34;, dtype=float, skiprows=1) print(data_float[9]) plt.scatter(data_float[:, 0], data_float[:, 1]) plt.xlabel(\u0026#39;time (min.)\u0026#39;) plt.ylabel(\u0026#39;percentage of larvae\u0026#39;) plt.show()   ['Time' 'Percent'] [0. 0.357] Much of the time you will need to import datasets which have different datatypes in different columns; one column may contain strings and another floats, for example. The function np.loadtxt() will freak at this. There is another function, genfromtxt( ), which can handle such structures. If we pass dtype=None to it, it will figure out what types each column should be.\n1 2 3 4 5 6 7  data = np.genfromtxt( \u0026#39;titanic.csv\u0026#39;, delimiter=\u0026#39;,\u0026#39;, names=True, dtype=None ) data[0:10]   array([( 1, 0, 3, b'male', 22., 1, 0, b'A/5 21171', 7.25 , b'', b'S'), ( 2, 1, 1, b'female', 38., 1, 0, b'PC 17599', 71.2833, b'C85', b'C'), ( 3, 1, 3, b'female', 26., 0, 0, b'STON/O2. 3101282', 7.925 , b'', b'S'), ( 4, 1, 1, b'female', 35., 1, 0, b'113803', 53.1 , b'C123', b'S'), ( 5, 0, 3, b'male', 35., 0, 0, b'373450', 8.05 , b'', b'S'), ( 6, 0, 3, b'male', nan, 0, 0, b'330877', 8.4583, b'', b'Q'), ( 7, 0, 1, b'male', 54., 0, 0, b'17463', 51.8625, b'E46', b'S'), ( 8, 0, 3, b'male', 2., 3, 1, b'349909', 21.075 , b'', b'S'), ( 9, 1, 3, b'female', 27., 0, 2, b'347742', 11.1333, b'', b'S'), (10, 1, 2, b'female', 14., 1, 0, b'237736', 30.0708, b'', b'C')], dtype=[('PassengerId', '\u0026lt;i8'), ('Survived', '\u0026lt;i8'), ('Pclass', '\u0026lt;i8'), ('Sex', 'S6'), ('Age', '\u0026lt;f8'), ('SibSp', '\u0026lt;i8'), ('Parch', '\u0026lt;i8'), ('Ticket', 'S18'), ('Fare', '\u0026lt;f8'), ('Cabin', 'S15'), ('Embarked', 'S1')]) In addition to genfromtxt, the numpy module provides several convenience functions derived from genfromtxt. These functions work the same way as the original, but they have different default values.\n  recfromtxt Returns a standard numpy.recarray (if usemask=False) or a MaskedRecords array (if usemaske=True). The default dtype is dtype=None, meaning that the types of each column will be automatically determined.\n  recfromcsv Like recfromtxt, but with a default delimiter=\u0026quot;,\u0026quot;.\n  1 2 3  file = \u0026#39;titanic.csv\u0026#39; d= np.recfromcsv(file) print(d[:3])   [(1, 0, 3, b'male', 22., 1, 0, b'A/5 21171', 7.25 , b'', b'S') (2, 1, 1, b'female', 38., 1, 0, b'PC 17599', 71.2833, b'C85', b'C') (3, 1, 3, b'female', 26., 0, 0, b'STON/O2. 3101282', 7.925 , b'', b'S')] 1.3 Importing flat files using Pandas If we wish to import a CSV in the most basic case all we need to do is to call the function read_csv( ) and supply it with a single argument, the name of the file, it will return a DataFrame object of the file.\n As Hadley Wickham tweeted, \u0026ldquo;A matrix has rows and columns. A data frame has observations and variables.\u0026rdquo;\n 1 2 3 4 5  file = \u0026#39;digits.csv\u0026#39; # Read the first 5 rows of the file except header into a DataFrame: data data=pd.read_csv(file, nrows=5, header=None) data_array=np.array(data) print(type(data_array))   \u0026lt;class 'numpy.ndarray'\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  import matplotlib.pyplot as plt file = \u0026#39;titanic_corrupt.txt\u0026#39; data = pd.read_csv( file, sep=\u0026#34;\\t\u0026#34;, # \u0026lt;-- the pandas version of delim comment=\u0026#34;#\u0026#34;, # \u0026lt;-- indicates remainder of line should not be parsed. na_values=\u0026#34;Nothing\u0026#34; # \u0026lt;-- takes str (in this case, \u0026#34;Nothing\u0026#34;) to recognize as NA/NaN ) data_origin = pd.read_csv(file) print(data.head()) print(data_origin.head()) pd.DataFrame.hist(data[[\u0026#39;Age\u0026#39;]]) plt.xlabel(\u0026#39;Age (years)\u0026#39;) plt.ylabel(\u0026#39;count\u0026#39;) plt.show()   PassengerId Survived Pclass Sex Age ... Parch Ticket Fare Cabin Embarked 0 1 0 3 male 22.0 ... 0 A/5 21171 7.250 NaN S 1 2 1 1 female 38.0 ... 0 PC 17599 NaN NaN NaN # \u0026lt;-- Notice here 2 3 1 3 female 26.0 ... 0 STON/O2. 3101282 7.925 NaN S 3 4 1 1 female 35.0 ... 0 113803 53.100 C123 S 4 5 0 3 male 35.0 ... 0 373450 8.050 NaN S PassengerId\\tSurvived\\tPclass\\tSex\\tAge\\tSibSp\\tParch\\tTicket\\tFare\\tCabin\\tEmbarked 0 1\\t0\\t3\\tmale\\t22.0\\t1\\t0\\tA/5 21171\\t7.25\\tNo... 1 2\\t1\\t1\\tfemale\\t38.0\\t1\\t0\\tPC 17599#to\\t71.2... # \u0026lt;-- Notice here \u0026quot;#\u0026quot; 2 3\\t1\\t3\\tfemale\\t26.0\\t0\\t0\\tSTON/O2. 3101282\\... 3 4\\t1\\t1\\tfemale\\t35.0\\t1\\t0\\t113803\\t53.1\\tC12... 4 5\\t0\\t3\\tmale\\t35.0\\t0\\t0\\t373450\\t8.05\\t\\tS 2. Importing data from other file types 2.1 Importing Pickle and Excel file Pickle Files is a file type native to Python, which can store data-types in python such as dictionaries, tuples, lists etc. It can serialize objects so that they can be saved into a file and loaded again later. when opening such a file, you can specify that it is read only and is a binary file, by passing the string \u0026lsquo;rb\u0026rsquo; as the second argument of open.\n1 2 3 4 5 6 7 8 9 10 11  d = dict(name=\u0026#39;Bob\u0026#39;, age=20, score=88) print(d) f = open(\u0026#39;dump.pkl\u0026#39;, \u0026#39;wb\u0026#39;) # wirte pickle.dump(d,f) f.close() # read g = open(\u0026#39;./dump.pkl\u0026#39;, \u0026#39;rb\u0026#39;) e=pickle.load(g) g.close() print(e)   {'name': 'Bob', 'age': 20, 'score': 88} {'name': 'Bob', 'age': 20, 'score': 88} 1  ! hexdump -x ./dump.pkl   0000000 0480 2495 0000 0000 0000 7d00 2894 048c 0000010 616e 656d 8c94 4203 626f 8c94 6103 6567 0000020 4b94 8c14 7305 6f63 6572 4b94 7558 002e 000002f An Excel working book generally consists of a number of working sheets, and each working sheets has a mass of working cells. There are many ways (like openpyxl and pyexcel module) to import Excel files and we\u0026rsquo;ll use pandas to do so because it produces dataframes natively.\nWe can use the function Excelfile to assign an Excel file to a variable data. As an Excel file consists of sheets, the first thing to do is figure out what the sheets are. This is straightforward with the data.sheet_names attribute.\n1 2  data = pd.ExcelFile(\u0026#34;/Users/wanghaoming/Documents/gpa.xlsx\u0026#34;) data.sheet_names   ['Sheet1', 'Sheet2', 'Sheet3', 'Sheet4', 'Sheet5', 'Sheet6'] To then load a particular sheet as a dataframe, we need only apply the method parse() to the object data with a single argument, which is either the name as a string or the index as a float of the sheet.\n1 2 3 4 5 6  df6 = data.parse( sheet_name=5, usecols=\u0026#34;A:F\u0026#34;, nrows=12 ) print(df6)       .1 .1 .1 0  3 97  4 93 1  4 96  4 93 2  4 96  5 93 3  4 96  4 93 4  3 95  4 92 5  4 95  4 91 6  4 94  5 91 7  4 94  4 91 8  4 94  3 90 9  3 94  4 90 10  4 94  4 90 11  4 93  4 80  pd.ExcelFile.parse() is equivalent to pd.read_excel method.\n 2.2 Importing SAS/Stata files The most common SAS files have the extension .sas7bdat and .sas7bcat, which are dataset files and catalog files respectively. We can import the former as DataFrames using the function SAS7BDAT (upper case) from the package sas7bdat (lower case). We can bind the variable file to a connection to the file \u0026lsquo;xxx.sas7bdat\u0026rsquo; in a context manager. Within this context, We can assign to a variable df_sas the result of applying method to_data_frame to file.\n1 2 3 4 5 6 7 8 9  from sas7bdat import SAS7BDAT with SAS7BDAT(\u0026#39;sales.sas7bdat\u0026#39;) as file: df_sas = file.to_data_frame() print(df_sas.head()) pd.DataFrame.hist(df_sas[[\u0026#39;P\u0026#39;]]) plt.ylabel(\u0026#39;count\u0026#39;) plt.show()    The other way to import sas (xport and sas7bdat) file is pd.read_sas.\n Stata files have extension .dta and we can import them using pandas. We don\u0026rsquo;t even need to initialize a context manager in this case. We merely pass the filename to the function pd.read_stata and assign it to a variable.\n1 2 3 4 5 6 7 8 9  import pandas as pd df = pd.read_stata(\u0026#39;disarea.dta\u0026#39;) print(df.head()) pd.DataFrame.hist(df[[\u0026#39;disa10\u0026#39;]]) plt.xlabel(\u0026#39;Extent of disease\u0026#39;) plt.ylabel(\u0026#39;Number of countries\u0026#39;) plt.show()   2.3 Importing HDF5 files  \u0026ldquo;In the Python world, consensus is rapidly converging on Hierarchical Data Format version 5, or \u0026lsquo;HDF5,\u0026rsquo; as the standard mechanism for storing large quantities of numerical data.\u0026rdquo; \u0026mdash;- 2013 O\u0026rsquo;Reilly book Python and HDF5 by Andrew Collette,\n Intro to HDF5 i. Group \u0026amp; Dataset The Hierarchical Data Format version 5 (HDF5) format can be thought of as a file system contained and described within one single file. Think about the files and folders stored on your computer. You might have a data directory with some temperature data for multiple field sites. These temperature data are collected every minute and summarized on an hourly, daily and weekly basis. Within one HDF5 file, you can store a similar set of data organized in the same way that you might organize files and folders on your computer. However in a HDF5 file, what we call \u0026ldquo;directories\u0026rdquo; or \u0026ldquo;folders\u0026rdquo; on our computers, are called groups and what we call files on our computer are called datasets.\n Group: A folder like element within an HDF5 file that might contain other groups or datasets within it. Dataset: The actual data contained within the HDF5 file. Datasets are often (but don\u0026rsquo;t have to be) stored within groups in the file.  ii. Metadata HDF5 format is self describing. This means that each file, group and dataset can have associated metadata that describes exactly what the data are. One key benefit of having metadata that are attached to each file, group and dataset, is that this facilitates automation without the need for a separate (and additional) metadata document. Using a programming language, like R or Python, we can grab information from the metadata that are already associated with the dataset, and which we might need to process the dataset.\niii. Compressed \u0026amp; Subsetting The HDF5 format is a compressed format. The size of all data contained within HDF5 is optimized which makes the overall file size smaller. Even when compressed, however, HDF5 files often contain big data and can thus still be quite large. A powerful attribute of HDF5 is data slicing, by which a particular subsets of a dataset can be extracted for processing. This means that the entire dataset doesn\u0026rsquo;t have to be read into memory (RAM); very helpful in allowing us to more efficiently work with very large (gigabytes or more) datasets!\niv. Heterogeneous HDF5 files can store many different types of data within in the same file. For example, one group may contain a set of datasets to contain integer (numeric) and text (string) data. Or, one dataset can contain heterogeneous data types (e.g., both text and numeric data even images in one dataset).\nv. Summary Points  Self-Describing: The datasets with an HDF5 file are self describing. This allows us to efficiently extract metadata without needing an additional metadata document. Supporta Heterogeneous Data: Different types of datasets can be contained within one HDF5 file. Supports Large, Complex Data: HDF5 is a compressed format that is designed to support large, heterogeneous, and complex datasets. Supports Data Slicing: \u0026ldquo;Data slicing\u0026rdquo;, or extracting portions of the dataset as needed for analysis, means large files don\u0026rsquo;t need to be completely read into the computers memory or RAM.  h5py package We import the package h5py and then import the file using h5py.File function, remembering to use \u0026lsquo;r\u0026rsquo; in order to specify read only.\nYou can explore HDF5\u0026rsquo;s hierarchical structure as you would that of a Python dictionary using the method keys. Each of keys is an HDF group. You can think of these groups as directories too.\n1 2 3 4 5 6 7 8  import numpy as np import h5py file = \u0026#34;LIGO_data.hdf5\u0026#34; data = h5py.File(file, \u0026#34;r\u0026#34;) print(type(data)) for key in data.keys(): print(key)   \u0026lt;class 'h5py._hl.files.File'\u0026gt; meta quality strain 1 2 3 4 5 6 7 8 9 10 11 12 13  group = data[\u0026#39;strain\u0026#39;] for key in group.keys(): print(key) strain = data[\u0026#39;strain\u0026#39;][\u0026#39;Strain\u0026#39;].value num_samples = 10000 time = np.arange(0, 1, 1/num_samples) plt.plot(time, strain[:num_samples]) plt.xlabel(\u0026#39;GPS Time (s)\u0026#39;) plt.ylabel(\u0026#39;strain\u0026#39;) plt.show()   Strain 2.4 Importing MATLAB files The standard library scipy.io has functions loadmat and savemat, which allow us to read and write .mat files, respectively.\nA .mat file is simply a collection of many objects like strings, floats, vectors and arrays. This means when importing a .mat file in Python, we should expect to see a number of different variables and objects.\nWe first import scipy.io and then load the .mat file. Then a dictionary object results returns. How this dictionary relates to a MATLAB workspace is straightforward: the keys of the Python dictionary are the MATLAB variable names and the values of the Python dictionary are the objects that are assigned to the variables.\n1 2 3 4  import scipy.io mat = scipy.io.loadmat(\u0026#34;albeck_gene_expression.mat\u0026#34;) print(type(mat))   \u0026lt;class 'dict'\u0026gt; 1 2 3 4 5 6 7 8 9 10  print(mat.keys()) print(type(mat[\u0026#34;CYratioCyt\u0026#34;])) print(mat[\u0026#34;CYratioCyt\u0026#34;].shape) data = mat[\u0026#39;CYratioCyt\u0026#39;][25, 5:] fig = plt.figure() plt.plot(data) plt.xlabel(\u0026#39;time (min.)\u0026#39;) plt.ylabel(\u0026#39;normalized fluorescence (measure of expression)\u0026#39;) plt.show()   dict_keys(['__header__', '__version__', '__globals__', 'rfpCyt', 'rfpNuc', 'cfpNuc', 'cfpCyt', 'yfpNuc', 'yfpCyt', 'CYratioCyt']) \u0026lt;class 'numpy.ndarray'\u0026gt; (200, 137) 3. Relational databases in Python 3.1 Creating a database engine in Python Intro to Relational database Relational database is a type of database that is based upon the Relational model of data, first described by Ted Codd in the late 1960s.\nA database consists of tables. A table generally represents one entity type. Notice that the table looks a great deal like a dataframe. In a relational database table, each row or record represents an instance of the entity type. Each column represents an attribute of each instance. In this sense, a table is entirely analogous to a dataframe. It is essential that each row contain a unique identifier, known as a primary key, that we can use to explicitly access the row in question.\nThe really cool thing about relational databases is not merely that you have a bunch of tables, but that the tables are linked. Some column in a table corresponds precisely to the primary keys in other tables. This is cool because it means that you don\u0026rsquo;t need to store all the details in one table, it saves an incredible amount of space.\nBuild engine We\u0026rsquo;ll use an SQLite database as an example because SQLite is fast and simple while still containing enough functionality to introduce the necessary concepts of querying a database. There are many packages we could use to access an SQLite database such as sqlite3 and SQLAlchemy. We\u0026rsquo;ll use SQLAlchemy as it works with many other Relational Database Management Systems, such as Postgres and MySQL.\nTo connect to database, we need to import the relevant funtion create_engine from the package sqlalchemy. We then use the function create_engine to fire up an SQL engine that will communicate our queries to the database. The only required argument of create_engine is a string that indicates the type of database you\u0026rsquo;re connecting to and the name of the database.\n1 2 3  from sqlalchemy import create_engine engine = create_engine(\u0026#34;sqlite:///Chinook.sqlite\u0026#34;)   We would like to know the names of the tables it contains. To do this, apply the method table_names to the object engine. This will return a list of the table names.\n1 2 3 4 5  from sqlalchemy import create_engine engine = create_engine(\u0026#34;sqlite:///Chinook.sqlite\u0026#34;) table_names = engine.table_names() print(table_names)   ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track'] 3.2 Querying relational databases in Python The workflow of SQL querying will be as follows.\n import the required packages and functions; create the engine; Create the engine object using the function create_engine(). connect to it; To connect to the database after creating the engine, you create a connection object by applying the method connect( ) to the engine object. query the database ; To query the DB, apply the method execute() to the connection object and pass it the relevant SQL query. The query SELECT * FROM Table_Name, where Table_name is the name of any of the tables in the database, returns all columns of all rows of the Table of interest. save the results to a dataframe; query manipulation creates a sqlalchemy results object. To turn the results object into a dataframe, we apply the method fetchmany(size=None) or fetchall( ) to result object and save it as a dataframe using the pandas function DataFrame(). Don\u0026rsquo;t forget setting dataframe columns name by df.columns = rs.keys() close the connection. To close the connection, execute close() method on connection object.  1 2 3 4 5 6 7 8 9 10 11  from sqlalchemy import create_engine import pandas as pd # step 1. import package engine = create_engine(\u0026#39;sqlite:///Chinook.sqlite\u0026#39;) # step2. create engine con = engine.connect() # step3. create connection rs = con.execute(\u0026#34;select * from Album\u0026#34;) # step4. query df = pd.DataFrame(rs.fetchall()) # step5. save result  df.columns = rs.keys() con.close() # step6. close connection print(df.head())    AlbumId Title ArtistId 0 1 For Those About To Rock We Salute You 1 1 2 Balls to the Wall 2 2 3 Restless and Wild 2 3 4 Let There Be Rock 1 4 5 Big Ones 3 You can also leverage the context manager construct to open a connection, which will save you the trouble of closing the connection.\n1 2 3 4 5 6 7 8  engine = create_engine(\u0026#39;sqlite:///Chinook.sqlite\u0026#39;) with engine.connect() as con: rs = con.execute(\u0026#34;select LastName, Title from Employee\u0026#34;) df = pd.DataFrame(rs.fetchmany(size=3)) df.columns = rs.keys() print(len(df)) print(df.head())   3 LastName Title 0 Adams General Manager 1 Edwards Sales Manager 2 Peacock Sales Support Agent 1 2 3 4 5 6 7 8 9 10 11 12 13  def query_sql(dbtype, dbname, query): engine = create_engine(dbtype+\u0026#34;:///\u0026#34;+dbname) with engine.connect() as con: rs = engine.execute(query) df = pd.DataFrame(rs.fetchall()) df.columns = rs.keys() return df df = query_sql( dbtype=\u0026#34;sqlite\u0026#34;, dbname=\u0026#34;Chinook.sqlite\u0026#34;, query=\u0026#34;select * from Employee where ReportsTo=2.0 order by BirthDate\u0026#34; ) print(df.head())   FirstName Title ReportsTo ... Country PostalCode Phone Fax Email 0 4 Park Margaret Sales Support Agent 2 ... Canada T2P 5G3 +1 (403) 263-4423 +1 (403) 263-4289 margaret@chinookcorp.com 1 5 Johnson Steve Sales Support Agent 2 ... Canada T3B 1Y7 1 (780) 836-9987 1 (780) 836-9543 steve@chinookcorp.com 2 3 Peacock Jane Sales Support Agent 2 ... Canada T2P 5M5 +1 (403) 262-3443 +1 (403) 262-6712 jane@chinookcorp.com 3.3 Querying relational databases directly with pandas You can actually query datebase in 2 line, utilizing the pandas function read_sql_query and passing it 2 arguments. The first argument will be the query you wish to make, the 2nd argument the engine you want to connect to.\n1 2 3 4 5 6  from sqlalchemy import create_engine import pandas as pd engine = create_engine(\u0026#34;sqlite:///Chinook.sqlite\u0026#34;) df = pd.read_sql_query(\u0026#34;select * from Album\u0026#34;, engine) print(df.head())    AlbumId Title ArtistId 0 1 For Those About To Rock We Salute You 1 1 2 Balls to the Wall 2 2 3 Restless and Wild 2 3 4 Let There Be Rock 1 4 5 Big Ones 3 1 2 3 4 5 6 7 8  from sqlalchemy import create_engine import pandas as pd engine = create_engine(\u0026#34;sqlite:///Chinook.sqlite\u0026#34;) query = \u0026#34;select * from Employee where EmployeeId \u0026gt;= 6 order by BirthDate\u0026#34; df = pd.read_sql_query(query, engine) print(df.head())   EmployeeId LastName FirstName Title ReportsTo ... Country PostalCode Phone Fax Email 0 8 Callahan Laura IT Staff 6 ... Canada T1H 1Y8 +1 (403) 467-3351 +1 (403) 467-8772 laura@chinookcorp.com 1 7 King Robert IT Staff 6 ... Canada T1K 5N8 +1 (403) 456-9986 +1 (403) 456-8485 robert@chinookcorp.com 2 6 Mitchell Michael IT Manager 1 ... Canada T3B 0C5 +1 (403) 246-9887 +1 (403) 246-9899 michael@chinookcorp.com Recall SQL inner join syntax:\n1  select*fromtable1innerjointable2ontable1.col1=table2.col2  1 2 3 4 5 6  with engine.connect() as con: rs = con.execute(\u0026#34;select Title, Name from Album inner join Artist on Album.ArtistID=Artist.ArtistID\u0026#34;) df = pd.DataFrame(rs.fetchall()) df.columns = rs.keys() print(df.head())    Title Name 0 For Those About To Rock We Salute You AC/DC 1 Balls to the Wall Accept 2 Restless and Wild Accept 3 Let There Be Rock AC/DC 4 Big Ones Aerosmith 1 2 3 4 5 6  df = pd.read_sql_query( \u0026#34;select * from PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId where Milliseconds \u0026lt; 250000\u0026#34;, engine ) print(df.head())    PlaylistId TrackId TrackId Name AlbumId ... GenreId Composer Milliseconds Bytes UnitPrice 0 1 3390 3390 One and the Same 271 ... 23 None 217732 3559040 0.99 1 1 3392 3392 Until We Fall 271 ... 23 None 230758 3766605 0.99 2 1 3393 3393 Original Fire 271 ... 23 None 218916 3577821 0.99 3 1 3394 3394 Broken City 271 ... 23 None 228366 3728955 0.99 4 1 3395 3395 Somedays 271 ... 23 None 213831 3497176 0.99 ","date":"2020-04-26T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/importing-data-in-python-i/","title":"Importing Data in Python I"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n [toc]\nSeaborn Introduction Distribution plot The pandas library supports simple plotting of data, which is very convenient when data is already likely to be in a pandas DataFrame.\n1 2 3 4  df[\u0026#39;Award_Amount\u0026#39;].plot.hist() plt.show() plt.clf() # \u0026lt;-- Clear out the pandas histogram   Seaborn generally does more statistical analysis on data and can provide more sophisticated insight into the data. (v.s. the seaborn distplot.)\n1 2 3 4  sns.distplot(df[\u0026#39;Award_Amount\u0026#39;]) plt.show() plt.clf()   Now we will customize distribution plots in Seaborn. The Seaborn API supports customizing the type of plot by using different arguments. We can use the same distplot() function to create a standard histogram and customize the number of bins. Trying different combinations of the kde and rug lot can yield important insights.\n1 2 3 4 5  sns.distplot(df[\u0026#39;Award_Amount\u0026#39;], kde=False, # \u0026lt;-- Kernel Density Estimate curve, default True bins=20) plt.show()   There are many functions in Seaborn that build upon each other. The distplot() function we have been discussing relies on using additional Seaborn functions such as the kdeplot() and rugplot(). By understanding this relationship, you can further customize Seaborn plots by passing additional arguments to the underlying functions. For example, we can tell the underlying kde() function to shade the plot by passing the kde_kws dictionary.\n1 2 3 4 5 6  sns.distplot(df[\u0026#39;Award_Amount\u0026#39;], hist=False, rug=True, kde_kws={\u0026#39;shade\u0026#39;:True}) plt.show()   Regression Plots Now we will transition to another basic visualization process by plotting linear regression lines.\nThe regplot() function is the basis function for building regression plots in Seaborn. We explicitly define the x and y variables as well as the source of the data.\n1 2 3 4 5 6  sns.regplot( x=\u0026#34;insurance_losses\u0026#34;, y=\u0026#34;premiums\u0026#34;, data=df ) plt.show()    As mentioned earlier, you can turn off the confidence interval by setting argument ci =False.\n One of the confusing points about Seaborn is that it may seem like there is more than one way to do the same plot. We have looked at distplot() and briefly discussed kde() plots as a building block for the more robust distplot(). In a similar manner, the lower level regplot() and higher level lmplot() are related. You can specify hue or col (row) in lmplot() to visualize category regression plot.\n   lower level func higher level func     kde(), rug() distplot()   countplot(), \u0026hellip; catplot()   scatterplot, \u0026hellip; relplot()   regplot() lmplot()    1 2 3 4 5 6 7 8  sns.lmplot( x=\u0026#34;insurance_losses\u0026#34;, y=\u0026#34;premiums\u0026#34;, data=df, ci=False ) plt.show()   1 2 3 4 5 6  sns.lmplot(data=df, x=\u0026#34;insurance_losses\u0026#34;, y=\u0026#34;premiums\u0026#34;, hue=\u0026#34;Region\u0026#34;) plt.show()   1 2 3 4 5 6  sns.lmplot(data=df, x=\u0026#34;insurance_losses\u0026#34;, y=\u0026#34;premiums\u0026#34;, row=\u0026#34;Region\u0026#34;) plt.show()   Customizing Seaborn Plots Seaborn styles Seaborn has several default built in themes that are more appealing than the default matplotlib styles. In order to set a default theme, use the sns.set() function.\n1 2 3 4  sns.set() df[\u0026#39;fmr_2\u0026#39;].plot.hist() plt.show() plt.clf()   1 2 3 4 5  sns.set_style(\u0026#34;whitegrid\u0026#34;) sns.distplot(df[\u0026#34;fmr_2\u0026#34;]) plt.show() plt.clf()   A common use case is to remove the lines around the axes called spines. Seaborn\u0026rsquo;s despine() function removes one or more of the spines on a chart. The default is to remove the top and right lines but more can be removed by passing arguments to the despine() function.\n1 2 3 4 5 6 7 8  sns.set_style(\u0026#39;white\u0026#39;)= sns.lmplot(data=df, x=\u0026#39;pop2010\u0026#39;, y=\u0026#39;fmr_2\u0026#39;) sns.despine() plt.show() plt.clf()   Colors in Seaborn Since Seaborn is built on top of matplotlib, it is able to interpret and apply matplotlib color codes. By using the familiar sns.set() function with color_codes=True, any matplotlib color codes, like color='g', will be appropriately mapped to the Seaborn palette.\n1 2 3 4  sns.set(color_codes=True) sns.distplot(df[\u0026#39;fmr_3\u0026#39;], color=\u0026#39;m\u0026#39;) plt.show()   You can set a palette of colors that can be cycled through in a plot, which can be helpful when there are many items that must be encoded with color. Seaborn has six default palettes in sns.palettes.SEABORN_PALETTES including deep, muted, pastel, bright, dark, and colorblind. Seaborn offers several convenience functions for working with palettes:\n sns.set_palette() method set current palette. sns.color_palette(p) function return palette p\u0026rsquo;s (R,G,B) tuples (default return the current palette\u0026rsquo;s (R,G,B) tuples). The second parameter specifies how many colors the palette contains. sns.palplot() function display palette with (R,G,B) tuples (in a Jupyter notebook).  1 2 3 4 5 6 7 8  t=0 for p in sns.palettes.SEABORN_PALETTES: t += 1 sns.set_palette(p) print(p) print(sns.color_palette()) if t == 4: break   deep [(0.2980392156862745, 0.4470588235294118, 0.6901960784313725), (0.8666666666666667, 0.5176470588235295, 0.3215686274509804), (0.3333333333333333, 0.6588235294117647, 0.40784313725490196), (0.7686274509803922, 0.3058823529411765, 0.3215686274509804), (0.5058823529411764, 0.4470588235294118, 0.7019607843137254), (0.5764705882352941, 0.47058823529411764, 0.3764705882352941), (0.8549019607843137, 0.5450980392156862, 0.7647058823529411), (0.5490196078431373, 0.5490196078431373, 0.5490196078431373), (0.8, 0.7254901960784313, 0.4549019607843137), (0.39215686274509803, 0.7098039215686275, 0.803921568627451)] deep6 [(0.2980392156862745, 0.4470588235294118, 0.6901960784313725), (0.3333333333333333, 0.6588235294117647, 0.40784313725490196), (0.7686274509803922, 0.3058823529411765, 0.3215686274509804), (0.5058823529411764, 0.4470588235294118, 0.7019607843137254), (0.8, 0.7254901960784313, 0.4549019607843137), (0.39215686274509803, 0.7098039215686275, 0.803921568627451)] muted [(0.2823529411764706, 0.47058823529411764, 0.8156862745098039), (0.9333333333333333, 0.5215686274509804, 0.2901960784313726), (0.41568627450980394, 0.8, 0.39215686274509803), (0.8392156862745098, 0.37254901960784315, 0.37254901960784315), (0.5843137254901961, 0.4235294117647059, 0.7058823529411765), (0.5490196078431373, 0.3803921568627451, 0.23529411764705882), (0.8627450980392157, 0.49411764705882355, 0.7529411764705882), (0.4745098039215686, 0.4745098039215686, 0.4745098039215686), (0.8352941176470589, 0.7333333333333333, 0.403921568627451), (0.5098039215686274, 0.7764705882352941, 0.8862745098039215)] muted6 [(0.2823529411764706, 0.47058823529411764, 0.8156862745098039), (0.41568627450980394, 0.8, 0.39215686274509803), (0.8392156862745098, 0.37254901960784315, 0.37254901960784315), (0.5843137254901961, 0.4235294117647059, 0.7058823529411765), (0.8352941176470589, 0.7333333333333333, 0.403921568627451), (0.5098039215686274, 0.7764705882352941, 0.8862745098039215)] 1 2 3 4 5 6 7 8 9  t=0 for p in sns.palettes.SEABORN_PALETTES: t += 1 sns.set_palette(p) print(p) sns.palplot(sns.color_palette()) plt.show() if t == 4: break   1 2 3 4  sns.palplot( sns.color_palette(\u0026#34;Purples\u0026#34;, 8) ) plt.show()   There are three main types of color palettes.\n Circular color palettes are used for categorical data that is not ordered. Sequential palettes are useful when the data has a consistent range from high to low values. Diverging color palette is best used when both the high and the low values are interesting.  1 2 3 4  cp = [\u0026#34;Paired\u0026#34;, \u0026#34;Blues\u0026#34;, \u0026#34;BrBG\u0026#34;] for i in cp: print(i) print(sns.color_palette(i))   Paired [(0.6509803921568628, 0.807843137254902, 0.8901960784313725), (0.12156862745098039, 0.47058823529411764, 0.7058823529411765), (0.6980392156862745, 0.8745098039215686, 0.5411764705882353), (0.2, 0.6274509803921569, 0.17254901960784313), (0.984313725490196, 0.6039215686274509, 0.6), (0.8901960784313725, 0.10196078431372549, 0.10980392156862745), (0.9921568627450981, 0.7490196078431373, 0.43529411764705883), (1.0, 0.4980392156862745, 0.0), (0.792156862745098, 0.6980392156862745, 0.8392156862745098), (0.41568627450980394, 0.23921568627450981, 0.6039215686274509), (1.0, 1.0, 0.6), (0.6941176470588235, 0.34901960784313724, 0.1568627450980392)] Blues [(0.8584083044982699, 0.9134486735870818, 0.9645674740484429), (0.7309496347558632, 0.8394771241830065, 0.9213225682429834), (0.5356862745098039, 0.746082276047674, 0.8642522106881968), (0.32628988850442137, 0.6186236063052672, 0.802798923490965), (0.16696655132641292, 0.48069204152249134, 0.7291503267973857), (0.044059976931949255, 0.3338869665513264, 0.6244521337946944)] BrBG [(0.6313725490196078, 0.3951557093425605, 0.09573241061130335), (0.8572856593617839, 0.7257977700884274, 0.4471357170319107), (0.9636293733179546, 0.9237985390234525, 0.8185313341022683), (0.8299115724721262, 0.9294886582083814, 0.9152633602460593), (0.4615916955017304, 0.7748558246828146, 0.7299500192233758), (0.0878892733564014, 0.479123414071511, 0.44775086505190315)] 1 2 3 4 5  cp = [\u0026#34;Paired\u0026#34;, \u0026#34;Blues\u0026#34;, \u0026#34;BrBG\u0026#34;] for i in cp: print(i) sns.palplot(sns.color_palette(i)) plt.show()   Customizing with matplotlib Since Seaborn is based on matplotlib, there is a wide variety of options for further modifying your Seaborn plots. By using matplotlib\u0026rsquo;s axes objects, you can customize almost any element of your plot.\nThe most important concept is to add additional code to create the subplots using matplotlib\u0026rsquo;s plt.subplots() functions and pass the resulting axes object to the Seaborn function. Seaborn will then plot the data on the given axes.\n1 2 3 4 5 6 7 8 9 10 11  fig, ax = plt.subplots() sns.set_palette(\u0026#34;Reds\u0026#34;) sns.distplot(df[\u0026#39;fmr_3\u0026#39;], ax=ax) ax.set( xlabel=\u0026#34;3 Bedroom Fair Market Rent\u0026#34;, ylabel=\u0026#34;rate\u0026#34;, xlim=(0,4000), title=\u0026#34;Title\u0026#34; ) plt.show()   You can use ax.axvline() or ax.axhline() to create a line in the plot, and then call ax.legned() to show it.\n1 2 3 4 5 6 7 8 9 10  fig, ax = plt.subplots() sns.distplot(df[\u0026#39;fmr_1\u0026#39;], ax=ax) ax.set(xlabel=\u0026#34;1 Bedroom Fair Market Rent\u0026#34;, xlim=(100,1500), title=\u0026#34;US Rent\u0026#34;) ax.axvline(x=df[\u0026#39;fmr_1\u0026#39;].median(), color=\u0026#39;g\u0026#39;, label=\u0026#39;Median\u0026#39;, linestyle=\u0026#39;--\u0026#39;, linewidth=2) ax.axvline(x=df[\u0026#39;fmr_1\u0026#39;].mean(), color=\u0026#39;r\u0026#39;, label=\u0026#39;Mean\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=2) ax.axhline(y=0.002, color=\u0026#39;b\u0026#39;, label=\u0026#39;y*\u0026#39;, linestyle=\u0026#39;:\u0026#39;, linewidth=2) ax.legend() plt.show()   1 2 3 4 5 6 7 8  fig, ax = plt.subplots() sns.set(color_codes=True) sns.distplot(df[\u0026#39;fmr_1\u0026#39;], ax=ax, color=\u0026#34;r\u0026#34;) sns.distplot(df[\u0026#39;fmr_2\u0026#39;], ax=ax, color=\u0026#34;c\u0026#34;) ax.set(xlabel=\u0026#34;1 Bedroom Fair Market Rent\u0026#34;, xlim=(100,1500)) plt.show()   1 2 3 4 5 6 7 8 9  fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, sharey=True) # \u0026lt;-- Make the Y-axis of the two subgraphs consistent sns.distplot(df[\u0026#39;fmr_1\u0026#39;], ax=ax0) ax0.set(xlabel=\u0026#34;1 Bedroom Fair Market Rent\u0026#34;, xlim=(100,1500)) sns.distplot(df[\u0026#39;fmr_2\u0026#39;], ax=ax1) ax1.set(xlabel=\u0026#34;2 Bedroom Fair Market Rent\u0026#34;, xlim=(100,1500)) plt.show()   You can also make this by\n1 2 3 4 5 6 7 8 9  fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True) sns.distplot(df[\u0026#39;fmr_1\u0026#39;], ax=ax[0]) ax[0].set(xlabel=\u0026#34;1 Bedroom Fair Market Rent\u0026#34;, xlim=(100,1500)) sns.distplot(df[\u0026#39;fmr_2\u0026#39;], ax=ax[1]) ax[1].set(xlabel=\u0026#34;2 Bedroom Fair Market Rent\u0026#34;, xlim=(100,1500)) plt.show()   Additional Plot Types Categorical Plot Types Seaborn supports many plot types with categorical data. Seaborn breaks categorical data plots into three groups.\n  The first group includes the stripplot() and swarmplot(), which show all the individual observations on the plot.\n  The second category contains the boxplot(), as well as the violinplot() and lvplot(). These plots show an abstract representation of the categorical data.\n  The final group of plots show statistical estimates of the categorical variables. The barplot() and pointplot() contain useful summaries of data. The countplot() shows the number of instances of each observation.\n  stripplot( ) shows every observation in the dataset. Sometimes, the data\u0026rsquo;s distribution is too busy, and we can set parameters jitter = True to scatter data rather than gather it in a single line.\n1 2 3 4 5  sns.stripplot(data=df, x=\u0026#39;Award_Amount\u0026#39;, y=\u0026#39;Model Selected\u0026#39;,) plt.show()   1 2 3 4 5 6  sns.stripplot(data=df, x=\u0026#39;Award_Amount\u0026#39;, y=\u0026#39;Model Selected\u0026#39;, jitter=True) plt.show()   We can plot a more sophisticated visualization of all the data using a swarmplot( ). This plot uses a complex algorithm to place the observations in a manner where they do not overlap. swarmplot() does not scale well to large datasets.\n1 2 3 4 5 6  sns.swarmplot(data=df, x=\u0026#39;Award_Amount\u0026#39;, y=\u0026#39;Model Selected\u0026#39;, hue=\u0026#39;Region\u0026#39;) plt.show()   boxplot( ) is the most common plots show abstract representations of the data. This plot is used to show several measures related to the distribution of data, including the median, upper and lower quartiles, as well as outliers.\n1 2 3 4 5 6  sns.boxplot(data=df, x=\u0026#39;Award_Amount\u0026#39;, y=\u0026#39;Model Selected\u0026#39;) plt.show() plt.clf()   The violinplot( ) is a combination of a kernel density plot and a box plot and can be suitable for providing an alternative view of the distribution of data.\n1 2 3 4 5 6 7  sns.violinplot(data=df, x=\u0026#39;Award_Amount\u0026#39;, y=\u0026#39;Model Selected\u0026#39;, palette=\u0026#39;husl\u0026#39;) plt.show() plt.clf()   lvplot() stands for Letter Value plot. The API is the same as the boxplot() and violinplot() but can scale more effectively to large datasets.\n1 2 3 4 5 6 7 8  sns.lvplot(data=df, x=\u0026#39;Award_Amount\u0026#39;, y=\u0026#39;Model Selected\u0026#39;, palette=\u0026#39;Paired\u0026#39;, hue=\u0026#39;Region\u0026#39;) plt.show() plt.clf()   The barplot( ) shows an estimate of the value as well as a confidence interval. The pointplot( ) is similar to the barplot() in that it shows a summary measure and confidence interval. the countplot( ) displays the number of instances of each variable.\nRegression Plots regplot( ), like most of the Seaborn functions we have reviewed, requires the definition of the data and the x and y variables and you include a marker for the observations by setting marker=\u0026quot;+\u0026quot;. If a value greater than 1 is passed to the order parameter of regplot(), then Seaborn will attempt a polynomial fit using underlying NumPy functions.\nThe residplot( ) plot is a very useful plot for understanding the appropriateness of a regression model. Ideally, the residual values in the plot should be plotted randomly across the horizontal line.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  fig, ax = plt.subplots(nrows=1, ncols=2) sns.regplot(data=df, y=\u0026#39;Tuition\u0026#39;, x=\u0026#34;SAT_AVG_ALL\u0026#34;, marker=\u0026#39;^\u0026#39;, color=\u0026#39;g\u0026#39;, ax=ax[0]) ax[0].set(title=\u0026#34;Regression plot\u0026#34;) sns.residplot(data=df, y=\u0026#39;Tuition\u0026#39;, x=\u0026#34;SAT_AVG_ALL\u0026#34;, color=\u0026#39;g\u0026#39;, ax=ax[1]) ax[1].set(title=\u0026#34;Residual plot\u0026#34;) plt.show()   Seaborn also supports regression plots with categorical variables. you can use the jitter parameter makes it easier to see the individual distribution of the categorical variable. In some cases, even with the jitter, it may be difficult to see if there are any trends based on the value of the variable. Using an x_estimator=func for the x value can apply this function to each unique value of x and plot the resulting estimate.\nWhen there are continuous variables, it can be helpful to break them into different bins by setting parameter x_bins. When this parameter is used, it implies that the default of x_estimator is numpy.mean.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  fig, ax = plt.subplots(nrows=2, ncols=2) sns.regplot( data=df, y=\u0026#39;Tuition\u0026#39;, x=\u0026#34;PCTPELL\u0026#34;, ax=ax[0][0] ) sns.regplot( data=df, y=\u0026#39;Tuition\u0026#39;, x=\u0026#34;PCTPELL\u0026#34;, x_bins=5, ax=ax[0][1] ) sns.regplot( data=df, y=\u0026#39;Tuition\u0026#39;, x=\u0026#34;PCTPELL\u0026#34;, x_bins=5, order=2, ax=ax[1][0] ) sns.residplot( data=df, y=\u0026#39;Tuition\u0026#39;, x=\u0026#34;PCTPELL\u0026#34;, order=2, ax=ax[1][1] ) ax[0][0].set(title=\u0026#34;Reg plot\u0026#34;) ax[0][1].set(title=\u0026#34;Reg bins plot\u0026#34;) ax[1][0].set(title=\u0026#34;2 orders Reg bins plot\u0026#34;) ax[1][1].set(title=\u0026#34;2 orders Resid plot\u0026#34;) plt.show()   Matrix plots Seaborn\u0026rsquo;s heatmap( ) function expects the data to be in a matrix. You can use pd.crosstab() or pd.pivot_table() to create this matirx.\n pd.crosstab(s1, s2) is a special kind of pd.pivot_table, it is a shortcut to count the number of (s1_i, s2_j).\n The display of the heatmap can be customized in multiple ways to present the most information as clearly as possible. First, we use annot equals True to turn on annotations in the individual cells. The fmt=d ensures that the results are displayed as integers. Next, we use a custom cmap to change the shading we use. By setting cbar equals False, the color bar is not displayed. we can use center to center the colormap at the value we want. Finally, passing a variable to linewidths puts some small spacing between the cells so that the values are simpler to view.\n1 2 3 4 5 6 7 8 9  pd_crosstab = pd.crosstab(df[\u0026#34;Group\u0026#34;], df[\u0026#34;YEAR\u0026#34;]) sns.heatmap(pd_crosstab, cbar=True, cmap=\u0026#34;BuGn\u0026#34;, linewidths=0.3, annot=True) plt.yticks(rotation=0) plt.xticks(rotation=90) plt.show() plt.clf()   One common usage for a heatmap is to visually represent the correlation between variables. pandas DataFrames have a corr() function that calculates the correlation between the values in the columns. The output of this function is ideally structured to be displayed as a heatmap.\nCreating Plots on Data Aware Grids FacetGrid, factorplot and lmplot One of Seaborn\u0026rsquo;s most powerful features is its ability to combine multiple smaller plots into a larger visualization that can help identify trends in data with many variables.\n One very important requirement for Seaborn to create these plots is that the data must be in tidy format. This means that each row of the data is a single observation and the columns contain the variables.\n There are two step processes to use FacetGrid( ) make multi subplots.\n Call sns.FacetGrid() to initialize the grid axes.  1 2 3 4 5 6 7 8  g = sns.FacetGrid( data, row=None, col=None, hue=None, palette=None, row_order=None, col_order=None, hue_order=None, hue_kws=None, col_wrap=None, sharex=True, sharey=True, height=3, aspect=1, dropna=True, legend_out=True, despine=True, margin_titles=False, xlim=None, ylim=None, subplot_kws=None, gridspec_kws=None, size=None )    Call g.map() to specify plot type, data, color and style.  1 2 3 4 5  # single variable plot g.map(plt.hist, \u0026#34;total_bill\u0026#34;, bins=bins, color=\u0026#34;r\u0026#34;) # double variables plot g.map(plt.scatter, \u0026#34;total_bill\u0026#34;, \u0026#34;tip\u0026#34;, edgecolor=\u0026#34;w\u0026#34;, marker=\u0026#34;.\u0026#34;)    1 2 3 4 5 6 7 8 9 10 11  g2 = sns.FacetGrid( df, row=\u0026#34;Degree_Type\u0026#34;, row_order=[ \u0026#39;Graduate\u0026#39;, \u0026#39;Bachelors\u0026#39;, \u0026#39;Associates\u0026#39;, \u0026#39;Certificate\u0026#39; ] ) g2.map(sns.pointplot, \u0026#39;SAT_AVG_ALL\u0026#39;) plt.show() plt.clf()   Seaborn\u0026rsquo;s FacetGrid() is very powerful and flexible but involves multiple steps to create. The factorplot() function returned value is also a FacetGrid but the process for creating one is much simpler.\n1 2 3 4 5 6 7 8  sns.factorplot(data=df, x=\u0026#39;SAT_AVG_ALL\u0026#39;, kind=\u0026#39;point\u0026#39;, row=\u0026#39;Degree_Type\u0026#39;, row_order=[\u0026#39;Graduate\u0026#39;, \u0026#39;Bachelors\u0026#39;, \u0026#39;Associates\u0026#39;, \u0026#39;Certificate\u0026#39;]) plt.show() plt.clf()   1 2 3 4 5 6 7  sns.factorplot(data=df, x=\u0026#39;Tuition\u0026#39;, kind=\u0026#39;box\u0026#39;, row=\u0026#39;Degree_Type\u0026#39;) plt.show() plt.clf()   1 2 3 4 5 6 7 8  seaborn.lmplot( x, y, data, hue=None, col=None, row=None, palette=None, hue_order=None, col_order=None, row_order=None, col_wrap=None, height=5, aspect=1, markers=\u0026#39;o\u0026#39;, sharex=True, sharey=True, legend=True, legend_out=True, x_estimator=None, x_bins=None, x_ci=\u0026#39;ci\u0026#39;, scatter=True, fit_reg=True, ci=95, n_boot=1000, units=None, order=1, logistic=False, lowess=False, robust=False, logx=False, x_partial=None, y_partial=None, truncate=False, x_jitter=None, y_jitter=None, scatter_kws=None, line_kws=None, size=None )   The lmplot( ) function combines regplot() and FacetGrid. It is intended as a convenient interface to fit regression models across conditional subsets of a dataset. When thinking about how to assign variables to different facets, a general rule is that it makes sense to use hue for the most important comparison, followed by col and row.\n1 2 3 4 5 6 7 8 9 10 11  g = sns.FacetGrid( df, col=\u0026#34;Ownership\u0026#34;, row=\u0026#39;Degree_Type\u0026#39;, row_order=[\u0026#39;Graduate\u0026#39;, \u0026#39;Bachelors\u0026#39;], hue=\u0026#39;WOMENONLY\u0026#39;, col_order=inst_ord ) g.map(plt.scatter, \u0026#39;SAT_AVG_ALL\u0026#39;, \u0026#34;Tuition\u0026#34;) plt.show() plt.clf()   1 2 3 4 5 6 7 8 9 10 11  sns.lmplot(data=df, x=\u0026#39;SAT_AVG_ALL\u0026#39;, y=\u0026#39;Tuition\u0026#39;, col=\u0026#34;Ownership\u0026#34;, row=\u0026#39;Degree_Type\u0026#39;, row_order=[\u0026#39;Graduate\u0026#39;, \u0026#39;Bachelors\u0026#39;], hue=\u0026#39;WOMENONLY\u0026#39;, col_order=inst_ord) plt.show() plt.clf()    Notice, factorplot(), lmplot(), relplot(), catplot().\n PairGrid and pairplot 1 2 3 4 5 6  PairGrid( data, hue=None, hue_order=None, palette=None, hue_kws=None, vars=None, x_vars=None, y_vars=None, diag_sharey=True, height=2.5, aspect=1, despine=True, dropna=True, size=None )   PairGrid and pairplot are similar to the FacetGrid, factorplot, and lmplots, exercise because they allow us to see interactions across different columns of data.\nThe PairGrid plot allows us to build plots that show the relationships between two data elements. The process for creating a PairGrid is similar to a FacetGrid in that we must create the grid.Notice that we define the variables instead of the row and column parameters. Three cases:\n seaborn will create row and column respectively for each of variable you specify in vars arguments. seaborn will create row for variable you specify in x_vars arguments, and column for variable in y_vars arguments. seaborn will create row and column respectively for each of volumns in the dateframe if you don\u0026rsquo;t specify vars argument.  Then map a plot type to the grid. You can use the map_diag() function to define the plotting function for the main diagonal. The map_offdiag() function defines the other diagonal.\n1 2 3 4 5 6 7 8  iris = sns.load_dataset(\u0026#34;iris\u0026#34;) g = sns.PairGrid(iris, hue=\u0026#34;species\u0026#34;) g.map_diag(plt.hist, histtype=\u0026#34;step\u0026#34;) g.map_offdiag(plt.scatter) g.add_legend() plt.show() plt.clf()   you can also use map_upper(), map_lower() to specify different plot type on the both sides of the diagonal.\n1 2 3 4 5 6 7 8  iris = sns.load_dataset(\u0026#34;iris\u0026#34;) g = sns.PairGrid(iris) g.map_upper(plt.scatter) g.map_lower(sns.kdeplot, cmap=\u0026#34;Blues_d\u0026#34;) g.map_diag(sns.kdeplot) plt.show() plt.clf()   1 2 3 4 5 6 7  seaborn.pairplot( data, hue=None, hue_order=None, palette=None, vars=None, x_vars=None, y_vars=None, kind=\u0026#39;scatter\u0026#39;, diag_kind=\u0026#39;auto\u0026#39;, markers=None, height=2.5, aspect=1, dropna=True, plot_kws=None, diag_kws=None, grid_kws=None, size=None )   Pairplot is a high-level interface for PairGrid that is intended to make it easy to draw a few common styles. You should use PairGrid directly if you need more flexibility. Two arguments:\n kind{scatter, reg}, optional Kind of plot for the non-identity relationships. diag_kind{auto, hist, kde}, optional Kind of plot for the diagonal subplots.  1 2 3 4 5 6 7 8 9 10 11 12 13  iris = sns.load_dataset(\u0026#34;iris\u0026#34;) sns.pairplot( iris, hue=\u0026#34;species\u0026#34;, palette=\u0026#34;husl\u0026#34;, markers=[\u0026#34;o\u0026#34;, \u0026#34;+\u0026#34;, \u0026#34;v\u0026#34;], diag_kind=\u0026#34;kde\u0026#34;, diag_kws={\u0026#39;alpha\u0026#39;:.5, \u0026#34;shade\u0026#34;:True}, kind=\u0026#34;reg\u0026#34; ) plt.show() plt.clf()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  sns.pairplot( data=df, vars=[\u0026#34;insurance_losses\u0026#34;, \u0026#34;premiums\u0026#34;], kind=\u0026#39;reg\u0026#39;, palette=\u0026#39;BrBG\u0026#39;, diag_kind = \u0026#39;kde\u0026#39;, diag_kws={ \u0026#34;shade\u0026#34;:True, \u0026#34;alpha\u0026#34;:.5 }, hue=\u0026#39;Region\u0026#39; ) plt.show() plt.clf()   JointGrid and jointplot 1 2 3 4 5  seaborn.JointGrid( x, y, data=None, height=6, ratio=5, space=0.2, dropna=True, xlim=None, ylim=None, size=None )   JoinGrid creates grid for drawing a bivariate plot with marginal univariate plots. Initialize the figure but dont draw any plots onto it:\n1 2 3 4 5  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) g = sns.JointGrid(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips) plt.show() plt.clf()   Add plots using default parameters:\n1 2 3 4 5 6  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) g = sns.JointGrid(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips) g.plot(sns.regplot, sns.distplot) plt.show() plt.clf()   Draw the join and marginal plots separately:\n1 2 3 4 5 6 7 8  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) g = sns.JointGrid(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips) g.plot_joint(plt.scatter, color=\u0026#34;.5\u0026#34;, edgecolor=\u0026#34;white\u0026#34;) g.plot_marginals(sns.distplot, kde=False, color=\u0026#34;.5\u0026#34;) plt.show() plt.clf()   or\n1 2 3 4 5 6 7 8 9 10 11  import numpy as np tips = sns.load_dataset(\u0026#34;tips\u0026#34;) g = sns.JointGrid(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips) g.plot_joint(plt.scatter, color=\u0026#34;m\u0026#34;, edgecolor=\u0026#34;white\u0026#34;) g.ax_marg_x.hist(tips[\u0026#34;total_bill\u0026#34;], color=\u0026#34;b\u0026#34;, alpha=.6, bins=np.arange(0, 60, 4)) g.ax_marg_y.hist(tips[\u0026#34;tip\u0026#34;], color=\u0026#34;r\u0026#34;, alpha=.6, orientation=\u0026#34;horizontal\u0026#34;, bins=np.arange(0, 12, .5)) plt.show() plt.clf()   Add an annotation with a statistic summarizing the bivariate relationship:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  from scipy import stats tips = sns.load_dataset(\u0026#34;tips\u0026#34;) g = sns.JointGrid(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips) g.plot_joint(plt.scatter) g.plot_marginals(sns.kdeplot, shade=True) rsquare = lambda a, b: stats.pearsonr(a, b)[0] ** 2 g.annotate( rsquare, template=\u0026#34;{stat}: {val:.2f}\u0026#34;, stat=\u0026#34;$R^2$\u0026#34;, loc=\u0026#34;upper left\u0026#34;, fontsize=12 ) plt.show() plt.clf()    1 2 3 4 5 6 7  seaborn.jointplot( x, y, data=None, kind=\u0026#39;scatter\u0026#39;, stat_func=None, color=None, height=6, ratio=5, space=0.2, dropna=True, xlim=None, ylim=None, joint_kws=None, marginal_kws=None, annot_kws=None, **kwargs )   jointplot( ) function provides a convenient interface to the JointGrid class, with several canned plot kinds. This is intended to be a fairly lightweight wrapper; if you need more flexibility, you should use JointGrid directly.\nDraw a default jointplot, i.e. scatterplot with marginal histograms:\n1 2  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) sns.jointplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips)   Add (3 order) regression and kernel density fits\n1 2 3 4 5 6 7 8 9 10 11 12  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) sns.jointplot( \u0026#34;total_bill\u0026#34;, \u0026#34;tip\u0026#34;, data=tips, kind=\u0026#34;reg\u0026#34;, order=3, xlim=(0,50) ) plt.show() plt.clf()   the corresponding residual plot:\n1 2 3 4 5 6 7 8 9 10 11 12  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) sns.jointplot( \u0026#34;total_bill\u0026#34;, \u0026#34;tip\u0026#34;, data=tips, kind=\u0026#34;resid\u0026#34;, order=3, xlim=(0,50) ) plt.show() plt.clf()   Replace the scatterplot with a joint histogram using hexagonal bins:\n1 2 3 4 5  tips = sns.load_dataset(\u0026#34;tips\u0026#34;) sns.jointplot(\u0026#34;total_bill\u0026#34;, \u0026#34;tip\u0026#34;, data=tips, kind=\u0026#34;hex\u0026#34;) plt.show() plt.clf()   Replace the scatterplots and histograms with density estimates and align the marginal Axes tightly with the joint Axes:\n1 2 3 4 5  iris = sns.load_dataset(\u0026#34;iris\u0026#34;) sns.jointplot(\u0026#34;sepal_width\u0026#34;, \u0026#34;petal_length\u0026#34;, data=iris, kind=\u0026#34;kde\u0026#34;, space=0, color=\u0026#34;g\u0026#34;) plt.show() plt.clf()   Draw a scatterplot, then add a joint density estimate:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  iris = sns.load_dataset(\u0026#34;iris\u0026#34;) ( sns.jointplot( \u0026#34;sepal_length\u0026#34;, \u0026#34;sepal_width\u0026#34;, data=iris, color=\u0026#34;b\u0026#34;, kind=\u0026#34;scatter\u0026#34; ).plot_joint( sns.kdeplot, n_levels=10, cmap=\u0026#34;RdBu\u0026#34; ) ) plt.show() plt.clf()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  iris = sns.load_dataset(\u0026#34;iris\u0026#34;) (sns.jointplot( \u0026#34;sepal_length\u0026#34;, \u0026#34;sepal_width\u0026#34;, data=iris, color=\u0026#34;b\u0026#34;, kind=\u0026#34;kde\u0026#34; ).plot_joint( sns.kdeplot, n_levels=10, cmap=\u0026#34;RdBu\u0026#34; ) ) plt.show() plt.clf()      Grid class high-level function     FacetGrid factorplot(), lmplot(), relplot(), catplot()   PairGrid pairplot   JointGrid jointplot, jointplot.plot_joint    Union Frame The final section of this blog will bring all of the concepts together and give a framework for deciding when to use each Seaborn plot.\nUnivariate Distribution Analysis One of the first steps in analyzing numerical data is looking at its distribution. Seaborn\u0026rsquo;s distplot() combines many of the features of the rugplot(), kdeplot(), and plt.histplot() into a single function. The distplot() function is the best place to start when trying to do distribution analysis with Seaborn.\nRegression Analysis A regression plot is an example of a plot that shows the relationship between two variables. matplotlib\u0026rsquo;s scatter() plot is a very simple method to compare the interaction of two variables on the x and y-axis. The lmplot() combines many of these features of the underlying regplot() and residplot() in addition to the ability to plot the data on a FacetGrid(). In many instances, lmplot() is the best function to use for determining linear relationships between data.\nCategorical Plots Seaborn has many types of categorical plots as well. In most scenarios, it makes sense to use one of the categorical plots such as the boxplot() or violinplot() to examine the distribution of the variables. Then, follow up with the statistical estimation plots such as the point, bar, or countplot. If you need to facet the data across rows or columns, use a factorplot().\nRelation The pairplot() and jointplot() visualizations are going to be most useful after you have done some preliminary analysis of regressions or distributions of the data. Once you are familiar with the data, the pairplot() and jointplot() can be very useful in understanding how two or more variables interact with each other.\n","date":"2020-04-23T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/data-visualization-iii-seaborn/","title":"Data Visualization III (Seaborn)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n [toc]\nIntroduction to Seaborn Using pandas with Seaborn 1 2 3  # Import Matplotlib and Seaborn import matplotlib.pyplot as plt import seaborn as sns   To create a count plot with a pandas DataFrame column instead of a list of data, set x (or y) equal to the name of the column in the DataFrame. Then, we\u0026rsquo;ll set the data parameter equal to DataFrame. Finally calling \u0026ldquo;plt.show()\u0026rdquo;.\n1 2 3 4 5 6  df = pd.read_csv(\u0026#34;xxxx.csv\u0026#34;) sns.countplot( x=\u0026#34;Spiders\u0026#34;, # column name data=df ) plt.show()    Notice that because we\u0026rsquo;re using a named column in the DataFrame, Seaborn automatically adds the name of the column as the x-axis label at the bottom.\n Adding a third variable with hue we\u0026rsquo;ll be using Seaborn\u0026rsquo;s built-in tips dataset. You can access it by using the \u0026ldquo;load_dataset\u0026rdquo; function in Seaborn and passing in the name of the dataset. These are the first five rows of the tips dataset.\n1 2  df=sns.load_dataset(\u0026#34;tips\u0026#34;) df.head()    total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 Let\u0026rsquo;s explore the relationship between the \u0026ldquo;total_bill\u0026rdquo; and \u0026ldquo;tip\u0026rdquo; columns using a scatter plot.\n1 2 3 4 5 6  sns.scatterplot( x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=df, ) plt.show()   If you want to add a third variable to plots by adding color, you can set the \u0026ldquo;hue\u0026rdquo; parameter equal to the DataFrame column name or a list, and then Seaborn will automatically color each point by groups.\n1 2 3 4 5 6 7  sns.scatterplot( x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;smoker\u0026#34;, data=df, ) plt.show()   The \u0026ldquo;hue_order\u0026rdquo; parameter takes in a list of values and will set the order of the values in the plot accordingly. You can control the colors assigned to each value using the \u0026ldquo;palette\u0026rdquo; parameter. This parameter takes in a dictionary. This dictionary should map the variable values to the colors you want to represent the value.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  hc = { \u0026#34;Yes\u0026#34;: \u0026#34;c\u0026#34;, \u0026#34;No\u0026#34;: \u0026#34;g\u0026#34; } sns.scatterplot( x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;smoker\u0026#34;, data=df, hue_order=[\u0026#34;No\u0026#34;, \u0026#34;Yes\u0026#34;], palette=hc ) plt.show()   Here is the list of Matplotlib colors and their names. Note that you can use a single-letter Matplotlib abbreviation instead of the full name.\nhue is available in most of Seaborn\u0026rsquo;s plot types. For example, countplot\n1 2 3 4 5 6 7 8 9 10  palette_colors = {\u0026#34;Rural\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;Urban\u0026#34;: \u0026#34;blue\u0026#34;} sns.countplot( x=\u0026#34;school\u0026#34;, data=student_data, hue=\u0026#34;location\u0026#34;, palette=palette_colors ) plt.show()    The other way to add the third variable into a plot is calling arguments col or row in sns.relplot() or sns.catplot() method.\n Relational Plot relational plots and subplots Sometimes we suspect that the relationship may be different within certain subgroups. In the last chapter, we started to look at subgroups by using the \u0026ldquo;hue\u0026rdquo; parameter to visualize each subgroup using a different color on the same plot.\nNow we\u0026rsquo;ll try out a different method relplot() (relationship plot) to creat a separate plot per subgroup. We call the \u0026ldquo;relplot()\u0026rdquo; method and use the \u0026ldquo;kind\u0026rdquo; parameter to specify what kind of relational plot to use - scatter plot or line plot.\nBy setting \u0026ldquo;col\u0026rdquo; equal to \u0026ldquo;smoker\u0026rdquo;, we get a separate scatter plot groups by \u0026ldquo;smoker\u0026rdquo;, arranged horizontally in columns. You can use the \u0026ldquo;col_wrap\u0026rdquo; parameter to specify how many subplots you want per row. If you want to arrange these vertically in rows instead, you can use the \u0026ldquo;row\u0026rdquo; parameter instead of \u0026ldquo;col\u0026rdquo;. We can also change the order of the subplots by using the \u0026ldquo;col_order\u0026rdquo; and \u0026ldquo;row_order\u0026rdquo; parameters and giving it a list of ordered values.\n1 2 3 4 5 6 7 8 9 10  sns.relplot( x=\u0026#34;G1\u0026#34;, y=\u0026#34;G3\u0026#34;, data=student_data, kind=\u0026#34;scatter\u0026#34;, col=\u0026#34;schoolsup\u0026#34;, col_order=[\u0026#34;yes\u0026#34;, \u0026#34;no\u0026#34;] ) plt.show()   1 2 3 4 5 6 7 8 9  sns.relplot( x=\u0026#34;absences\u0026#34;, y=\u0026#34;G3\u0026#34;, data=student_data, kind=\u0026#34;scatter\u0026#34;, row=\u0026#34;study_time\u0026#34; ) plt.show()   It is possible to use both \u0026ldquo;col\u0026rdquo; and \u0026ldquo;row\u0026rdquo; at the same time. Here, we set \u0026ldquo;col\u0026rdquo; equal to smoking status and \u0026ldquo;row\u0026rdquo; equal to the time of day (lunch or dinner). Now we have a subplot for each combination of these two categorical variables.\n1 2 3 4 5 6 7 8 9 10 11 12  sns.relplot( x=\u0026#34;G1\u0026#34;, y=\u0026#34;G3\u0026#34;, data=student_data, kind=\u0026#34;scatter\u0026#34;, col=\u0026#34;schoolsup\u0026#34;, col_order=[\u0026#34;yes\u0026#34;, \u0026#34;no\u0026#34;], row=\u0026#34;famsup\u0026#34;, row_order=[\u0026#39;yes\u0026#39;, \u0026#39;no\u0026#39;] ) plt.show()   Customizing scatter plots Seaborn allows you to add more information to scatter plots by varying the size, the style, and the transparency of the points.\n All of these options can be used in both the \u0026ldquo;scatterplot()\u0026rdquo; and \u0026ldquo;relplot()\u0026rdquo; functions, but we\u0026rsquo;ll continue to use \u0026ldquo;relplot()\u0026rdquo; since it\u0026rsquo;s more flexible and allows us to create subplots.\n We want each point on the scatter plot to be sized based on the number of variable with larger number having bigger points on the plot. To do this, we\u0026rsquo;ll set the \u0026ldquo;size\u0026rdquo; parameter equal to the variable name from the dataset.\nWe can make it easier by using the \u0026ldquo;size\u0026rdquo; parameter in combination with the \u0026ldquo;hue\u0026rdquo; parameter. To do this, set \u0026ldquo;hue\u0026rdquo; equal to the variable name.\n1 2 3 4 5 6 7 8 9 10  sns.relplot( x=\u0026#34;horsepower\u0026#34;, y=\u0026#34;mpg\u0026#34;, data=mpg, kind=\u0026#34;scatter\u0026#34;, size=\u0026#34;cylinders\u0026#34;, hue=\u0026#34;cylinders\u0026#34; ) plt.show()    Notice that because \u0026ldquo;size\u0026rdquo; is a quantitative variable, Seaborn will automatically color the points the same color with different depth, instead of different colors per category value like we saw in previous plots.\n We use \u0026ldquo;hue\u0026rdquo; to create different colored points based on groups. Setting \u0026ldquo;style\u0026rdquo; equal to allows us to better distinguish these subgroups by plotting points with a different point style in addition to a different color.\n1 2 3 4 5 6 7 8 9 10  sns.relplot( x=\u0026#34;acceleration\u0026#34;, y=\u0026#34;mpg\u0026#34;, data=mpg, hue=\u0026#34;origin\u0026#34;, style=\u0026#34;origin\u0026#34;, kind=\u0026#34;scatter\u0026#34; ) plt.show()   Setting the \u0026ldquo;alpha\u0026rdquo; parameter to a value between 0 and 1 will vary the transparency of the points in the plot, with 0 being completely transparent and 1 being completely non-transparent.\nLine plots In Seaborn, we have two types of relational plots: scatter plots and line plots. Line plots are the visualization of choice when we need to track the same thing over time.\nBy specifying \u0026ldquo;kind\u0026rdquo; equals \u0026ldquo;line\u0026rdquo;, we can create a line plot. We can also track subgroups over time with line plots. Setting the \u0026ldquo;style\u0026rdquo; and \u0026ldquo;hue\u0026rdquo; parameters equal to the variable name creates different lines for each group that vary in both line style and color.\n1 2 3 4 5 6 7 8 9 10 11  sns.relplot( x=\u0026#34;model_year\u0026#34;, y=\u0026#34;horsepower\u0026#34;, data=mpg, kind=\u0026#34;line\u0026#34;, ci=None, style=\u0026#34;origin\u0026#34;, hue=\u0026#34;origin\u0026#34; ) plt.show()   1 2 3 4 5 6 7 8 9 10 11 12  sns.relplot( x=\u0026#34;model_year\u0026#34;, y=\u0026#34;horsepower\u0026#34;, data=mpg, kind=\u0026#34;line\u0026#34;, ci=None, col=\u0026#34;origin\u0026#34;, hue=\u0026#34;origin\u0026#34;, style=\u0026#34;origin\u0026#34; ) plt.show()   Setting the \u0026ldquo;markers\u0026rdquo; parameter equal to \u0026ldquo;True\u0026rdquo; will display a marker for each data point. The marker will vary based on the subgroup you\u0026rsquo;ve set using the \u0026ldquo;style\u0026rdquo; parameter. If you don\u0026rsquo;t want the line styles to vary by subgroup, set the \u0026ldquo;dashes\u0026rdquo; parameter equal to \u0026ldquo;False\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  sns.relplot( x=\u0026#34;model_year\u0026#34;, y=\u0026#34;horsepower\u0026#34;, data=mpg, kind=\u0026#34;line\u0026#34;, ci=None, col=\u0026#34;origin\u0026#34;, hue=\u0026#34;origin\u0026#34;, style=\u0026#34;origin\u0026#34;, markers=True, dashes=False ) plt.show()   Line plots can also be used when you have more than one observation per x-value. If a line plot is given multiple observations per x-value, it will aggregate them into a single summary measure. By default, it will display the mean. This is the scatter plot\n1 2 3 4 5 6 7  sns.relplot( x=\u0026#34;model_year\u0026#34;, y=\u0026#34;mpg\u0026#34;, data=mpg, ) plt.show()   and the line plot\n1 2 3 4 5 6 7 8  sns.relplot( x=\u0026#34;model_year\u0026#34;, y=\u0026#34;mpg\u0026#34;, data=mpg, kind=\u0026#34;line\u0026#34; ) plt.show()    Notice that Seaborn will automatically calculate a confidence interval for the mean, displayed by the shaded region.\n Set the \u0026ldquo;ci\u0026rdquo; parameter equal to the string \u0026ldquo;sd\u0026rdquo; to make the shaded area represent the standard deviation instead of the confidence interval for the mean which shows the spread of the distribution of observations at each x value. We can also turn off the confidence interval by setting the \u0026ldquo;ci\u0026rdquo; parameter equal to \u0026ldquo;None\u0026rdquo;.\n1 2 3 4 5 6 7 8 9  sns.relplot( x=\u0026#34;model_year\u0026#34;, y=\u0026#34;mpg\u0026#34;, data=mpg, kind=\u0026#34;line\u0026#34;, ci=\u0026#34;sd\u0026#34; ) plt.show()   Categorical Plot Count plots and bar plots Count plots and bar plots are two types of visualizations that Seaborn calls \u0026ldquo;categorical plots\u0026rdquo;.\nWe call catplot() method and use the \u0026ldquo;kind\u0026rdquo; parameter to specify what kind of categorical plot to use. \u0026ldquo;catplot()\u0026rdquo; offers the same flexibility that \u0026ldquo;relplot()\u0026rdquo; does, which means it will be easy to create subplots if we need to using the same \u0026ldquo;col\u0026rdquo; and \u0026ldquo;row\u0026rdquo; parameters.\n1 2 3 4  sns.catplot(x=\u0026#34;Internet usage\u0026#34;, data=survey_data, kind=\u0026#34;count\u0026#34;, row=\u0026#34;Age Category\u0026#34;) plt.show()   Use the \u0026ldquo;order\u0026rdquo; parameter which accepts a list can specific ordering of categories. This works for all types of categorical plots.\nBar plots look similar to count plots, but instead of the count of observations in each category, they show the mean of a quantitative variable among observations in each category. To create this bar plot, we use \u0026ldquo;catplot\u0026rdquo;. Specify the categorical variable on the x-axis, the quantitative variable on the y-axis, and set the \u0026ldquo;kind\u0026rdquo; parameter equal to \u0026ldquo;bar\u0026rdquo;.\n Of course, you can also change the orientation of the bars in bar plots and count plots by switching the x and y parameters.\n 1 2 3 4 5 6 7 8 9 10 11  category_order = [\u0026#34;\u0026lt;2 hours\u0026#34;, \u0026#34;2 to 5 hours\u0026#34;, \u0026#34;5 to 10 hours\u0026#34;, \u0026#34;\u0026gt;10 hours\u0026#34;] sns.catplot(x=\u0026#34;study_time\u0026#34;, y=\u0026#34;G3\u0026#34;, data=student_data, kind=\u0026#34;bar\u0026#34;, order=category_order) plt.show()   Notice also that Seaborn automatically shows 95% confidence intervals for these means. we can turn off these confidence intervals by setting the \u0026ldquo;ci\u0026rdquo; parameter equal to \u0026ldquo;None\u0026rdquo;.\nBox plot A box plot shows the distribution of quantitative data. The colored box represents the 25th to 75th percentile, and the line in the middle of the box represents the median. The whiskers give a sense of the spread of the distribution, and the floating points represent outliers.\nWe can use the \u0026ldquo;catplot()\u0026rdquo; function to crate a box plot. We\u0026rsquo;ll put the categorical variable on the x-axis and the quantitative variable on the y-axis, and specify kind=\u0026quot;box\u0026quot;.\n As a reminder, \u0026ldquo;catplot\u0026rdquo; allows you to change the order of the categories with a list by \u0026ldquo;order\u0026rdquo; parameter.\n 1 2 3 4 5 6 7 8 9 10 11 12  study_time_order = [\u0026#34;\u0026lt;2 hours\u0026#34;, \u0026#34;2 to 5 hours\u0026#34;, \u0026#34;5 to 10 hours\u0026#34;, \u0026#34;\u0026gt;10 hours\u0026#34;] sns.catplot( x=\u0026#34;study_time\u0026#34;, y=\u0026#34;G3\u0026#34;, data=student_data, kind=\u0026#34;box\u0026#34;, order=study_time_order ) plt.show()   You can omit the outliers using the specify sym=\u0026quot;\u0026quot;. \u0026ldquo;sym\u0026rdquo; can also be used to change the appearance of the outliers instead of omitting them.\nYou can change the way the whiskers using the \u0026ldquo;whis\u0026rdquo; parameter. You can have the whiskers define specific lower and upper percentiles by passing in a list of the lower and upper values such as whis=[5, 95] or whis=[0, 100].\n1 2 3 4 5 6 7 8 9 10 11  sns.catplot( x=\u0026#34;internet\u0026#34;, y=\u0026#34;G3\u0026#34;, data=student_data, kind=\u0026#34;box\u0026#34;, sym=\u0026#34;\u0026#34;, whis=[0, 100], # \u0026lt;-- Min \u0026amp; Max hue=\u0026#34;location\u0026#34; ) plt.show()   Point plots Point plots show the mean of a quantitative variable for the observations in each category, plotted as a single point.\nBoth line plots and point plots show the mean of a quantitative variable and 95% confidence intervals for the mean. However, there is a key difference. Line plots are relational plots, so both the x- and y-axis are quantitative variables. In a point plot, one axis - usually the x-axis - is a categorical variable, making it a categorical plot.\nSince Point plot is a categorical plot, we use \u0026ldquo;catplot\u0026rdquo; and set \u0026ldquo;kind\u0026rdquo; equal to \u0026ldquo;point\u0026rdquo;. To remove the lines connecting each point, set the \u0026ldquo;join\u0026rdquo; parameter equal to False. You can also add caps to the end of the confidence intervals, set the capsize parameter equal to the desired width of the caps. Finally, you can turn the confidence intervals off by setting the \u0026ldquo;ci\u0026rdquo; parameter equal to None.\n1 2 3 4 5 6  sns.catplot(x=\u0026#34;famrel\u0026#34;, y=\u0026#34;absences\u0026#34;, data=student_data, kind=\u0026#34;point\u0026#34;, capsize=0.2) plt.show()   To have the points and confidence intervals be calculated for the median instead of the mean, set \u0026ldquo;estimator=np.median.\n1 2 3 4 5 6 7 8 9 10  from numpy import median sns.catplot(x=\u0026#34;romantic\u0026#34;, y=\u0026#34;absences\u0026#34;, data=student_data, kind=\u0026#34;point\u0026#34;, hue=\u0026#34;school\u0026#34;, ci=None, estimator=median) plt.show()   Customizing Seaborn Plots Changing plot style and color Seaborn has five preset figure styles which change the background and axes of the plot. You can refer to them by name: \u0026ldquo;white\u0026rdquo; (default), \u0026ldquo;dark\u0026rdquo;, \u0026ldquo;whitegrid\u0026rdquo;, \u0026ldquo;darkgrid\u0026rdquo;, and \u0026ldquo;ticks\u0026rdquo;. To set one of these as the global style for all of your plots, use the \u0026ldquo;sns.set_style()\u0026rdquo; method.\nYou can change the color of the main elements of the plot with Seaborn\u0026rsquo;s \u0026ldquo;sns.set_palette()\u0026rdquo; method. Seaborn has many preset color palettes that you can refer to by name, or you can create your own custom palette with list.\nSeaborn has a group of preset palettes called diverging palettes that are great to use if your visualization deals with a scale where the two ends of the scale are opposites and there is a neutral midpoint.\nAnother group of palettes are called sequential palettes. These are a single color (or two colors blended) moving from light to dark values. Sequential palettes are great for emphasizing a variable on a continuous scale.\nYou can also create your own custom palettes by passing in a list of color names.\n1 2 3 4 5 6 7 8 9 10 11 12  sns.set_style(\u0026#34;whitegrid\u0026#34;) sns.set_palette(\u0026#34;RdBu\u0026#34;) category_order = [\u0026#34;Never\u0026#34;, \u0026#34;Rarely\u0026#34;, \u0026#34;Sometimes\u0026#34;, \u0026#34;Often\u0026#34;, \u0026#34;Always\u0026#34;] sns.catplot(x=\u0026#34;Parents Advice\u0026#34;, data=survey_data, kind=\u0026#34;count\u0026#34;, order=category_order) plt.show()   1 2 3 4 5 6 7 8  sns.set_style(\u0026#34;darkgrid\u0026#34;) color = [\u0026#34;#39A7D0\u0026#34;, \u0026#34;#36ADA4\u0026#34;] sns.set_palette(color) sns.catplot(x=\u0026#34;Gender\u0026#34;, y=\u0026#34;Age\u0026#34;, data=survey_data, kind=\u0026#34;box\u0026#34;) plt.show()   Finally, you can change the scale of your plot by using the \u0026ldquo;sns.set_context()\u0026rdquo; method. The scale options from smallest to largest are \u0026ldquo;paper\u0026rdquo; (default), \u0026ldquo;notebook\u0026rdquo;, \u0026ldquo;talk\u0026rdquo;, and \u0026ldquo;poster\u0026rdquo;.\nAdding titles and labels In this section, we will focus on the title, label and x axis tick label in the plot.\nSeaborn\u0026rsquo;s plot functions create two different types of objects: FacetGrids and AxesSubplots. To figure out which type of object you\u0026rsquo;re working with, first assign the plot output to a variable such as g, and call the function type(g) .\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  g1 = sns.relplot(x=\u0026#34;weight\u0026#34;, y=\u0026#34;horsepower\u0026#34;, data=mpg, kind=\u0026#34;scatter\u0026#34;) g2 = sns.catplot(x=\u0026#34;weight\u0026#34;, data=mpg, kind=\u0026#34;count\u0026#34;) g3 = sns.scatterplot(x=\u0026#34;weight\u0026#34;, y=\u0026#34;horsepower\u0026#34;, data=mpg) g4 = sns.countplot(x=\u0026#34;weight\u0026#34;, data=mpg) print(type(g1), type(g2), type(g3), type(g4)   \u0026lt;class 'seaborn.axisgrid.FacetGrid'\u0026gt; \u0026lt;class 'seaborn.axisgrid.FacetGrid'\u0026gt; \u0026lt;class 'matplotlib.axes._subplots.AxesSubplot'\u0026gt; \u0026lt;class 'matplotlib.axes._subplots.AxesSubplot'\u0026gt; Recall that \u0026ldquo;relplot()\u0026rdquo; and \u0026ldquo;catplot()\u0026rdquo; both support making subplots. This means that they are creating FacetGrid objects. In contrast, single-type plot functions like \u0026ldquo;scatterplot()\u0026rdquo; and \u0026ldquo;countplot()\u0026rdquo; return a single AxesSubplot object.\n A FacetGrid consists of one or more AxesSubplots, which is how it supports subplots.\n To add a title to a FacetGrid object, first assign the plot to the variable \u0026ldquo;g\u0026rdquo;. And then, you can set the title using \u0026ldquo;g.fig.suptitle()\u0026rdquo;. To adjust the height of the title, you can use the \u0026ldquo;y\u0026rdquo; parameter whose default value is 1.\n1 2 3 4 5 6 7  g = sns.relplot(x=\u0026#34;weight\u0026#34;, y=\u0026#34;horsepower\u0026#34;, data=mpg, kind=\u0026#34;scatter\u0026#34;) g.fig.suptitle(\u0026#34;Car Weight vs. Horsepower\u0026#34;, y=1) plt.show()   1 2 3 4 5 6 7 8  sns.set_palette(\u0026#34;Blues\u0026#34;) g = sns.catplot(x=\u0026#34;Gender\u0026#34;, y=\u0026#34;Age\u0026#34;, data=survey_data, kind=\u0026#34;box\u0026#34;, hue=\u0026#34;Interested in Pets\u0026#34;) g.fig.suptitle(\u0026#34;Age of Those Interested in Pets vs. Not\u0026#34;) plt.show()   To add a title to an AxesSubplot object, call the method g.set_titles(). You can also use the y parameter here to adjust the height of the title. Notice that a FacetGrid consists of one or more AxesSubplots, thus you can set title on each of subplot in a plot.\nSince g is a FacetGrid object, using \u0026ldquo;g.fig.suptitle()\u0026rdquo; will add a title to the figure as a whole. To alter the subplot titles, call \u0026ldquo;g.set_titles()\u0026rdquo; repeatly to set the titles for each AxesSubplot. If you want to use the variable name in the title, you can use {col_name} to reference the column value.\n1 2 3 4 5 6 7 8 9 10  g = sns.relplot(x=\u0026#34;model_year\u0026#34;, y=\u0026#34;mpg_mean\u0026#34;, data=mpg_mean, col=\u0026#34;origin\u0026#34;, kind=\u0026#34;line\u0026#34;) g.fig.suptitle(\u0026#34;Average MPG Over Time\u0026#34;) g.set_titles(\u0026#34;EU\u0026#34;) g.set_titles(\u0026#34;JP\u0026#34;) g.set_titles(\u0026#34;US\u0026#34;) plt.show()   or\n1 2 3 4 5 6 7 8  g = sns.relplot(x=\u0026#34;model_year\u0026#34;, y=\u0026#34;mpg_mean\u0026#34;, data=mpg_mean, col=\u0026#34;origin\u0026#34;, kind=\u0026#34;line\u0026#34;) g.fig.suptitle(\u0026#34;Average MPG Over Time\u0026#34;) g.set_titles(\u0026#34;{col_name}\u0026#34;) plt.show()   To add axis labels, assign the plot to a variable and then call the \u0026ldquo;set\u0026rdquo; function. Set the parameters \u0026ldquo;xlabel\u0026rdquo; and \u0026ldquo;ylabel\u0026rdquo; to set the desired x-axis and y-axis labels, respectively. This works with both FacetGrid and AxesSubplot objects.\n1 2 3 4 5 6 7 8 9 10 11  g = sns.lineplot(x=\u0026#34;model_year\u0026#34;, y=\u0026#34;mpg_mean\u0026#34;, data=mpg_mean, hue=\u0026#34;origin\u0026#34;) g.set_title(\u0026#34;Average MPG Over Time\u0026#34;) g.set( xlabel=\u0026#34;Car Model Year\u0026#34;, ylabel=\u0026#34;Average MPG\u0026#34; ) plt.show()   1 2 3 4 5 6 7 8 9 10 11 12  sns.set_style(\u0026#34;darkgrid\u0026#34;) sns.set_palette(\u0026#34;RdBu_r\u0026#34;) g = sns.catplot(x=\u0026#34;Village - town\u0026#34;, y=\u0026#34;Likes Techno\u0026#34;, data=survey_data, kind=\u0026#34;bar\u0026#34;, col=\u0026#34;Gender\u0026#34;) g.fig.suptitle(\u0026#34;Percentage of Young People Who Like Techno\u0026#34;, y=1.02) g.set_titles(\u0026#34;{col_name}\u0026#34;) g.set(xlabel=\u0026#34;Location of Residence\u0026#34;, ylabel=\u0026#34;% Who Like Techno\u0026#34;) plt.show()   Sometimes your tick labels may overlap, one way to address this is by rotating the tick labels. To do this, after we create the plot, we call the matplotlib function \u0026ldquo;plt.xticks()\u0026rdquo; and set \u0026ldquo;rotation=90\u0026rdquo;. This works with both FacetGrid and AxesSubplot objects.\n1 2 3 4 5 6 7 8 9 10  sns.catplot(x=\u0026#34;origin\u0026#34;, y=\u0026#34;acceleration\u0026#34;, data=mpg, kind=\u0026#34;point\u0026#34;, join=False, capsize=0.1) plt.xticks(rotation=90) plt.show()   ","date":"2020-04-20T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/data-visualization-ii-seaborn/","title":"Data Visualization II (Seaborn)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Introduction to Matplotlib Intro to data visualization with matplotlib we will use the main object-oriented interface. This interface is provided through the pyplot submodule. The plt.subplots() command, when called without any inputs, creates two different objects: a Figure object and an Axes object.\n1 2  import matplotlib.pyplot as plt fig, ax = plt.subplots()   The Figure object is a container that holds everything that you see on the page. Meanwhile, the Axes is the part of the page that holds the data. It is the canvas on which we will draw with our data.\nTo add the data to the Axes, we call a ax.plot() command. If you want, you can add more data to the plot by this way. Finally, we call the plt.show() function to show the effect of the plotting command.\n1 2 3 4 5 6 7 8  import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.plot(x=df.col1, y=df.col2) ax.plot(x=df.col1, y=df.col3) ax.plot(df2[\u0026#34;col1\u0026#34;], df2[\u0026#34;col2\u0026#34;]) # you can also add data to a figure from different tables. plt.show()   Here is what all of the code to create this figure would then look like.\n First, we create the Figure and the Axes objects. We call the Axes method plot to add first the df.col2, and then the df.col3 to the Axes. Finally, we ask Matplotlib to show us the figure.  Customizing Plots The plot method takes an optional keyword argument, marker, which lets you indicate what kind of markers you\u0026rsquo;d like. To see all the possible marker styles, you can visit this page in the Matplotlib online documentation.\nYou can change the appearance of these connecting lines by adding the linestyle keyword argument. Like marker shapes, there are a few linestyles you can choose from, listed in this page.\nYou can even go so far as to eliminate the lines altogether, by passing the string \u0026ldquo;None\u0026rdquo; as input to this keyword argument.\nyou can also add the color argument. For example, here we\u0026rsquo;ve chosen to show this data in red, indicated by the letter \u0026ldquo;r\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # Plot Seattle data, setting data appearance ax.plot( seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;], color=\u0026#34;b\u0026#34;, marker=\u0026#34;o\u0026#34;, linestyle=\u0026#34;--\u0026#34; ) # Plot Austin data, setting data appearance ax.plot( austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;], color=\u0026#34;r\u0026#34;, marker=\u0026#34;v\u0026#34;, linestyle=\u0026#34;--\u0026#34; ) plt.show()   The Axes object has several methods that start with the word set. These are methods that you can use to change certain properties of the object, before calling show to display it.\nFor example, there is a set_xlabel method that you can use to set the value of the label of the x-axis.\nSimilarly, a set_ylabel method customizes the label that is associated with the y-axis.\nFinally, you can also add a title to your Axes using the set_title method.\n1 2 3 4 5 6 7 8  ax.plot(seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;]) ax.plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;]) ax.set_xlabel(\u0026#34;Time (months)\u0026#34;) ax.set_ylabel(\u0026#34;Precipitation (inches)\u0026#34;) ax.set_title(\u0026#34;Weather patterns in Austin and Seattle\u0026#34;) plt.show()   Small Multiples In some cases, adding more data to a plot can make the plot too busy, obscuring patterns rather than revealing them. One way to overcome this kind of mess is to use what are called small multiples. These are multiple small plots that show similar data across different conditions.\nIn Matplotlib, small multiples are called sub-plots. That is also the reason that the function that creates these is called subplots.\nPreviously, we called this function with no inputs. This creates one subplot. Now, we\u0026rsquo;ll give it some inputs. Small multiples are typically arranged on the page as a grid with rows and columns. Here, we are creating a Figure object with 2 rows of subplots, and 2 columns. This is what this would look like before we add any data to it.\n1 2  fig, ax = plt.sublpots(2, 2) plt.show()   In this case, the variable ax is no longer only one Axes object. Instead, it is an numpy.ndarray of Axes objects with a shape of 2 by 2.\n1 2  print(ax.shape) print(type(ax))   (2, 2) \u0026lt;class 'numpy.ndarray'\u0026gt; To add data, we would now have to index into this object and call the plot method on an element of the array.\n1 2 3 4 5 6 7 8  fig, ax = plt.subplots(2, 2) ax[0, 0].plot(seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;]) ax[0, 1].plot(seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-TAVG-NORMAL\u0026#34;]) ax[1, 0].plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;]) ax[1, 1].plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-TAVG-NORMAL\u0026#34;]) plt.show()   There is a special case for situations where you have only one row or only one column of plots. In this case, the resulting array will be one-dimensional and you will only have to provide one index to access the elements of this array.\n1 2  fig, ax = plt.subplots(3, 1) ax.shape   (3,) To make sure that all the subplots have the same range of y-axis values, we initialize the figure and its subplots with the key-word argument sharey set to True. This means that both subplots will have the same range of y-axis values.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  fig, ax = plt.subplots(2, 1, sharey=True) ax[0].plot(seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;], color = \u0026#34;b\u0026#34;) ax[0].plot(seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-PRCP-25PCTL\u0026#34;], color = \u0026#34;b\u0026#34;, linestyle = \u0026#34;--\u0026#34;) ax[0].plot(seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-PRCP-75PCTL\u0026#34;], color = \u0026#34;b\u0026#34;, linestyle = \u0026#34;--\u0026#34;) ax[1].plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-PRCP-NORMAL\u0026#34;], color = \u0026#34;r\u0026#34;) ax[1].plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-PRCP-25PCTL\u0026#34;], color = \u0026#34;r\u0026#34;, linestyle = \u0026#34;--\u0026#34;) ax[1].plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-PRCP-75PCTL\u0026#34;], color = \u0026#34;r\u0026#34;, linestyle = \u0026#34;--\u0026#34;) ax[0].set_ylabel(\u0026#34;Seattle\u0026#34;) ax[1].set_ylabel(\u0026#34;Austin\u0026#34;) ax[1].set_xlabel(\u0026#34;MONTH\u0026#34;) plt.show()   Plotting time-series Plotting time-series data If we want pandas to recognize column with YYYY-MM-DD format as time-series, we\u0026rsquo;ll need to tell it to parse the this column as a date by parse_dates argument in read_csv method. To use the full power of pandas indexing facilities, we\u0026rsquo;ll also designate the date column as our index by using the index_col argument in read_csv method.\n1 2 3 4 5 6 7  (base) wanghaoming@localhost ~ % head -6 ~/Downloads/climate_change.csv date,co2,relative_temp 1958-03-06,315.71,0.1 1958-04-06,317.45,0.01 1958-05-06,317.5,0.08 1958-06-06,,-0.05 1958-07-06,315.86,0.06   1 2 3 4 5  climate_change = pd.read_csv( \u0026#34;~/Downloads/climate_change.csv\u0026#34;, parse_dates=[\u0026#34;date\u0026#34;], index_col=\u0026#34;date\u0026#34; )   Matplotlib automatically chooses to show the time on the x-axis with intervals of 10 years or years or month depending on the length of data.\n1 2 3 4 5 6 7 8 9 10 11  fig, ax = plt.subplots() ax.plot( climate_change.index, climate_change[\u0026#34;relative_temp\u0026#34;] ) ax.set_xlabel(\u0026#34;Time\u0026#34;) ax.set_ylabel(\u0026#34;Relative temperature (Celsius)\u0026#34;) plt.show()   1 2 3 4 5 6  fig, ax = plt.subplots() seventies = climate_change[\u0026#34;1970-01-01\u0026#34;: \u0026#34;1979-12-31\u0026#34;] ax.plot(seventies.index, seventies[\u0026#34;co2\u0026#34;]) plt.show()   Plotting time-series with different variables You could plot two time-series in separate sub-plots. Here, we\u0026rsquo;re going to plot them in the same sub-plot, using two different y-axis scales.\nWe start by adding the first variable to the Axes. Then, we use the twinx method to create a twin of this Axes. This means that the two Axes share the same x-axis, but the y-axes are separate.\nWe add the other variable to this second Axes object and show the figure. There is one y-axis scale on the left, and another y-axis scale to the right.\n1 2 3 4 5 6 7 8  fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[\u0026#34;co2\u0026#34;], color=\u0026#34;b\u0026#34;) ax2 = ax.twinx() ax2.plot(climate_change.index, climate_change[\u0026#34;relative_temp\u0026#34;], color=\u0026#34;r\u0026#34;) plt.show()   We set color argument in our calls to plot and set_ylabel method from the origin Axes twin Axes object. In the resulting figure, each variable has its own color and the y-axis labels clearly tell us which scale belongs to which variable.\nWe can also make encoding by color more distinct by setting y-axis ticks and the y-axis tick labels. This is done by adding a call to the tick_params method. This method takes either y or x as its first argument, pointing to the fact that we are modifying the parameters of the y-axis or x-axis ticks and tick labels. To change their color, we use the colors key-word argument.\n1 2 3 4 5 6 7 8 9 10 11 12 13  fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[\u0026#34;co2\u0026#34;], color=\u0026#34;b\u0026#34;) ax.set_ylabel(\u0026#34;CO2\u0026#34;, color=\u0026#34;b\u0026#34;) ax.tick_params(\u0026#34;y\u0026#34;, colors=\u0026#34;b\u0026#34;) # \u0026lt;-- Notice \u0026#34;s\u0026#34; ax.set_xlabel(\u0026#34;Year\u0026#34;) ax2 = ax.twinx() ax2.plot(climate_change.index, climate_change[\u0026#34;relative_temp\u0026#34;], color=\u0026#34;r\u0026#34;) ax2.set_ylabel(\u0026#34;Relative Temp\u0026#34;, color=\u0026#34;r\u0026#34;) ax2.tick_params(\u0026#34;y\u0026#34;, colors=\u0026#34;r\u0026#34;) plt.show()   We can implement this as a function that we can reuse. Using our function, we don\u0026rsquo;t have to repeat these calls, and the code is simpler.\n1 2 3 4 5  def plot_timeseries(axes, x, y, color, xlabel, ylabel): axes.plot(x, y, color=color) axes.set_xlabel(xlabel) axes.set_ylabel(ylabel, color=color) axes.tick_params(\u0026#39;y\u0026#39;, colors=color)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  fig, ax = plt.subplots() plot_timeseries( ax, climate_change.index, climate_change[\u0026#34;co2\u0026#34;], \u0026#34;blue\u0026#34;, \u0026#34;Time (years)\u0026#34;, \u0026#34;CO2 levels\u0026#34; ) ax2 = ax.twinx() plot_timeseries( ax2, climate_change.index, climate_change[\u0026#34;relative_temp\u0026#34;], \u0026#34;red\u0026#34;, \u0026#34;Time (years)\u0026#34;, \u0026#34;Relative temperature (Celsius)\u0026#34; ) plt.show()   Annotating time-series data Annotations are usually small pieces of text that refer to a particular part of the visualization, focusing our attention on some feature of the data and explaining this feature.\nWe call a method of the Axes object called annotate. This function takes the annotation text as input, and the xy coordinate that we would like to annotate. The annotate method takes an optional xy text argument that selects the xy position of the text.\n1 2 3 4 5 6 7 8 9 10  fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[\u0026#34;relative_temp\u0026#34;]) ax.annotate( \u0026#34;\u0026gt;1 degree\u0026#34;, xy=(pd.Timestamp(\u0026#39;2015-10-06\u0026#39;), 1), xytext=(pd.Timestamp(\u0026#39;2010-10-06\u0026#39;), 0) ) plt.show()   To connect between the annotation text and the annotated data, we can add an arrow. The key-word argument of annotate method to do this is called arrowprops. This argument takes as input a dictionary that defines the properties of the arrow that we would like to use.\nIf we pass an empty dictionary into the key-word argument, the arrow will have the default properties. We can also customize the appearance of the arrow, by dictionary key \u0026ldquo;arrowstyle\u0026rdquo; and \u0026ldquo;color\u0026rdquo;. Look more customizing method here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  fig, ax = plt.subplots() plot_timeseries( ax, climate_change.index, climate_change[\u0026#34;co2\u0026#34;], \u0026#39;blue\u0026#39;, \u0026#34;Time (years)\u0026#34;, \u0026#34;CO2 levels\u0026#34; ) ax2 = ax.twinx() plot_timeseries( ax2, climate_change.index, climate_change[\u0026#34;relative_temp\u0026#34;], \u0026#39;red\u0026#39;, \u0026#34;Time (years)\u0026#34;, \u0026#34;Relative temp (Celsius)\u0026#34; ) ax2.annotate( \u0026#34;\u0026gt;1 degree\u0026#34;, xy=(pd.Timestamp(\u0026#39;2015-10-06\u0026#39;), 1), xytext=(pd.Timestamp(\u0026#39;2008-10-06\u0026#39;), -0.2), arrowprops = { \u0026#34;arrowstyle\u0026#34;: \u0026#34;-\u0026gt;\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;gray\u0026#34; } ) plt.show()   Quantitative comparisons and statistical visualizations Quantitative comparisons: bar-charts We will use a dataset that contains information about the number of medals won by a few countries in the 2016 Olympic Games.\n1 2 3 4 5 6 7  (base) wanghaoming@localhost ~ % head -6 ~/Downloads/medals_by_country_2016.csv ,Bronze,Gold,Silver United States,67,137,52 Germany,67,47,43 Great Britain,26,64,55 Russia,35,50,28 China,35,44,30   We tell pandas to create a DataFrame from a file that contains the data and to use the first column as the index for the DataFrame.\n1  medals = pd.read_csv(\u0026#34;~/Downloads/medals_by_country_2016.csv\u0026#34;, index_col=0)   We create a Figure and an Axes object and call the Axes bar method to create a bar chart. The labels of the x-axis ticks can be rotated by using the set_xticklabels method of the Axes.\n1 2 3 4 5 6 7  fig, ax = plt.subplots() ax.bar(medals.index, medals[\u0026#34;Gold\u0026#34;]) ax.set_xticklabels(medals.index, rotation=90) # \u0026lt;-- Notice here ax.set_ylabel(\u0026#34;Number of medals\u0026#34;) plt.show()   To add others information into the same plot, we\u0026rsquo;ll create a stacked bar chart. We add another call to the bar method to add the other data. We add the bottom key-word argument to tell Matplotlib that the bottom of this data should be at the height of the previous data.\nWe also need to add the label key-word argument to each call of the bar method with the label for the bars plotted in this call. Then add a call to the Axes legend method before calling show.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  fig, ax = plt.subplots() ax.bar(medals.index, medals[\u0026#34;Gold\u0026#34;], label=\u0026#34;Gold\u0026#34;) ax.bar( medals.index, medals[\u0026#34;Silver\u0026#34;], bottom=medals[\u0026#34;Gold\u0026#34;] , label=\u0026#34;Silver\u0026#34; ) ax.bar( medals.index, medals[\u0026#34;Bronze\u0026#34;], bottom=medals[\u0026#34;Gold\u0026#34;] + medals[\u0026#34;Silver\u0026#34;], # \u0026lt;-- Notice here label=\u0026#34;Bronze\u0026#34; ) ax.legend() plt.show()   Quantitative comparisons: histograms A histogram would show the full distribution of values within each variable. We call the Axes hist method with the column of the DataFrame.\nAs before, we can label a variable by calling the hist method with the label key-word argument and then calling the legend method before we call plt.show.\n1 2 3 4 5 6 7 8 9 10  fig, ax = plt.subplots() ax.hist(mens_rowing[\u0026#34;Weight\u0026#34;], label=\u0026#34;Rowing\u0026#34;) ax.hist(mens_gymnastics[\u0026#34;Weight\u0026#34;], label=\u0026#34;Gymnastics\u0026#34;) ax.set_xlabel(\u0026#34;Weight (kg)\u0026#34;) ax.set_ylabel(\u0026#34;# of observations\u0026#34;) ax.legend() plt.show()   The number of default bars or bins in a histogram is 10, but we can customize that. If we provide an scalar to the bins key-word argument, the histogram will have that number of bins.\nIf we instead provide a list, these numbers will be set to be the boundaries between the bins.\nThe occlusion of two histogram plots can be eliminated by changing the type of histogram that is used. you can specify a histtype of \u0026ldquo;step\u0026rdquo;, which displays the histogram as thin lines, instead of solid bars.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  fig, ax = plt.subplots() ax.hist( mens_rowing[\u0026#34;Weight\u0026#34;], label=\u0026#34;Rowing\u0026#34;, bins=5, histtype=\u0026#34;step\u0026#34; ) ax.hist( mens_gymnastics[\u0026#34;Weight\u0026#34;], label=\u0026#34;Gymnastics\u0026#34;, bins=5, histtype=\u0026#34;step\u0026#34; ) ax.set_xlabel(\u0026#34;Weight (kg)\u0026#34;) ax.set_ylabel(\u0026#34;# of observations\u0026#34;) ax.legend() plt.show()   Statistical plotting The first is the use of error bars in plots. These are additional markers on a plot or bar chart that tell us something about the distribution of the data.\n Histograms show the entire distribution. Error bars instead summarize the distribution of the data in one number, such as the standard deviation of the values.\n There are at least two different ways to display error bars. Here, we add the error bar as an argument to a bar chart. Each call to the ax.bar method takes an x argument and a y argument. The yerr key-word argument takes an additional number.\n1 2 3 4 5 6 7  fig, ax = plt.subplots() ax.bar(\u0026#34;Rowing\u0026#34;, mens_rowing[\u0026#34;Height\u0026#34;].mean(), yerr=mens_rowing[\u0026#34;Height\u0026#34;].std()) ax.bar(\u0026#34;Gymnastics\u0026#34;, mens_gymnastics[\u0026#34;Height\u0026#34;].mean(), yerr=mens_gymnastics[\u0026#34;Height\u0026#34;].std()) ax.set_ylabel(\u0026#34;Height (cm)\u0026#34;) plt.show()   It summarizes the full distribution that you saw in the histograms in two numbers: the mean value, and the spread of values, quantified as the standard deviation.\nWe can also add error bars to a line plot. To plot this data with error bars, we will use the ax.errorbar method. This method takes a sequence of x values, y values and yerr values.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  fig, ax = plt.subplots() ax.errorbar( seattle_weather[\u0026#34;MONTH\u0026#34;], seattle_weather[\u0026#34;MLY-TAVG-NORMAL\u0026#34;], seattle_weather[\u0026#34;MLY-TAVG-STDDEV\u0026#34;] ) ax.errorbar( austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-TAVG-NORMAL\u0026#34;], austin_weather[\u0026#34;MLY-TAVG-STDDEV\u0026#34;] ) ax.set_ylabel(\u0026#34;Temperature (Fahrenheit)\u0026#34;) plt.show()   The second statistical visualization technique is the boxplot. It is implemented by ax.boxplot method. We can call it with a sequence of sequences. We add labels on each of the variables separately by calling ax.set_xticklabels method, labeling the y-axis as well.\n1 2 3 4 5 6 7 8 9 10 11 12 13  fig, ax = plt.subplots() ax.boxplot( [ mens_rowing[\u0026#34;Height\u0026#34;], mens_gymnastics[\u0026#34;Height\u0026#34;], ] ) ax.set_xticklabels([\u0026#34;Rowing\u0026#34;, \u0026#34;Gymnastics\u0026#34;]) ax.set_ylabel(\u0026#34;Height (cm)\u0026#34;) plt.show()   Quantitative comparisons: scatter plots A standard visualization for bi-variate comparisons is a scatter plot. To create this plot, we initialize a Figure and Axes objects and call the Axes scatter method.\nWe can customize scatter plots in a manner that is similar to the customization that we introduced in other plots. For example, we set the color argument to set color on each group of points.\n1 2 3 4 5 6 7 8 9 10 11 12 13  fig, ax = plt.subplots() eighty = climate_change[\u0026#34;1980-01-01\u0026#34;: \u0026#34;1989-12-31\u0026#34;] ninety = climate_change[\u0026#34;1990-01-01\u0026#34;: \u0026#34;1999-12-31\u0026#34;] ax.scatter(eighty[\u0026#34;co2\u0026#34;], eighty[\u0026#34;relative_temp\u0026#34;], color=\u0026#34;b\u0026#34;, label=\u0026#34;1980s\u0026#34;) ax.scatter(ninety[\u0026#34;co2\u0026#34;], ninety[\u0026#34;relative_temp\u0026#34;], color=\u0026#34;r\u0026#34;, label=\u0026#34;1990s\u0026#34;) ax.set_xlabel(\u0026#34;CO2 (ppm)\u0026#34;) ax.set_ylabel(\u0026#34;Relative temperature (C)\u0026#34;) ax.legend() plt.show()   But we can also enter a Serise as input to the c key-word argument, this variable will get encoded as color.\n Note that this is not the color key-word argument that we used before, but is instead just the letter c.\n Now, time of the measurements is encoded in the brightness of the color applied to the points, with dark blue points early on and later points in bright yellow.\n1 2 3 4 5 6 7 8 9 10 11 12  fig, ax = plt.subplots() ax.scatter( climate_change[\u0026#34;co2\u0026#34;], climate_change[\u0026#34;relative_temp\u0026#34;], c=climate_change.index ) ax.set_xlabel(\u0026#34;CO2 (ppm)\u0026#34;) ax.set_ylabel(\u0026#34;Relative temperature (C)\u0026#34;) plt.show()   Sharing visualizations with others Plot style we\u0026rsquo;ll change the overall style of the figure. we call plt.style.use() method before the plotting code, the figure style will look completely different.\nwe can choose \u0026ldquo;ggplot\u0026rdquo; style, or \u0026ldquo;seaborn-colorblind\u0026rdquo;, click here to find more style.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  plt.style.use(\u0026#34;seaborn\u0026#34;) fig, ax = plt.subplots() ax2 = ax.twinx() ax.plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-TAVG-NORMAL\u0026#34;], color=\u0026#34;b\u0026#34;) ax2.plot(austin_weather[\u0026#34;MONTH\u0026#34;], austin_weather[\u0026#34;MLY-TAVG-STDDEV\u0026#34;], color=\u0026#34;r\u0026#34;) ax.set_xlabel(\u0026#34;MONTH\u0026#34;) ax.set_ylabel(\u0026#34;MLY-TAVG-NORMAL\u0026#34;, color=\u0026#34;b\u0026#34;) ax.tick_params(\u0026#34;y\u0026#34;, colors=\u0026#34;b\u0026#34;) ax2.set_ylabel(\u0026#34;MLY-TAVG-NORMAL\u0026#34;, color=\u0026#34;r\u0026#34;) ax2.tick_params(\u0026#34;y\u0026#34;, colors=\u0026#34;r\u0026#34;) ax.set_title(\u0026#34;Title\u0026#34;) plt.show()   Saving visualizations Now, we replace the call to plt.show() with a call to the Figure object\u0026rsquo;s fig.savefig method. We provide a file-path as input to the function. If we do this, the figure will no longer appear on our screen, but instead appear as a file on our file-system.\nUsing savefig method, we can saved the figure as a png, jpg, svg \u0026hellip; file. You can control how small the resulting file will be, and the degree of loss of quality, by setting the quality key-word argument. This will be a number between 1 and 100. you can also control the quality through setting the dpi key-word argument.\n1  fig.savefig(\u0026#34;~/Documents/figure.png\u0026#34;, dpi=300)   Finally, you can call set_size_inches method to control is the size of the figure. This function takes a list. The first number sets the width of the figure and the second number sets the height of the figure.\n1 2  fig.set_size_inches([3,5]) fig.savefig(\u0026#34;~/Documents/figure_3_5.png\u0026#34;)   Automating figures from data you can write functions and programs that automatically adjust what they are doing based on the input data.\n1 2 3 4 5 6 7 8 9 10 11 12 13  sports_column = summer_2016_medals[\u0026#34;Sport\u0026#34;] sports = sports_column.unique() fig, ax = plt.subplots() for sport in sports: sport_df = summer_2016_medals[summer_2016_medals[\u0026#34;Sport\u0026#34;]==sport] ax.bar(sport, sport_df[\u0026#34;Weight\u0026#34;].mean(), yerr=sport_df[\u0026#34;Weight\u0026#34;].std()) ax.set_ylabel(\u0026#34;Weight\u0026#34;) ax.set_xticklabels(sports, rotation=90) plt.show()   You can also do this to achieve a similar effect:\n1  summer_2016_medals.groupby(\u0026#34;Sport\u0026#34;)[\u0026#34;Weight\u0026#34;].mean().plot(kind=\u0026#34;bar\u0026#34;)   ","date":"2020-04-16T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/data-visualization-i-matplotlib/","title":"Data Visualization I (Matplotlib)"},{"content":"","date":"2020-03-04T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/high-performance-python-v-big-data/","title":"High-Performance Python V (Big Data)"},{"content":"1 2 3 4  import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline   Linear separable Generate the data: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  randlist1 = np.array([np.random.uniform(-5,5) for i in range(500)]) randlist2 = np.array([np.random.uniform(0,3) for i in range(500)]) data_p1_0 = pd.DataFrame( { \u0026#34;x0\u0026#34;: 1, \u0026#34;x1\u0026#34;: randlist1, \u0026#34;x2\u0026#34;: randlist1 + randlist2, \u0026#34;y\u0026#34; : -1 } ) data_m1_0 = pd.DataFrame( { \u0026#34;x0\u0026#34;: 1, \u0026#34;x1\u0026#34;: randlist1, \u0026#34;x2\u0026#34;: randlist1 - randlist2, \u0026#34;y\u0026#34; : +1 } ) df = pd.concat([data_m1_0, data_p1_0], axis=0)   PLA model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  plt.figure(figsize=(8,6)) plt.scatter(data_m1_0[\u0026#34;x1\u0026#34;], data_m1_0[\u0026#34;x2\u0026#34;], c=\u0026#34;r\u0026#34;, alpha=0.5, linewidths=0) plt.scatter(data_p1_0[\u0026#34;x1\u0026#34;], data_p1_0[\u0026#34;x2\u0026#34;], c=\u0026#34;b\u0026#34;, alpha=0.5, linewidths=0) w = pd.Series([0, 0, 0], index=[\u0026#34;x0\u0026#34;, \u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;]); times = 1; wl = []; x=np.linspace(-5,5,100) wl.append(w) mis_p = df[df[\u0026#34;y\u0026#34;]==+1][np.dot(df[df[\u0026#34;y\u0026#34;]==+1].iloc[:,0:df.shape[1]-1], w) \u0026lt;= 0] mis_n = df[df[\u0026#34;y\u0026#34;]==-1][np.dot(df[df[\u0026#34;y\u0026#34;]==-1].iloc[:,0:df.shape[1]-1], w) \u0026gt; 0] mis = pd.concat([mis_p, mis_n], axis=0) length = mis.shape[0] while length \u0026gt; 0: mis_point = mis.iloc[np.random.randint(length),:] w = w + mis_point[\u0026#34;y\u0026#34;] * mis_point.iloc[0: mis_point.shape[0]-1] mis_p = df[df[\u0026#34;y\u0026#34;]==+1][np.dot(df[df[\u0026#34;y\u0026#34;]==+1].iloc[:,0:df.shape[1]-1], w) \u0026lt;= 0] mis_n = df[df[\u0026#34;y\u0026#34;]==-1][np.dot(df[df[\u0026#34;y\u0026#34;]==-1].iloc[:,0:df.shape[1]-1], w) \u0026gt; 0] mis = pd.concat([mis_p, mis_n], axis=0) length = mis.shape[0] times += 1; wl.append(w) if times in [1, 2, 3, 4, 5, 10, 15, 50, 100]: plt.plot(x, -(w[1]/w[2])*x - (w[0]/w[2]), \u0026#34;--\u0026#34;, label=f\u0026#34;{times}hypothesis\u0026#34;) if times \u0026gt; 50000 : print(\u0026#34;over 50000 times loops\u0026#34;) break plt.plot(x, -(w[1]/w[2])*x - (w[0]/w[2]), \u0026#34;c\u0026#34;, label=f\u0026#34;end hypothesis (times={times})\u0026#34;) plt.legend() plt.savefig(\u0026#34;/Users/wanghaoming/Documents/LaTeX_doc/Machine_Learning/pla.png\u0026#34;, bbox_inches=\u0026#39;tight\u0026#39;, dpi=500)   Convergence process Convergence process of weight vector: $\\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_t}{||\\mathbf{w}_f||\\cdot||\\mathbf{w}_t||}$\n1 2 3 4 5 6 7 8 9 10 11 12  wl1 = pd.DataFrame(wl) wl2 = wl1[[\u0026#34;x0\u0026#34;, \u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;]].reset_index(drop=True) wf = np.array([0,1,-1]) wt = wl2.dot(wf) / (np.array([wl2.loc[v].dot(wl2.loc[v]) for v in wl2.index]) * wf.dot(wf)) ** 0.5 plt.figure(figsize=(8,6)) plt.plot( range(wt.shape[0]), wt, label= r\u0026#34;$\\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_t}{||\\mathbf{w}_f||\\cdot||\\mathbf{w}_t||}$\u0026#34; ) plt.legend() plt.savefig(\u0026#34;/Users/wanghaoming/Documents/LaTeX_doc/Machine_Learning/converg.png\u0026#34;, bbox_inches=\u0026#39;tight\u0026#39;, dpi=500)   Calculate the maximum number of iterations: we have\n$$ \\mathbb{w}_{f}^{T} = \\mathbb{w}_{t} $$\nand\n$$ \\begin{aligned} ||\\mathbf{w}_{t+1}||^2 \u0026amp;= || \\mathbf{w}_{t}+y_{n(t)}\\mathbf{x}_{n(t)} ||^2\\\\ \u0026amp;= ||\\mathbf{w}_{t}||^2+2y_{n(t)}\\mathbf{w}_{t}^{T}\\mathbf{x}_{n(t)} + y_{n(t)}^{2}||\\mathbf{x}_{n}||^2\\\\ \u0026amp;\\leq ||\\mathbf{w}_{t}||^2+||\\mathbf{x}_{n}||^2\\\\ \u0026amp;\\leq ||\\mathbf{w}_{t}||^2 + \\max_{n}||\\mathbf{x}_{n}||^2. \\end{aligned} $$\ndefine $R^2 = \\max_{n}||\\mathbf{x}||^2$ and $\\rho = \\min_{n}\\frac{y_n\\mathbf{w}_{f}^{T}\\mathbf{x}_n}{||\\mathbf{w}_f||}$\nwe have\n$$ \\begin{aligned} ||\\mathbf{w}t||^2 \u0026amp;\\leq ||\\mathbf{w}{t-1}||^2 + R^2\\ \u0026amp;\\leq ||\\mathbf{w}{t-2}||^2 + 2R^2\\ \u0026amp;\\cdots\\ \u0026amp;\\leq ||\\mathbf{w}{0}||^2 + tR^2\\ \u0026amp;=tR^2. \\end{aligned} $$\ni.e. $||\\mathbf{w}_t||\\leq R\\sqrt{t}$\nsince\n1 2 3 4 5 6  $$\\begin{aligned} x +y \u0026amp;=z \\\\z +s \u0026amp;=t \\end{aligned} $$   $$ \\begin{aligned} x + y \u0026amp;= z \\\\ z + s \u0026amp;= t \\end{aligned} $$\n1 2 3 4 5 6 7 8 9 10 11 12  $$\\begin{aligned} \\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_t}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} \u0026amp; =\\frac{\\mathbf{w}_{f}^{T}(\\mathbf{w}_{t-1}+y_{n(t-1)}\\mathbf{x}_{n(t-1)})}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||}\\\\\u0026amp;=\\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_{t-1}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} +\\frac{y_{n(t-1)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(t-1)}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||}\\\\\u0026amp;=\\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_{t-2}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} +\\frac{y_{n(t-2)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(t-2)}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} +\\frac{y_{n(t-1)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(t-1)}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||}\\\\\u0026amp;\\cdots\\\\\u0026amp;=\\frac{1}{||\\mathbf{w}_t||}\\cdot\\sum^{t}_{i=1}\\frac{y_{n(i)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(i)}}{||\\mathbf{w}_{f}^{T}||} \\\\\u0026amp;\\geq\\frac{1}{||\\mathbf{w}_t||}\\cdott\\cdot\\rho\\geq\\frac{t\\rho}{R\\sqrt{t}}\\\\\u0026amp;=\\frac{\\rho}{R}\\sqrt{t}. \\end{aligned} $$   $$ \\begin{aligned} \\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_t}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} \u0026amp; = \\frac{\\mathbf{w}_{f}^{T}(\\mathbf{w}_{t-1}+y_{n(t-1)}\\mathbf{x}_{n(t-1)})}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||}\\\\ \u0026amp;=\\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_{t-1}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} + \\frac{y_{n(t-1)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(t-1)}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||}\\\\ \u0026amp;= \\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}_{t-2}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} + \\frac{y_{n(t-2)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(t-2)}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||} + \\frac{y_{n(t-1)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(t-1)}}{||\\mathbf{w}_{f}^{T}||\\cdot||\\mathbf{w}_{t}||}\\\\ \u0026amp;\\cdots\\\\ \u0026amp;=\\frac{1}{||\\mathbf{w}_t||}\\cdot\\sum^{t}_{i=1}\\frac{y_{n(i)}\\mathbf{w}_{f}^{T}\\mathbf{x}_{n(i)}}{||\\mathbf{w}_{f}^{T}||} \\\\ \u0026amp;\\geq \\frac{1}{||\\mathbf{w}_t||}\\cdot t\\cdot \\rho\\geq \\frac{t\\rho}{R\\sqrt{t}}\\\\ \u0026amp;=\\frac{\\rho}{R}\\sqrt{t}. \\end{aligned} $$\nwe have that $\\frac{\\sqrt{t}\\rho}{R}\\leq 1$ and $t\\leq \\frac{R^2}{\\rho^2}$\n1 2 3 4 5 6  x_vec = df[[\u0026#34;x0\u0026#34;, \u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;]].reset_index(drop=True) y = df[\u0026#34;y\u0026#34;] R = (np.array([x_vec.loc[r].dot(x_vec.loc[r]) for r in x_vec.index]).max()) ** 0.5 rhot = ((np.array([wf.dot(x_vec.loc[v]) for v in x_vec.index]) * y) / (wf.dot(wf))**0.5) rho = rhot.min() print(R, rho)   9.391078270239522 0.0014954984184524147  therefore the maximum iterations of this model is\n1 2  maxt = (R/rho)**2 maxt   39432926.04282378  Non-Linear separable Generate the data: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  randlist1 = np.array([np.random.uniform(-5,5) for i in range(500)]) randlist2 = np.array([np.random.uniform(-0.1,3) for i in range(500)]) data_p1_0 = pd.DataFrame( { \u0026#34;x0\u0026#34;: 1, \u0026#34;x1\u0026#34;: randlist1, \u0026#34;x2\u0026#34;: randlist1 + randlist2, \u0026#34;y\u0026#34; : -1 } ) data_m1_0 = pd.DataFrame( { \u0026#34;x0\u0026#34;: 1, \u0026#34;x1\u0026#34;: randlist1, \u0026#34;x2\u0026#34;: randlist1 - randlist2, \u0026#34;y\u0026#34; : +1 } ) df = pd.concat([data_m1_0, data_p1_0], axis=0) plt.figure(figsize=(8,6)) plt.scatter(data_m1_0[\u0026#34;x1\u0026#34;], data_m1_0[\u0026#34;x2\u0026#34;], c=\u0026#34;r\u0026#34;, alpha=0.5, linewidths=0) plt.scatter(data_p1_0[\u0026#34;x1\u0026#34;], data_p1_0[\u0026#34;x2\u0026#34;], c=\u0026#34;b\u0026#34;, alpha=0.5, linewidths=0) plt.savefig(\u0026#34;/Users/wanghaoming/Documents/LaTeX_doc/Machine_Learning/non_sep.png\u0026#34;, bbox_inches=\u0026#39;tight\u0026#39;, dpi=500)   Building Pocket Model: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  plt.figure(figsize=(8,6)) plt.scatter(data_m1_0[\u0026#34;x1\u0026#34;], data_m1_0[\u0026#34;x2\u0026#34;], c=\u0026#34;r\u0026#34;, alpha=0.5, linewidths=0) plt.scatter(data_p1_0[\u0026#34;x1\u0026#34;], data_p1_0[\u0026#34;x2\u0026#34;], c=\u0026#34;b\u0026#34;, alpha=0.5, linewidths=0) w = pd.Series([0, 0, 0], index=[\u0026#34;x0\u0026#34;, \u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;]); times = 1; x=np.linspace(-5,5,100) mis_p = df[df[\u0026#34;y\u0026#34;]==+1][np.dot(df[df[\u0026#34;y\u0026#34;]==+1].iloc[:,0:df.shape[1]-1], w) \u0026lt;= 0] mis_n = df[df[\u0026#34;y\u0026#34;]==-1][np.dot(df[df[\u0026#34;y\u0026#34;]==-1].iloc[:,0:df.shape[1]-1], w) \u0026gt; 0] mis = pd.concat([mis_p, mis_n], axis=0) length = mis.shape[0] lengthl = [] T = 50000 for i in range(T): lengthl.append(length) mis_point = mis.iloc[np.random.randint(mis.shape[0]),:] w1 = w + mis_point[\u0026#34;y\u0026#34;] * mis_point.iloc[0: mis_point.shape[0]-1] mis_p = df[df[\u0026#34;y\u0026#34;]==+1][np.dot(df[df[\u0026#34;y\u0026#34;]==+1].iloc[:,0:df.shape[1]-1], w1) \u0026lt;= 0] mis_n = df[df[\u0026#34;y\u0026#34;]==-1][np.dot(df[df[\u0026#34;y\u0026#34;]==-1].iloc[:,0:df.shape[1]-1], w1) \u0026gt; 0] mis1 = pd.concat([mis_p, mis_n], axis=0) length1 = mis1.shape[0] if length1 \u0026lt; length: w = w1 mis = mis1 length=length1 lengthl.append(length) plt.plot(x, -(w[1]/w[2])*x - (w[0]/w[2]), \u0026#34;c\u0026#34;, label=f\u0026#34;end hypothesis (times={T})\u0026#34;) plt.legend() plt.savefig(\u0026#34;/Users/wanghaoming/Documents/LaTeX_doc/Machine_Learning/pocket.png\u0026#34;, bbox_inches=\u0026#39;tight\u0026#39;, dpi=500)   Error prediction number 1 2 3 4 5 6 7 8  ll = pd.Series(lengthl) plt.figure(figsize=(8,6)) ll.plot() plt.xscale(\u0026#34;log\u0026#34;) for i in range(1, len(ll)): if ll[i] \u0026lt; ll[i-1]: plt.text(i ,ll[i], ll[i]) plt.savefig(\u0026#34;/Users/wanghaoming/Documents/LaTeX_doc/Machine_Learning/minlen.png\u0026#34;, bbox_inches=\u0026#39;tight\u0026#39;, dpi=500)   $\\mathbb{R}\\times\\mathbb{R}$ $$ \\mathbf{R}\\mathbf{R} $$\n\\href{https://katex.org/}{\\frac{\\mathbf{w}_{f}^{T}\\mathbf{w}t}{||\\mathbf{w}{f}^{T}||\\cdot}}\n$$ \\mathbf{w}_{t}^T\\mathbf{w}_{t} $$\n$$ \\mathbf{R}^T\\mathbf{R} $$\n","date":"2020-03-04T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/perceptron-learning-algorithm/","title":"Perceptron Learning Algorithm"},{"content":"","date":"2020-02-16T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/high-performance-python-iv-distribute-computing-pyspark/","title":"High-Performance Python IV (Distribute Computing \u0026 PySpark)"},{"content":"","date":"2020-02-01T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/high-performance-python-iii-multi-process/","title":"High-Performance Python III (Multi Process)"},{"content":"","date":"2020-01-13T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/high-performance-python-ii-multi-threading/","title":"High-Performance Python II (Multi Threading)"},{"content":"","date":"2020-01-01T00:00:00Z","permalink":"https://wanghaoming177.netlify.app/p/high-performance-python-i-asyncio-coroutine/","title":"High-Performance Python I (Asyncio \u0026 Coroutine)"},{"content":"We have discussed many special objects in Python, like __doc__, __closure__, __name__, __defaults__ and __init()__, __eq__(), __str__(), __repr__, the former names (without parentheses) are called Special Attributes; the later names are called Magic Method. We will elaborate others spacial attribute and magic method detailedly in this blog.\nMagic Method __init__() Constructor constructor will be called automatically when class is called:\n1 2 3 4 5 6  class Cat: def __init__(self, name): self.name = name print(\u0026#34;this is a cat, named {}.\u0026#34;.format(self.name)) cat = Cat(\u0026#34;yaochong\u0026#34;)   this is a cat, named yaochong. __del__() Destructor Destructor will be called automatically when the object is collected (call del keyword).\n1 2 3 4 5 6 7 8 9 10 11 12  class Cat: def __init__(self, name): self.name = name print(\u0026#34;this is a cat, named {}.\u0026#34;.format(self.name)) def __del__(self): print(\u0026#34;cat {}is collected by system.\u0026#34;.format(self.name)) cat = Cat(\u0026#34;yaochong\u0026#34;) print(cat) del cat print(cat)   this is a cat, named yaochong. \u0026lt;__main__.Cat object at 0x104500dc0\u0026gt; cat yaochong is collected by system. --------------------------------------------------------------------------- NameError Traceback (most recent call last) /Users/wanghaoming/Documents/doc1/oop/test2.ipynb Cell 2' in \u0026lt;module\u0026gt; 9 cat = Cat(\u0026quot;yaochong\u0026quot;) 10 del cat ---\u0026gt; 11 print(cat) NameError: name 'cat' is not defined __call__() As we discussed before, there are two kinds of objects in Python, the callable objects and uncallable ones. For example, str, list, dirt can not be called, while function/method, class can. You can check weather or not an object could be called by callable() function.\n1 2  print(callable([1,2,3])) print(callable(list)) # \u0026lt;-- No parenthese   False True __call__() method is a callable object.\n1  callable(object.__call__)   True It will be called when you use a pair of parentheses behind the instance of the classes defined __call__(), and return the result. Thus __call__() method can transform a uncallabel object (instance) to a callable object. As a method, __call__() has nothing special, you can also call it by calling instance.__call__() directly.\nFor example, if we want to map a function to every element of a list, we would have three ways to attain that: The first is define a method; the second and three are transmuting the list into a method and call __call__() (inside the class) in different ways.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class mapls(list): def __init__(self, ls): self.ls = ls def maps(self,func): return [func(i) for i in self.ls] def __call__(self, func): return [func(i) for i in self.ls] ls = mapls([1,2,3]) print(ls.maps(lambda x: x+1)) print(ls(lambda x: x+1)) print(ls.__call__(lambda x: x+1))   [2, 3, 4] [2, 3, 4] [2, 3, 4] __len__() len is a function to get the length of a collection. It works by calling an object\u0026rsquo;s __len__ method, in the other word, __len__() (inside the class) is called when len()/__len__() (outside the class) is called, just like __call__(). Thus when you call len(instance) outside the class, it is equivalent with instance.__len__().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class Cat: def __init__(self, attr1, attr2): self.attr1 = attr1 self.attr2 = attr2 def func1(self): print(\u0026#34;func1\u0026#34;) def __len__(self): return self.attr2 ** 2 cat = Cat(\u0026#34;haoming\u0026#34;, 15) print(len(cat)) print(cat.__len__())   225 225 __iter__() Just like __call__() method, the object in Python can also be classified as iterable and non-iterable. Using __iter__() method can transmute a non-iterable object into iterabel one.\nLike __call__() and __len__(), __iter__()(inside the class) will be called when you call iter(instance)/instance.__iter__() outside the class, and return a interator.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  class Cat: def __init__(self, attr1, attr2): self.attr1 = attr1 self.attr2 = attr2 def func1(self): print(\u0026#34;func1\u0026#34;) def __iter__(self): print(\u0026#34;__iter__() method has been called\u0026#34;) return iter(self.attr2) cat = Cat(\u0026#34;haoming\u0026#34;, \u0026#34;abcdefg\u0026#34;) itcat1 = iter(cat) itcat2 = cat.__iter__() print(next(itcat1)) print(next(itcat1)) print(next(itcat1)) print(\u0026#34;------------\u0026#34;) print(next(itcat2)) print(next(itcat2)) print(next(itcat2))   __iter__() method has been called __iter__() method has been called a b c ------------ a b c But you can also replace iter(iterable) with iterable.__iter__() inside the class:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  class Dog: def __init__(self, attr1, attr2): self.attr1 = attr1 self.attr2 = attr2 def func1(self): print(\u0026#34;func1\u0026#34;) def __iter__(self): print(\u0026#34;__iter__() method has been called\u0026#34;) return self.attr2.__iter__() dog = Dog(\u0026#34;yaochong\u0026#34;, \u0026#34;abcdefg\u0026#34;) itcat1 = iter(dog) itcat2 = dog.__iter__() print(next(itcat1)) print(next(itcat1)) print(next(itcat1)) print(\u0026#34;------------\u0026#34;) print(next(itcat2)) print(next(itcat2)) print(next(itcat2))   __iter__() method has been called __iter__() method has been called a b c ------------ a b c __getitem__(), __setitem__() and __delitem__() Let\u0026rsquo;s recall the operations on dict class, we can refer, modify and delete the element of dictionary using brackets:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  dct = { k:v for k,v in zip( list(\u0026#34;abcd\u0026#34;), range(4) ) } # refer print(dct[\u0026#34;a\u0026#34;]) # modify dct[\u0026#34;a\u0026#34;] = 4 print(dct[\u0026#34;a\u0026#34;]) # delete del dct[\u0026#34;b\u0026#34;] print(dct)   0 4 {'a': 4, 'c': 2, 'd': 3} But how to add this property to our general class? The solution is __getitem__(), __setitem__() and __delitem__(). These methods will be called when you add bracket with the key behind the instance:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  class Dog: def __init__(self, attr1, attr2, attr3=10): self.attr1 = attr1 self.attr2 = attr2 self.attr3 = attr3 def __getitem__(self, key): print(\u0026#34;__getitem__ is called\u0026#34;) if key in self.__dict__: return self.__dict__[key] else: print(\u0026#34;No such attr\u0026#34;) def __setitem__(self, key, value): print(\u0026#34;__setitem__ is called\u0026#34;) self.__dict__[key] = value def __delitem__(self, key): print(\u0026#34;__delitem__ is called\u0026#34;) del self.__dict__[key] dog = Dog(1,2,3) print(dog[\u0026#34;attr1\u0026#34;]) dog[\u0026#34;attr1\u0026#34;] = 4 print(dog[\u0026#34;attr1\u0026#34;]) del dog[\u0026#34;attr1\u0026#34;] print(dog.__dict__)   __getitem__ is called 1 __setitem__ is called __getitem__ is called 4 __delitem__ is called {'attr2': 2, 'attr3': 3} In the same way, you can also call them by instance.__xxxitem__(\u0026quot;key\u0026quot;) directly.\n1 2 3 4 5 6 7 8 9  dog = Dog(1,2,3) print(dog.__getitem__(\u0026#34;attr1\u0026#34;)) dog.__setitem__(\u0026#34;attr1\u0026#34;,4) print(dog[\u0026#34;attr1\u0026#34;]) dog.__delitem__(\u0026#34;attr1\u0026#34;) print(dog.__dict__)   __getitem__ is called 1 __setitem__ is called __getitem__ is called 4 __delitem__ is called {'attr2': 2, 'attr3': 3} Mathematics operations The reason why we name the method mentioned in this blog as Magic Method is that it can magically change the operation and relationship of the objects. For example, we can transform uncallable/unlenable/uniterable/unindexedable object to callable/lenable/iterable/indexedable ones. Something even more amazing is you can even define mathematics operations on general class.\nFor example, there are two list class instance here\n1 2  ls1 = [i for i in range(10)] ls2 = [i for i in range(10)]   the following operation rules are allowed:\n list class instance + list class instance list class instance * int calss instance  but the others are invalid. The former operation concatenates two list; the later concatenates same list multi-times. We now are going to modify this operation rules as called vector operation.\n1 2 3 4 5 6  ls1 = [i for i in range(4)] ls2 = [i for i in range(4)] print(ls1 + ls2) print(ls1 * 3) print(ls1 - ls2)   --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /Users/wanghaoming/Documents/doc1/oop/test2.ipynb Cell 63' in \u0026lt;module\u0026gt; 4 print(ls1 + ls2) 5 print(ls1 * 3) ----\u0026gt; 6 print(ls1 - ls2) TypeError: unsupported operand type(s) for -: 'list' and 'list' These are magic methods of mathematics operation:\n +: __add__() -: __sub__() *: __mul__() /: __div__() %: __mod__() ^: __pow__()  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  class vlist(list): def __add__(self,other): if isinstance(other, vlist): lses = [self[i]+other[i] for i in range(len(self))] return lses else: raise ValueError(\u0026#34;Must be vlist obejct\u0026#34;) def __sub__(self,other): if isinstance(other, vlist): lses = [self[i]-other[i] for i in range(len(self))] return lses else: raise ValueError(\u0026#34;Must be vlist obejct\u0026#34;) ls1 = vlist([i for i in range(10)]) ls2 = vlist([i for i in range(10)]) print(ls1 + ls2) print(ls1 - ls2)   [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] Special Attribute __name__ When a Python module or package is imported, __name__ is set to the module\u0026rsquo;s name. Usually, this is the name of the Python file itself without the .py extension. If the file is part of a package, __name__ will also include the parent package\u0026rsquo;s path.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import os filename = \u0026#34;./test_pkg/test_pkg_module.py\u0026#34; os.makedirs(os.path.dirname(filename), exist_ok=True) tm = \u0026#34;\u0026#34;\u0026#34; class Cat: def __init__(self, name): self.name = name def cat_func(self): print(\u0026#34;cat_func is called\u0026#34;) \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;test_module.py\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(tm) with open(filename, \u0026#34;w\u0026#34;) as f: f.write(tm) import test_module as tm import test_pkg.test_pkg_module as tpm print(tm.__name__) print(tpm.__name__)   test_module test_pkg.test_pkg_module However, if the module is executed in the top-level code environment, its __name__ is set to the string \u0026lsquo;__main__'.\n__main__ So what is the \u0026ldquo;top-level code environment\u0026rdquo;? __main__ is the name of the environment where top-level code is run. \u0026ldquo;Top-level code\u0026rdquo; is the first user-specified Python module that starts running. It\u0026rsquo;s \u0026ldquo;top-level\u0026rdquo; because it imports all other modules that the program needs. Sometimes \u0026ldquo;top-level code\u0026rdquo; is called an entry point to the application. The most common top-level code environment are the scope of an interactive prompt \u0026gt;\u0026gt;\u0026gt;, script, IPython console or Jupyter notebook cells:\n interactive prompt:  1 2 3 4 5 6  (base) wanghaoming@localhost oop % python3 Python 3.9.1 (v3.9.1:1e5d33e9b9, Dec 7 2020, 12:44:01) [Clang 12.0.0 (clang-1200.0.32.27)] on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; __name__ \u0026#39;__main__\u0026#39;    python script:  1  print(__name__)   (base) wanghaoming@localhost doc1 % /usr/local/bin/python3 /Users/wanghaoming/Documents/doc1/oop/test3.py __main__  IPython console:  1 2  In[1]: __name__ Out[1]: \u0026#39;__main__\u0026#39;    Jupyter notebook cells:  1  __name__   '__main__'  You can click here for more top-level environment occasions.\n As a result, a module can discover whether or not it is running in the top-level environment by checking its own __name__, which allows a common idiom, if __name__ == '__main__':, for conditionally executing code when the module is not initialized from an import statement. We add lines of codes to test_module.py module:\n1 2 3 4 5 6 7 8 9 10  codes = \u0026#34;\u0026#34;\u0026#34; print(__name__) if __name__ == \u0026#39;__main__\u0026#39;: print(\u0026#34;__name__ = \u0026#39;__main__\u0026#39;\u0026#34;) \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;test_module.py\u0026#34;, \u0026#34;a\u0026#34;) as f: f.write(codes) !cat test_module.py   class Cat: def __init__(self, name): self.name = name def cat_func(self): print(\u0026quot;cat_func is called\u0026quot;) print(__name__) if __name__ == '__main__': print(\u0026quot;__name__ = '__main__'\u0026quot;) and execute it, the outcome is desired:\n1  !python test_module.py   __main__ __name__ = '__main__' But if we import the module, we have\n1 2  import test_module as tm print(tm.__name__)   test_module test_module So you will find that a module can contain executable statements as well as function definitions. These statements are intended to initialize the module. They are executed in __main__ the first time the module name is encountered in an import statement. (They are also run if the file is executed as a script.)\nSome modules contain code that is intended for script use only, like parsing command-line arguments or fetching data from standard input. If a module like this was imported from a different module, the script code would unintentionally execute as well. This is where using the code block comes in handy. Code within if __name__ == '__main__': block won\u0026rsquo;t run unless the module is executed in the top-level environment.\nThere is a confusing concept here: __main__ with __main__.py. You can create a directory or zipfile full of code, and include a __main__.py.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  import os from pathlib import Path dir_name = \u0026#34;./pkg/\u0026#34; os.makedirs(dir_name) Path(\u0026#34;./pkg/__init__.py\u0026#34;).touch() tm = \u0026#34;\u0026#34;\u0026#34; class Cat: \u0026#39;\u0026#39;\u0026#39; this is a Cat class in pkg.module. Args: name(str): name of the cat. \u0026#39;\u0026#39;\u0026#39; def __init__(self, name): \u0026#39;\u0026#39;\u0026#39; this is a constructor in pkg.module. \u0026#39;\u0026#39;\u0026#39; self.name = name print(\u0026#39;Cat in pkg.module has been called.\u0026#39;) def cat_func(self): print(\u0026#34;cat_func in pkg.module is called\u0026#34;) print(\u0026#39;pkg.module has been initialized\u0026#39;) \u0026#34;\u0026#34;\u0026#34; tm1 = \u0026#34;\u0026#34;\u0026#34; import test_pkg_module as tpm catt = tpm.Cat(\u0026#34;yachong\u0026#34;) catt.cat_func() \u0026#34;\u0026#34;\u0026#34; with open(dir_name+\u0026#34;test_pkg_module.py\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(tm) with open(dir_name+\u0026#34;__main__.py\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(tm1)   Then you can simply name the directory or zipfile on the command line, and it executes the __main__.py automatically.\n1  (base) wanghaoming@localhost oop % python pkg   pkg.module has been initialized Cat in module has been called. cat_func in pkg.module is called Note that a __main__ module usually doesn\u0026rsquo;t come from a __main__.py file. It can, but it usually doesn\u0026rsquo;t. When you run a script like python my_program.py, the script will run as the __main__ module instead of the my_program module. This also happens for modules run as $ python -m my_module, or in several other ways. If you saw the name __main__ in an error message, that doesn\u0026rsquo;t necessarily mean you should be looking for a __main__.py file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54  class Cat: \u0026#34;\u0026#34;\u0026#34; this is a Cat class in main. Args: name(str): name of the cat. \u0026#34;\u0026#34;\u0026#34; def __init__(self, name): \u0026#34;\u0026#34;\u0026#34; this is a constructor in main. \u0026#34;\u0026#34;\u0026#34; self.name = name print(\u0026#39;Cat in main has been called.\u0026#39;) def cat_func(self): print(\u0026#34;cat_func in main is called\u0026#34;) tm = \u0026#34;\u0026#34;\u0026#34; class Cat: \u0026#39;\u0026#39;\u0026#39; this is a Cat class in module. Args: name(str): name of the cat. \u0026#39;\u0026#39;\u0026#39; def __init__(self, name): \u0026#39;\u0026#39;\u0026#39; this is a constructor in module. \u0026#39;\u0026#39;\u0026#39; self.name = name print(\u0026#39;Cat in module has been called.\u0026#39;) def cat_func(self): print(\u0026#34;cat_func in module is called\u0026#34;) print(\u0026#39;module has been initialized\u0026#39;) \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;test_module.py\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(tm) import __main__ as mm import test_module as tm import pkg.test_pkg_module as tpm cat0 = Cat(\u0026#34;000\u0026#34;) cat1 = mm.Cat(\u0026#34;111\u0026#34;) cat2 = tm.Cat(\u0026#34;222\u0026#34;) cat3 = tpm.Cat(\u0026#34;333\u0026#34;) cats = [cat0, cat1, cat2, cat3] print(\u0026#34;------------------\u0026#34;) cls = [cat.__class__ for cat in cats] [ [ True if clsa==clsb else False for clsa in cls ] for clsb in cls ]   module has been initialized pkg.module has been initialized ------------------ Cat in main has been called. Cat in main has been called. Cat in module has been called. Cat in pkg.module has been called. ------------------ [[True, True, False, False], [True, True, False, False], [False, False, True, False], [False, False, False, True]] __doc__, __module__ \u0026amp; __class__ __doc__/__module__/__class__ represents the docstring/module/class of the object.\nDefine two class, one is defined in __main__ and another is defined in test_module.py module. Import the outside module, and created two instances with __main__.Cat and test_module.Cat respectively.\n1 2  cat0 = Cat(\u0026#34;yaochong\u0026#34;) cat1 = tm.Cat(\u0026#34;shaonan\u0026#34;)   Cat in main has been called. Cat in module has been called. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  print(\u0026#34;1.--------docstring of class and method---------\u0026#34;) print(cat0.__doc__) print(cat0.__init__.__doc__) print(cat1.__doc__) print(cat1.__init__.__doc__,\u0026#39;\\n\u0026#39;) print(\u0026#34;2.-------class \u0026amp; module of class--------\\n\u0026#34;) print(cat0.__module__) print(cat1.__module__) print(cat0.__class__) print(cat1.__class__) print(\u0026#34;\\n3.-------class of attribute--------\\n\u0026#34;) # print(cat0.name.__module__) # print(cat1.name.__module__) print(cat0.name.__class__) print(cat1.name.__class__) print(\u0026#34;\\n4.-------class \u0026amp; module of method---------\\n\u0026#34;) print(cat0.cat_func.__module__) print(cat1.cat_func.__module__) print(cat0.cat_func.__class__) print(cat1.cat_func.__class__)   1.--------docstring of class and method--------- this is a Cat class in main. Args: name(str): name of the cat. this is a constructor in main. this is a Cat class in module. Args: name(str): name of the cat. this is a constructor in module. 2.-------class \u0026amp; module of class-------- __main__ test_module \u0026lt;class '__main__.Cat'\u0026gt; \u0026lt;class 'test_module.Cat'\u0026gt; 3.-------class of attribute-------- \u0026lt;class 'str'\u0026gt; \u0026lt;class 'str'\u0026gt; 4.-------class \u0026amp; module of method--------- __main__ test_module \u0026lt;class 'method'\u0026gt; \u0026lt;class 'method'\u0026gt; __dict__ __dict__ represents a dictatory that contains the attributes with values of an object excluding special attribute. Notice that __dict__ does not incorporate the method. dic() is similar with __dict__ in a way, it return a list that contains all attribute and method of the object, but is does not provide those values.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  class Cat: def __init__(self, attr1): self.attr1 = attr1 self._attr1 = attr1 self.__attr1 = attr1 def func1(self): print(\u0026#34;func1\u0026#34;) def _func1(self): print(\u0026#34;func1\u0026#34;) def __func1(self): print(\u0026#34;func1\u0026#34;) cat = Cat(\u0026#34;haoming\u0026#34;) print(cat.__dict__) print(dir(cat))   {'attr1': 'haoming', '_attr1': 'haoming', '_Cat__attr1': 'haoming'} ['_Cat__attr1', '_Cat__func1', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_attr1', '_func1', 'attr1', 'func1'] ","date":"2019-09-15T00:00:00Z","image":"https://wanghaoming177.netlify.app/head_img/oop_hi/oo4.png","permalink":"https://wanghaoming177.netlify.app/p/object-oriented-programming-v-special-attributes-magic-methods/","title":"Object Oriented Programming V (Special Attributes \u0026 Magic Methods)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n OOP Fundamental The fundamental concepts of OOP are objects and classes. An object is a data structure incorporating information about state and behavior. For example, an object representing a customer can have a certain phone number and email associated with them, and behaviors like placeOrder or cancelOrder. The distinctive feature of OOP is that state and behavior are bundled together: instead of thinking of customer data separately from customer actions, we think of them as one unit representing a customer. This is called encapsulation, and it\u0026rsquo;s one of the core tenets of object-oriented programming.\nClasses are like blueprints for objects. They describe the possible states and behaviors that every object of a certain type could have. For example, if you say \u0026ldquo;every customer will have a phone number and an email, and will be able to place and cancel orders\u0026rdquo;, you just defined a class. This way, you can talk about customers in a unified way. In Python, everything is an object. In particular, everything you deal with in Python has a class, a blueprint associated with it under the hood. The existence of these unified interfaces, is why you can use one kind of object in the same way.\nClasses incorporate information about state and behavior. State information in Python is contained in attributes, and behavior information \u0026ndash; in methods. For example, every numpy array has an attribute \u0026ldquo;shape\u0026rdquo; and a method \u0026ldquo;reshape()\u0026rdquo;. Attributes (or states) in Python objects are represented by variables \u0026ndash; like numbers, or strings, or tuples. Methods, or behaviors, are represented by functions. Both are accessible from an object using the dot . syntax.\nYou can call type() on any Python object to find out its class. And You can list all the attributes and methods that an object has by calling dir() on it.\nAttributes and Method To start a new class definition, all you need is a class statement, followed by the name of the class, followed by a colon. Everything in the indented block after will be considered a part of the class. You can create an \u0026ldquo;empty\u0026rdquo; class by including the pass statement after the class declaration.\nWe can create objects of the class by specifying the name of the class, followed by parentheses. Here, c1 and c2 are two different objects of the empty class Customer.\n1 2 3 4 5 6 7 8  class Customer: pass c1 = Customer() c2 = Customer() print(c1) print(c2)   \u0026lt;__main__.Customer object at 0x7fbc4944ad00\u0026gt; \u0026lt;__main__.Customer object at 0x7fbc4944adc0\u0026gt; Defining a method is simple. Methods are functions, so the definition of a method looks just like a regular Python function, with one exception: the self argument that every method will have as the first argument, possibly followed by other arguments.\nClasses are templates. Objects of a class don\u0026rsquo;t yet exist when a class is being defined, but we often need a way to refer to the data of a particular object within class definition. That is the purpose of self - it\u0026rsquo;s a stand-in for the future object. That\u0026rsquo;s why every method should have the self argument \u0026ndash; so we could use it to access attributes and call other methods from within the class definition even when no objects were created yet. Python will handle self when the method is called from an object using the object.method() syntax. In fact, using object.method() is equivalent to passing that object as an argument. That\u0026rsquo;s why we don\u0026rsquo;t specify it explicitly when calling the method from an existing object.\n1 2 3 4 5 6  class Customer: def identify(self, name): print(\u0026#34;This is \u0026#34;+name) c1 = Customer() c1.identify(\u0026#34;haoming\u0026#34;)   This is haoming By the principles of OOP, the data describing the state of the object should be bundled into the object. For example, customer name should be an attribute of a customer object, instead of a parameter passed to a method. In Python attributes \u0026ndash; like variables \u0026ndash; are created by assignment, meaning an attribute manifests into existence only when a value is assigned to it. (thus you can modify attr by assignment either inside the class, i.e. method; or outside the class, assignment.)\nHere is a method set_name with arguments self and new_name. To create an attribute of the Customer class called \u0026ldquo;name\u0026rdquo;, all we need to do is to assign something to self.name. Here, we set the name attribute to the new_name parameter of the function. When we create a customer, it does not yet have a name attribute. But after the set_name method was called, the name attribute is created, and we can access it through .name.\n Remember, self is a stand-in for object, so self.attribute should remind you of the object.attribute syntax.\n 1 2 3 4 5 6 7 8  class Customer: def set_name(self, name): self.name = name def identify(self): print(\u0026#34;This is \u0026#34;+self.name) c1 = Customer() print(c1.name)   --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_74663/4057745890.py in \u0026lt;module\u0026gt; 6 7 c1 = Customer() ----\u0026gt; 8 print(c1.name) AttributeError: 'Customer' object has no attribute 'name' 1 2  c1.set_name(\u0026#34;haoming\u0026#34;) c1.identify()   This is haoming 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  # -------- Example -------- # class Employee: def set_name(self, new_name): self.name = new_name def set_salary(self, new_salary): self.salary = new_salary def give_raise(self, amount): self.salary = self.salary + amount def monthly_salary(self): return self.salary/12 emp = Employee() emp.set_name(\u0026#39;Korel Rossi\u0026#39;) emp.set_salary(50000) emp.give_raise(15000) mon_sal = emp.monthly_salary() print(\u0026#34;annual package: \u0026#34;, emp.salary) print(\u0026#34;month salary: \u0026#34;, mon_sal)   annual package: 65000 month salary: 5416.666666666667 __init__ constructor We have discussed that methods are functions within class with a special first argument self, and attributes are created by assignment and referred to using the self variable within methods.\nA better strategy would be to add data to the object when creating it, like you do when creating a numpy array or a DataFrame. Python allows you to add a special method __init__ called the constructor that is automatically called every time an object is created.\nSo now, we can pass the parameter in the parentheses when creating the customer object, and the __init__ method will be automatically called, and the attribute created. The init constructor is also a good place to set the default values for attributes.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # -------- Example -------- # 1. from datetime import datetime class Employee: def __init__(self, name, salary=0): self.name = name if salary \u0026gt; 0: self.salary = salary else: self.salary = 0 print(\u0026#34;Invalid salary!\u0026#34;) self.hire_date = datetime.today() self.hire_date = datetime.today() def give_raise(self, amount): self.salary += amount def monthly_salary(self): return self.salary/12 emp = Employee(\u0026#34;Korel Rossi\u0026#34;, -1000)   Invalid salary! 1 2 3 4 5 6  print(emp.name) print(emp.salary) emp.give_raise(12324) print(emp.monthly_salary) print(emp.monthly_salary()) print(emp.hire_date)   Korel Rossi 0 \u0026lt;bound method Employee.monthly_salary of \u0026lt;__main__.Employee object at 0x7fbc68aa6b80\u0026gt;\u0026gt; 1027.0 2019-08-21 11:56:58.019767 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # -------- Example -------- # 2. class Point: def __init__(self,x=0,y=0): \u0026#34;\u0026#34;\u0026#34;Accepts two arguments that initialize the corresponding attributes. Args: x(float,optional) y(float,optional) \u0026#34;\u0026#34;\u0026#34; self.x = x self.y = y def distance_to_origin(self): \u0026#34;\u0026#34;\u0026#34;Returns the distance from the point to the origin.\u0026#34;\u0026#34;\u0026#34; dis = (self.x ** 2 + self.y ** 2) ** .5 return dis def reflect(self, axis): \u0026#34;\u0026#34;\u0026#34;Reflects the point with respect to the x- or y-axis\u0026#34;\u0026#34;\u0026#34; if axis == \u0026#34;x\u0026#34;: self.y = -self.y elif axis == \u0026#34;y\u0026#34;: self.x = -self.x else: print(\u0026#34;error message\u0026#34;) pt = Point(x=3.0) pt.reflect(\u0026#34;y\u0026#34;) print((pt.x, pt.y)) pt.y = 4.0 print(pt.distance_to_origin())   (-3.0, 0) 5.0 There are some conventions that will make your code more reader-friendly.\n  There are two ways to define attributes: we can define an attribute in any method in a class; and then calling the method will add the attribute to the object. Alternatively, we can define them all together in the constructor. If possible, use latter, which makes your code more organized, readable, and maintainable code.\n  For classes, words should be camel case, which means that if your class name contains several words, they should be written without delimiters, and each word should start with a capital letter. For methods and attributes, words should be separated by underscores and start with lowercase letters.\n  the name \u0026ldquo;self\u0026rdquo; is a convention. You could actually use any name for the first variable of a method, it will always be treated as the object reference regardless.\n  classes, like functions, allow for docstrings which are displayed when help() is called on the object.\n  Inheritance and Polymorphism Instance and class data Class-level attribute Remember the class we defined above. It had attributes like name and salary, and we were able to assign specific values to them for each new instance of the class. These were Instance-level attributes. We used self to bind them to a particular instance.\nBut if you needed to store some data that is shared among all the instances of a class, you can define an attribute directly in the class body. This will create a Class-level attribute, that will serve as a \u0026ldquo;global variable\u0026rdquo; within a class.\nWe can define Class-level attribute MIN_SALARY, and set it to 30000. We refer to the attribute this attribute inside the class like we would use any global variable, only follow by the class name, instead of self. This MIN_SALARY variable will be shared among all the instances of the employee class. We can access it like any other attribute from an object instance, and the value will be the same across instances. Here we print the MIN_SALARY class attribute from two employee objects.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  class Employee: MIN_SALARY = 30000 # \u0026lt;-- assign Class-level attr, directly without `self` def __init__(self, name, salary=0): self.name = name # \u0026lt;-- assign Instance-level attr, bounded with instance by `self` if salary \u0026gt; Employee.MIN_SALARY: # \u0026lt;-- refer Class-level attr inside the class self.salary = salary else: self.salary = Employee.MIN_SALARY emp1 = Employee(\u0026#34;Haoming\u0026#34;, 37000) emp2 = Employee(\u0026#34;Yachong\u0026#34;, 36999) print(emp1.name) print(emp1.salary) print(emp1.MIN_SALARY) # \u0026lt;-- refer Class-level attr outside the class print(emp2.MIN_SALARY) print(Employee.MIN_SALARY)   Haoming 37000 30000 30000 30000 Notice that you can modify Class-level attr by assigning value to obj.ATTR directly. But if you do that, the only thing you changed is the specific instance\u0026rsquo;s attr, rather than the all instance.\n1 2 3 4 5  emp1.MIN_SALARY = 34000 print(emp1.MIN_SALARY) print(emp2.MIN_SALARY) print(Employee.MIN_SALARY)   34000 30000 30000 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # -------- Example -------- # 1 class Player: MAX_POSITION = 10 def __init__(self): self.position = 0 def move(self, steps): if self.position + steps \u0026lt; Player.MAX_POSITION: self.position += steps else: self.position = Player.MAX_POSITION def draw(self): drawing = \u0026#34;-\u0026#34; * self.position + \u0026#34;|\u0026#34; +\u0026#34;-\u0026#34;*(Player.MAX_POSITION - self.position) print(drawing) p = Player(); p.draw() p.move(4); p.draw() p.move(5); p.draw() p.move(3); p.draw()   |---------- ----|------ ---------|- ----------| Class-level method Instance-level methods are already shared between instances: the same code gets executed for every instance. The only difference is the data (different instance\u0026rsquo;s attr) that is fed into it.\nIt is possible to define methods bound to class rather than an instance, but they have a narrow application scope, because these methods will not be able to use any instance-level data. To define a class method, you start with a classmethod decorator, followed by a method definition. The only difference is that now the first argument is not self, but cls, referring to the class, just like the self argument was a reference to a particular instance. To call a class method, we use class.method syntax, rather than object.method syntax.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  class Employee: MIN_SALARY = 30000 def __init__(self, name, salary=0): self.name = name if salary \u0026gt; Employee.MIN_SALARY: self.salary = salary else: self.salary = Employee.MIN_SALARY @classmethod def minsalary(cls, min_salary): Employee.MIN_SALARY = min_salary @classmethod def delta_minsalary(cls, delta): Employee.MIN_SALARY += delta emp1 = Employee(\u0026#34;Haoming\u0026#34;, 37000) emp2 = Employee(\u0026#34;Yachong\u0026#34;, 36999) emp1.MIN_SALARY = 27000 print(\u0026#34;1: \u0026#34;, emp1.MIN_SALARY) print(\u0026#34;2: \u0026#34;, emp2.MIN_SALARY) Employee.minsalary(25000) print(\u0026#34;3: \u0026#34;, emp1.MIN_SALARY) print(\u0026#34;4: \u0026#34;, emp2.MIN_SALARY) Employee.delta_minsalary(500) print(\u0026#34;5: \u0026#34;, emp1.MIN_SALARY) print(\u0026#34;6: \u0026#34;, emp2.MIN_SALARY)   1: 27000 2: 30000 3: 27000 4: 25000 5: 27000 6: 25500 The main use of class methods is alternative constructors. A class can only have one __init__ method, but there might be multiple ways to initialize an object. For example, we might want to create an Employee object from data stored in a file. We can\u0026rsquo;t use a instance method, because it would require an instance, and there isn\u0026rsquo;t one yet.\nHere we introduce a class method from_file that accepts a file name, reads the first line from the file that presumably contains the name of the employee, and returns an object instance. In the return statement, we use the cls variable, which refers to the class that class method follows, so this will call (notice the parentheses) the class and activities the __init__ constructor, just like using Employee with parentheses, and finally return object instance.\nThen we can call the method from_file by using class.method syntax, which will create an employee object without explicitly calling the constructor.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  !echo haoming \u0026gt; ~/Documents/names.txt !echo yachong \u0026gt;\u0026gt; ~/Documents/names.txt !echo shaonan \u0026gt;\u0026gt; ~/Documents/names.txt class Employee: MIN_SALARY = 30000 def __init__(self, name, salary=0): self.name = name if salary \u0026gt; Employee.MIN_SALARY: self.salary = salary else: self.salary = Employee.MIN_SALARY @classmethod def from_file(cls, filename, index): with open(filename, \u0026#34;r\u0026#34;) as f: names = f.readlines() name = names[index] return cls(name) filename = \u0026#34;/Users/wanghaoming/Documents/names.txt\u0026#34; objs = [ Employee.from_file(filename, i) for i in range(3) ] info = [(obj.name, obj.salary) for obj in objs] print(info)   [('haoming\\n', 30000), ('yachong\\n', 30000), ('shaonan\\n', 30000)]  Notice that class is also an object, just like function.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # -------- Example -------- # from datetime import datetime class BetterDate: def __init__(self, year, month, day): self.year, self.month, self.day = year, month, day @classmethod def from_str(cls, datestr): year, month, day = map(int, datestr.split(\u0026#34;-\u0026#34;)) return cls(year, month, day) @classmethod def from_datetime(cls, datetime): year, month, day = datetime.year, datetime.month, datetime.day return cls(year, month, day) today = datetime.today() bd = BetterDate.from_datetime(today) print(bd.year) print(bd.month) print(bd.day)   2019 9 4 Class inheritance Object-oriented programming is fundamentally about code reuse. There are millions of people out there writing code, so there\u0026rsquo;s a good chance that someone has already written code that solves a part of your problem! But what if that code doesn\u0026rsquo;t match your needs exactly? For example, you might want to modify the to_csv method of a pandas DataFrame to adjust the output format. You could do that by importing pandas and writing a new function, but it will not be integrated into the DataFrame interface. OOP will allow you to keep interface consistent while customizing functionality.\nWe can accomplish this with inheritance. Class inheritance is mechanism by which we can define a new class that gets all the the functionality of another class plus maybe something extra without re-implementing the code.\nDeclaring a class that inherits from another class is very straightforward: you simply add parentheses after the class name, and then specify the class to inherit from. Here, we define a rudimentary BankAccount class and a seemingly empty SavingsAccount class inherited from it.\u0026ldquo;Seemingly\u0026rdquo; because SavingsAccount actually has exactly as much in it as the BankAccount class.\n1 2 3 4 5 6 7 8 9 10 11 12 13  class BankAccount: def __init__(self, balance): self.balance = balance def withdraw(self, amount): self.balance -= amount class SavingsAccount(BankAccount): pass sa1 = SavingsAccount(10000) sa1.withdraw(1024) print(sa1.balance)   8976 Inheritance represents \u0026ldquo;is-a\u0026rdquo; relationship: a savings account is a bank account (The opposite is not true), just with some extra features. Calling isinstance function on a SavingsAccount object shows that Python treats it like an instance of both SavingsAccount and BankAccount classes, which is not the case for a generic BankAccount object.\n1 2 3 4 5 6  print(isinstance(sa1, BankAccount)) print(isinstance(sa1, SavingsAccount)) ba1 = BankAccount(1000) print(isinstance(ba1, BankAccount)) print(isinstance(ba1, SavingsAccount))   True True True False Class-level attributes CAN be inherited, and the value of class attributes CAN be overwritten in the child class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  class Player: MAX_POSITION = 10 MAX_SPEED = 3 def __init__(self): self.position = 0 def move(self, steps): if self.position + steps \u0026lt; Player.MAX_POSITION: self.position += steps else: self.position = Player.MAX_POSITION class Racer(Player): MAX_SPEED = 5 p = Player() r = Racer() print(\u0026#34;p.MAX_SPEED = \u0026#34;, p.MAX_SPEED) print(\u0026#34;r.MAX_SPEED = \u0026#34;, r.MAX_SPEED) print(\u0026#34;p.MAX_POSITION = \u0026#34;, p.MAX_POSITION) print(\u0026#34;r.MAX_POSITION = \u0026#34;, r.MAX_POSITION)   p.MAX_SPEED = 3 r.MAX_SPEED = 5 p.MAX_POSITION = 10 r.MAX_POSITION = 10 Customizing functionality via inheritance Let\u0026rsquo;s customize SavingsAccount class by adding a constructor specifically for SavingsAccount. It will take a balance parameter, just like BankAccount, and an additional interest_rate parameter. In that constructor, we first run the code for creating a generic BankAccount by explicitly calling the __init__ method follows the bankAccount class name.\nNotice that we use BankAccount.__init__() to tell Python to call (note the parentheses) the constructor from the parent class, and we also pass self to that constructor. self in this case refers a SavingsAccount object, but also a BankAccount object. so we can pass it to the __init__ method of BankAccount. Then we can add more functionality, in this case just initializing an attribute.\nNow when we create an instance of the SavingsAccount class, the new constructor will be called, and the interest_rate attribute will be initialized.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  class BankAccount: def __init__(self, balance): self.balance = balance def withdraw(self, amount): self.balance -= amount class SavingsAccount(BankAccount): def __init__(self, balance, interest_rate): BankAccount.__init__(self, balance) self.interest_rate = interest_rate sa = SavingsAccount(1000, 0.05) print(sa.balance, sa.interest_rate)   1000 0.05  Remember that in Python, instances of a subclass are also instances of the parent class.\n SavingsAccount inherits the withdraw method from the parent BankAccount class. Calling withdraw on a savings account instance will execute exactly the same code as calling it on a generic bank account instance.\nYou can add new methods to a subclass. In these methods you can use data from both the child and the parent class. For example here, we add a compute_interest method by an expression involving the balance attribute that inherits from the parent clsss ,and interest_rate attribute that exists only in the child SavingsAccount class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  class BankAccount: def __init__(self, balance): self.balance = balance def withdraw(self, amount): self.balance -= amount class SavingsAccount(BankAccount): def __init__(self, balance, interest_rate): BankAccount.__init__(self, balance) self.interest_rate = interest_rate def compute_interest(self, n_periods=1): return self.balance * ((1 + self.interest_rate) ** n_periods - 1) sa = SavingsAccount(10000, .05) print(sa.compute_interest(5)) sa.withdraw(2048) print(sa.compute_interest(5))   2762.8156250000034 2196.9909850000026 You can also modify the method of parent class in the subclass. We now define another subclass of BankAccount. Start by inheriting from the parent class, add a customized constructor that also executes the parent code, a new deposit method, and a withdraw method with a new argument to withdraw fee.\n Notice that we can change the signature of the method in the subclass by adding a parameter, and we again, just like in the constructor, call the parent version of the method directly by using parentclassname.method(self) syntax and passing self. Notice the parentheses, that means we call this function.\n( you can view function signature as number of arguments + type of argument + return value )\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  class BankAccount: def __init__(self, balance): self.balance = balance def withdraw(self, amount): self.balance -= amount class CheckingAccount(BankAccount): def __init__(self, balance, limit): BankAccount.__init__(self, balance) self.limit = limit def deposit(self, amount): self.balance += amount def withdraw(self, amount, fee=0): if fee \u0026lt;= self.limit: BankAccount.withdraw(self, amount+fee) else: BankAccount.withdraw(self, amount+self.limit) ca = CheckingAccount(10000, 50) ba = BankAccount(10000) print(ca.balance) ca.withdraw(6000, 34) ba.withdraw(6000) print(ca.balance) print(ba.balance)   10000 3966 4000 Now when you call withdraw from an object that is a CheckingAccount instance, the new customized version will be used, but when you call it from regular BankAccount, the basic version will be used. For a CheckingAccount instance, we could call the method with 2 parameters. But trying this call for a generic BankAccount instance would cause an error, because the method defined in the BankAccount class was not affected by the changes in the subclass.\nThe interface of the call is the same, and the actual method that is called is determined by the instance class. This is an application of polymorphism.\n1  ba.withdraw(300, 30)   --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_74663/1465132301.py in \u0026lt;module\u0026gt; ----\u0026gt; 1 ba.withdraw(300, 30) TypeError: withdraw() takes 2 positional arguments but 3 were given  withdraw() takes self and amount 2 positional arguments.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # -------- Example -------- # class Employee: def __init__(self, name, salary=30000): self.name = name self.salary = salary def give_raise(self, amount): self.salary += amount class Manager(Employee): def display(self): print(\u0026#34;Manager \u0026#34;, self.name) def __init__(self, name, salary=50000, project=None): Employee.__init__(self, name, salary) self.project = project def give_raise(self, amount, bonus=1.05): Employee.give_raise(self, amount*bonus) mngr = Manager(\u0026#34;Ashta Dunbar\u0026#34;, 78500) mngr.give_raise(1000) print(mngr.salary) mngr.give_raise(2000, bonus=1.03) print(mngr.salary)   79550.0 81610.0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  # -------- Example -------- # import pandas as pd class LoggedDF(pd.DataFrame): def __init__(self, *args, **kwargs): pd.DataFrame.__init__(self, *args, **kwargs) self.created_at = datetime.today() def to_csv(self, *args, **kwargs): temp = self.copy() # \u0026lt;-- `self` refers the object that method follows.  temp[\u0026#34;created_at\u0026#34;] = self.created_at pd.DataFrame.to_csv(temp, *args, **kwargs) LDF = LoggedDF( { \u0026#34;a\u0026#34;: [1,2,3,4,5], \u0026#34;b\u0026#34;: [2,3,4,5,6], \u0026#34;c\u0026#34;: [3,4,5,6,7] } ) LDF.to_csv(\u0026#34;~/Documents/LDF.csv\u0026#34;) !cat ~/Documents/LDF.csv   ,a,b,c,created_at 0,1,2,3,2019-09-04 20:05:50.471649 1,2,3,4,2019-09-04 20:05:50.471649 2,3,4,5,2019-09-09 20:05:50.471649 3,4,5,6,2019-09-09 20:05:50.471649 4,5,6,7,2019-09-09 20:05:50.471649 Integrating with Standard Python object and type* This is a deep-level knowledge, which indeed bothers me for a long time. But reader could just skip it. When we say inheritance previously, It generally follows the rule: class C2(C1):. But if we do not specify what class does class inherits from, it inherits from object class. There are two rules in Python:\n every class (except object) inherits from class object. every class is an instance of the class type.  Furthermore, type is an instance of itself, and inherits from object, thus type is an instance of object. object is an instance of type, and type is inherits from object, thus object is an instance of object.\n1 2 3 4 5 6 7  print(isinstance(object, type)) print(isinstance(type, object)) print(isinstance(object, object)) print(isinstance(type, type), \u0026#34;\\n\u0026#34;) print(issubclass(type, object)) print(issubclass(object, type))   True True True True True False Every class (except object) inherits from object.\n1 2  for x in object, type, int, float, str, list, dict: print(f\u0026#39;{x.__name__:6}: {x.__bases__}\u0026#39;)   object: () type : (\u0026lt;class 'object'\u0026gt;,) int : (\u0026lt;class 'object'\u0026gt;,) float : (\u0026lt;class 'object'\u0026gt;,) str : (\u0026lt;class 'object'\u0026gt;,) list : (\u0026lt;class 'object'\u0026gt;,) dict : (\u0026lt;class 'object'\u0026gt;,) Every class is an instance of the class type.\n1 2  for x in object, type, int, float, str, list, dict: print(f\u0026#39;{x.__name__:6}: {x.__class__}\u0026#39;)   object: \u0026lt;class 'type'\u0026gt; type : \u0026lt;class 'type'\u0026gt; int : \u0026lt;class 'type'\u0026gt; float : \u0026lt;class 'type'\u0026gt; str : \u0026lt;class 'type'\u0026gt; list : \u0026lt;class 'type'\u0026gt; dict : \u0026lt;class 'type'\u0026gt; Since type inherits from object, every class is an instance of the class object too.\n1 2  for x in object, type, int, float, str, list, dict: print(f\u0026#39;{x.__name__:6}is an instance of object : {isinstance(x, object)}\u0026#39;)   object is an instance of object : True type is an instance of object : True int is an instance of object : True float is an instance of object : True str is an instance of object : True list is an instance of object : True dict is an instance of object : True Thus you should know \u0026ldquo;Everything in Python is an object\u0026rdquo; better. Any classes that we define are objects (inherits), and of course, instances of those classes are objects (\u0026rsquo;s instance) as well.\n Tips: print(f\u0026quot;{'aaa':5}bbb\u0026quot;) prints out aaa bbb.\n Comparison Here are two objects of the Customer class that have the same data. If we ask Python if these objects are equal, the answer is \u0026ldquo;False\u0026rdquo;. When an object is created, Python allocates a chunk of memory to that object, and the variable that the object is assigned to actually contains just the reference to the memory chunk.\n1 2 3 4 5 6 7 8 9  class Customer: def __init__(self, name, balance): self.name = name self.balance = balance customer1 = Customer(\u0026#34;haoming\u0026#34;, 20000) customer2 = Customer(\u0026#34;haoming\u0026#34;, 20000) customer1 == customer2   False When we call == to compare variables, Python acquiescently compares references of the memory chunk, not the data. Because customer1 and customer2 point to different chunks in memory, they are not considered equal.\n1 2  print(customer1) print(customer2)   \u0026lt;__main__.Customer object at 0x7fe21895adf0\u0026gt; \u0026lt;__main__.Customer object at 0x7fe21894f430\u0026gt; But we can define a special method for customizing comparison. The __eq__ method is implicitly called whenever two objects are compared to each other. We can re-define this method to execute custom comparison code. The method should accept two arguments, referring to the objects to be compared. They are usually called self and other by convention.\nNow, if we create two objects containing the same data and try to compare them using double equality sign, we see from the diagnostic printout that the __eq__ method is called, and the comparison returns \u0026ldquo;True\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13  class Customer: def __init__(self, name, balance): self.name = name self.balance = balance def __eq__(self, other): print(\u0026#34;__eq__() is called\u0026#34;) return (self.name == other.name) and (self.balance == other.balance) customer1 = Customer(\u0026#34;haoming\u0026#34;, 20000) customer2 = Customer(\u0026#34;haoming\u0026#34;, 20000) customer1 == customer2   __eq__() is called True Notice that the redefined __eq__ method compares instances by only attributes, which means though we\u0026rsquo;re comparing a Phone class instance with a BankAccount class instance, Python still returns True.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  class Phone: def __init__(self, number): self.number = number def __eq__(self, other): return self.number == other.number class BankAccount: def __init__(self, number, balance=0): self.number, self.balance = number, balance def withdraw(self, amount): self.balance -= amount def __eq__(self, other): return (self.number == other.number) acct = BankAccount(873555333) pn = Phone(873555333) print(acct == pn)   True But we can modify it by type function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  class Phone: def __init__(self, number): self.number = number def __eq__(self, other): print(\u0026#34;Phone class\u0026#39;s __eq__\u0026#34;) return self.number == other.number class BankAccount: def __init__(self, number, balance=0): self.number, self.balance = number, balance def withdraw(self, amount): self.balance -= amount def __eq__(self, other): print(\u0026#34;BankAccount class\u0026#39;s __eq__\u0026#34;) return (self.number == other.number) and (type(self) == type(other)) acct = BankAccount(873555333) pn = Phone(873555333) print(acct == pn) print(pn == acct)   BankAccount class's __eq__ False Phone class's __eq__ True Notice that when comparing two object without inheriting relationship, the __eq__ of former object\u0026rsquo;s class will be called. self refers to this object, and other refers the later one, thus acct==pn is not equivalent with pn==acct.\nBut When we comparing two object with inheriting relationship, the subclass\u0026rsquo;s __eq__ will always be called.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  class Parent: def __eq__(self, other): print(\u0026#34;Parent\u0026#39;s __eq__() called\u0026#34;) return True class Child(Parent): def __eq__(self, other): print(\u0026#34;Child\u0026#39;s __eq__() called\u0026#34;) return True p = Parent() c = Child() print(p == c) print(c == p) print(c == c) print(p == p)   Child's __eq__() called True Child's __eq__() called True Child's __eq__() called True Parent's __eq__() called True  Recall, __init__ implicitly called as creating an instance; __eq__ implicitly called as calling ==.\n Python allows you to implement all the comparison operators in your custom class using special methods:\n   Operator Method     == __eq__()   != __ne__()   \u0026gt;= __ge__()   \u0026lt;= __le__()   \u0026gt; __gt__()   \u0026lt; __lt__()    String representation When calling print on an object of a custom class returns the object\u0026rsquo;s address in memory by default. But there are plenty of classes for which the printout is much more informative. For example, if we print a numpy array or a DataFrame, we\u0026rsquo;ll see the actual data contained in the object.\nThere are two special methods that we can define in a class that will return a printable representation of an object.\n__str__ method is executed when we call print or str on an object. __str__ is supposed to give an informal representation, suitable for an end user;\n1 2 3 4 5  import numpy as np npls = np.array([1,2,3]) print(npls) str(npls)   [1 2 3] '[1 2 3]' __repr__ method is executed when we call repr on the object, or when we print it in the console without calling print explicitly. __repr__ is mainly used by developers.\n1  repr(npls)   'array([1, 2, 3])' 1  npls   array([1, 2, 3]) __str__ and __repr__ accept only self argument and return a string. The best practice is to use __repr__ to print a string that can be used to reproduce the object. (reproduce means the return of __repr__ could create the same object directly.)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  class Customer: def __init__(self, name, balance): self.name = name self.balance = balance def __eq__(self, other): print(\u0026#34;__eq__() is called\u0026#34;) return (self.name == other.name) and (self.balance == other.balance) def __repr__(self): return \u0026#34;Customer(\u0026#39;{name}\u0026#39;, {balance})\u0026#34;.format(name=self.name, balance=self.balance) c1 = Customer(\u0026#34;haoming\u0026#34;, 142857) cc1 = eval(repr(c1)) print(type(cc1)) print(cc1 == c1)   \u0026lt;class '__main__.Customer'\u0026gt; __eq__() is called True  eval function accepts the string of an expression and return the value of the expression.\n If you only choose to implement one of them, chose __repr__, because it is also used as a fallback for print when __str__ is not defined.\n1 2 3 4 5 6 7 8 9  class Customer: def __init__(self, name, balance): self.name = name self.balance = balance def __repr__(self): return \u0026#34;Customer(\u0026#39;{name}\u0026#39;, {balance})\u0026#34;.format(name=self.name, balance=self.balance) c1 = Customer(\u0026#34;haoming\u0026#34;, 142857)   1  repr(c1)   Customer('haoming', 142857) 1  print(c1)   Customer('haoming', 142857) In this class we didn\u0026rsquo;t define the __str__ method, so __repr__ will be used as a fallback for the actual print method as well. Notice the single quotes around the name in the return statement, the point of __repr__ is to give the exact call needed to reproduce the the object, where the name should be in quotes.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # -------- Example -------- # class Employee: def __init__(self, name, salary=30000): self.name, self.salary = name, salary def __str__(self): s = \u0026#34;Employee name: {name}\\nEmployee salary: {salary}\u0026#34;.format(name=self.name, salary=self.salary) return s def __repr__(self): return \u0026#34;Employee(\u0026#39;{}\u0026#39;, {})\u0026#34;.format(self.name, self.salary) emp1 = Employee(\u0026#34;Amar Howard\u0026#34;, 30000) emp2 = Employee(\u0026#34;Carolyn Ramirez\u0026#34;, 35000) print(emp1) print(emp2) print(\u0026#34;\\n\u0026#34;) print(repr(emp1)) print(repr(emp2))   Employee name: Amar Howard Employee salary: 30000 Employee name: Carolyn Ramirez Employee salary: 35000 Employee('Amar Howard', 30000) Employee('Carolyn Ramirez', 35000)  Recall, the triple quotes are used in Python to define multi-line strings, and the format method is used on strings to substitute values inside curly brackets with variables.\n Exceptions Some statements in Python will cause an error when you try to execute them. These errors are called exceptions. Many exceptions have special names, like ZeroDivisionError or TypeError. If exceptions not handled correctly, they will stop the execution of your program entirely.\nInstead, you might want to execute special code to handle this case. To catch an exception and handle it, use the try-except-finally code: wrap the code that you\u0026rsquo;re worried about in a try block, then add an except block, followed by the name of the particular exception you want to handle (exceptions names are optional) , and the code that should be executed when the exception is raised. Then if this particular exception does happen, the program will not terminate, but execute the code in the except block instead. You can also have multiple exception blocks. And finally, you can use the optional \u0026ldquo;finally\u0026rdquo; block that will contain the code that runs no matter what. This block is best used for cleaning up like, for example, closing opened files.\n1 2 3 4 5 6 7 8  try: # Try running some code except ZeroDivisionError: # Run this code if ZeroDivisionError happens except TypeError: # Run this code if TypeError happens finally: # \u0026lt;-- optional # Run this code no matter what   Sometimes, you want to raise exceptions yourself, for example when some conditions aren\u0026rsquo;t satisfied. You can use the raise keyword, optionally followed by a specific error message in parentheses. The user of the code can then decide to handle the error using try/except.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  def make_list_of_ones(length): if length \u0026lt;=0 : raise ValueError(\u0026#34;Invalid length!\u0026#34;) return [1]*length def tef(func, arg): try: func(arg) except ValueError: print(\u0026#34;length should be positive!\u0026#34;) finally: print(\u0026#34;program is over.\u0026#34;) tef(make_list_of_ones, -1)   length should be positive! program is over. You can define your own exceptions in Python by inheriting from built-in classes BaseException or Exception. To define a custom exception, just define a class that inherits from the built-in Exception class or one of its subclasses. The class itself can be empty - inheritance alone is enough to ensure that Python will treat this class as an exception class.\nFor example, let\u0026rsquo;s define a BalanceError class that inherits from Exception. Then, in Customer class we raise an exception if a negative balance value is passed to the constructor.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  class BalanceError(Exception): pass class Customer: def __init__(self, name, balance): if balance \u0026lt; 0: raise BalanceError(\u0026#34;balance must be positive\u0026#34;) else: self.name = name self.balance = balance try: c1 = Customer(\u0026#34;haoming\u0026#34;, -1000) except BalanceError: c1 = Customer(\u0026#34;haoming\u0026#34;, 0) print(\u0026#34;balance must be positive, the account with 0 balance value has been created.\u0026#34;)   except block for a parent exception will catch child exceptions, but the opposite is not true:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  class SalaryError(ValueError): pass class BonusError(SalaryError): pass class Employee: MIN_SALARY = 30000 MAX_BONUS = 5000 def __init__(self, name, salary = 30000): self.name = name if salary \u0026lt; Employee.MIN_SALARY: raise SalaryError(\u0026#34;Salary is too low!\u0026#34;) self.salary = salary def give_bonus(self, amount): if amount \u0026gt; Employee.MAX_BONUS: raise BonusError(\u0026#34;The bonus amount is too high!\u0026#34;) elif self.salary + amount \u0026lt; Employee.MIN_SALARY: raise SalaryError(\u0026#34;The salary after bonus is too low!\u0026#34;) else: self.salary += amount emp = Employee(\u0026#34;Katze Rik\u0026#34;, salary=50000)   1 2 3 4  try: emp.give_bonus(7000) # bonus \u0026gt; MAX_BONUS --\u0026gt; BonusError Caught except SalaryError: print(\u0026#34;SalaryError caught!\u0026#34;)   SalaryError caught! 1 2 3 4  try: emp.give_bonus(7000) # bonus \u0026gt; MAX_BONUS --\u0026gt; BonusError Caught except BonusError: print(\u0026#34;BonusError caught!\u0026#34;)   BonusError caught! 1 2 3 4  try: emp.give_bonus(-100000) # salary + amount \u0026lt; MIN_SALARY --\u0026gt; SalaryError Caught except SalaryError: print(\u0026#34;SalaryError caught again!\u0026#34;)   SalaryError caught again! 1 2 3 4  try: emp.give_bonus(-100000) # salary + amount \u0026lt; MIN_SALARY --\u0026gt; SalaryError Caught except BonusError: print(\u0026#34;BonusError caught again!\u0026#34;)   --------------------------------------------------------------------------- SalaryError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_57175/1321339394.py in \u0026lt;module\u0026gt; 1 try: ----\u0026gt; 2 emp.give_bonus(-100000) # salary + amount \u0026lt; MIN_SALARY --\u0026gt; SalaryError Caught 3 except BonusError: 4 print(\u0026quot;BonusError caught again!\u0026quot;) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_57175/1909163033.py in give_bonus(self, amount) 17 18 elif self.salary + amount \u0026lt; Employee.MIN_SALARY: ---\u0026gt; 19 raise SalaryError(\u0026quot;The salary after bonus is too low!\u0026quot;) 20 21 else: SalaryError: The salary after bonus is too low! If you don\u0026rsquo;t specify the exception name behind the except, then Python views it as Exception, thus every exceptions will be caught.\n1 2 3 4  try: emp.give_bonus(-100000) except Exception: # or just `except:`  print(\u0026#34;BonusError caught again!\u0026#34;)   BonusError caught again! It\u0026rsquo;s better to include an except block for a child exception before the block for a parent exception, otherwise the child exceptions will be always be caught in the parent block, and the except block for the child will never be executed.\n1 2 3 4 5 6 7 8  emp = Employee(\u0026#34;Katze Rik\u0026#34;,\\ 50000) try: emp.give_bonus(7000) # sub error except BonusError: print(\u0026#34;BonusError caught\u0026#34;) except SalaryError: print(\u0026#34;SalaryError caught\u0026#34;)   BonusError caught 1 2 3 4 5 6 7 8  emp = Employee(\u0026#34;Katze Rik\u0026#34;,\\ 50000) try: emp.give_bonus(7000) # sub error except SalaryError: print(\u0026#34;SalaryError caught\u0026#34;) except BonusError: print(\u0026#34;BonusError caught\u0026#34;)   SalaryError caught Class Design Liskov substitution principle Polymorphism means using a unified interface to operate on objects of different classes just as We\u0026rsquo;ve already dealt with pd.DataFrame.\nRecall that we defined a BankAccount class, and two classes inherited from it: a CheckingAccount class and a SavingsAccount class. All of them had a withdraw method, but the CheckingAccount\u0026rsquo;s method was executing different code.\nLet\u0026rsquo;s say we defined a function to withdraw the same amount of money from a whole list of accounts at once. This function doesn\u0026rsquo;t know whether the objects passed to it are CheckingAccount, SavingsAccount or just BankAccount. All that matters is that they have a withdraw method that accepts one argument. That is enough to make the function work. It does not check which withdraw it should call \u0026ndash; the original or the modified. When the withdraw method is actually called, Python will dynamically pull the correct method: modified withdraw for whenever a CheckingAccount is being processed,and base withdraw for whenever a SavingAccount or generic BankAccount is processed. So you, as a person writing this batch processing function, don\u0026rsquo;t need to worry about what exactly is being passed to it, only what kind of interface it has. To really make use of this idea, you have to design your classes with inheritance and polymorphism - the uniformity of interface - in mind.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  class BankAccount: def __init__(self, balance): self.balance = balance def withdraw(self, amount): self.balance -= amount class SavingsAccount(BankAccount): pass class CheckingAccount(BankAccount): def __init__(self, balance, limit): BankAccount.__init__(self, balance) self.limit = limit def deposit(self, amount): self.balance += amount def withdraw(self, amount, fee=20): if fee \u0026lt;= self.limit: BankAccount.withdraw(self, amount+fee) else: BankAccount.withdraw(self, amount+self.limit) def batch_withdraw(list_of_accounts, amount): for acct in list_of_accounts: acct.withdraw(amount) a1 = BankAccount(1000) a2 = SavingsAccount(2000) a3 = CheckingAccount(3000, 50) accounts = [a1, a2, a3] batch_withdraw(accounts, 500) repr([ac.balance for ac in accounts])   '[500, 1500, 2480]' There is a fundamental object-oriented design principle of when and how to use inheritance properly, called \u0026ldquo;Liskov substitution principle (LSP)\u0026quot;: A base class should be interchangeable with any of its subclasses without altering any properties of the surrounding program.\nOn the one hand, the method in a subclass should have a signature with parameters and returned values compatible with the method in the parent class. On the other hand, the state of objects also must stay consistent; the subclass method shouldn\u0026rsquo;t rely on stronger input conditions, should not provide weaker output conditions, it should not throw additional exceptions and so on.\n The state of an object is it\u0026rsquo;s attributes with it\u0026rsquo;s values. To modify an object\u0026rsquo;s state, a method need to have access to this object. For ordinary methods, this is provided by the self parameter. Python\u0026rsquo;s classes are objects, so Python has \u0026ldquo;classmethods\u0026rdquo; which can be invoked on either an instance or the class itself, but get the class object itself instead of an instance. Those classmethods can then modify the class\u0026rsquo;s state (class attributes, which are shared by all instances of the class).\n The ultimate rule is that if your class hierarchy violates the Liskov substitution principle, then you should not be using inheritance, because it is likely to cause the code to behave in unpredictable ways somewhere down the road.\nUsing the example of our Account hierarchy, that means that wherever in your application you use a BankAccount object instance, substituting a CheckingAaccount instead should not affect anything in the surrounding program. For example, the batch withdraw functions worked regardless of what kind of account was used.\nLet\u0026rsquo;s illustrate some possible violations of LSP on our account classes: for example,\n the parent\u0026rsquo;s \u0026ndash; or base\u0026rsquo;s \u0026ndash; withdraw method could require 1 parameter, but the subclass method could require 2. Then we couldn\u0026rsquo;t use the subclass\u0026rsquo;s withdraw in place of the parent\u0026rsquo;s. But if the subclass method has a default value for the second parameter, then there is no problem. If the subclass method only accepts certain amounts, unlike the base one, then sometimes the subclass could not be used in place of the base class, if those unsuitable amounts are used. If the base withdraw had a check for whether the resulting balance is positive, and only performed the withdraw in that case, but the subclass did not do that, we wouldn\u0026rsquo;t be able to use subclass in place of the base class, because it\u0026rsquo;s possible that ambient program depends on the fact that the balance is always positive after withdraw.  The classic example of a problem that violates the Liskov Substitution Principle is the Circle-Ellipse problem, sometimes called the Square-Rectangle problem. By all means, it seems like you should be able to define a class Rectangle, with attributes h and w (for height and width), and then define a class Square that inherits from the Rectangle. After all, a square \u0026ldquo;is-a\u0026rdquo; rectangle! Unfortunately, this intuition doesn\u0026rsquo;t apply to object-oriented design.\n1 2 3 4 5 6 7 8 9 10 11 12 13  class Rectangle: def __init__(self, h, w): self.h = h self.w = w class Square(Rectangle): def __init__(self, w): self.h = w self.w = w s1 = Square(4) s1.h = 7 print(s1.w, s1.h)   4 7 For example, we create a Square object with side length 4. Then the 4x4 Square object would no longer be a square if we assign 7 to h. A Square inherited from a Rectangle will always have both the h and w attributes, but we can\u0026rsquo;t allow them to change independently of each other. We can make such modifies:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  class Rectangle: def __init__(self, w,h): self.w, self.h = w,h def set_h(self, h): self.h = h def set_w(self, w): self.w = w class Square(Rectangle): def __init__(self, w): self.w, self.h = w, w def set_h(self, h): self.h = h self.w = h def set_w(self, w): self.h = w self.w = w   But these setter methods violate Liskov Substitution principle. Each of the setter methods of Square change both h and w attributes, while setter methods of Rectangle change only one attribute at a time, so the Square objects cannot be substituted for Rectangle into programs that rely on one attribute staying constant.\nPrivate attributes Any attribute or method of any class in Python can be accessed by anyone. There are a few ways to manage access to data. We can use some universal naming conventions to signal that the data is not for external consumption.\n_xxx When involving the attribute and method, the leading underscore has an conventional meaning. It is a tips to developer that Python community reach a consensus where this attribute or method are for internal use only. This convention is defined in PEP 8.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class MyClass: def my_func(self): print(\u0026#34;I Love Python.\u0026#34;) self.at1 = 1 self._at2 = 2 def _my_func(self): print(\u0026#34;I Love Python too.\u0026#34;) self.at3 = 3 self._at4 = 4 mc = MyClass() mc.my_func() mc._my_func() print(mc.at1, mc._at2, mc.at3, mc._at4)   I Love Python. I Love Python too. 1 2 3 4 You can see that _my_func() or _at2 does not prevent us from entering the class. This because it is not mandatory, just conventional. However, the leading underscore does generate affection when importing from modules.\nNow, if you use wildcards * to import all names from a module, Python does not import names with leading underscores (but the methods and attributes warped by unprefixed object can be imported).\nFor example, we write such a module, and name it as my_module.py.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class MyClass: def my_func(self): self.at1 = 1 self._at2 = 2 def _my_func(self): self.at3 = 3 self._at4 = 4 def my_func3(): def _my_func4(): print(\u0026#34;test\u0026#34;) return _my_func4 def _my_func2(): print(\u0026#34;test\u0026#34;)   import the module with wildcard:\n1 2 3 4 5 6 7  from my_module import * mc = MyClass() mc.my_func() mc._my_func() print(mc.at1, mc._at2, mc.at3, mc._at4) my_func3()()   1 2 3 4 test 1  _my_func2()   --------------------------------------------------------------------------- NameError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_25437/3445096031.py in \u0026lt;module\u0026gt; ----\u0026gt; 1 _my_func2() NameError: name '_my_func2' is not defined Unlike wildcard imports, regular imports are not affected by the leading single underscore naming convention:\n1 2  import my_module as mm mm._my_func2()   test In general, you should avoid wildcard imports and calling the leading underlined methods and attributes in public space.\n__xxx The double leading underscore prefix causes the Python interpreter to override attribute/method names to avoid naming conflicts in subclasses. This is also called name mangling - the interpreter changes the name of a variable so that it is less likely to collide when the class is extended.\n1 2 3 4 5 6 7 8 9 10  class Test: def __init__(self): self.foo = 11 self._bar = 23 self.__baz = 23 def __func(self): print(\u0026#34;test\u0026#34;) t = Test() dir(t)   ['_Test__baz', # \u0026lt;--- '_Test__func', # \u0026lt;--- ... '_bar', 'foo'] You will see that there is a attribute on this object named _Test__baz and a method named _Test_func. This is the name modification that the Python interpreter does. It does this to avoid subclasses overwriting base class attributes and methods.\nLet\u0026rsquo;s create a subclass that extends the Test class and try to override the existing attributes/method added in the constructor:\n1 2 3 4 5 6 7 8 9 10 11 12 13  class ExtendedTest(Test): def __init__(self): super().__init__() self.foo = \u0026#39;overridden\u0026#39; self.__baz = \u0026#39;overridden\u0026#39; def __func(self): print(\u0026#34;test1\u0026#34;) t2 = ExtendedTest() print(t2.foo) print(t2._bar) print(t2.__baz)   overridden 23 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_29055/1866563237.py in \u0026lt;module\u0026gt; 11 print(t2.foo) 12 print(t2._bar) ---\u0026gt; 13 print(t2.__baz) AttributeError: 'ExtendedTest' object has no attribute '__baz' 1  dir(t2)   ['_ExtendedTest__baz', # \u0026lt;-- '_ExtendedTest__func', # \u0026lt;-- '_Test__baz', # \u0026lt;-- '_Test__func', # \u0026lt;-- ... '_bar', 'foo'] You will find the __baz attribute and __func method of subclass has been renemed by Python interpreter as _ExtendedTest__baz and _ExtendedTest__func. Thus __baz/__func of base class has not been overrided by subclass.\n1 2 3 4  print(t2._Test__baz) t2._Test__func() print(t2._ExtendedTest__baz) t2._ExtendedTest__func()   23 test overridden test1  Click here or here for more information.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # -------- Example -------- # class BetterDate: _MAX_DAYS = 30 _MAX_MONTHS = 12 def __init__(self, year, month, day): self.year, self.month, self.day = year, month, day @classmethod def from_str(cls, datestr): year, month, day = map(int, datestr.split(\u0026#34;-\u0026#34;)) return cls(year, month, day) def _is_valid(self): return (self.day \u0026lt;= BetterDate._MAX_DAYS) and (self.month \u0026lt;= BetterDate._MAX_MONTHS) bd1 = BetterDate(2020, 4, 30) print(bd1._is_valid()) bd2 = BetterDate(2020, 6, 45) print(bd2._is_valid())   True False Properties We have worked with an Employee class where we defined methods like set_salary that were used to set the values for attributes. Later, we use the constructor to initialize the attributes . We can access and change the attributes directly by assignment. But this means that with a simple equality we can assign anything to salary: a million, a negative number, or even the word \u0026ldquo;Hello\u0026rdquo;. But salary should only be positive number.\nSo how do we control attribute access, validate it or even make the attribute read-only? We could modify the set_salary method, but that wouldn\u0026rsquo;t help, because we could still use the dot syntax and assignment via equality.\nWe can reach there using the property decorator.\n Start by defining an \u0026ldquo;internal\u0026rdquo; attribute that will store the data, it is recommended to start the name with one leading underscore. Here, we defined a _salary attribute. Next, we define a method whose name is the exact name we\u0026rsquo;d like the restricted attribute to have, and put a decorator \u0026ldquo;property\u0026rdquo; on it. In our case that method is called salary, without underscore, because that\u0026rsquo;s how we\u0026rsquo;d like to refer to it. If we were writing a DataFrame class, this could be \u0026ldquo;columns\u0026rdquo;, or \u0026ldquo;shape\u0026rdquo;. The method just returns the actual internal attribute that is storing the data. To customize how the attribute is set, we implement a method with a decorator \u0026lt;attribute-name\u0026gt;.setter: salary.setter in our case. The method itself is again named exactly the same as the property \u0026ndash; salary - and it will be called when a value is assigned to the property attribute. It has a self argument, and an argument that represents the value to be assigned. Here we raise an exception if the value is negative, otherwise change the internal attribute.  1 2 3 4 5 6 7 8 9 10 11 12 13  class Employer: def __init__(self, name, new_salary): self._salary = new_salary @property def salary(self): return self._salary @salary.setter def salary(self, new_salary): if new_salary \u0026lt; 0: raise ValueError(\u0026#34;Invalid salary\u0026#34;) self._salary = new_salary   So there are two methods called salary \u0026ndash; the name of the property \u0026ndash; that have different decorators. The method with property decorator returns the data, and the method with salary.setter decorator implements validation and sets the attribute.\nWe can use this property just as if it was a regular attribute (remember the only real attribute we have is the internal underscore-salary). Use the dot syntax and equality sign to assign a value to the salary property. Then, the setter method will be called. If we try to assign a negative value to salary, an exception will be raised.\n1 2 3 4 5  emp = Employer(\u0026#34;haoming\u0026#34;, 35000) print(emp.salary) # \u0026lt;-- accessing the property emp.salary = 60000 # \u0026lt;-- @salary.setter print(emp.salary) # \u0026lt;-- accessing the property again emp.salary = -1   35000 60000 --------------------------------------------------------------------------- ValueError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_29055/2378834019.py in \u0026lt;module\u0026gt; ----\u0026gt; 1 emp.salary = -1 /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_29055/3054542886.py in salary(self, new_salary) 10 def salary(self, new_salary): 11 if new_salary \u0026lt; 0: ---\u0026gt; 12 raise ValueError(\u0026quot;Invalid salary\u0026quot;) 13 self._salary = new_salary 14 ValueError: Invalid salary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # -------- Example -------- # class Customer: def __init__(self, name, new_bal): self.name = name if new_bal \u0026lt; 0: raise ValueError(\u0026#34;Invalid balance!\u0026#34;) self._balance = new_bal @property def balance(self): print(\u0026#34;property called\u0026#34;) return self._balance @balance.setter def balance(self, new_bal): if new_bal \u0026lt; 0: raise ValueError(\u0026#34;Invalid balance!\u0026#34;) self._balance = new_bal print(\u0026#34;Setter method called\u0026#34;) cust = Customer(\u0026#34;Belinda Lutz\u0026#34;, 2000) print(cust.balance) cust.balance = 3000 print(cust.balance)   property called 2000 Setter method called property called 3000 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # -------- Example -------- # import pandas as pd from datetime import datetime class LoggedDF(pd.DataFrame): def __init__(self, *args, **kwargs): pd.DataFrame.__init__(self, *args, **kwargs) self._created_at = datetime.today() def to_csv(self, *args, **kwargs): temp = self.copy() temp[\u0026#34;created_at\u0026#34;] = self._created_at pd.DataFrame.to_csv(temp, *args, **kwargs) # Add a read-only property: _created_at @property def created_at(self): return self._created_at # Instantiate a LoggedDF called ldf ldf = LoggedDF({\u0026#34;col1\u0026#34;: [1,2], \u0026#34;col2\u0026#34;:[3,4]}) print(ldf.created_at)   2019-09-10 14:56:45.115035 There are a few other things you can do with properties: if you do not define a setter method, the property will be read-only, like Dataframe shape. A method with an \u0026lt;attribute-name\u0026gt;.getter decorator will be called when the property\u0026rsquo;s value is just retrieved, and the method with the \u0026lt;attribute-name\u0026gt;.deleter \u0026ndash; when an attribute is deleted.\n","date":"2019-09-10T00:00:00Z","image":"https://wanghaoming177.netlify.app/head_img/oop_hi/oo5.png","permalink":"https://wanghaoming177.netlify.app/p/object-oriented-programming-iv-class/","title":"Object Oriented Programming IV (Class)"},{"content":"Intro Docstrings Docstrings, which describes what the arguments are supposed to be, and what it returns, makes code much easier to use, read, and maintain. A docstring is a string enclosed in triple quotes and written as the first line of a function.\n1 2 3 4 5  def func() \u0026#34;\u0026#34;\u0026#34; docstring \u0026#34;\u0026#34;\u0026#34; pass   Every docstring has some (although usually not all) of these five key pieces of information:\n what the function does what the arguments are what the return value or values should be info about any errors raised, anything else you\u0026rsquo;d like to say about the function.  Python community has evolved several standards for how to format your docstrings. Google-style and Numpydoc are the most popular formats, so we\u0026rsquo;ll focus on those.\nIn Google style, the docstring starts with a concise description of what the function does. This should be in imperative language.\n1 2 3 4 5 6 7  def count_letter(content, letter): \u0026#34;\u0026#34;\u0026#34; Count the number of times `letter` appears in `content` \u0026#34;\u0026#34;\u0026#34; if (not isinstance(letter, str)) or len(letter) != 1: raise ValueError(\u0026#39;`letter` must be a single character string.\u0026#39;) return len([char for char in content if char == letter])   Next comes the \u0026ldquo;Args\u0026rdquo; section where you list each argument name, followed by its expected type in parentheses, and then what its role is in the function. If you need extra space, you can break to the next line and indent. If an argument has a default value, mark it as \u0026ldquo;optional\u0026rdquo; when describing the type. If the function does not take any parameters, feel free to leave this section out.\n1 2 3 4 5 6 7 8 9 10  def count_letter(content, letter): \u0026#34;\u0026#34;\u0026#34;Count the number of times `letter` appears in `content`. Args: content (str): The string to search. letter (str): The letter to search for. \u0026#34;\u0026#34;\u0026#34; if (not isinstance(letter, str)) or len(letter) != 1: raise ValueError(\u0026#39;`letter` must be a single character string.\u0026#39;) return len([char for char in content if char == letter])   The next section is the \u0026ldquo;Returns\u0026rdquo; section, where you list the expected type or types of what gets returned. You can also provide some comment about what gets returned, but often the name of the function and the description will make this clear. Finally, if your function intentionally raises any errors, you should add a \u0026ldquo;Raises\u0026rdquo; section. You can also include any additional notes or examples of usage in free form text at the end.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def count_letter(content, letter): \u0026#34;\u0026#34;\u0026#34;Count the number of times `letter` appears in `content`. Args: content (str): The string to search. letter (str): The letter to search for. Returns: int Raises: ValueError: If `letter` is not a one-character string. \u0026#34;\u0026#34;\u0026#34; if (not isinstance(letter, str)) or len(letter) != 1: raise ValueError(\u0026#39;`letter` must be a single character string.\u0026#39;) return len([char for char in content if char == letter])    Personally, I think the Numpydoc format looks better than the Google style.\n you can access the contents of your function\u0026rsquo;s docstring using __doc__ attribute. Notice that the __doc__ attribute contains the raw docstring, including any tabs or spaces.\nTo remove those leading spaces, you can use the getdoc() function from the inspect module. The inspect module contains a lot of useful methods for gathering information about functions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  import inspect def build_tooltip(function): \u0026#34;\u0026#34;\u0026#34;Create a tooltip for any function that shows the function\u0026#39;s docstring. Args: function (callable): The function we want a tooltip for. Returns: str \u0026#34;\u0026#34;\u0026#34; docstring = inspect.getdoc(function) border = \u0026#39;#\u0026#39; * 28 return \u0026#39;{}\\n{}\\n{}\u0026#39;.format(border, docstring, border) print(build_tooltip(count_letter)) print(build_tooltip(range)) print(build_tooltip(print))   ############################ Count the number of times `letter` appears in `content`. Args: content (str): The string to search. letter (str): The letter to search for. Returns: int Raises: ValueError: If `letter` is not a one-character string. ############################ ############################ range(stop) -\u0026gt; range object range(start, stop[, step]) -\u0026gt; range object Return an object that produces a sequence of integers from start (inclusive) to stop (exclusive) by step. range(i, j) produces i, i+1, i+2, ..., j-1. start defaults to 0, and stop is omitted! range(4) produces 0, 1, 2, 3. These are exactly the valid indices for a list of 4 elements. When step is given, it specifies the increment (or decrement). ############################ ############################ print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream. ############################ DRY \u0026amp; Do one thing DRY (also known as \u0026ldquo;don\u0026rsquo;t repeat yourself\u0026rdquo;) and the \u0026ldquo;Do One Thing\u0026rdquo; principle are good ways to ensure that functions are well designed and easy to test.\nWhen you are writing code to look for answers to a research question, it is totally normal to copy and paste a bit of code, tweak it slightly, and return it. But it can lead to real problems. For instance, it is easy to accidentally introduce errors that are hard to spot. Another problem is that if you want to change something, you often have to do it in multiple places.\nWrapping the repeated logic in a function and then calling that function several times makes it much easier to avoid the kind of errors introduced by copying and pasting. And if you ever need to change something, you only have to do it in one or two places.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def standardize(column): \u0026#34;\u0026#34;\u0026#34;Standardize the values in a column. Args: column (pandas Series): The data to standardize. Returns: pandas Series: the values as z-scores \u0026#34;\u0026#34;\u0026#34; z_score = (column - column.mean()) / column.std() return z_score df[\u0026#39;y1_z\u0026#39;] = standardize(df.y1_gpa) df[\u0026#39;y2_z\u0026#39;] = standardize(df.y2_gpa) df[\u0026#39;y3_z\u0026#39;] = standardize(df.y3_gpa) df[\u0026#39;y4_z\u0026#39;] = standardize(df.y4_gpa)   Another software engineering principle: Do One Thing. Every function should have a single responsibility. Instead of one big function, we could have a more nimble function. We get several advantages from splitting the big function into smaller functions. First of all, our code has become more flexible. The code will also be easier for other developers to understand, and it will be more pleasant to test and debug. Finally, if you ever need to update your code, functions that each have a single responsibility make it easier to predict how changes in one place will affect the rest of the code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # split it into two simpler functions: mean() and median() def mean_and_median(values): \u0026#34;\u0026#34;\u0026#34;Get the mean and median of a sorted list of `values` Args: values (iterable of float): A list of numbers Returns: tuple (float, float): The mean and median \u0026#34;\u0026#34;\u0026#34; mean = sum(values) / len(values) midpoint = int(len(values) / 2) if len(values) % 2 == 0: median = (values[midpoint - 1] + values[midpoint]) / 2 else: median = values[midpoint] return mean, median   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  # function mean() def mean(values): \u0026#34;\u0026#34;\u0026#34;Get the mean of a sorted list of values Args: values (iterable of float): A list of numbers Returns: float \u0026#34;\u0026#34;\u0026#34; mean = sum(values)/len(values) return mean # function median() def median(values): \u0026#34;\u0026#34;\u0026#34;Get the median of a sorted list of values Args: values (iterable of float): A list of numbers Returns: float \u0026#34;\u0026#34;\u0026#34; midpoint = int(len(values) / 2) if len(values) % 2 == 0: median = (values[midpoint - 1] + values[midpoint]) / 2 else: median = values[midpoint] return median   Pass by assignment The way that Python passes information to functions is called \u0026ldquo;pass by assignment\u0026rdquo;.\nConsider function foo() that takes a list and sets the first value of the list to 99. Then we set \u0026ldquo;my_list\u0026rdquo; to the value [1, 2, 3] and pass it to foo(). The value of \u0026ldquo;my_list\u0026rdquo; would be [99, 2, 3] after calling foo(). Lists in Python are mutable objects, meaning that they can be changed.\n1 2 3 4 5  def foo(x): x[0] = 99 my_list = [1, 2, 3] foo(my_list) print(my_list)   [99, 2, 3] Consider function bar() that takes an argument and adds 90 to it. Then we assign the value 3 to the variable \u0026ldquo;my_var\u0026rdquo; and call bar() with \u0026ldquo;my_var\u0026rdquo; as the argument. The value of \u0026ldquo;my_var\u0026rdquo; would be 3 after we\u0026rsquo;ve called bar(). In Python, integers are immutable objects, meaning they can\u0026rsquo;t be changed.\n1 2 3 4 5  def bar(x): x = x + 90 my_var = 3 bar(my_var) print(my_var)   3 Imagine that this gray bar is the computer\u0026rsquo;s memory. When we set the variable \u0026ldquo;a\u0026rdquo; equal to the list [1, 2, 3], the Python interpreter makes \u0026lsquo;a\u0026rsquo; points to this location in memory. Then if we type \u0026ldquo;b = a\u0026rdquo;, the interpreter makes \u0026lsquo;b\u0026rsquo; points to whatever \u0026lsquo;a\u0026rsquo; is pointing to. (certainly, \u0026lsquo;a\u0026rsquo; points to whatever \u0026lsquo;b\u0026rsquo; is pointing to correspondingly.)\nIf we append 4 to the end of \u0026ldquo;a\u0026rdquo;, both variables get it because there is only one list. Likewise, if we append 5 to \u0026ldquo;b\u0026rdquo;, both variables get it.\nHowever, if we assign \u0026ldquo;a\u0026rdquo; to a different object (like int) in memory, that does not change where \u0026ldquo;b\u0026rdquo; is pointing. Now, things that happen to \u0026ldquo;a\u0026rdquo; are no longer happening to \u0026ldquo;b\u0026rdquo;, and vice versa.\nWhen we assign a list to the variable \u0026ldquo;my_list\u0026rdquo;, it sets up a location in memory for it. Then, when we pass \u0026ldquo;my_list\u0026rdquo; to the function foo(), the parameter \u0026ldquo;x\u0026rdquo; gets assigned to that same location. So when the function modifies the thing that \u0026ldquo;x\u0026rdquo; points to, it is also modifying the thing that \u0026ldquo;my_list\u0026rdquo; points to.\nIn the other example, we created a variable \u0026ldquo;my_var\u0026rdquo; and assigned it the value 3. Then we passed it to the function bar(), which caused the argument \u0026ldquo;x\u0026rdquo; to point to the same place \u0026ldquo;my_var\u0026rdquo; is pointing. But the bar() function assigns \u0026ldquo;x\u0026rdquo; to a new value , so the \u0026ldquo;my_var\u0026rdquo; variable isn\u0026rsquo;t touched. In fact, there is no way in Python to have changed \u0026ldquo;x\u0026rdquo; or \u0026ldquo;my_var\u0026rdquo; directly, because integers are immutable variables.\n You can comprehend it as: Python interpreter kills the old x, and release the relationship between x and my_var, and then create a new object x which has nothing to do with my_var.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def store_lower(_dict, _string): # \u0026lt;-- d \u0026amp; (associates with) _dict; s \u0026amp; _string \u0026#34;\u0026#34;\u0026#34;Add a mapping between `_string` and a lowercased version of `_string` to `_dict` Args: _dict (dict): The dictionary to update. _string (str): The string to add. \u0026#34;\u0026#34;\u0026#34; orig_string = _string # \u0026lt;-- orig_string \u0026amp; _string \u0026amp; s  _string = _string.lower() # \u0026lt;-- _string # (breaks up with) orig_string \u0026amp; s _dict[orig_string] = _string d = {} s = \u0026#39;Hello\u0026#39; store_lower(d, s) print(d,s)   {'Hello': 'hello'} Hello # {'orig_string': '_string'} s There are only a few immutable data types in Python because almost everything is represented as an object. The only way to tell if something is mutable is to see if there is a function or method that will change the object without assigning it to a new variable.\nNotice, Consider foo() is a function that appends the value 1 to the end of a list with an empty list as a default value.\n1 2 3  def foo(var=[]): var.append(1) return var   When we call foo() the first time, we will get a list with one entry as expected. But, when we call foo() again, the default value has already been modified.\n1 2  foo() foo()   [1] [1, 1] If you want a mutable variable as a default value, consider defaulting to None and setting the argument in the function.\n1 2 3 4 5 6 7  def foo(var=None): if var is None: var = [] var.append(1) return var foo() foo()   [1] [1] I wan to explain exactly what\u0026rsquo;s happening when we call a function with mutable object default value. Consider following example,\n1 2 3 4 5 6 7 8 9  def addend(lt=[]): # 0. lt.append(\u0026#39;end\u0026#39;) return lt lst = [1, 2, 3, 4] # 1. print(addend(lst)) # 2. print(lst) print(addend()) # 3. print(addend()) # 4.   [1, 2, 3, 4, 'end'] [1, 2, 3, 4, 'end'] ['end'] ['end', 'end']  step 0: def addend(lt=[]): As we define the function append(), Python interpreter assigns a memory space block, which stored an empty list, to argument lt. Notice, lt points to this block. step 1: lst = [1,2,3,4] We we define a global list object lst, Python interpreter assigns another memory, which stored a corresponding list, to variable lst. lst points to this block. step 2: print(addend(lst)) When we call function addend with parameter lst, Python interpreter reads this global object in the local scope, and then makes pointer lt point to the block pointed by lst. Thus lt, lst point to the same memory block, which loaded list object ([1,2,3,4]). And then interpreter appends a new element \u0026ldquo;end\u0026rdquo; to this block, which means it changes the value of lt and lst simultaneously. This is the reason why the outcome of print(lst) is [1, 2, 3, 4, 'end'].   Notice that if lt, lst is an immutable object, the python interpreter will kill the older lt pointer, and assigns a new memory block to lt, instead of appending the element to the memory block. Thus it will not modify the value of lst. Collectively,\n immutable object: change the space pointed by variable. mutable object: change the value of space pointed by variable.    step 3/4: print(addend()) When we call addend() without parameter, the Python interpreter will add an element \u0026ldquo;end\u0026rdquo; to the space pointed by lt; when we call addend() again, the space pointed by lt has already been changed.  Above example is adapted from link.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # Modify this func using an immutable variable for the default argument. def add_column(values, df=pandas.DataFrame()): \u0026#34;\u0026#34;\u0026#34;Add a column of `values` to a DataFrame `df`. The column will be named \u0026#34;col_\u0026lt;n\u0026gt;\u0026#34; where \u0026#34;n\u0026#34; is the numerical index of the column. Args: values (iterable): The values of the new column df (DataFrame, optional): The DataFrame to update. If no DataFrame is passed, one is created by default. Returns: DataFrame \u0026#34;\u0026#34;\u0026#34; df[\u0026#39;col_{}\u0026#39;.format(len(df.columns))] = values return df   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  def better_add_column(values, df=None): \u0026#34;\u0026#34;\u0026#34;Add a column of `values` to a DataFrame `df`. The column will be named \u0026#34;col_\u0026lt;n\u0026gt;\u0026#34; where \u0026#34;n\u0026#34; is the numerical index of the column. Args: values (iterable): The values of the new column df (DataFrame, optional): The DataFrame to update. If no DataFrame is passed, one is created by default. Returns: DataFrame \u0026#34;\u0026#34;\u0026#34; if df is None: df = pandas.DataFrame() df[\u0026#39;col_{}\u0026#39;.format(len(df.columns))] = values return df   Context Managers Using context managers A context manager is a type of function that\n sets up a context for code to run in, runs your code, and then removes the context.  The \u0026ldquo;open()\u0026rdquo; function is a context manager. When you write \u0026ldquo;with open()\u0026rdquo;, it opens a file that you can read from or write to. Then, it gives control back to your code so that you can perform operations on the file object. When the code inside the indented block is done, the \u0026ldquo;open()\u0026rdquo; function makes sure that the file is closed before continuing on in the script.\nAny time you use a context manager, it will look like this:\n The keyword \u0026ldquo;with\u0026rdquo; lets Python know that you are trying to enter a context. Then you call a function. You can call any function that is built to work as a context manager. A context manager can take arguments like any normal function. You end the \u0026ldquo;with\u0026rdquo; statement with a colon \u0026lsquo;:\u0026rsquo;, as if you were writing a for loop or an if statement. Any code that you want to run inside the context that the context manager created needs to be indented. When the indented block is done, the context manager gets a chance to clean up anything that it needs to, like when the \u0026ldquo;open()\u0026rdquo; context manager closed the file.  Collectively:\n1 2  with \u0026lt;context-manager\u0026gt;(\u0026lt;args\u0026gt;): \u0026lt;code\u0026gt;    Statements in Python that have an indented block after them, like for loops, if/else statements, function/class definitions def/Class, etc. are called \u0026ldquo;compound statements\u0026rdquo;. The \u0026ldquo;with\u0026rdquo; statement is another type of compound statement.\n Some context managers want to return a value that you can use inside the context. By adding \u0026ldquo;as\u0026rdquo; and a variable name at the end of the \u0026ldquo;with\u0026rdquo; statement, you can assign the returned value to the variable name.\n1 2 3 4 5 6 7 8 9  with open(\u0026#39;alice.txt\u0026#39;) as file: text = file.read() n = 0 for word in text.split(): if word.lower() in [\u0026#39;cat\u0026#39;, \u0026#39;cats\u0026#39;]: n += 1 print(\u0026#39;Lewis Carroll uses the word \u0026#34;cat\u0026#34; {}times\u0026#39;.format(n))   Lewis Carroll uses the word \u0026quot;cat\u0026quot; 24 times Writing context managers There are two ways to define a context manager in Python:\n by using a class that has special __enter__() and __exit__() methods by decorating a certain kind of function.  We will focus on the function-based method here. There are five parts to creating a context manager.\n define a function. (optional) add any setup code your context needs. use the \u0026ldquo;yield\u0026rdquo; keyword to signal to Python that this is a special kind of function. (optional) add any teardown code that your context needs. decorate the function on the line above your context manager function with @contextlib.contextmanager.  1 2 3 4 5 6  import contextlib @contextlib.contextmanager def context_`func()`: # setup code yield # teardown code   When you write yield keyword, it means that you are going to return a value, but you expect to finish the rest of the function at some point in the future. As we say last blog, the \u0026ldquo;yield\u0026rdquo; keyword as a thing that gets used when creating generators. In fact, a context manager function is technically a generator that yields a single value. The value that your context manager yields can be assigned to a variable in the \u0026ldquo;with\u0026rdquo; statement by adding \u0026ldquo;as \u0026lt;variable name\u0026gt;\u0026rdquo;.\nHere, we\u0026rsquo;ve assigned the value 42 that my_context() yields to the variable \u0026ldquo;foo\u0026rdquo;. By running this code, you can see that after the context block is done executing, the rest of the my_context() function gets run, printing \u0026ldquo;goodbye\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import contextlib @contextlib.contextmanager def my_context(): \u0026#34;\u0026#34;\u0026#34;Show the order of the execution of CM code. Yield: a certain value. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;1\u0026#34;) yield 34 print(\u0026#34;2\u0026#34;) with my_context() as foo: print(foo)   1 34 2 This context manager is an example of code that accesses a database. Like most context managers, it has some setup code that runs before the function yields. This context manager uses that setup code to connect to the database. Most context managers also have some teardown or cleanup code when they get control back after yielding.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  @contextlib.contextmanager def database(url): \u0026#34;\u0026#34;\u0026#34;Connect to a database and close the connection after query. Yield: connection object \u0026#34;\u0026#34;\u0026#34; db = postgres.connect(url) yield db db.disconnect() url = \u0026#34;https://xxxx\u0026#34; with database(url) as d: outcome = d.execute(\u0026#34;select * from table\u0026#34;)   This setup/teardown behavior allows a context manager to hide things like connecting and disconnecting from a database so that a programmer using the context manager can just perform operations on the database without worrying about the underlying details.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  @contextlib.contextmanager def open_read_only(filename): \u0026#34;\u0026#34;\u0026#34;Open a file in read-only mode. Args: filename (str): The location of the file to read. Yields: str: the content of the opened file object. \u0026#34;\u0026#34;\u0026#34; f = open(filename, mode=\u0026#39;r\u0026#39;) content = f.read() yield content f.close() with open_read_only(\u0026#39;my_file.txt\u0026#39;) as content: print(content)   Congratulations! You wrote a context manager that acts like `open()` but responses the content of the file directly without `read()` method. The database() context manager that we\u0026rsquo;ve been looking at yields a specific value - the database connection - that can be used in the context block. Some context managers don\u0026rsquo;t yield an explicit value. in_dir() is a context manager that changes the current working directory to a specific path and then changes it back after the context block is done. It does not need to return anything with its \u0026ldquo;yield\u0026rdquo; statement.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  import os @contextlib.contextmanager def in_dir(path=\u0026#34;../\u0026#34;): \u0026#34;\u0026#34;\u0026#34;changes the current working directory to a specific path and then changes it back after the context block is done. Yield: None \u0026#34;\u0026#34;\u0026#34; old_dir = os.getcwd() print(os.getcwd()) os.chdir(path) yield os.chdir(old_dir) print(os.getcwd()) with in_dir(): print(os.getcwd()) print(os.listdir())   /Users/wanghaoming/PycharmProjects/daliy_test/save /Users/wanghaoming/PycharmProjects/daliy_test ['testpdf1.pdf', 'skl.ipynb', 'testpdf.pdf', 'pdf.ipynb', 'save', '.idea'] /Users/wanghaoming/PycharmProjects/daliy_test/save 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  @contextlib.contextmanager def timer(): \u0026#34;\u0026#34;\u0026#34;Time the execution of a context block. Yields: None \u0026#34;\u0026#34;\u0026#34; start = time.time() yield end = time.time() print(\u0026#39;Elapsed: {:.2f}s\u0026#39;.format(end - start)) with timer(): print(\u0026#39;This should take approximately 0.25 seconds\u0026#39;) time.sleep(0.25)   This should take approximately 0.25 seconds Elapsed: 0.25s Advanced topics Nested contexts Imagine you are implementing this copy() function that copies the contents of one file to another file. One way you could write this function would be to open the source file, store the contents of the file in the \u0026ldquo;contents\u0026rdquo; variable, then open the destination file and write the contents to it. This approach works fine until you try to copy a file that is too large to fit in memory.\n1 2 3 4 5 6 7 8 9 10 11  def copy(src, dst): \u0026#34;\u0026#34;\u0026#34;Copy the contents of one file to another. Args: src (str): File name of the file to be copied. dst (str): Where to write the new file. \u0026#34;\u0026#34;\u0026#34; with open(src,\u0026#34;r\u0026#34;) as f_src: contents = f_src.read() with open(dst,\u0026#34;w\u0026#34;) as f_dst: f_dst.write(contents)   A good ideal is that we could open both files at once and copy over one line at a time. The file object that the \u0026ldquo;open()\u0026rdquo; context manager returns can be iterated over in a for loop. In Python, nested \u0026ldquo;with\u0026quot; statements are perfectly legal.\n1 2 3 4 5 6 7 8 9 10 11  def copy(src, dst): \u0026#34;\u0026#34;\u0026#34;Copy the contents of one file to another. Args: src (str): File name of the file to be copied. dst (str): Where to write the new file. \u0026#34;\u0026#34;\u0026#34; with open(src,\u0026#34;r\u0026#34;) as f_src: with open(dst,\u0026#34;w\u0026#34;) as f_dst: for line in f_src: f_dst.write(line)   The context manager stock('NVDA') will connect to the NASDAQ and return an object that you can use to get the latest price by calling its .price() method. We now connect to stock('NVDA') and record 10 timesteps of price data by writing it to the file NVDA.txt. Notice that the object returned by stock() would be very large, thus we use nested context manager:\n1 2 3 4 5 6  with stock(\u0026#34;NVDA\u0026#34;) as nvda: with open(\u0026#39;NVDA.txt\u0026#39;, \u0026#39;w\u0026#39;) as f_out: for _ in range(10): value = nvda.price() print(\u0026#39;Logging ${:.2f}for NVDA\u0026#39;.format(value)) f_out.write(\u0026#39;{:.2f}\\n\u0026#39;.format(value))   Opening stock ticker for NVDA Logging $139.50 for NVDA Logging $139.54 for NVDA Logging $139.61 for NVDA Logging $139.65 for NVDA Logging $139.72 for NVDA Logging $139.73 for NVDA Logging $139.80 for NVDA Logging $139.78 for NVDA Logging $139.73 for NVDA Logging $139.64 for NVDA Closing stock ticker Handling errors If the programmer who uses your context manager writes code that causes an error after setup code but before teardown code, then the file or connection would not be close.\nTo cope with an error, you may be familiar with the \u0026ldquo;try\u0026rdquo; statement. It allows you to write code that might raise an error inside the \u0026ldquo;try\u0026rdquo; block and catch that error inside the \u0026ldquo;except\u0026rdquo; block. You can choose to ignore the error or re-raise it. The \u0026ldquo;try\u0026rdquo; statement also allows you to add a \u0026ldquo;finally\u0026rdquo; block. This is code that runs no matter what an exception occurred or not.\nThe solution is to put a \u0026ldquo;try\u0026rdquo; statement before the \u0026ldquo;yield\u0026rdquo; statement in our context manager function and a \u0026ldquo;finally\u0026rdquo; statement before teardown code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  def in_dir(directory): \u0026#34;\u0026#34;\u0026#34;Change current working directory to `directory`, allow the user to run some code, and change back. Args: directory (str): The path to a directory to work in. \u0026#34;\u0026#34;\u0026#34; current_dir = os.getcwd() os.chdir(directory) try: yield finally: os.chdir(current_dir)   If you notice that your code is following any of these patterns, you might consider using a context manager.\n   setup teardown     open close   lock release   change reset   enter exit   start stop   setup teardown   connect disconnect   Adapted from Dave Brondsema\u0026rsquo;s talk at PyCon 2012.     Decorators Functions are objects Functions are just like any other object in Python. They are not fundamentally different from lists, dictionaries, DataFrames, strings, integers, floats, modules, or anything else in Python. And because functions are just another type of object, you can do anything to or with them that you would do with any other kind of object.\nYou can take a function and assign it to a variable, like assign the print() function to p, and use it as your print() function.\n1 2  p = print p(\u0026#34;test\u0026#34;)   test You can also add functions to a list or dictionary. Here, we\u0026rsquo;ve added the functions my_function(), open(), and print() to the list \u0026ldquo;list_of_functions\u0026rdquo;. Below that, we\u0026rsquo;ve added the same three functions to a dictionary dict_of_functions, under the keys \u0026ldquo;func1\u0026rdquo;, \u0026ldquo;func2\u0026rdquo;, and \u0026ldquo;func3\u0026rdquo;. We can call the element of the list or reference values of the dictionary and use them as if we were calling the functions directly.\n1 2 3 4 5 6 7 8 9 10  def my_function(): return 42 list_of_functions = [my_function, open, print] dict_of_functions = { \u0026#34;func1\u0026#34;: my_function, \u0026#34;func2\u0026#34;: open, \u0026#34;func3\u0026#34;: print } dict_of_functions[\u0026#34;func3\u0026#34;](list_of_functions[0]())   42 Notice that when you assign a function to a variable, you do not include the parentheses after the function name. This is a subtle but very important distinction. When you type my_function() with the parentheses, you are calling that function. It evaluates to the value that the function returns. However, when you type \u0026ldquo;my_function\u0026rdquo; without the parentheses, you are referencing the function itself. It evaluates to a function object.\n1 2  print(my_function()) print(my_function)   42 \u0026lt;function my_function at 0x7fe6c1691b80\u0026gt; Since a function is just an object like anything else in Python, you can pass one as an argument to another function. The has_docstring() function checks to see whether the function that is passed to it has a docstring or not. We could define these two functions, no() and yes(), and pass them as arguments to the has_docstring() function. Since the no() function doesn\u0026rsquo;t have a docstring, the has_docstring() function returns False. Likewise, has_docstring() returns True for the yes() function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  def has_docstring(func): \u0026#34;\u0026#34;\u0026#34;Check to see if the function `func` has a docstring. Args: func (callable): A function. Returns: bool \u0026#34;\u0026#34;\u0026#34; return func.__doc__ is not None def yes(): \u0026#34;\u0026#34;\u0026#34;Return the value 42 \u0026#34;\u0026#34;\u0026#34; return 42 def no(): return 42 print(has_docstring(yes)) print(has_docstring(no)) print(has_docstring(has_docstring))   True False True Functions can also be defined inside other functions. These kinds of functions are called nested functions. A nested function can make your code easier to read. In this example, if x and y are within some bounds, foo() prints x times y. We can make that if statement easier to read by defining an in_range() function.\n1 2 3 4 5 6 7 8 9 10 11 12  def foo1(x,y): if (x\u0026gt;4 and x\u0026lt;10) and (y\u0026gt;4 and y \u0026lt;10): print(x*y) def foo2(x,y): def in_range(v): return v\u0026gt;4 and v\u0026lt;10 if in_range(x) and in_range(y): print(x*y) foo1(5,6) foo2(7,8)   30 56 Function also could return a function. For instance, the function get_function() creates a new function, print_me(), and then returns it. If we assign the result of calling get_function() to the variable \u0026ldquo;new_func\u0026rdquo;, we are assigning the return value, \u0026ldquo;print_me()\u0026rdquo; to \u0026ldquo;new_func\u0026rdquo;. We can then call new_func()`` as if it were the print_me() function.\n1 2 3 4 5 6 7  def get_function(): def print_me(s): print(s) return print_me new_func = get_function() new_func(\u0026#34;test\u0026#34;)   test 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  def create_math_function(func_name): if func_name == \u0026#39;add\u0026#39;: def add(a, b): return a + b return add elif func_name == \u0026#39;subtract\u0026#39;: def subtract(a,b): return a-b return subtract else: print(\u0026#34;I don\u0026#39;t know that one\u0026#34;) add = create_math_function(\u0026#39;add\u0026#39;) print(\u0026#39;5 + 2 = {}\u0026#39;.format(add(5, 2))) subtract = create_math_function(\u0026#39;subtract\u0026#39;) print(\u0026#39;5 - 2 = {}\u0026#39;.format(subtract(5, 2)))   5 + 2 = 7 5 - 2 = 3 Scope We have discussed scope in python before, now we gonna briefly recall it. Python has to have strict rules about which variable you are referring to when using a particular variable name.\n First, the interpreter looks in the Local Scope. When you are inside a function, the local scope is made up of the arguments and any variables defined inside the function. In the case of nested functions, where one function is defined inside another function, Python will check the scope of the parent function after checking the local scope. This is called the Nonlocal Scope to show that it is not the local scope of the child function and not the global scope. If the interpreter can\u0026rsquo;t find the variable in the (non)local scope, it expands its search to the Global Scope. These are the things defined outside the function. Finally, if it can\u0026rsquo;t find the thing it is looking for in the global scope, the interpreter checks the Builtin Scope. These are things that are always available in Python. For instance, the print() function is in the builtin scope, which is why we are able to use it in any scope.  Note that Python only gives you read access, instead of write, to variables defined outside of your current scope. But you can write it indirectly using global or nonlocal keywords.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  x = 50 def one(): x = 10 def two(): global x x = 30 def three(): x = 100 print(x) for func in [one, two, three]: `func()` print(x)   50 30 100 30 Closure A closure in Python is a tuple of variables that are no longer in scope, but that a function needs in order to run.\nConsider an example. The function foo() defines a nested function bar() that prints the value of \u0026ldquo;a\u0026rdquo;. foo() returns this new function, so when we say \u0026ldquo;func = foo()\u0026rdquo; we are assigning the bar() function to the variable \u0026ldquo;func\u0026rdquo;. When we call func(), it prints the value of variable \u0026ldquo;a\u0026rdquo;, which is 5.\n1 2 3 4 5 6 7 8  def foo(): a = 5 def bar(): # Notice, if you set `def bar(a):`, then you must assign it a value when you call bar(). print(a) return bar func = foo() func()   a How does function \u0026ldquo;func()\u0026rdquo; know anything about variable \u0026ldquo;a\u0026rdquo;? \u0026ldquo;a\u0026rdquo; is defined in foo()\u0026rsquo;s scope, not bar()\u0026rsquo;s. You would think that \u0026ldquo;a\u0026rdquo; would not be observable outside of the scope of foo(). That\u0026rsquo;s where closures come in. When foo() returned the new bar() function, Python helpfully attached any nonlocal variable that bar() was going to need to the function object.\nThose variables get stored in a tuple in the \u0026ldquo;__closure__\u0026rdquo; attribute, which could be iterated, of the function. The closure for \u0026ldquo;func\u0026rdquo; has one variable, and you can view the value of that variable by accessing the \u0026ldquo;cell_contents\u0026rdquo; of the item.\n1 2 3 4  print(func.__closure__) print(type(func.__closure__)) print(len(func.__closure__)) print(func.__closure__[0].cell_contents)   (\u0026lt;cell at 0x7fe6b033aa30: int object at 0x7fe70002e9b0\u0026gt;,) \u0026lt;class 'tuple'\u0026gt; 1 5 1 2 3 4 5 6 7 8 9 10 11 12 13  def parent(arg_1, arg_2): value = 22 my_dict = {\u0026#39;chocolate\u0026#39;: \u0026#39;yummy\u0026#39;} def child(): print(2 * value) print(my_dict[\u0026#39;chocolate\u0026#39;]) print(arg_1 + arg_2) return child new_function = parent(3, 4) print([cell.cell_contents for cell in new_function.__closure__])   [3, 4, {'chocolate': 'yummy'}, 22] Let\u0026rsquo;s examine this bit of code. Here, x is defined in the global scope. foo() creates a function bar() that prints whatever argument was passed to foo(). When we call foo() and assign the result to \u0026ldquo;my_func\u0026rdquo;, we pass in \u0026ldquo;x\u0026rdquo;. So, as expected, calling my_func() prints the value of x. Now let\u0026rsquo;s delete x and call my_func() again, we would still print 25. That\u0026rsquo;s because foo()\u0026rsquo;s \u0026ldquo;value\u0026rdquo; argument gets added to the closure attached to the new \u0026ldquo;my_func\u0026rdquo; function. So even though x doesn\u0026rsquo;t exist anymore, the value persists in its closure.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  x = 25 def foo(value=1): def bar(): print(value) return bar my_func = foo(x) # \u0026lt;-- Create closure here my_func() del(x) my_func() print(my_func.__closure__[0].cell_contents) x = 52 my_func() print(my_func.__closure__[0].cell_contents)   25 25 25 25 25 Notice that nothing changes if we overwrite \u0026ldquo;x\u0026rdquo; instead of deleting it. Here we\u0026rsquo;ve passed x into foo() and then assigned the new function to the variable x. The old value of \u0026ldquo;x\u0026rdquo;, 25, is still stored in the new function\u0026rsquo;s closure, even though the new function is now stored in the \u0026ldquo;x\u0026rdquo; variable.\nDecorators Let\u0026rsquo;s say you have a function that takes some inputs and returns some outputs. Decorators are just functions that take a function as an argument and return a modified version of that function that changes that function\u0026rsquo;s behavior. You can modify the inputs, modify the outputs, or even change the behavior of the function itself.\nTo start off, let\u0026rsquo;s define a function double_args does not modify anything of the input function.\n1 2  def double_args(func): return func   It just takes a function and immediately returns it. If we call this version of double_args() that does nothing and pass it the multiply() function and then assign the result to the variable \u0026ldquo;new_multiply\u0026rdquo;, then we can call new_multiply(1, 5) and get the same value we would have gotten from multiply(1, 5).\n1 2 3 4 5 6 7 8  def multiply(a,b): return a * b def double_args(func): return func new_multiply = double_args(multiply) print(new_multiply(1,5) == multiply(1,5))   True In order for your decorator to return a modified function, it is usually helpful for it to define a nested function to return. We\u0026rsquo;ll call that nested function \u0026ldquo;wrapper()\u0026rdquo;. All wrapper() does is take two arguments and passes them on to whatever function was passed to double_args() in the first place, assuming that the function passed to double_args() also takes exactly two arguments.\nSo, double_args() is still not doing anything to actually modify the function it is decorating. Once again, we\u0026rsquo;ll pass multiply() to double_args() and assign the result to new_multiply(). If we then call new_multiply(), which is now equal to the wrapper() function, wrapper() calls multiply() because it is the function that was passed to double_args(). So wrapper() calls multiply() with the arguments 1 and 5, which returns 5.\n1 2 3 4 5 6 7  def double_args(func): def wrapper(a,b): return(func(a,b)) return wrapper new_multiply = double_args(multiply) # ==\u0026gt; **refer** `wrapper` with 2 arguments `a`,`b` and a closure `multiply`. new_multiply(1,5) # ==\u0026gt; **call** wrapper(1,5) return multiply(1,5)   5 Now let\u0026rsquo;s actually modify the function our decorator is decorating. This time, wrapper() will still call whatever function is passed to double_args(), but it will double every argument when it calls the original function. As usual, we will call double_args() on the multiply() function and assign the result to new_multiply. Now, when we call new_multiply() with 1 and 5 as arguments, new_multiply() is equal to wrapper(), which calls multiply() after doubling each argument. So 1 becomes 2 and 5 becomes 10, giving us 2 times 10, which equals 20.\n1 2 3 4 5 6 7  def double_args(func): def wrapper(a,b): return(func(2*a,2*b)) return wrapper new_multiply = double_args(multiply) # ==\u0026gt; e.t. wrapper with closure `multiply` new_multiply(1,5) # ==\u0026gt; e.t. `wrapper(1,5)` returns `multiply(2,10)`   20 This time, instead of assigning the new function to \u0026ldquo;new_multiply\u0026rdquo;, we\u0026rsquo;re going to overwrite the \u0026ldquo;multiply\u0026rdquo; variable. And then calling multiply() with arguments 1 and 5 gives us 20 instead of 5. Remember that we can do this because Python stores the original multiply function in the new function\u0026rsquo;s closure.\n1 2 3  multiply = double_args(multiply) print(multiply(1,5)) print(multiply.__closure__[0].cell_contents)   20 \u0026lt;function multiply at 0x7f0060c9e620\u0026gt; Now we can implement above code in a concise way, using @ symbol followed by the decorator\u0026rsquo;s name on the line directly above the function you are decorating.\n1 2 3 4 5 6 7 8 9 10  def double_args(func): def wrapper(a,b): return func(a*2, b*2) return wrapper @double_args def multiply(a,b): return a * b multiply(1,5)   20 This is just a Python convenience for saying \u0026ldquo;multiply\u0026rdquo; equals the value returned by calling double_args() with \u0026ldquo;multiply\u0026rdquo; as the only argument, i.e. multiply = double_args(multiply). The above code is exactly equivalent to the following code,\n1 2 3 4 5 6 7 8 9 10  def double_args(func): def wrapper(a,b): return func(a*2, b*2) return wrapper def multiply(a,b): return a * b multiply = double_args(multiply) multiply(1,5)   20 More on Decorators Real-world examples count time cost We will walk through some real-world decorators so that we can start to recognize common decorator patterns. The timer() decorator runs the decorated function and then prints how long it took for the function to run.\n you wind up adding some version of this to all of projects because it is a pretty easy way to figure out where your computational bottlenecks are.\n All decorators have fairly similar-looking docstrings because they all take and return a single function. Like most decorators, we\u0026rsquo;ll start off by defining a wrapper() function. This is the function that the decorator will return. wrapper() takes any number of positional and keyword arguments so that it can be used to decorate any function. The first thing the new function will do is record the time that it was called with the time() function. Then wrapper() gets the result of calling the decorated function. We don\u0026rsquo;t return that value yet though. After calling the decorated function, wrapper() checks the time again, and prints a message about how long it took to run the decorated function. Once we\u0026rsquo;ve done that, we need to return the value that the decorated function calculated.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import time def timer(func): \u0026#34;\u0026#34;\u0026#34;A decorator that prints how long a function took to run. Args: func (callable): The function being decorated. Returns: callable: The decorated function. \u0026#34;\u0026#34;\u0026#34; def wrapper(*args, **kargs): start_time = time.time() result = func(*args, **kargs) print(f\u0026#34;time cost: {time.time()-start_time}s\u0026#34;) return result return wrapper @timer def sleep_second(n): time.sleep(n) print(\u0026#34;get up ~ \u0026#34;) sleep_second(3)   get up ~ time cost: 3.0057671070098877s Alternatively, you can also modify it as\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  import time import numpy as np def timer(func): \u0026#34;\u0026#34;\u0026#34;A decorator that prints how long a function took to run. Args: func (callable): The function being decorated. Returns: callable: The decorated function. \u0026#34;\u0026#34;\u0026#34; def wrapper(*args, **kargs): start_time = time.time() result = func(*args, **kargs) cost = time.time()-start_time timecost[func.__name__]=cost return result return wrapper @timer def sleep_second1(n): time.sleep(n) print(\u0026#34;get up ~ \u0026#34;) @timer def sleep_second2(n): time.sleep(n) print(\u0026#34;get up ~ \u0026#34;) @timer def sleep_second3(n): time.sleep(n) print(\u0026#34;get up ~ \u0026#34;) timecost = {} funcls = [sleep_second1,sleep_second2,sleep_second3] [f(np.random.uniform(.5,2.5)) for f in funcls] print(timecost) # \u0026lt;-- Notice, we can accept the desired dict because dict is mutable object.    get up ~ get up ~ get up ~ {'sleep_second1': 0.689359188079834, 'sleep_second2': 1.0407230854034424, 'sleep_second3': 1.763228178024292} store the result Memoizing is the process of storing the results of a function so that the next time the function is called with the same arguments; you can just look up the answer.\nWe start by setting up a dictionary that will map arguments to results. Then, as usual, we create wrapper() to be the new decorated function that this decorator returns. When the new function gets called, we check to see whether we\u0026rsquo;ve ever seen these arguments before. If we haven\u0026rsquo;t, we send them to the decorated function and store the arguments and result in the dictionary. The next time we call this function with those same arguments, the return value will already be in the dictionary, and we can return the value very quickly.\nThere are something deserve attentions:\n Fitstly, when we use @ symbol, Python actions as multiply = memoize(multiply), which means Python packages all objects required by wrapper function, include (original)multiply and result_dict , in to the closure. Secondly, when we call multiply(1,2,3,4,5), the result_dict will be written the arguments and result by Python. But notice that dict is mutable object, which means the closure\u0026rsquo;s second element, result_dict, will be changed simultaneously.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  import time def memoize(func): \u0026#34;\u0026#34;\u0026#34;A decorator that stores the result of decorated function with specific arguments. Args: func (callable): The function being decorated. Returns: callable: The decorated function. \u0026#34;\u0026#34;\u0026#34; result_dict = {} def wrapper(*args): if args not in result_dict: result_dict[args] = func(*args) return result_dict[args] return wrapper @memoize # \u0026lt;--- e.t. multiply = memoize(multiply) def multiply(*args): j = 1 for i in args: j *= i time.sleep(3) print(\u0026#34;time cost: 3s\u0026#34;) return j print([cell.cell_contents for cell in multiply.__closure__]) print(multiply(1,2,3,4,5)) print([cell.cell_contents for cell in multiply.__closure__]) print(multiply(1,2,3,4,5)) print([cell.cell_contents for cell in multiply.__closure__]) print(multiply(1,2,3,4,5,6)) print([cell.cell_contents for cell in multiply.__closure__])   [\u0026lt;function multiply at 0x7fe6d00fe3a0\u0026gt;, {}] time cost: 3s 120 [\u0026lt;function multiply at 0x7fe6d00fe3a0\u0026gt;, {(1, 2, 3, 4, 5): 120}] 120 [\u0026lt;function multiply at 0x7fe6d00fe3a0\u0026gt;, {(1, 2, 3, 4, 5): 120}] time cost: 3s 720 [\u0026lt;function multiply at 0x7fe6d00fe3a0\u0026gt;, {(1, 2, 3, 4, 5): 120, (1, 2, 3, 4, 5, 6): 720}]  You should consider using a decorator when you want to add some common bit of code to multiple functions.\n multi decorators You can use @ symbol folded to make multi decorators, like:\n1 2 3 4  @decorator2 @decorator1 def func(): pass   Recall that when we use @decorator, what Python exactly does is overwrite the decorated function func, like func = decorator(func), which assigns the child function of decorator with the closure containing origin func to new func. Homogeneously, multi decorators just do nestedly overwrite the decorated function with inner-to-outer order. For instance, if we use above codes, then decorator1 overwrites func firstly, assigns child function with closure to func; And then decorator2 overwrites func again, assigns child function with closure to func. There is an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  def decorator1(func): a1 = 1 print(\u0026#34;Get in decorator1\u0026#34;) def wrapper1(*args): print(\u0026#34;Get in wrapper1\u0026#34; ,a1) return func(*args) return wrapper1 def decorator2(func): a2 = 2 print(\u0026#34;Get in decorator2\u0026#34;) def wrapper2(*args): print(\u0026#34;Get in wrapper2\u0026#34; ,a2) return func(*args) return wrapper2 @decorator1 def f(): print(\u0026#34;Ger in f\u0026#34;)   Get in decorator1 Python implements f = decorator1(f), thus print the \u0026ldquo;Get in decorator1\u0026rdquo;.\n1 2  print(f.__closure__) print([cell.cell_contents for cell in f.__closure__])   (\u0026lt;cell at 0x7fe6c168c7f0: int object at 0x7fe70002e930\u0026gt;, \u0026lt;cell at 0x7fe6c168c2b0: function object at 0x7fe6d00fdee0\u0026gt;) [1, \u0026lt;function f at 0x7fe6d00fdee0\u0026gt;] We can see the global object f and nonlocal object a1 has been in closure of decorated f. Now call the multi decorators:\n1 2 3 4 5 6 7  @decorator2 @decorator1 def f(): print(\u0026#34;Ger in f\u0026#34;) print(f.__closure__) print([cell.cell_contents for cell in f.__closure__])   Get in decorator1 Get in decorator2 (\u0026lt;cell at 0x7fe6e0242df0: int object at 0x7fe70002e950\u0026gt;, \u0026lt;cell at 0x7fe6e0242280: function object at 0x7fe7014f6160\u0026gt;) [2, \u0026lt;function decorator1.\u0026lt;locals\u0026gt;.wrapper1 at 0x7fe7014f6160\u0026gt;] we can see Python implements f = decorator1(f) and f = decorator2(f) successively, and print the statement in corresponding order. Notice that the nonlocal object a2 and decorated f (wrapper1) were written into the closure of final f. So where is the closure of wrapper1 that we discussed? We can find them disassembling \u0026lt;function decorator1.\u0026lt;locals\u0026gt;.wrapper1 at 0x7fe7014f6160\u0026gt;\n1 2  print(f.__closure__[1].cell_contents.__closure__) print([cell1.cell_contents for cell1 in f.__closure__[1].cell_contents.__closure__])   (\u0026lt;cell at 0x7fe7014f8af0: int object at 0x7fe70002e930\u0026gt;, \u0026lt;cell at 0x7fe7014f87c0: function object at 0x7fe7014f61f0\u0026gt;) [1, \u0026lt;function f at 0x7fe7014f61f0\u0026gt;]  f.__closure__[1] is a cell object, you should call cell_contents attribute to attain the content of it.\n Everything we discussed above happened before calling the double-decorated function f. Since f is a nested function, like f=decorator2(decorator1(f)), when we call f(), the inner function wrapper2 will conduct firstly, and then wrapper1 and finally original f.\n1  f()   Get in wrapper2 2 Get in wrapper1 1 Ger in f other examples print_return_type() will print out the type of the variable that gets returned from every call of any function it is decorating\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def print_return_type(func): def wrapper(v): result = func(v) print(\u0026#39;{}() returned type {}\u0026#39;.format( func.__name__, type(result) )) return result return wrapper @print_return_type def foo(value): return value print(foo(42)) print(foo([1, 2, 3])) print(foo({\u0026#39;a\u0026#39;: 42}))   foo() returned type \u0026lt;class 'int'\u0026gt; 42 foo() returned type \u0026lt;class 'list'\u0026gt; [1, 2, 3] foo() returned type \u0026lt;class 'dict'\u0026gt; {'a': 42} Counting how many times some functions is called:\n1 2 3 4 5 6 7 8 9 10 11 12 13  def counter(func): # \u0026lt;-- called when use `@` def wrapper(*args, **kwargs): # \u0026lt;-- called everytime calling `foo()` wrapper.count += 1 return func() # \u0026lt;-- Notice the parentheses, we need to implement the passed-in function and attain the outcome, instead of assigning it to something. wrapper.count = 0 return wrapper @counter def foo(): print(\u0026#39;calling foo()\u0026#39;) [foo() for _ in range(5)] print(\u0026#39;foo() was called {}times.\u0026#39;.format(foo.count))   calling foo() calling foo() calling foo() calling foo() calling foo() foo() was called 5 times.  You can define the attribute for function object by func.attr or setattr()/getattr(). (more)\n Metadata One of the problems with decorators is that they obscure the decorated function\u0026rsquo;s metadata. Here we have a function, sleep_n_seconds(), with a docstring that explains exactly what it does. We can access the metadata for the function, like its docstring, name and default arguments.\n1 2 3 4 5 6 7 8 9 10 11  def sleep_n_seconds(n=10): \u0026#34;\u0026#34;\u0026#34;Pause processing for n seconds. Args: n (int): The number of seconds to pause for. \u0026#34;\u0026#34;\u0026#34; time.sleep(n) print(sleep_n_seconds.__doc__) print(sleep_n_seconds.__name__) print(sleep_n_seconds.__defaults__)   Pause processing for n seconds. Args: n (int): The number of seconds to pause for. sleep_n_seconds (10,) But if we decorate sleep_n_seconds() with the timer() decorator, when we try to print the docstring or default, we get nothing back. Furthermore, when we try to look up the function\u0026rsquo;s name, Python tells us that sleep_n_seconds()\u0026rsquo;s name is \u0026ldquo;wrapper\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import time def timer(func): def wrapper(*args, **kargs): start_time = time.time() result = func(*args, **kargs) print(time.time()-start_time) return result return wrapper @timer def sleep_n_seconds(n=10): \u0026#34;\u0026#34;\u0026#34;Pause processing for n seconds. Args: n (int): The number of seconds to pause for. \u0026#34;\u0026#34;\u0026#34; time.sleep(n) print(sleep_n_seconds.__doc__) print(sleep_n_seconds.__name__) print(sleep_n_seconds.__defaults__)   None wrapper None Remember that when we write decorators, we almost always define a nested function to return. Because the decorator overwrites the sleep_n_seconds() function, when you ask for sleep_n_seconds()\u0026rsquo;s docstring or name, you are actually referencing the nested function that was returned by the decorator. In this case, the nested function was called wrapper() and it didn\u0026rsquo;t have a docstring. You can access to this function via the closure, like\n1 2 3  print(sleep_n_seconds.__closure__[0].cell_contents.__doc__) print(sleep_n_seconds.__closure__[0].cell_contents.__name__) print(sleep_n_seconds.__closure__[0].cell_contents.__defaults__)   Pause processing for n seconds. Args: n (int): The number of seconds to pause for. sleep_n_seconds (10,) But there is an easy way to get to it if you need it. The wraps() function from the functools module is a decorator that you use when defining a decorator. If you use it to decorate the wrapper function that your decorator returns, it will modify wrapper()\u0026rsquo;s metadata to look like the function you are decorating. Notice that the wraps() decorator takes the function you are decorating as an argument, we will talked about decorators that take arguments in the next section.\nIf we use this updated version of the timer() decorator to decorate sleep_n_seconds() and then try to print sleep_n_seconds()\u0026rsquo;s metadata, Python gives you the metadata from the function being decorated rather than the metadata of the wrapper() function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  import time from functools import wraps def timer(func): @wraps(func) def wrapper(*args, **kargs): start_time = time.time() result = func(*args, **kargs) print(time.time()-start_time) return result return wrapper @timer def sleep_n_seconds(n=10): \u0026#34;\u0026#34;\u0026#34;Pause processing for n seconds. Args: n (int): The number of seconds to pause for. \u0026#34;\u0026#34;\u0026#34; time.sleep(n) print(f\u0026#34;sleep {n}s\u0026#34;) print(sleep_n_seconds.__doc__) print(sleep_n_seconds.__name__) print(sleep_n_seconds.__defaults__) # \u0026lt;-- why???   Pause processing for n seconds. Args: n (int): The number of seconds to pause for. sleep_n_seconds None # \u0026lt;-- why??? As an added bonus, using wraps() when creating your decorator also gives you easy access to the original undecorated function via the __wrapped__ attribute, or .__closure__[0].cell_content__\n1 2 3 4 5 6 7  orgfuc = sleep_n_seconds.__wrapped__ print(orgfuc) print(sleep_n_seconds.__closure__[0].cell_contents) print(orgfuc.__doc__) print(orgfuc.__name__) print(orgfuc.__defaults__)   \u0026lt;function sleep_n_seconds at 0x7fdc88b33af0\u0026gt; \u0026lt;function sleep_n_seconds at 0x7fdc88b33af0\u0026gt; Pause processing for n seconds. Args: n (int): The number of seconds to pause for. sleep_n_seconds (10,) Notice that __wrapped__ attribute is a function object, thus you can call it directly.\n1  orgfuc(3)   sleep 3s Decorators that take arguments To add arguments to our decorator, we need another level of function nesting.\nLet\u0026rsquo;s consider this run_three_times() decorator. If you use it to decorate a function, it will run that function three times.\n1 2 3 4 5  def run_three_times(func): def wrapper(*args, **kargs): for _ in range(3): func(*args, **kargs) return wrapper   Let\u0026rsquo;s think about what we would need to change if we wanted to write a run_n_times() decorator. A decorator is only supposed to take one argument - the function it is decorating. Also, when you use decorator syntax, you\u0026rsquo;re not supposed to use the parentheses. To make run_n_times() work, we have to turn it into a function that returns a decorator, rather than a function that is a decorator.\nSo run_n_times() takes n as an argument, instead of func. Then, inside of run_n_times(), we\u0026rsquo;ll define a new decorator function. This function takes \u0026ldquo;func\u0026rdquo; as an argument because it is the function that will be acting as our decorator. We start our new decorator with a nested wrapper() function, as usual. Finally, run_n_times() returns the decorator() function we just defined, then we can use that return value as a decorator.\n1 2 3 4 5 6 7 8 9 10 11 12 13  def run_n_times(n): def decorator(func): def wrapper(*arg, **kargs): for _ in range(n): func(*arg, **kargs) return wrapper return decorator @run_n_times(4) def print_sum(a,b): print(a+b) print(1,3)   4 4 4 4 Notice how when we decorate print_sum() with run_n_times(), we use parentheses after @run_n_times. This indicates that we are actually calling run_n_times() and decorating print_sum() with the result of that function call. Since the return value from run_n_times() is a decorator function, we can use it to decorate print_sum().\nNow let\u0026rsquo;s explain exactly how this works without using decorator syntax. Like before, we have a function, run_n_times() that returns a decorator function when you call it. If we call run_n_times() with the argument 3, it will return a decorator. In fact, it returns the decorator just like run_three_times defined above. The argument 3 is in the closure of returned decorator:\n1 2  # run_n_times(3) almost e.t. run_three_times print([cell.cell_contents for cell in run_n_times(3).__closure__])   [3] We could decorate print_sum() with this returned new decorator using decorator syntax. Python makes it convenient to do both of those in a single step though. When we use decorator syntax, the thing that comes after the @ symbol must be a reference to a decorator function. We can use the name of a specific decorator, or we can call a function that returns a decorator.\nNotice that decorating a function almost is equivalent with overwriting it. Thus we can decorate an already has been defined function by overwriting like:\n1 2  print = run_n_times(5)(print) # run_n_times(5) returns a decorator, and then the decorator overwrites `print` and assigns the result to `print`. print(\u0026#34;hello\u0026#34;)   hello hello hello hello hello  Remember implement del print after test.\n There is another application:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  def html(open_tag, close_tag): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): msg = func(*args, **kwargs) return \u0026#39;{}{}{}\u0026#39;.format(open_tag, msg, close_tag) return wrapper return decorator @html(\u0026#34;\u0026lt;b\u0026gt;\u0026#34;, \u0026#34;\u0026lt;/b\u0026gt;\u0026#34;) def hello(name): return \u0026#39;Hello {}!\u0026#39;.format(name) @html(\u0026#34;\u0026lt;i\u0026gt;\u0026#34;, \u0026#34;\u0026lt;/i\u0026gt;\u0026#34;) def goodbye(name): return \u0026#39;Goodbye {}.\u0026#39;.format(name) @html(\u0026#34;\u0026lt;div\u0026gt;\u0026#34;, \u0026#34;\u0026lt;/div\u0026gt;\u0026#34;) def hello_goodbye(name): return \u0026#39;\\n{}\\n{}\\n\u0026#39;.format(hello(name), goodbye(name)) print(hello_goodbye(\u0026#39;Alice\u0026#39;))   \u0026lt;div\u0026gt; \u0026lt;b\u0026gt;Hello Alice!\u0026lt;/b\u0026gt; \u0026lt;i\u0026gt;Goodbye Alice.\u0026lt;/i\u0026gt; \u0026lt;/div\u0026gt; Other real world examples timeout Let\u0026rsquo;s imagine that we have some functions that occasionally either run for longer than we want them to or just hang and never return. It would be nice if we could add some kind of timeout() decorator to those functions that will raise an error if the function runs for longer than expected.\nTo create the timeout() decorator, we are going to use some functions from Python\u0026rsquo;s signal module.\n The signal.signal() function tells Python, \u0026ldquo;When you see the signal whose number is signalnum, call the handler function.\u0026rdquo;. In this case, we tell Python to call raise_timeout() whenever it sees the alarm signal. The raise_timeout() function raises a TimeoutError when it is called. The signal.alarm() function lets us set an alarm for some number of seconds in the future. Passing 0 to the alarm() function cancels the alarm. timeout() is a function that returns a decorator. It just like a decorator factory. When you call timeout(), it cranks out a brand new decorator that times out in 5 seconds, or 20 seconds, or whatever value we pass as n_seconds. wrapper() sets an alarm for 5 seconds in the future. Then it calls the function being decorated. It wraps that call in a try block so that in a finally block we can cancel the alarm. This ensures that the alarm either rings or gets canceled. Remember, when the alarm rings, Python calls the raise_timeout() function.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  import time import signal import functools def raise_timeout(*args, **kwargs): # \u0026lt;-- Notice the arguments, why??? raise TimeoutError(\u0026#34;you timeout\u0026#34;) signal.signal( signalnum=signal.SIGALRM, handler=raise_timeout ) def timeout(n_seconds): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): signal.alarm(n_seconds) try: return func(*args, **kwargs) finally: signal.alarm(0) return wrapper return decorator @timeout(3) def foo(): time.sleep(5) print(\u0026#39;foo!\u0026#39;) @timeout(20) def bar(): time.sleep(5) print(\u0026#39;bar!\u0026#39;) foo()   --------------------------------------------------------------------------- TimeoutError Traceback (most recent call last) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_38592/2588934869.py in \u0026lt;module\u0026gt; 31 print('bar!') 32 ---\u0026gt; 33 foo() /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_38592/2588934869.py in wrapper(*args, **kwargs) 15 signal.alarm(n_seconds) 16 try: ---\u0026gt; 17 return func(*args, **kwargs) 18 finally: 19 signal.alarm(0) /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_38592/2588934869.py in foo() 23 @timeout(3) 24 def foo(): ---\u0026gt; 25 time.sleep(5) 26 print('foo!') 27 /var/folders/p6/qtqch15n41v436lkcl5zb7g80000gn/T/ipykernel_38592/2588934869.py in raise_timeout(*args, **kwargs) 2 3 def raise_timeout(*args, **kwargs): ----\u0026gt; 4 raise TimeoutError(\u0026quot;you timeout\u0026quot;) 5 6 signal.signal( TimeoutError: you timeout 1  bar()   bar! Notice that wrapper() returns the result of calling func(), decorator() returns wrapper, and timeout() returns decorator. So when we call foo(), which has a 5-second timeout, it will timeout like before. But bar(), which has a 20-second timeout, prints its message in 10 seconds, so the alarm gets canceled.\ntag your functions 1 2 3 4 5 6 7 8 9 10 11 12 13 14  def tag(*tags): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): return func(*args, **kwargs) wrapper.tags = tags return wrapper return decorator @tag(\u0026#39;test\u0026#39;, \u0026#39;this is a tag\u0026#39;) def foo(): pass print(foo.tags)   ('test', 'this is a tag') Check the return type 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  def returns(return_type): def decorator(func): def wrapper(*args, **kargs): result = func(*args, **kargs) assert type(result) == return_type return result return wrapper return decorator @returns(dict) def foo(value): return value try: print(foo([1,2,3])) except AssertionError: print(\u0026#39;foo() did not return a dict!\u0026#39;)   foo() did not return a dict! assert \u0026lt;condition\u0026gt; return an AssertionError if \u0026lt;condition\u0026gt; is False.\n","date":"2019-08-19T00:00:00Z","image":"https://wanghaoming177.netlify.app/head_img/oop_hi/oo3.png","permalink":"https://wanghaoming177.netlify.app/p/object-oriented-programming-iii-context-managers-decorators/","title":"Object Oriented Programming III (Context Managers \u0026 Decorators)"},{"content":" All figures in this blog are embedded by Github Image Hosting Service. These figures may not be displayed on mobile devices.\n Iterators in PythonLand Introduction to iterators When you use a for loop to print out each element of a list, you\u0026rsquo;re iterating over the list. You can also use a for loop to iterate over characters in a string. You can also use a for loop to iterate a over a sequence of numbers produced by a special range object. The reason that we can loop over such objects is that they are special objects called iterables: lists, strings and range objects are all iterables, as are many other Python objects, such as dictionaries and file connections.\nThe actual definition of an iterable is an object that has an associated iter() method. Once this iter() method is applied to an iterable, an iterator object is created. An iterator is defined as an object that has an associated next() method that produces the consecutive values. To create an iterator from an iterable, all we need to do is use the function iter() and pass it the iterable. Once we have the iterator defined, we pass it to the function next() and this returns the first value. Calling next() again on the iterator returns the next value until there are no values left to return and then it throws us a StopIteration error.\n iter(iterable) \u0026mdash;-\u0026gt; iterator next(iterator) \u0026mdash;-\u0026gt; iterate the elements in iterator  For example, a list object is an iterable, which means you can transform a list to a iterator calling iter() on it. But you can not call next() on it directly.\n1 2 3 4 5  ls = [1,2,3,4] itls = iter(ls) print(next(itls)) print(next(itls)) print(next(itls))   1 2 3 1  next(ls)   --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /Users/wanghaoming/Documents/doc1/oop/test2.ipynb Cell 35' in \u0026lt;module\u0026gt; ----\u0026gt; 1 next(ls) TypeError: 'list' object is not an iterator  Under the hood, this is actually what a for loop is doing: it takes an iterable, creates the associated iterator object, and iterates over it.\n 1 2 3 4 5 6 7 8 9 10  flash = [\u0026#39;jay garrick\u0026#39;, \u0026#39;barry allen\u0026#39;, \u0026#39;wally west\u0026#39;, \u0026#39;bart allen\u0026#39;] for item in flash: print(item) # e.t. superhero = iter(flash) print(next(superhero)) print(next(superhero)) print(next(superhero)) print(next(superhero))   jay garrick barry allen wally west bart allen jay garrick barry allen wally west bart allen You can also print all values of an iterator in one fell swoop using the * operator. This * operator unpacks all elements of an iterator or an iterable. Be warned, however, once you do so, you cannot do it (the iterator instead of the iterable) again as there are no more values to iterate through. We would have to redefine our iterator to do so.\n1 2 3 4 5 6 7 8 9 10 11 12  def pr(ob): print(\u0026#34;ob: \u0026#34;, ob) print(\u0026#34;*ob: \u0026#34;, *ob) print(\u0026#34;type: \u0026#34;, type(ob), \u0026#34;\\n\u0026#34;) word = \u0026#34;dfghjhgfds\u0026#34; word1 = iter(word) pr(word) pr(word1) pr(word) pr(word1)   ob: dfghjhgfds *ob: d f g h j h g f d s type: \u0026lt;class 'str'\u0026gt; ob: \u0026lt;str_iterator object at 0x7fc5787b2ac0\u0026gt; *ob: d f g h j h g f d s type: \u0026lt;class 'str_iterator'\u0026gt; ob: dfghjhgfds *ob: d f g h j h g f d s type: \u0026lt;class 'str'\u0026gt; ob: \u0026lt;str_iterator object at 0x7fc5787b2ac0\u0026gt; *ob: type: \u0026lt;class 'str_iterator'\u0026gt; You can also assembles the elements, in the iterator, into a list with list() method\n1 2 3 4  word = \u0026#34;dfghjhgfds\u0026#34; word1 = iter(word) print(word1) print(list(word1))   \u0026lt;str_iterator object at 0x7fc5787b2df0\u0026gt; ['d', 'f', 'g', 'h', 'j', 'h', 'g', 'f', 'd', 's'] We mentioned before that dictionaries and file connections are iterables as well. To iterate over the key-value pairs of a Python dictionary, we need to unpack them by applying the items() method to the dictionary.\nw.r.t. file connections, here you can see how to use the iter() and next() methods to return the lines from a file.\n1 2 3 4 5 6 7 8  (base) wanghaoming@localhost oop_ii_slide % touch file.txt (base) wanghaoming@localhost oop_ii_slide % echo \u0026#34;the first line\u0026#34; \u0026gt; file.txt (base) wanghaoming@localhost oop_ii_slide % echo \u0026#34;the second line\u0026#34; \u0026gt;\u0026gt; file.txt (base) wanghaoming@localhost oop_ii_slide % echo \u0026#34;the third line\u0026#34; \u0026gt;\u0026gt; file.txt (base) wanghaoming@localhost oop_ii_slide % cat file.txt the first line the second line the third line   1 2 3 4 5 6 7 8 9 10  def pr(ob): print(\u0026#34;ob: \u0026#34;, ob) print(next(ob)) # Placing it below *ob will cause an error because there are no more elements in the iterator print(\u0026#34;*ob: \u0026#34;, *ob) print(\u0026#34;type: \u0026#34;, type(ob), \u0026#34;\\n\u0026#34;) file = open(\u0026#34;file.txt\u0026#34;) it = iter(file) pr(it)   ob: \u0026lt;_io.TextIOWrapper name='file.txt' mode='r' encoding='UTF-8'\u0026gt; the first line *ob: the second line # \u0026lt;-- Notice here the third line type: \u0026lt;class '_io.TextIOWrapper'\u0026gt; Operating iterator Now, we are going to introduce two useful functions. The first function, enumerate, will allow us to add a counter to any iterable while the second function, zip, will allow us to stitch together an arbitrary number of iterables.\nenumerate is a function that takes any iterable as argument, such as a list, and returns a special enumerate object, which consists of pairs containing the elements of the original iterable, along with their index within the iterable. The enumerate object itself is also an iterable and we can loop over it while unpacking its elements using the clause for index, value in enumerate(avengers). The default behavior of enumerate to begin indexing at 0. However, you can alter this with startargument, such as start=10\n1 2 3 4 5 6 7 8  tx = \u0026#34;asdfghjkll\u0026#34; etx = enumerate(tx) print(*tx) print(list(tx)) print(*etx) print(list(etx))   a s d f g h j k l l ['a', 's', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'l'] (0, 'a') (1, 's') (2, 'd') (3, 'f') (4, 'g') (5, 'h') (6, 'j') (7, 'k') (8, 'l') (9, 'l') [] # \u0026lt;-- lazy evaluation Notice that The enumerate object has the characteristics of lazy evaluation. The elements that have been visited in the enumerate object cannot be accessed again, and the use of square brackets to index the elements is not supported. (Objects like zip, filter, map, etc. have similar characteristics.)\n1 2 3 4 5 6 7 8  tx = \u0026#34;asdfghjkll\u0026#34; mtx = map( lambda x: x+\u0026#34;!\u0026#34;, tx ) print(mtx) print(*mtx) print(list(mtx))   \u0026lt;map object at 0x7fc5a95e7b20\u0026gt; a! s! d! f! g! h! j! k! l! l! [] Now let\u0026rsquo;s move on to zip, which accepts an arbitrary number of iterables and returns an iterator of tuples. We could use a for loop to iterate over the zip object and print the tuples. We could also have used the * operator or list method to print all the elements.\n1 2 3 4 5 6 7 8  s = \u0026#34;asdfghjkl\u0026#34; t = \u0026#34;qwertyuio\u0026#34; st = zip(s,t) print(st) print(next(st), \u0026#34;\\n\u0026#34;) for i in st: print(i)   \u0026lt;zip object at 0x7fc5787fce40\u0026gt; ('a', 'q') ('s', 'w') ('d', 'e') ('f', 'r') ('g', 't') ('h', 'y') ('j', 'u') ('k', 'i') ('l', 'o') Notice the difference between zip(), *zip(), and zip(*).\n zip(it1, it2) can assemble two iterable object it1, it2 to a zip object; zip(it1) assemble a iterable object it with an empty iterable object. *z can show a zip object; zip(*z) can disassemble a zip object z to the original iterable objects.  1 2 3 4 5 6 7 8 9  s = list(\u0026#34;asdfghjkl\u0026#34;) t = list(\u0026#34;qwertyuio\u0026#34;) print(\u0026#34;zip:\u0026#34;, zip(s,t)) print(\u0026#34;*zip:\u0026#34;, *zip(s,t)) print(\u0026#34;zip(*):\u0026#34;, *zip(*zip(s,t))) s1, t1 = zip(*zip(s,t)) print(list(s1) == s and list(t1) == t)   zip: \u0026lt;zip object at 0x7fc5a95b2ec0\u0026gt; *zip: ('a', 'q') ('s', 'w') ('d', 'e') ('f', 'r') ('g', 't') ('h', 'y') ('j', 'u') ('k', 'i') ('l', 'o') zip(*): ('a', 's', 'd', 'f', 'g', 'h', 'j', 'k', 'l') ('q', 'w', 'e', 'r', 't', 'y', 'u', 'i', 'o') True 1 2 3 4 5 6 7  z1 = zip(mutants, powers) print(*z1) z1 = zip(mutants, powers) result1, result2 = zip(*z1) print(result1 == mutants) print(result2 == powers)   ('charles xavier', 'telepathy') ('bobby drake', 'thermokinesis') ('kurt wagner', 'teleportation') ('max eisenhardt', 'magnetokinesis') ('kitty pryde', 'intangibility') True True Using iterators to load large files into memory If you are pulling data that you can\u0026rsquo;t hold it in memory. One solution is to load the data in chunks, perform the desired operation or operations on each chuck, store the result, discard the chunk and then load the next chunk. To surmount this challenge, we can use the pandas function pd.read_csv()and specify the argument chunksize.\nThe object created by the read_csv() call is an iterable so you can can iterate over it, using a for loop, in which each chunk will be a DataFrame.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def count_entries(csv_file, c_size, colname): \u0026#34;\u0026#34;\u0026#34;Return a dictionary with counts of occurrences as value for each key.\u0026#34;\u0026#34;\u0026#34; counts_dict = {} for chunk in pd.read_csv(csv_file, chunksize=c_size): for entry in chunk[colname]: if entry in counts_dict.keys(): counts_dict[entry] += 1 else: counts_dict[entry] = 1 return counts_dict result_counts = count_entries(\u0026#34;tweets.csv\u0026#34;, 10, \u0026#34;lang\u0026#34;) print(result_counts)   {'en': 97, 'et': 1, 'und': 2} List comprehensions and generators List comprehensions You can finish a for loop in one line of code by comprehensions. The syntax is as follows:\n1  [ \u0026lt;expression\u0026gt; for \u0026lt;variable\u0026gt; in \u0026lt;iterable\u0026gt; ]    You can write a list comprehension over any iterable.\n 1 2 3 4 5  matrix = [[i for i in range(6)] for j in range(5)] Print the matrix for row in matrix: print(row)   [0, 1, 2, 3, 4, 5] [0, 1, 2, 3, 4, 5] [0, 1, 2, 3, 4, 5] [0, 1, 2, 3, 4, 5] [0, 1, 2, 3, 4, 5] 1 2 3 4 5  tweet_time = df[\u0026#34;created_at\u0026#34;] print(tweet_time.head()) tweet_clock_time = [entry[12:20] for entry in tweet_time] print(tweet_clock_time)   0 Tue Mar 29 23:40:17 +0000 2016 1 Tue Mar 29 23:40:17 +0000 2016 2 Tue Mar 29 23:40:17 +0000 2016 3 Tue Mar 29 23:40:17 +0000 2016 4 Tue Mar 29 23:40:17 +0000 2016 Name: created_at, dtype: object ['23:40:17', '23:40:17', '23:40:17', '23:40:17', '23:40:17', '23:40:17', '23:40:18', '23:40:17', '23:40:18', '23:40:18', '23:40:18', '23:40:17', '23:40:18', '23:40:18', '23:40:17', '23:40:18', '23:40:18', '23:40:17', '23:40:18', '23:40:17', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:17', '23:40:18', '23:40:18', '23:40:17', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:18', '23:40:19', '23:40:18', '23:40:18', '23:40:18', '23:40:19', '23:40:19', '23:40:19', '23:40:18', '23:40:19', '23:40:19', '23:40:19', '23:40:18', '23:40:19', '23:40:19', '23:40:19', '23:40:18', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19'] We can also use list comprehensions to create nested loop. The syntax is as follows:\n1  [\u0026lt;exp.\u0026gt; \u0026lt;outer for loop\u0026gt; \u0026lt;inner for loop\u0026gt;]   1 2  ls = [(x,y) for x in range(0,3) for y in range(3,6)] print(ls)   [(0, 3), (0, 4), (0, 5), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5)] Advanced comprehensions We can filter the output of a list comprehension using a conditional on the iterable, the syntax is as follows:\n1  [\u0026lt;exp.\u0026gt; \u0026lt;for loop\u0026gt; if \u0026lt;con.\u0026gt;]   We can also condition the list comprehension on the output expression, like\n1  [\u0026lt;exp.\u0026gt; if \u0026lt;con.\u0026gt; else \u0026lt;exp.\u0026gt; \u0026lt;for loop\u0026gt;]   1 2 3 4 5 6 7 8 9 10 11 12 13 14  [ \u0026lt;exp.1\u0026gt; if \u0026lt;con.1\u0026gt; else \u0026lt;exp.2\u0026gt; if \u0026lt;con.2\u0026gt; else \u0026lt;exp3\u0026gt; \u0026lt;for loop\u0026gt; ] # e.t. if con.1: exp.1 elif con.2: exp.2 else: exp.3   some example:\n1 2 3 4 5  fellowship = [\u0026#39;frodo\u0026#39;, \u0026#39;samwise\u0026#39;, \u0026#39;merry\u0026#39;, \u0026#39;aragorn\u0026#39;, \u0026#39;legolas\u0026#39;, \u0026#39;boromir\u0026#39;, \u0026#39;gimli\u0026#39;] new_fellowship = [member for member in fellowship if len(member)\u0026gt;=7] print(new_fellowship)   ['samwise', 'aragorn', 'legolas', 'boromir'] 1 2 3 4 5  fellowship = [\u0026#39;frodo\u0026#39;, \u0026#39;samwise\u0026#39;, \u0026#39;merry\u0026#39;, \u0026#39;aragorn\u0026#39;, \u0026#39;legolas\u0026#39;, \u0026#39;boromir\u0026#39;, \u0026#39;gimli\u0026#39;] new_fellowship = [member if len(member)\u0026gt;=7 else \u0026#34;\u0026#34; for member in fellowship] print(new_fellowship)   ['', 'samwise', '', 'aragorn', 'legolas', 'boromir', ''] 1 2 3 4 5  tweet_time = df[\u0026#34;created_at\u0026#34;] tweet_clock_time = [entry[11:19] for entry in tweet_time if entry[17:19] == \u0026#34;19\u0026#34;] print(tweet_clock_time)   ['23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19', '23:40:19'] Dictionary comprehensions Now we can also write dictionary comprehensions to create new dictionaries from iterables. The syntax is almost the same as in list comprehensions and there are 2 differences.\n One, we use curly braces instead of square brackets. Two, the key and value are separated by a colon in the output expression.  We can generate a dictionary from two list:\n1 2 3 4 5 6  keys = list(\u0026#34;abcdefg\u0026#34;) vals = list(\u0026#34;1234567\u0026#34;) dic = { key: value for key, value in zip(keys, vals) } print(dic)   {'a': '1', 'b': '2', 'c': '3', 'd': '4', 'e': '5', 'f': '6', 'g': '7'} 1 2 3 4 5 6  import numpy as np import pandas as pd ks = list(\u0026#34;abcde\u0026#34;) vs = [np.random.randn(10) for i in range(5)] df = pd.DataFrame({k:v for k,v in zip(ks,vs)}) print(df)    a b c d e 0 0.108696 0.405694 0.018458 0.529013 0.284126 1 0.645469 0.937125 -0.420953 0.210035 0.151663 2 1.414896 1.298583 1.485383 -0.096663 0.036326 3 0.883876 -3.009644 1.328881 -0.127821 -0.111707 4 1.888317 -2.507632 0.157957 2.036903 2.164004 5 0.144894 1.298034 0.160900 -0.022732 0.080893 6 -0.233574 0.076446 -0.146595 0.842054 1.818775 7 0.509977 -0.424521 0.657196 -0.913793 0.131405 8 -0.049865 -0.816738 -0.975876 0.170173 -0.569519 9 -0.269802 0.755459 0.021207 0.102335 0.119438 1 2 3 4 5 6 7 8 9 10 11  import numpy as np import pandas as pd ks = list(\u0026#34;abcde\u0026#34;) vs = [ np.random.rand(10) if i \u0026lt;2 else np.random.uniform(5,6,10) if i\u0026lt;4 else np.random.randint(0,9,10) for i in range(5) ] df = pd.DataFrame({k:v for k,v in zip(ks,vs)}) print(df)    a b c d e 0 0.797270 0.079468 5.167152 5.205353 7 1 0.036326 0.752136 5.421250 5.275327 4 2 0.698624 0.703501 5.322341 5.295758 8 3 0.306367 0.092034 5.659722 5.430591 8 4 0.217272 0.251593 5.353020 5.078669 2 5 0.404077 0.882906 5.145223 5.866817 3 6 0.677442 0.012514 5.186332 5.221120 8 7 0.080858 0.226505 5.724067 5.976189 7 8 0.366588 0.884782 5.561158 5.070010 6 9 0.694882 0.928411 5.180124 5.449207 2 1 2 3 4 5 6 7  fellowship = [\u0026#39;frodo\u0026#39;, \u0026#39;samwise\u0026#39;, \u0026#39;merry\u0026#39;, \u0026#39;aragorn\u0026#39;, \u0026#39;legolas\u0026#39;, \u0026#39;boromir\u0026#39;, \u0026#39;gimli\u0026#39;] new_fellowship = { member: len(member) for member in fellowship } print(new_fellowship)   {'frodo': 5, 'samwise': 7, 'merry': 5, 'aragorn': 7, 'legolas': 7, 'boromir': 7, 'gimli': 5} Generator expressions Recall that this list comprehension will create a list. Now lets replace the square brackets with round parentheses. Something called a generator object has been created.\nA generator is like a list comprehension except it does not store the list in memory: it does not construct the list, but is an object we can iterate over to produce elements of the list as required. Like any other iterator, we can pass a generator to the function next() in order to iterate through its elements.\n1 2 3 4 5 6 7 8 9 10 11  result = (num for num in range(0,10)) print(next(result)) print(next(result)) print(next(result)) print(next(result)) print(next(result), \u0026#34;\\n\u0026#34;) # Print the rest of the values for value in result: print(value)   0 1 2 3 4 5 6 7 8 9 Generator is an example of something called lazy evaluation, whereby the evaluation of the expression is delayed until its value is needed. (This means that you can\u0026rsquo;t index values by square brackets.) This can help a great deal when working with extremely large sequences as you don\u0026rsquo;t want to store the entire list in memory, which is what comprehensions would do; you can generate elements of the sequence on the fly. Let\u0026rsquo;s say that we wanted to iterate over a very large sequence of numbers, such as from 0 up to 10 to the power of a million.\n1  [i for i in range(10 ** 1000000)]    DO NOT try this on your personal computer.\n This can cause outages, since the list we are trying to create are so large that can\u0026rsquo;t even be stored in memory. however: We can easily create the analogous generator object because it does not yet create the entire list.\n1 2  g = (i for i in range(10 ** 1000000)) print(next(g))   0 The thing we can do in a list comprehension such as filtering and applying conditionals, can also be done in a generator expression.\nGenerator functions are functions that, when called, produce generator objects. Generator functions are written with the syntax of any other user-defined function, however instead of returning values using the keyword return, they yield sequences of values using the keyword yield.\n1 2 3 4 5 6 7 8 9 10 11  lannister = [\u0026#39;cersei\u0026#39;, \u0026#39;jaime\u0026#39;, \u0026#39;tywin\u0026#39;, \u0026#39;tyrion\u0026#39;, \u0026#39;joffrey\u0026#39;] def get_lengths(input_list): \u0026#34;\u0026#34;\u0026#34;Generator function that yields the length of the strings in input_list.\u0026#34;\u0026#34;\u0026#34; for person in input_list: yield len(person) for value in get_lengths(lannister): print(value)   6 5 5 6 7 World Bank Data Analysis Case Here, we will use the skills of writing user-defined functions, iterators, list comprehensions and generators to wrangle and extract meaningful information from a real-world dataset, the World Bank World Development Indicators dataset.\nThis dataset contains data on 217 world economies for over half a century, from 1960 up until 2015. The data contains hundreds of indicators from population, electricity consumption and CO2 emissions to literacy rates, unemployment and mortality rates.\n The first list feature_names contains header names of the dataset and the second list row_vals contains actual values of a row from the dataset, corresponding to each of the header names. Create a zip object by calling zip() and passing to it feature_names and row_vals. Assign the result to zipped_lists. Create a dictionary from the zipped_lists zip object by calling dict() with zipped_lists. Assign the resulting dictionary to rs_dict.\n1 2 3  zipped_lists = zip(feature_names, row_vals) rs_dict = dict(zipped_lists) # e.t. rs_dict = {k:v for k,v in zipped_lists} print(rs_dict)   {'CountryName': 'Arab World', 'CountryCode': 'ARB', 'IndicatorName': 'Adolescent fertility rate (births per 1,000 women ages 15-19)', 'IndicatorCode': 'SP.ADO.TFRT', 'Year': '1960', 'Value': '133.56090740552298'} Define the function lists2dict() with two parameters: first is list1 and second is list2. Return the resulting dictionary rs_dict in lists2dict(). Call the lists2dict() function with the arguments feature_names and row_vals. Assign the result of the function call to rs_fxn.\n1 2 3 4 5 6 7 8 9  def lists2dict(list1, list2): \u0026#34;\u0026#34;\u0026#34;Return a dictionary where list1 provides the keys and list2 provides the values.\u0026#34;\u0026#34;\u0026#34; zipped_lists = zip(list1, list2) rs_dict = dict(zipped_lists) return rs_dict rs_fxn = lists2dict(feature_names, row_vals) print(rs_fxn)   {'CountryName': 'Arab World', 'CountryCode': 'ARB', 'IndicatorName': 'Adolescent fertility rate (births per 1,000 women ages 15-19)', 'IndicatorCode': 'SP.ADO.TFRT', 'Year': '1960', 'Value': '133.56090740552298'} row_lists is a list of lists, where each sublist is a list of actual values of a row from the dataset. Inspect the contents of row_lists by printing the first two lists in row_lists. Create a list comprehension that generates a dictionary using lists2dict() for each sublist in row_lists. The keys are from the feature_names list and the values are the row entries in row_lists. Use sublist as your iterator variable and assign the resulting list of dictionaries to list_of_dicts. Look at the first two dictionaries in list_of_dicts by printing them out.\n1 2 3 4 5 6 7  print(row_lists[0]) print(row_lists[1]) list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists] print(list_of_dicts[0]) print(list_of_dicts[1])   ['Arab World', 'ARB', 'Adolescent fertility rate (births per 1,000 women ages 15-19)', 'SP.ADO.TFRT', '1960', '133.56090740552298'] ['Arab World', 'ARB', 'Age dependency ratio (% of working-age population)', 'SP.POP.DPND', '1960', '87.7976011532547'] {'CountryName': 'Arab World', 'CountryCode': 'ARB', 'IndicatorName': 'Adolescent fertility rate (births per 1,000 women ages 15-19)', 'IndicatorCode': 'SP.ADO.TFRT', 'Year': '1960', 'Value': '133.56090740552298'} {'CountryName': 'Arab World', 'CountryCode': 'ARB', 'IndicatorName': 'Age dependency ratio (% of working-age population)', 'IndicatorCode': 'SP.POP.DPND', 'Year': '1960', 'Value': '87.7976011532547'} To use the DataFrame() function, first import the pandas package with the alias pd. Create a DataFrame from the list of dictionaries in list_of_dicts by calling pd.DataFrame(). Assign the resulting DataFrame to df. Inspect the contents of df printing the head of the DataFrame. Head of the DataFrame df can be accessed by calling df.head().\n1 2 3 4 5  import pandas as pd list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists] df = pd.DataFrame(list_of_dicts) df.head()    CountryName CountryCode IndicatorName IndicatorCode Year Value 0 Arab World ARB Adolescent fertility rate (births per 1,000 wo... SP.ADO.TFRT 1960 133.56090740552298 1 Arab World ARB Age dependency ratio (% of working-age populat... SP.POP.DPND 1960 87.7976011532547 2 Arab World ARB Age dependency ratio, old (% of working-age po... SP.POP.DPND.OL 1960 6.634579191565161 3 Arab World ARB Age dependency ratio, young (% of working-age ... SP.POP.DPND.YG 1960 81.02332950839141 4 Arab World ARB Arms exports (SIPRI trend indicator values) MS.MIL.XPRT.KD 1960 3000000.0 In the function read_large_file(), read a line from file_object by using the method readline(). Assign the result to data. In the function read_large_file(), yield the line read from the file data. In the context manager, create a generator object gen_file by calling your generator function read_large_file() and passing file to it. Print the first three lines produced by the generator object gen_file using next().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def read_large_file(file_object): \u0026#34;\u0026#34;\u0026#34;A generator function to read a large file lazily.\u0026#34;\u0026#34;\u0026#34; while True: data = file_object.readline() if not data: # \u0026lt;-- Break if this is the end of the file break yield data # \u0026lt;-- After calling read_large_file(), in each loop, programme will freeze here until calling next() with open(\u0026#39;world_dev_ind.csv\u0026#39;) as file: gen_file = read_large_file(file) print(next(gen_file)) print(next(gen_file)) print(next(gen_file))   CountryName,CountryCode,IndicatorName,IndicatorCode,Year,Value Arab World,ARB,\u0026quot;Adolescent fertility rate (births per 1,000 women ages 15-19)\u0026quot;,SP.ADO.TFRT,1960,133.56090740552298 Arab World,ARB,Age dependency ratio (% of working-age population),SP.POP.DPND,1960,87.7976011532547 Bind the file \u0026lsquo;world_dev_ind.csv\u0026rsquo; to file in the context manager with open(). Complete the for loop so that it iterates over the generator from the call to read_large_file() to process all the rows of the file.\n1 2 3 4 5 6 7 8 9 10 11 12 13  counts_dict = {} with open(\u0026#34;world_dev_ind.csv\u0026#34;) as file: for line in read_large_file(file): # \u0026lt;-- Each loop is a call to next() row = line.split(\u0026#39;,\u0026#39;) # \u0026lt;-- Separate each line of the CSV with a comma and generate a list first_col = row[0] if first_col in counts_dict.keys(): counts_dict[first_col] += 1 else: counts_dict[first_col] = 1 print(counts_dict)   {'CountryName': 1, 'Arab World': 80, 'Caribbean small states': 77, 'Central Europe and the Baltics': 71, 'East Asia \u0026amp; Pacific (all income levels)': 122, 'East Asia \u0026amp; Pacific (developing only)': 123, 'Euro area': 119, 'Europe \u0026amp; Central Asia (all income levels)': 109, 'Europe \u0026amp; Central Asia (developing only)': 89, 'European Union': 116, 'Fragile and conflict affected situations': 76, 'Heavily indebted poor countries (HIPC)': 99, 'High income': 131, 'High income: nonOECD': 68, 'High income: OECD': 127, 'Latin America \u0026amp; Caribbean (all income levels)': 130, 'Latin America \u0026amp; Caribbean (developing only)': 133, 'Least developed countries: UN classification': 78, 'Low \u0026amp; middle income': 138, 'Low income': 80, 'Lower middle income': 126, 'Middle East \u0026amp; North Africa (all income levels)': 89, 'Middle East \u0026amp; North Africa (developing only)': 94, 'Middle income': 138, 'North America': 123, 'OECD members': 130, 'Other small states': 63, 'Pacific island small states': 66, 'Small states': 69, 'South Asia': 36} Use pd.read_csv() to read in \u0026lsquo;ind_pop.csv\u0026rsquo; in chunks of size 10. Assign the result to df_reader. Print the first two chunks from df_reader.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import pandas as pd df_reader = pd.read_csv(\u0026#34;ind_pop.csv\u0026#34;, chunksize=10) print(next(df_reader)) print(next(df_reader)) # e.t. # t=0 # for i in df_reaedr: # \u0026lt;-- Each loop is a next() # t += 1 # print(i) # if t == 2: # break    CountryName CountryCode IndicatorName IndicatorCode Year Value 0 Arab World ARB Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 31.285 1 Caribbean small states CSS Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 31.597 2 Central Europe and the Baltics CEB Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 44.508 3 East Asia \u0026amp; Pacific (all income levels) EAS Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 22.471 4 East Asia \u0026amp; Pacific (developing only) EAP Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 16.918 5 Euro area EMU Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 62.097 6 Europe \u0026amp; Central Asia (all income levels) ECS Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 55.379 7 Europe \u0026amp; Central Asia (developing only) ECA Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 38.066 8 European Union EUU Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 61.213 9 Fragile and conflict affected situations FCS Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 17.892 CountryName CountryCode IndicatorName IndicatorCode Year Value 10 Heavily indebted poor countries (HIPC) HPC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 12.236 11 High income HIC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 62.680 12 High income: nonOECD NOC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 56.108 13 High income: OECD OEC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 64.285 14 Latin America \u0026amp; Caribbean (all income levels) LCN Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 49.285 15 Latin America \u0026amp; Caribbean (developing only) LAC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 44.863 16 Least developed countries: UN classification LDC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 9.616 17 Low \u0026amp; middle income LMY Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 21.273 18 Low income LIC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 11.498 19 Lower middle income LMC Urban population (% of total) SP.URB.TOTL.IN.ZS 1960 19.811 Use pd.read_csv() to read in the file in \u0026lsquo;ind_pop_data.csv\u0026rsquo; in chunks of size 1000. Assign the result to urb_pop_reader. Get the first DataFrame chunk from the iterable urb_pop_reader and assign this to df_urb_pop.\nSelect only the rows of df_urb_pop that have a \u0026lsquo;CountryCode\u0026rsquo; of \u0026lsquo;CEB\u0026rsquo;. Using zip(), zip together the \u0026lsquo;Total Population\u0026rsquo; and \u0026lsquo;Urban population (% of total)\u0026rsquo; columns of df_pop_ceb. Assign the resulting zip object to pops.\n1 2 3  urb_pop_reader = pd.read_csv(\u0026#34;ind_pop_data.csv\u0026#34;, chunksize=1000) df_urb_pop = next(urb_pop_reader) print(df_urb_pop.head())    CountryName CountryCode Year Total Population Urban population (% of total) 0 Arab World ARB 1960 9.250e+07 31.285 1 Caribbean small states CSS 1960 4.191e+06 31.597 2 Central Europe and the Baltics CEB 1960 9.140e+07 44.508 3 East Asia \u0026amp; Pacific (all income levels) EAS 1960 1.042e+09 22.471 4 East Asia \u0026amp; Pacific (developing only) EAP 1960 8.965e+08 16.918 1 2 3 4  df_pop_ceb = df_urb_pop[df_urb_pop[\u0026#34;CountryCode\u0026#34;]==\u0026#34;CEB\u0026#34;] pops = zip(df_pop_ceb[\u0026#39;Total Population\u0026#39;], df_pop_ceb[\u0026#39;Urban population (% of total)\u0026#39;]) pops_list = list(pops) print(pops_list)   [(91401583.0, 44.5079211390026), (92237118.0, 45.206665319194), (93014890.0, 45.866564696018), (93845749.0, 46.5340927663649), (94722599.0, 47.2087429803526)] Write a list comprehension to generate a list of values from pops_list for the new column \u0026lsquo;Total Urban Population\u0026rsquo;. The output expression should be the product of the first and second element in each tuple in pops_list. Because the 2nd element is a percentage, you also need to either multiply the result by 0.01 or divide it by 100. In addition, note that the column \u0026lsquo;Total Urban Population\u0026rsquo; should only be able to take on integer values. To ensure this, make sure you cast the output expression to an integer with int().\nCreate a scatter plot where the x-axis are values from the \u0026lsquo;Year\u0026rsquo; column and the y-axis are values from the \u0026lsquo;Total Urban Population\u0026rsquo; column.\n1 2 3 4 5 6 7 8 9 10  urb_pop_reader = pd.read_csv(\u0026#39;ind_pop_data.csv\u0026#39;, chunksize=1000) df_urb_pop = next(urb_pop_reader) df_pop_ceb = df_urb_pop[df_urb_pop[\u0026#39;CountryCode\u0026#39;] == \u0026#39;CEB\u0026#39;] pops = zip(df_pop_ceb[\u0026#39;Total Population\u0026#39;], df_pop_ceb[\u0026#39;Urban population (% of total)\u0026#39;]) pops_list = list(pops) df_pop_ceb[\u0026#39;Total Urban Population\u0026#39;] = [int(x[0]*x[1]*0.01) for x in pops_list ] df_pop_ceb.plot(kind=\u0026#34;scatter\u0026#34;, x=\u0026#34;Year\u0026#34;, y=\u0026#34;Total Urban Population\u0026#34;) plt.show()   Now let\u0026rsquo;s show the panoramic view. Initialize an empty DataFrame data using pd.DataFrame(). In the for loop, iterate over urb_pop_reader to be able to process all the DataFrame chunks in the dataset. Using the method append() of the DataFrame data, append df_pop_ceb to data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  urb_pop_reader = pd.read_csv(\u0026#39;ind_pop_data.csv\u0026#39;, chunksize=1000) data = pd.DataFrame() for df_urb_pop in urb_pop_reader: df_pop_ceb = df_urb_pop[df_urb_pop[\u0026#39;CountryCode\u0026#39;] == \u0026#39;CEB\u0026#39;] pops = zip( df_pop_ceb[\u0026#39;Total Population\u0026#39;], df_pop_ceb[\u0026#39;Urban population (% of total)\u0026#39;] ) pops_list = list(pops) df_pop_ceb[\u0026#39;Total Urban Population\u0026#39;] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list] data = data.append(df_pop_ceb) data.plot(kind=\u0026#39;scatter\u0026#39;, x=\u0026#39;Year\u0026#39;, y=\u0026#39;Total Urban Population\u0026#39;) plt.show()   Now we encapsulate the above code into a function. Define the function plot_pop() that has two arguments: first is filename for the file to process and second is country_code for the country to be processed in the dataset. Call plot_pop() to process the data for country code \u0026lsquo;CEB\u0026rsquo; in the file \u0026lsquo;ind_pop_data.csv\u0026rsquo;. Call plot_pop() to process the data for country code \u0026lsquo;ARB\u0026rsquo; in the file \u0026lsquo;ind_pop_data.csv\u0026rsquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  def plot_pop(filename, country_code): urb_pop_reader = pd.read_csv(filename, chunksize=1000) data = pd.DataFrame() for df_urb_pop in urb_pop_reader: df_pop_ceb = df_urb_pop[df_urb_pop[\u0026#39;CountryCode\u0026#39;] == country_code] pops = zip( df_pop_ceb[\u0026#39;Total Population\u0026#39;], df_pop_ceb[\u0026#39;Urban population (% of total)\u0026#39;] ) pops_list = list(pops) df_pop_ceb[\u0026#39;Total Urban Population\u0026#39;] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list] data = data.append(df_pop_ceb) data.plot(kind=\u0026#39;scatter\u0026#39;, x=\u0026#39;Year\u0026#39;, y=\u0026#39;Total Urban Population\u0026#39;) plt.show() fn = \u0026#39;ind_pop_data.csv\u0026#39; # plot_pop(fn, \u0026#34;CEB\u0026#34;) plot_pop(fn, \u0026#34;ARB\u0026#34;)   ","date":"2019-08-16T00:00:00Z","image":"https://wanghaoming177.netlify.app/head_img/oop_hi/oo2.png","permalink":"https://wanghaoming177.netlify.app/p/object-oriented-programming-ii-iterators-comprehensions-generators/","title":"Object Oriented Programming II (Iterators, Comprehensions \u0026 Generators)"},{"content":"Writing Functions1 Define function To define the function, We begin with the keyword def, followed by the function name; this is then followed by a set of parentheses and a colon. This piece of code is called a function header. To complete the function definition, We write the function body inside the indentation.\nThere\u0026rsquo;s an essential aspect of writing functions in Python: docstrings. Docstrings serve as documentation for your function so that anyone who reads your function\u0026rsquo;s docstring understands what your function does, without having to trace through all the code in the function definition. Function docstrings are placed in the immediate line after the function header and are placed in between triple quotation marks.\n1 2 3 4 5 6  def shout(): \u0026#34;\u0026#34;\u0026#34;Print a string with three exclamation marks\u0026#34;\u0026#34;\u0026#34; shout_word = \u0026#34;congratulations\u0026#34; + \u0026#34;!!!\u0026#34; print(shout_word) shout()   congratulations!!! You can add a parameter to the function definition in between the parentheses. When you define a function, you write parameters in the function header. When you call a function, you pass arguments into the function.\nWhat if we don\u0026rsquo;t want to print that outcome directly and instead we want to return the it and assign it to some variable? You can have your function return the new value by adding the return keyword, followed by the outcome to return.\n1 2 3 4 5 6 7  def shout(word): \u0026#34;\u0026#34;\u0026#34;Return a string with three exclamation marks\u0026#34;\u0026#34;\u0026#34; shout_word = word + \u0026#34;!!!\u0026#34; return shout_word yell = shout(\u0026#34;congratulations\u0026#34;) print(yell)   congratulations!!! Multiple parameters and return values We can imput multi parameters to the function.\n You should also change your function name and docstrings to reflect this new behavior.\n 1 2 3 4 5 6 7 8 9  def shout(word1, word2): \u0026#34;\u0026#34;\u0026#34;Concatenate strings with three exclamation marks\u0026#34;\u0026#34;\u0026#34; shout1 = word1 + \u0026#34;!!!\u0026#34; shout2 = word2 + \u0026#34;!!!\u0026#34; new_shout = shout1 + shout2 return new_shout yell = shout(\u0026#34;congratulations\u0026#34;, \u0026#34;you\u0026#34;) print(yell)   congratulations!!!you!!! You can also make your function return multiple values. You can do that by constructing objects known as tuples in your functions. In the function body, we construct a tuple consisting of the values we want the function to return ,and also we return the tuple.\n1 2 3 4 5 6 7 8 9 10  def shout_all(word1, word2): shout1 = word1 + \u0026#34;!!!\u0026#34; shout2 = word2 + \u0026#34;!!!\u0026#34; shout_words = (shout1, shout2) return shout_words yell1, yell2 = shout_all(\u0026#34;congratulations\u0026#34;, \u0026#34;you\u0026#34;) print(yell1) print(yell2)   congratulations!!! you!!! 1 2 3 4 5 6 7 8 9 10 11 12 13 14  def count_entries(df, col_name): \u0026#34;\u0026#34;\u0026#34;Return a dictionary with counts of occurrences as value for each key.\u0026#34;\u0026#34;\u0026#34; langs_count = {} col = df[col_name] for entry in col: if entry in langs_count.keys(): langs_count[entry] += 1 else: langs_count[entry] = 1 return langs_count result = count_entries(tweets_df, \u0026#34;lang\u0026#34;) print(result)   {'en': 97, 'et': 1, 'und': 2} Default arguments, variable-length arguments and scope Scope We\u0026rsquo;ll now talk about the idea of scope in the context of user-defined functions, which tells you which part of a program an object or a name may be accessed.\nGenerally, there are three types of scope:\n global scope. A name that is in the global scope means that it is defined in the main body of a script or a Python program. local scope. A name that is in a local scope means that it is defined within a function. Once the execution of a function is done, any name inside the local scope ceases to exist, which means you cannot access those names anymore outside of the function definition. built-in scope: this consists of names in the pre-defined built-ins module Python provides, such as print and sum.  The rule for referencing global variable inside the function is to search for local scope first, and then global scope if not. Notice that\n if global variable was not defined in the function body, you can refer it directly:  1 2 3 4  v = 3 def func0(): print(v) func0()   3 if global variable was defined in the function body after you call the variable, you need use global key word te declare te variable:  1 2 3 4 5 6 7  v = 3 def func1(): global v print(v) v = 4 print(v) func1()   3 4 otherwise python will report Errors:\n1 2 3 4 5 6  v = 3 def func1(): print(v) v = 4 print(v) func1()   UnboundLocalError: local variable 'v' referenced before assignment if the variable was defined in the function before you call it, python will use local variable:  1 2 3 4 5  v = 3 def func2(): v=4 print(v) func2()   4 In general, we cannot refer to a local scope outside a function, unless we declare a local scope as global using the global keyword. Notice that once you use the global keyword for a variable, any changes made to that variable within the function are reflected in the global variable. For example\n1 2 3 4 5 6 7 8 9 10 11 12 13  n = 5 def func1(): n=3 print(\u0026#34;n1:\u0026#34;, n) def func2(): global n print(\u0026#34;n2:\u0026#34;, n) n = 4 print(\u0026#34;n3:\u0026#34;, n) func1() func2() print(\u0026#34;n4:\u0026#34;, n)   n1: 3 n2: 5 n3: 4 n4: 4 1 2 3 4 5 6 7 8 9  team = \u0026#34;teen titans\u0026#34; def change_team(): \u0026#34;\u0026#34;\u0026#34;Change the value of the global variable team.\u0026#34;\u0026#34;\u0026#34; global team team = \u0026#34;justice league\u0026#34; print(team) change_team() print(team)   teen titans justice league Nested functions There are a number of good reasons to do write nested function. For example, we want a function that takes 3 numbers as parameters and performs the same function on each of them. One way would be to write out the computation 3 times.\n1 2 3 4 5 6  def func(v1, v2, v3): v1 = v1 + 5 v2 = v2 + 5 v3 = v3 + 5 return (v1, v2, v3) func(1,2,3)   (6,7,8) but this definitely does not scale if you need to perform the computation many times. What we can do instead is define an inner function within our function definition, such that\n1 2 3 4 5  def func1(v1,v2,v3): def func2(x): return x + 5 return(func2(v1), func2(v2), func2(v3)) func1(1,2,3)   (6,7,8) This is called a nested function. The syntax for the inner function is exactly the same as that for any other function.\n1 2 3 4 5 6 7 8 9  def three_shouts(word1, word2, word3): \u0026#34;\u0026#34;\u0026#34;Returns a tuple of strings concatenated with \u0026#39;!!!\u0026#39;.\u0026#34;\u0026#34;\u0026#34; def inner(word): \u0026#34;\u0026#34;\u0026#34;Returns a string concatenated with \u0026#39;!!!\u0026#39;.\u0026#34;\u0026#34;\u0026#34; return word + \u0026#39;!!!\u0026#39; return (inner(word1), inner(word2), inner(word3)) print(three_shouts(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;))    ('a!!!', 'b!!!', 'c!!!') 1 2 3 4 5 6 7 8 9 10 11 12  def echo(n): \u0026#34;\u0026#34;\u0026#34;Return the inner_echo function.\u0026#34;\u0026#34;\u0026#34; def inner_echo(word1): \u0026#34;\u0026#34;\u0026#34;Concatenate n copies of word1.\u0026#34;\u0026#34;\u0026#34; echo_word = word1 * n return echo_word return inner_echo twice = echo(2) thrice = echo(3) print(twice(\u0026#39;hello\u0026#39;), thrice(\u0026#39;hello\u0026#39;))   hellohello hellohellohello What if we have a function inner defined within another function outer and we reference a name x in the inner function? The answer is intuitive: Python searches the local scope of the function inner, then if it doesn\u0026rsquo;t find x, it searches the scope of the function outer, which is called an enclosing function because it encloses the function inner. If Python can\u0026rsquo;t find x in the scope of the enclosing function, it only then searches the global scope and then the built-in scope. This is known as the LEGB rule, where L is for local, E for enclosing, G for global and B for built-ins\n1 2 3 4 5 6 7 8 9 10 11  n = 1 # \u0026lt;-- G scope def outter(): n = 2 # \u0026lt;-- E scope def inner(): n = 3 # \u0026lt;-- L scope print(n) # \u0026lt;-- L scope inner() print(n) # \u0026lt;-- E scope outter() print(n) # \u0026lt;-- G scope   3 2 1 1 2 3 4 5 6 7 8 9 10  n = 1 # \u0026lt;-- G scope def outter(): n = 2 # \u0026lt;-- E scope def inner(): print(n) # \u0026lt;-- L scope inner() print(n) # \u0026lt;-- E scope outter() print(n) # \u0026lt;-- G scope   2 2 1 1 2 3 4 5 6 7 8 9  n = 1 # \u0026lt;-- G scope def outter(): def inner(): print(n) # \u0026lt;-- L scope inner() print(n) # \u0026lt;-- E scope outter() print(n) # \u0026lt;-- G scope   1 1 1  Notice that The E scope variable can reference the values of L and G scope variables, but cannot change the values of L and G scope variables, unless declare keyword global or nonlocol.\n Let\u0026rsquo;s now look at another important use case of nested functions. In this example, we define a function func1, which contains an inner function called func2. func1 returns the inner function func2. func1 takes an argument x and creates a function inner that returns the yth power of x.\n1 2 3 4 5 6 7  def func1(x): def func2(y): return x ** y return func2 f = func1(2) print(type(f)) f(3)   \u0026lt;class 'function'\u0026gt; 8 One interesting detail: when we call the function f, it remembers the value x=2, although the enclosing scope defined by func1 and to which x=2 is local, has finished execution. This is a subtlety referred to as a closure in Computer Science.\nRecall that you can use the keyword global in function definitions to create and change global names; similarly, in a nested function, you can use the keyword nonlocal to create and changes names in an enclosing scope. In simple terms, global: L -\u0026gt; G or global: E -\u0026gt; G; and nonlocal: L -\u0026gt; E.\n outer + inner(nonlocal)  1 2 3 4 5 6 7 8 9 10 11 12  n=0 # \u0026lt;-- G scope def func1(): n=1 # \u0026lt;-- E scope def func2(): nonlocal n # \u0026lt;== [L -\u0026gt; E] n=2 # \u0026lt;-- E scope print(\u0026#34;n1:\u0026#34;,n) # \u0026lt;-- E scope print(\u0026#34;n2:\u0026#34;,n) # \u0026lt;-- E scope func2() print(\u0026#34;n3:\u0026#34;,n) # \u0026lt;-- E scope func1() print(\u0026#34;n4:\u0026#34;,n) # \u0026lt;-- G scope   n2: 1 n1: 2 n3: 2 n4: 0 outer(global) + inner  1 2 3 4 5 6 7 8 9 10 11 12  n=0 # \u0026lt;-- G scope def func1(): global n # \u0026lt;== [E -\u0026gt; G] n=1 # \u0026lt;-- G scope def func2(): n=2 # \u0026lt;-- L scope print(\u0026#34;n1:\u0026#34;,n) # \u0026lt;-- L scope print(\u0026#34;n2:\u0026#34;,n) # \u0026lt;-- G scope func2() print(\u0026#34;n3:\u0026#34;,n) # \u0026lt;-- G scope func1() print(\u0026#34;n4:\u0026#34;,n) # \u0026lt;-- G scope   n2: 1 n1: 2 n3: 1 n4: 1 outer + inner(global)  1 2 3 4 5 6 7 8 9 10 11 12 13 14  n=0 def func1(): n=1 # \u0026lt;-- E scope def func2(): global n # \u0026lt;== [L -\u0026gt; G] print(\u0026#34;n0:\u0026#34;,n) # \u0026lt;-- G scope n=2 # \u0026lt;-- G scope print(\u0026#34;n1:\u0026#34;,n) # \u0026lt;-- G scope print(\u0026#34;n2:\u0026#34;,n) # \u0026lt;-- E scope func2() print(\u0026#34;n3:\u0026#34;,n) # \u0026lt;-- E scope func1() print(\u0026#34;n4:\u0026#34;,n) # \u0026lt;-- G scope   n2: 1 n0: 0 n1: 2 n3: 1 # \u0026lt;-- Notice, E scope n4: 2 outer(global) + inner(global)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  n=0 # \u0026lt;-- G scope def func1(): global n # \u0026lt;== [E -\u0026gt; G] n=1 # \u0026lt;-- G scope def func2(): global n # \u0026lt;== [L -\u0026gt; G] print(\u0026#34;n0:\u0026#34;,n) # \u0026lt;-- G scope n=2 # \u0026lt;-- G scope print(\u0026#34;n1:\u0026#34;,n) # \u0026lt;-- G scope print(\u0026#34;n2:\u0026#34;,n) # \u0026lt;-- G scope func2() print(\u0026#34;n3:\u0026#34;,n) # \u0026lt;-- G scope func1() print(\u0026#34;n4:\u0026#34;,n) # \u0026lt;-- G scope   n2: 1 n0: 1 n1: 2 n3: 2 n4: 2 outer(global) + inner(nonlocal)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  n=0 # \u0026lt;-- G scope def func1(): global n # \u0026lt;== [E -\u0026gt; G] n=1 # \u0026lt;-- G scope def func2(): nonlocal n # \u0026lt;== [L -\u0026gt; E] ERROR! print(\u0026#34;n0:\u0026#34;,n) n=2 print(\u0026#34;n1:\u0026#34;,n) print(\u0026#34;n2:\u0026#34;,n) func2() print(\u0026#34;n3:\u0026#34;,n) func1() print(\u0026#34;n4:\u0026#34;,n)   SyntaxError: no binding for nonlocal 'n' found This leads an error, since E scope variable n has been transform to be G scope variable, which means there is no E scope variable n, and nonlocal n cannot match n.\n1 2 3 4 5 6 7 8 9 10 11 12 13  def echo_shout(word): \u0026#34;\u0026#34;\u0026#34;Change the value of a nonlocal variable\u0026#34;\u0026#34;\u0026#34; echo_word = word * 2 print(echo_word) def shout(): \u0026#34;\u0026#34;\u0026#34;Alter a variable in the enclosing scope\u0026#34;\u0026#34;\u0026#34; nonlocal echo_word echo_word = echo_word + \u0026#34;!!!\u0026#34; shout() print(echo_word) echo_shout(\u0026#34;hello\u0026#34;)   hellohello hellohello!!! Default and flexible arguments To define a function with a default argument value, in the function header we follow the parameter of interest with an equals sign and the default argument value.\n1 2 3 4 5 6 7 8 9 10 11 12 13  # Define shout_echo def shout_echo(word1, echo=1): \u0026#34;\u0026#34;\u0026#34;Concatenate echo copies of word1 and three exclamation marks at the end of the string.\u0026#34;\u0026#34;\u0026#34; echo_word = word1 * echo shout_word = echo_word + \u0026#39;!!!\u0026#39; return shout_word no_echo = shout_echo(\u0026#34;Hey\u0026#34;) with_echo = shout_echo(\u0026#34;Hey\u0026#34;, echo=5) print(no_echo) print(with_echo)   Hey!!! HeyHeyHeyHeyHey!!! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def shout_echo(word1, echo=1, intense=False): \u0026#34;\u0026#34;\u0026#34;Concatenate echo copies of word1 and three exclamation marks at the end of the string.\u0026#34;\u0026#34;\u0026#34; echo_word = word1 * echo if intense is True: echo_word_new = echo_word.upper() + \u0026#39;!!!\u0026#39; else: echo_word_new = echo_word + \u0026#39;!!!\u0026#39; return echo_word_new with_big_echo = shout_echo(\u0026#34;Hey\u0026#34;, echo=5, intense=True) big_no_echo = shout_echo(\u0026#34;Hey\u0026#34;, intense=True) print(with_big_echo) print(big_no_echo)   HEYHEYHEYHEYHEY!!! HEY!!! In the function definition, we use the parameter *arg: this then turns all the arguments passed to a function call into a tuple called args in the function body;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  def gibberish(*args): \u0026#34;\u0026#34;\u0026#34;Concatenate strings in *args together.\u0026#34;\u0026#34;\u0026#34; hodgepodge=\u0026#34;\u0026#34; for word in args: hodgepodge += word return hodgepodge one_word = gibberish(\u0026#34;luke\u0026#34;) many_words = gibberish(\u0026#34;luke\u0026#34;, \u0026#34;leia\u0026#34;, \u0026#34;han\u0026#34;, \u0026#34;obi\u0026#34;, \u0026#34;darth\u0026#34;) ls = [\u0026#34;luke\u0026#34;, \u0026#34;leia\u0026#34;, \u0026#34;han\u0026#34;, \u0026#34;obi\u0026#34;, \u0026#34;darth\u0026#34;] many_words2 = gibberish(*ls) print(one_word) print(many_words) print(many_words2)   luke lukeleiahanobidarth lukeleiahanobidarth You can also use the parameter kwargs preceded by a double star. This turns the identifier-keyword pairs into a dictionary within the function body.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def report_status(**kwargs): \u0026#34;\u0026#34;\u0026#34;Print out the status of a movie character.\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;\\nBEGIN: REPORT\\n\u0026#34;) for key, value in kwargs.items(): print(key + \u0026#34;: \u0026#34; + value) print(\u0026#34;\\nEND REPORT\u0026#34;) report_status(name=\u0026#34;luke\u0026#34;, affiliation=\u0026#34;jedi\u0026#34;, status=\u0026#34;missing\u0026#34;) params = { \u0026#34;name\u0026#34;:\u0026#34;anakin\u0026#34;, \u0026#34;affiliation\u0026#34;:\u0026#34;sith lord\u0026#34;, \u0026#34;status\u0026#34;:\u0026#34;deceased\u0026#34; } report_status(**params)   BEGIN: REPORT name: luke affiliation: jedi status: missing END REPORT BEGIN: REPORT name: anakin affiliation: sith lord status: deceased END REPORT  Note that it is NOT the names args and kwargs that are important when using flexible arguments, but rather that they\u0026rsquo;re preceded by a single and double star, respectively.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  def count_entries(df, *args): \u0026#34;\u0026#34;\u0026#34;Return a dictionary with counts of occurrences as value for each key.\u0026#34;\u0026#34;\u0026#34; cols_count = {} for col_name in args: col = df[col_name] for entry in col: if entry in cols_count.keys(): cols_count[entry] += 1 else: cols_count[entry] = 1 return cols_count result1 = count_entries(tweets_df, \u0026#34;lang\u0026#34;) result2 = count_entries(tweets_df, \u0026#34;lang\u0026#34;, \u0026#34;source\u0026#34;) print(result1) print(result2)   {'en': 97, 'et': 1, 'und': 2} {'en': 97, 'et': 1, 'und': 2, '\u0026lt;a href=\u0026quot;http://twitter.com\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Twitter Web Client\u0026lt;/a\u0026gt;': 24, '\u0026lt;a href=\u0026quot;http://www.facebook.com/twitter\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Facebook\u0026lt;/a\u0026gt;': 1, '\u0026lt;a href=\u0026quot;http://twitter.com/download/android\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Twitter for Android\u0026lt;/a\u0026gt;': 26, '\u0026lt;a href=\u0026quot;http://twitter.com/download/iphone\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Twitter for iPhone\u0026lt;/a\u0026gt;': 33, '\u0026lt;a href=\u0026quot;http://www.twitter.com\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Twitter for BlackBerry\u0026lt;/a\u0026gt;': 2, '\u0026lt;a href=\u0026quot;http://www.google.com/\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Google\u0026lt;/a\u0026gt;': 2, '\u0026lt;a href=\u0026quot;http://twitter.com/#!/download/ipad\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Twitter for iPad\u0026lt;/a\u0026gt;': 6, '\u0026lt;a href=\u0026quot;http://linkis.com\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Linkis.com\u0026lt;/a\u0026gt;': 2, '\u0026lt;a href=\u0026quot;http://rutracker.org/forum/viewforum.php?f=93\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;newzlasz\u0026lt;/a\u0026gt;': 2, '\u0026lt;a href=\u0026quot;http://ifttt.com\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;IFTTT\u0026lt;/a\u0026gt;': 1, '\u0026lt;a href=\u0026quot;http://www.myplume.com/\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Plume\\xa0for\\xa0Android\u0026lt;/a\u0026gt;': 1} Lambda functions and error-handling Lambda functions There\u0026rsquo;s a quicker way to write functions on the fly and these are called lambda functions because you use the keyword lambda. To do so, after the keyword lambda, we specify the names of the arguments; then we use a colon followed by the expression that specifies what we wish the function to return, such as\n1  lambda x,y: x + y   1 2  f = lambda x: x ** 2 f(3)   9 1 2 3 4  echo_word = (lambda word1, echo: word1 * echo) result = echo_word(\u0026#34;hey\u0026#34;, 5) print(result)   heyheyheyheyhey Here we introduce three useful function: map(), filter() and reduce().\nFirstly, check out the map function, which takes two arguments, a function and a sequence such as a list and applies the function over all elements of the sequence. We can pass lambda functions to map without even naming them and in this case we refer to them as anonymous functions.\n1 2 3 4 5 6  ls = [1,2,3,4,5] ls1 = map( lambda x: x ** 2, ls ) print(ls1, list(ls1))   \u0026lt;map object at 0x7fc5a95e76d0\u0026gt; [1, 4, 9, 16, 25] 1 2 3 4 5 6 7 8 9  spells = [\u0026#34;protego\u0026#34;, \u0026#34;accio\u0026#34;, \u0026#34;expecto patronum\u0026#34;, \u0026#34;legilimens\u0026#34;] shout_spells = map( lambda item: item + \u0026#34;!!!\u0026#34;, spells ) shout_spells_list = list(shout_spells) print(shout_spells_list)   ['protego!!!', 'accio!!!', 'expecto patronum!!!', 'legilimens!!!'] The function filter() offers a way to filter out elements from a list that don\u0026rsquo;t satisfy certain criteria. We now use filter() to create, from an input list of strings, a new list that contains only strings that have more than 6 characters.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  fellowship = [\u0026#39;frodo\u0026#39;, \u0026#39;samwise\u0026#39;, \u0026#39;merry\u0026#39;, \u0026#39;pippin\u0026#39;, \u0026#39;aragorn\u0026#39;, \u0026#39;boromir\u0026#39;, \u0026#39;legolas\u0026#39;, \u0026#39;gimli\u0026#39;, \u0026#39;gandalf\u0026#39;] result_f = filter( lambda x: len(x) \u0026gt; 6, fellowship ) result_m = map( lambda x: len(x) \u0026gt; 6, fellowship ) result_f_list = list(result_f) result_m_list = list(result_m) print(result_f_list) print(result_m_list)   ['samwise', 'aragorn', 'boromir', 'legolas', 'gandalf'] [False, True, False, False, True, True, True, False, True] The reduce() function is useful for performing some computation on a list and, unlike map() and filter(), returns a single value as a result. The operation rule of reduce() is to use the input function (with 2 arguments) to operate on the first and second elements in the iterator (e.g. list), and then use the function to operate the result with the third data, and finally get a result. To use reduce(), you must import it from the functools module.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  from functools import reduce stark = [\u0026#39;robb\u0026#39;, \u0026#39;sansa\u0026#39;, \u0026#39;arya\u0026#39;, \u0026#39;brandon\u0026#39;, \u0026#39;rickon\u0026#39;] nums = [1,1,1,1,1,1] result_s = reduce( lambda x, y: x + y, stark ) result_n = reduce( lambda x, y: x + y, nums ) print(result_s) print(result_n)   robbsansaaryabrandonrickon 6 Error handling A error caught during execution, commonly called exceptions. The main way to catch exceptions is the try-except clause, in which Python tries to run the code following try and if it can, all is well. If it cannot due to an exception, it runs the code following except.\nWe may also wish to only catch specific type of error and let other errors pass through, in which case we would use except \u0026lt;error type\u0026gt;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  def count_entries(df, col_name=\u0026#39;lang\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Return a dictionary with counts of occurrences as value for each key.\u0026#34;\u0026#34;\u0026#34; cols_count = {} try: col = df[col_name] for entry in col: if entry in cols_count.keys(): cols_count[entry] += 1 else: cols_count[entry] = 1 return cols_count except: print(\u0026#39;The DataFrame does not have a \u0026#39; + col_name + \u0026#39; column.\u0026#39;) result1 = count_entries(tweets_df, \u0026#39;lang\u0026#39;) print(result1)   {'en': 97, 'et': 1, 'und': 2} More often than not, instead of merely printing an error message, we\u0026rsquo;ll want to actually raise an error by clause raise \u0026lt;error type\u0026gt;(\u0026quot;xxx\u0026quot;). For example\n1 2 3  a = 4 if a \u0026lt; 5: raise TypeError(\u0026#34;ddd\u0026#34;)   TypeError: ddd The other method to catch error for the last example is that\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  def count_entries(df, col_name=\u0026#39;lang\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Return a dictionary with counts of occurrences as value for each key.\u0026#34;\u0026#34;\u0026#34; if col_name not in df.columns: raise ValueError(\u0026#39;The DataFrame does not have a \u0026#39; + col_name + \u0026#39; column.\u0026#39;) cols_count = {} col = df[col_name] for entry in col: if entry in cols_count.keys(): cols_count[entry] += 1 else: cols_count[entry] = 1 return cols_count result1 = count_entries(tweets_df, \u0026#34;lang\u0026#34;) print(result1)    {'en': 97, 'et': 1, 'und': 2} ","date":"2019-08-13T00:00:00Z","image":"https://wanghaoming177.netlify.app/head_img/oop_hi/oo1.png","permalink":"https://wanghaoming177.netlify.app/p/object-oriented-programming-i-function/","title":"Object Oriented Programming I (Function)"}]